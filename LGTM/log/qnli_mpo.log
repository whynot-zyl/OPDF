04/29/2024 11:32:33 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/29/2024 11:32:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=100,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/runs/Apr29_11-32-33_ubuntu-gd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Using the latest cached version of the module from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Tue Apr  2 12:05:36 2024) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.
04/29/2024 11:32:33 - WARNING - datasets.load - Using the latest cached version of the module from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Tue Apr  2 12:05:36 2024) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.
Loading Dataset Infos from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
04/29/2024 11:32:33 - INFO - datasets.info - Loading Dataset Infos from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
Overwrite dataset info from restored data version if exists.
04/29/2024 11:32:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
04/29/2024 11:32:33 - INFO - datasets.info - Loading Dataset info from /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
Found cached dataset glue (/home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
04/29/2024 11:32:33 - INFO - datasets.builder - Found cached dataset glue (/home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading Dataset info from /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
04/29/2024 11:32:33 - INFO - datasets.info - Loading Dataset info from /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
[INFO|configuration_utils.py:646] 2024-04-29 11:32:33,870 >> loading configuration file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/config.json
[INFO|configuration_utils.py:684] 2024-04-29 11:32:33,870 >> Model config BertConfig {
  "_name_or_path": "/mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "qnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1703] 2024-04-29 11:32:33,870 >> Didn't find file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1784] 2024-04-29 11:32:33,871 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/vocab.txt
[INFO|tokenization_utils_base.py:1784] 2024-04-29 11:32:33,871 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/tokenizer.json
[INFO|tokenization_utils_base.py:1784] 2024-04-29 11:32:33,871 >> loading file None
[INFO|tokenization_utils_base.py:1784] 2024-04-29 11:32:33,871 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/special_tokens_map.json
[INFO|tokenization_utils_base.py:1784] 2024-04-29 11:32:33,871 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/tokenizer_config.json
[INFO|modeling_utils.py:1429] 2024-04-29 11:32:33,892 >> loading weights file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/pytorch_model.bin
[WARNING|modeling_utils.py:1693] 2024-04-29 11:32:34,464 >> Some weights of the model checkpoint at /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/ were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2024-04-29 11:32:34,464 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
[INFO|configuration_utils.py:646] 2024-04-29 11:32:34,465 >> loading configuration file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/config.json
[INFO|configuration_utils.py:684] 2024-04-29 11:32:34,466 >> Model config BertConfig {
  "_name_or_path": "/mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "qnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1429] 2024-04-29 11:32:34,466 >> loading weights file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/pytorch_model.bin
[INFO|modeling_utils.py:1702] 2024-04-29 11:32:35,269 >> All model checkpoint weights were used when initializing BertForSequenceClassification.

[INFO|modeling_utils.py:1710] 2024-04-29 11:32:35,269 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-368bd2ca39bd82ef.arrow
04/29/2024 11:32:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-368bd2ca39bd82ef.arrow
Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2a45f481588e528.arrow
04/29/2024 11:32:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2a45f481588e528.arrow
Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-aca439316fa99b17.arrow
04/29/2024 11:32:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/zhanyuliang/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-aca439316fa99b17.arrow
04/29/2024 11:32:35 - INFO - __main__ - The following columns  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, question, idx.
04/29/2024 11:32:35 - INFO - __main__ - Sample 83810 of the training set: {'label': 0, 'input_ids': [101, 2054, 9887, 1997, 1996, 26189, 2050, 2020, 2149, 1998, 20996, 2243, 3629, 2025, 4810, 2000, 5047, 1029, 102, 2006, 2676, 2281, 2012, 1996, 4759, 2789, 2392, 1010, 1037, 1057, 1012, 1055, 1012, 5504, 3939, 2407, 17604, 4337, 2136, 1006, 1017, 1010, 2199, 3548, 1007, 1998, 1996, 1057, 1012, 1055, 1012, 3083, 3884, 2407, 1006, 2260, 1010, 2199, 1516, 2321, 1010, 2199, 9622, 1007, 2020, 4895, 28139, 19362, 2098, 2005, 1996, 26189, 2050, 6280, 2390, 2177, 1005, 1055, 2093, 1011, 4013, 21558, 4372, 6895, 21769, 3672, 9887, 2012, 1996, 2645, 1997, 16480, 11493, 8071, 1010, 2021, 2027, 3266, 2000, 4019, 2104, 2250, 2486, 1998, 1060, 3650, 2490, 2543, 1517, 12167, 2007, 2070, 2321, 1010, 2199, 7268, 8664, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
04/29/2024 11:32:35 - INFO - __main__ - Sample 14592 of the training set: {'label': 1, 'input_ids': [101, 2129, 2106, 1996, 6811, 2111, 2514, 2055, 26099, 1005, 1055, 3433, 2000, 1996, 2886, 1029, 102, 26099, 2106, 2025, 2514, 2008, 1996, 6811, 2390, 2001, 3201, 2005, 1037, 13111, 1998, 2106, 2025, 2128, 9080, 13143, 23689, 6590, 11272, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
04/29/2024 11:32:35 - INFO - __main__ - Sample 3278 of the training set: {'label': 0, 'input_ids': [101, 2054, 2785, 1997, 2948, 2024, 5214, 1997, 2635, 2125, 20018, 1029, 102, 2348, 2358, 4492, 2140, 2948, 2024, 5214, 1997, 2635, 2125, 20018, 2013, 1037, 3962, 2006, 1996, 5877, 1010, 2478, 1996, 13276, 1998, 1037, 2770, 2707, 2003, 2521, 2062, 4762, 8114, 1998, 14245, 1037, 11907, 4888, 3635, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
run_glue_mpo_laterloss.py:789: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("glue", data_args.task_name)
HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [0.3333333333333333]
04/29/2024 11:32:35 - INFO - datasets.utils.file_utils - HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [0.3333333333333333]
HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [0.6666666666666666]
04/29/2024 11:32:35 - INFO - datasets.utils.file_utils - HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [0.6666666666666666]
HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [1.0]
04/29/2024 11:32:36 - INFO - datasets.utils.file_utils - HEAD request to https://raw.githubusercontent.com/huggingface/datasets/2.15.0/metrics/glue/glue.py timed out, retrying... [1.0]
Using the latest cached version of the module from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/metrics/glue/91f3cfc5498873918ecf119dbf806fb10815786c84f41b85a5d3c47c1519b343 (last modified on Tue Apr  2 15:49:35 2024) since it couldn't be found locally at glue, or remotely on the Hugging Face Hub.
04/29/2024 11:32:38 - WARNING - datasets.load - Using the latest cached version of the module from /home/zhanyuliang/.cache/huggingface/modules/datasets_modules/metrics/glue/91f3cfc5498873918ecf119dbf806fb10815786c84f41b85a5d3c47c1519b343 (last modified on Tue Apr  2 15:49:35 2024) since it couldn't be found locally at glue, or remotely on the Hugging Face Hub.
#########################################################################################################
分解前参数量：66956546
#########################################################################################################
bert.encoder.layer.0.attention.self.query.weight
bert.encoder.layer.0.attention.self.key.weight
bert.encoder.layer.0.attention.self.value.weight
bert.encoder.layer.0.attention.output.dense.weight
bert.encoder.layer.0.intermediate.dense.weight
bert.encoder.layer.0.output.dense.weight
bert.encoder.layer.1.attention.self.query.weight
bert.encoder.layer.1.attention.self.key.weight
bert.encoder.layer.1.attention.self.value.weight
bert.encoder.layer.1.attention.output.dense.weight
bert.encoder.layer.1.intermediate.dense.weight
bert.encoder.layer.1.output.dense.weight
bert.encoder.layer.2.attention.self.query.weight
bert.encoder.layer.2.attention.self.key.weight
bert.encoder.layer.2.attention.self.value.weight
bert.encoder.layer.2.attention.output.dense.weight
bert.encoder.layer.2.intermediate.dense.weight
bert.encoder.layer.2.output.dense.weight
bert.encoder.layer.3.attention.self.query.weight
bert.encoder.layer.3.attention.self.key.weight
bert.encoder.layer.3.attention.self.value.weight
bert.encoder.layer.3.attention.output.dense.weight
bert.encoder.layer.3.intermediate.dense.weight
bert.encoder.layer.3.output.dense.weight
bert.encoder.layer.4.attention.self.query.weight
bert.encoder.layer.4.attention.self.key.weight
bert.encoder.layer.4.attention.self.value.weight
bert.encoder.layer.4.attention.output.dense.weight
bert.encoder.layer.4.intermediate.dense.weight
bert.encoder.layer.4.output.dense.weight
bert.encoder.layer.5.attention.self.query.weight
bert.encoder.layer.5.attention.self.key.weight
bert.encoder.layer.5.attention.self.value.weight
bert.encoder.layer.5.attention.output.dense.weight
bert.encoder.layer.5.intermediate.dense.weight
bert.encoder.layer.5.output.dense.weight
#########################################################################################################
分解后参数量：186395906
#########################################################################################################
#########################################################################################################
分解前参数量：109483778
#########################################################################################################
bert.encoder.layer.0.attention.self.query.weight
bert.encoder.layer.0.attention.self.key.weight
bert.encoder.layer.0.attention.self.value.weight
bert.encoder.layer.0.attention.output.dense.weight
bert.encoder.layer.0.intermediate.dense.weight
bert.encoder.layer.0.output.dense.weight
bert.encoder.layer.1.attention.self.query.weight
bert.encoder.layer.1.attention.self.key.weight
bert.encoder.layer.1.attention.self.value.weight
bert.encoder.layer.1.attention.output.dense.weight
bert.encoder.layer.1.intermediate.dense.weight
bert.encoder.layer.1.output.dense.weight
bert.encoder.layer.2.attention.self.query.weight
bert.encoder.layer.2.attention.self.key.weight
bert.encoder.layer.2.attention.self.value.weight
bert.encoder.layer.2.attention.output.dense.weight
bert.encoder.layer.2.intermediate.dense.weight
bert.encoder.layer.2.output.dense.weight
bert.encoder.layer.3.attention.self.query.weight
bert.encoder.layer.3.attention.self.key.weight
bert.encoder.layer.3.attention.self.value.weight
bert.encoder.layer.3.attention.output.dense.weight
bert.encoder.layer.3.intermediate.dense.weight
bert.encoder.layer.3.output.dense.weight
bert.encoder.layer.4.attention.self.query.weight
bert.encoder.layer.4.attention.self.key.weight
bert.encoder.layer.4.attention.self.value.weight
bert.encoder.layer.4.attention.output.dense.weight
bert.encoder.layer.4.intermediate.dense.weight
bert.encoder.layer.4.output.dense.weight
bert.encoder.layer.5.attention.self.query.weight
bert.encoder.layer.5.attention.self.key.weight
bert.encoder.layer.5.attention.self.value.weight
bert.encoder.layer.5.attention.output.dense.weight
bert.encoder.layer.5.intermediate.dense.weight
bert.encoder.layer.5.output.dense.weight
bert.encoder.layer.6.attention.self.query.weight
bert.encoder.layer.6.attention.self.key.weight
bert.encoder.layer.6.attention.self.value.weight
bert.encoder.layer.6.attention.output.dense.weight
bert.encoder.layer.6.intermediate.dense.weight
bert.encoder.layer.6.output.dense.weight
bert.encoder.layer.7.attention.self.query.weight
bert.encoder.layer.7.attention.self.key.weight
bert.encoder.layer.7.attention.self.value.weight
bert.encoder.layer.7.attention.output.dense.weight
bert.encoder.layer.7.intermediate.dense.weight
bert.encoder.layer.7.output.dense.weight
bert.encoder.layer.8.attention.self.query.weight
bert.encoder.layer.8.attention.self.key.weight
bert.encoder.layer.8.attention.self.value.weight
bert.encoder.layer.8.attention.output.dense.weight
bert.encoder.layer.8.intermediate.dense.weight
bert.encoder.layer.8.output.dense.weight
bert.encoder.layer.9.attention.self.query.weight
bert.encoder.layer.9.attention.self.key.weight
bert.encoder.layer.9.attention.self.value.weight
bert.encoder.layer.9.attention.output.dense.weight
bert.encoder.layer.9.intermediate.dense.weight
bert.encoder.layer.9.output.dense.weight
bert.encoder.layer.10.attention.self.query.weight
bert.encoder.layer.10.attention.self.key.weight
bert.encoder.layer.10.attention.self.value.weight
bert.encoder.layer.10.attention.output.dense.weight
bert.encoder.layer.10.intermediate.dense.weight
bert.encoder.layer.10.output.dense.weight
bert.encoder.layer.11.attention.self.query.weight
bert.encoder.layer.11.attention.self.key.weight
bert.encoder.layer.11.attention.self.value.weight
bert.encoder.layer.11.attention.output.dense.weight
bert.encoder.layer.11.intermediate.dense.weight
bert.encoder.layer.11.output.dense.weight
#########################################################################################################
分解后参数量：348362498
#########################################################################################################
######################################student_model##########################################################
bert.embeddings.word_embeddings.weight
torch.Size([30522, 768])
bert.embeddings.position_embeddings.weight
torch.Size([512, 768])
bert.embeddings.token_type_embeddings.weight
torch.Size([2, 768])
bert.embeddings.LayerNorm.weight
torch.Size([768])
bert.embeddings.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.0.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.0.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.0.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.0.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.0.output.dense.bias
torch.Size([768])
bert.encoder.layer.0.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.0.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.0.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.0.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.1.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.1.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.1.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.1.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.1.output.dense.bias
torch.Size([768])
bert.encoder.layer.1.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.1.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.1.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.1.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.2.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.2.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.2.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.2.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.2.output.dense.bias
torch.Size([768])
bert.encoder.layer.2.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.2.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.2.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.2.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.3.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.3.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.3.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.3.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.3.output.dense.bias
torch.Size([768])
bert.encoder.layer.3.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.3.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.3.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.3.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.4.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.4.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.4.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.4.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.4.output.dense.bias
torch.Size([768])
bert.encoder.layer.4.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.4.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.4.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.4.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.5.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.5.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.5.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.5.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.5.output.dense.bias
torch.Size([768])
bert.encoder.layer.5.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.5.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.5.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.5.output.LayerNorm.bias
torch.Size([768])
bert.pooler.dense.weight
torch.Size([768, 768])
bert.pooler.dense.bias
torch.Size([768])
classifier.weight
torch.Size([2, 768])
classifier.bias
torch.Size([2])
######################################teacher_model##########################################################
bert.embeddings.word_embeddings.weight
torch.Size([30522, 768])
bert.embeddings.position_embeddings.weight
torch.Size([512, 768])
bert.embeddings.token_type_embeddings.weight
torch.Size([2, 768])
bert.embeddings.LayerNorm.weight
torch.Size([768])
bert.embeddings.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.0.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.0.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.0.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.0.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.0.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.0.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.0.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.0.output.dense.bias
torch.Size([768])
bert.encoder.layer.0.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.0.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.0.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.0.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.0.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.1.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.1.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.1.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.1.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.1.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.1.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.1.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.1.output.dense.bias
torch.Size([768])
bert.encoder.layer.1.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.1.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.1.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.1.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.1.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.2.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.2.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.2.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.2.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.2.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.2.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.2.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.2.output.dense.bias
torch.Size([768])
bert.encoder.layer.2.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.2.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.2.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.2.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.2.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.3.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.3.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.3.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.3.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.3.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.3.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.3.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.3.output.dense.bias
torch.Size([768])
bert.encoder.layer.3.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.3.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.3.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.3.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.3.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.4.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.4.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.4.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.4.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.4.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.4.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.4.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.4.output.dense.bias
torch.Size([768])
bert.encoder.layer.4.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.4.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.4.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.4.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.4.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.5.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.5.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.5.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.5.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.5.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.5.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.5.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.5.output.dense.bias
torch.Size([768])
bert.encoder.layer.5.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.5.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.5.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.5.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.5.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.6.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.6.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.6.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.6.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.6.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.6.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.6.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.6.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.6.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.6.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.6.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.6.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.6.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.6.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.6.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.6.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.6.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.6.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.6.output.dense.bias
torch.Size([768])
bert.encoder.layer.6.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.6.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.6.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.6.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.6.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.7.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.7.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.7.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.7.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.7.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.7.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.7.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.7.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.7.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.7.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.7.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.7.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.7.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.7.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.7.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.7.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.7.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.7.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.7.output.dense.bias
torch.Size([768])
bert.encoder.layer.7.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.7.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.7.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.7.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.7.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.8.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.8.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.8.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.8.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.8.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.8.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.8.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.8.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.8.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.8.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.8.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.8.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.8.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.8.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.8.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.8.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.8.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.8.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.8.output.dense.bias
torch.Size([768])
bert.encoder.layer.8.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.8.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.8.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.8.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.8.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.9.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.9.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.9.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.9.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.9.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.9.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.9.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.9.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.9.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.9.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.9.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.9.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.9.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.9.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.9.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.9.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.9.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.9.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.9.output.dense.bias
torch.Size([768])
bert.encoder.layer.9.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.9.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.9.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.9.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.9.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.10.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.10.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.10.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.10.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.10.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.10.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.10.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.10.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.10.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.10.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.10.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.10.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.10.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.10.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.10.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.10.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.10.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.10.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.10.output.dense.bias
torch.Size([768])
bert.encoder.layer.10.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.10.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.10.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.10.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.10.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.11.attention.self.query.bias
torch.Size([768])
bert.encoder.layer.11.attention.self.query.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.11.attention.self.query.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.query.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.query.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.query.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.query.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.11.attention.self.key.bias
torch.Size([768])
bert.encoder.layer.11.attention.self.key.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.11.attention.self.key.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.key.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.key.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.key.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.key.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.11.attention.self.value.bias
torch.Size([768])
bert.encoder.layer.11.attention.self.value.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.11.attention.self.value.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.value.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.value.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.value.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.self.value.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.11.attention.output.dense.bias
torch.Size([768])
bert.encoder.layer.11.attention.output.dense.tensor_set.0
torch.Size([1, 32, 32, 576])
bert.encoder.layer.11.attention.output.dense.tensor_set.1
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.output.dense.tensor_set.2
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.output.dense.tensor_set.3
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.output.dense.tensor_set.4
torch.Size([576, 1, 1, 576])
bert.encoder.layer.11.attention.output.dense.tensor_set.5
torch.Size([576, 24, 24, 1])
bert.encoder.layer.11.attention.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.11.attention.output.LayerNorm.bias
torch.Size([768])
bert.encoder.layer.11.intermediate.dense.bias
torch.Size([3072])
bert.encoder.layer.11.intermediate.dense.tensor_set.0
torch.Size([1, 64, 32, 1152])
bert.encoder.layer.11.intermediate.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.intermediate.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.intermediate.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.intermediate.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.intermediate.dense.tensor_set.5
torch.Size([1152, 48, 24, 1])
bert.encoder.layer.11.output.dense.bias
torch.Size([768])
bert.encoder.layer.11.output.dense.tensor_set.0
torch.Size([1, 32, 64, 1152])
bert.encoder.layer.11.output.dense.tensor_set.1
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.output.dense.tensor_set.2
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.output.dense.tensor_set.3
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.output.dense.tensor_set.4
torch.Size([1152, 1, 1, 1152])
bert.encoder.layer.11.output.dense.tensor_set.5
torch.Size([1152, 24, 48, 1])
bert.encoder.layer.11.output.LayerNorm.weight
torch.Size([768])
bert.encoder.layer.11.output.LayerNorm.bias
torch.Size([768])
bert.pooler.dense.weight
torch.Size([768, 768])
bert.pooler.dense.bias
torch.Size([768])
classifier.weight
torch.Size([2, 768])
classifier.bias
torch.Size([2])
04/29/2024 11:40:42 - INFO - __main__ - ***** Running training *****
04/29/2024 11:40:42 - INFO - __main__ -   Num examples = 104743
04/29/2024 11:40:42 - INFO - __main__ -   Num Epochs = 3
04/29/2024 11:40:42 - INFO - __main__ -   Instantaneous batch size per device = 32
04/29/2024 11:40:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
04/29/2024 11:40:42 - INFO - __main__ -   Gradient Accumulation steps = 1
04/29/2024 11:40:42 - INFO - __main__ -   Total optimization steps = 9822
/home/zhanyuliang/miniconda3/envs/lgtm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/9822 [00:00<?, ?it/s]/home/zhanyuliang/Project/DistillingMPO/OPF/LGTM/utils_glue.py:132: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/9822 [00:02<5:43:11,  2.10s/it]  0%|          | 2/9822 [00:03<4:56:44,  1.81s/it]  0%|          | 3/9822 [00:05<4:42:13,  1.72s/it]  0%|          | 4/9822 [00:06<4:35:20,  1.68s/it]  0%|          | 5/9822 [00:08<4:32:56,  1.67s/it]  0%|          | 6/9822 [00:10<4:31:58,  1.66s/it]  0%|          | 7/9822 [00:11<4:31:23,  1.66s/it]  0%|          | 8/9822 [00:13<4:31:02,  1.66s/it]  0%|          | 9/9822 [00:15<4:30:42,  1.66s/it]  0%|          | 10/9822 [00:16<4:29:49,  1.65s/it]  0%|          | 11/9822 [00:18<4:29:36,  1.65s/it]  0%|          | 12/9822 [00:20<4:34:35,  1.68s/it]  0%|          | 13/9822 [00:21<4:32:55,  1.67s/it]  0%|          | 14/9822 [00:23<4:31:49,  1.66s/it]  0%|          | 15/9822 [00:25<4:31:05,  1.66s/it]  0%|          | 16/9822 [00:26<4:30:32,  1.66s/it]  0%|          | 17/9822 [00:28<4:30:21,  1.65s/it]  0%|          | 18/9822 [00:30<4:30:12,  1.65s/it]  0%|          | 19/9822 [00:31<4:29:57,  1.65s/it]  0%|          | 20/9822 [00:33<4:29:51,  1.65s/it]  0%|          | 21/9822 [00:35<4:29:50,  1.65s/it]  0%|          | 22/9822 [00:36<4:29:59,  1.65s/it]  0%|          | 23/9822 [00:38<4:32:03,  1.67s/it]  0%|          | 24/9822 [00:40<4:30:22,  1.66s/it]  0%|          | 25/9822 [00:41<4:29:23,  1.65s/it]  0%|          | 26/9822 [00:43<4:29:02,  1.65s/it]  0%|          | 27/9822 [00:44<4:28:26,  1.64s/it]  0%|          | 28/9822 [00:46<4:28:06,  1.64s/it]  0%|          | 29/9822 [00:48<4:27:32,  1.64s/it]  0%|          | 30/9822 [00:49<4:27:42,  1.64s/it]  0%|          | 31/9822 [00:51<4:28:04,  1.64s/it]  0%|          | 32/9822 [00:53<4:28:00,  1.64s/it]  0%|          | 33/9822 [00:54<4:27:43,  1.64s/it]  0%|          | 34/9822 [00:56<4:27:52,  1.64s/it]  0%|          | 35/9822 [00:58<4:27:57,  1.64s/it]  0%|          | 36/9822 [00:59<4:28:26,  1.65s/it]  0%|          | 37/9822 [01:01<4:28:08,  1.64s/it]  0%|          | 38/9822 [01:03<4:27:47,  1.64s/it]  0%|          | 39/9822 [01:04<4:27:53,  1.64s/it]  0%|          | 40/9822 [01:06<4:28:43,  1.65s/it]  0%|          | 41/9822 [01:08<4:29:05,  1.65s/it]  0%|          | 42/9822 [01:09<4:28:41,  1.65s/it]  0%|          | 43/9822 [01:11<4:28:04,  1.64s/it]  0%|          | 44/9822 [01:12<4:27:41,  1.64s/it]  0%|          | 45/9822 [01:14<4:33:43,  1.68s/it]  0%|          | 46/9822 [01:16<4:32:45,  1.67s/it]  0%|          | 47/9822 [01:18<4:32:17,  1.67s/it]  0%|          | 48/9822 [01:19<4:31:58,  1.67s/it]  0%|          | 49/9822 [01:21<4:32:04,  1.67s/it]  1%|          | 50/9822 [01:23<4:32:06,  1.67s/it]  1%|          | 51/9822 [01:24<4:31:07,  1.66s/it]  1%|          | 52/9822 [01:26<4:30:48,  1.66s/it]  1%|          | 53/9822 [01:27<4:29:38,  1.66s/it]  1%|          | 54/9822 [01:29<4:29:19,  1.65s/it]  1%|          | 55/9822 [01:31<4:29:03,  1.65s/it]  1%|          | 56/9822 [01:32<4:28:30,  1.65s/it]  1%|          | 57/9822 [01:34<4:28:18,  1.65s/it]  1%|          | 58/9822 [01:36<4:27:54,  1.65s/it]  1%|          | 59/9822 [01:37<4:28:06,  1.65s/it]  1%|          | 60/9822 [01:39<4:28:16,  1.65s/it]  1%|          | 61/9822 [01:41<4:28:13,  1.65s/it]  1%|          | 62/9822 [01:42<4:29:51,  1.66s/it]  1%|          | 63/9822 [01:44<4:30:01,  1.66s/it]  1%|          | 64/9822 [01:46<4:30:25,  1.66s/it]  1%|          | 65/9822 [01:47<4:31:02,  1.67s/it]  1%|          | 66/9822 [01:49<4:31:25,  1.67s/it]  1%|          | 67/9822 [01:51<4:36:48,  1.70s/it]  1%|          | 68/9822 [01:52<4:35:41,  1.70s/it]  1%|          | 69/9822 [01:54<4:34:31,  1.69s/it]  1%|          | 70/9822 [01:56<4:33:24,  1.68s/it]  1%|          | 71/9822 [01:57<4:32:57,  1.68s/it]  1%|          | 72/9822 [01:59<4:32:54,  1.68s/it]  1%|          | 73/9822 [02:01<4:32:32,  1.68s/it]  1%|          | 74/9822 [02:03<4:32:27,  1.68s/it]  1%|          | 75/9822 [02:04<4:32:33,  1.68s/it]  1%|          | 76/9822 [02:06<4:32:37,  1.68s/it]  1%|          | 77/9822 [02:08<4:32:07,  1.68s/it]  1%|          | 78/9822 [02:09<4:32:16,  1.68s/it]  1%|          | 79/9822 [02:11<4:32:26,  1.68s/it]  1%|          | 80/9822 [02:13<4:32:27,  1.68s/it]  1%|          | 81/9822 [02:14<4:32:05,  1.68s/it]  1%|          | 82/9822 [02:16<4:30:49,  1.67s/it]  1%|          | 83/9822 [02:18<4:30:19,  1.67s/it]  1%|          | 84/9822 [02:19<4:31:03,  1.67s/it]  1%|          | 85/9822 [02:21<4:31:40,  1.67s/it]  1%|          | 86/9822 [02:23<4:29:15,  1.66s/it]  1%|          | 87/9822 [02:24<4:30:23,  1.67s/it]  1%|          | 88/9822 [02:26<4:31:02,  1.67s/it]  1%|          | 89/9822 [02:28<4:30:59,  1.67s/it]  1%|          | 90/9822 [02:29<4:31:05,  1.67s/it]  1%|          | 91/9822 [02:31<4:31:41,  1.68s/it]  1%|          | 92/9822 [02:33<4:32:04,  1.68s/it]  1%|          | 93/9822 [02:34<4:31:47,  1.68s/it]  1%|          | 94/9822 [02:36<4:37:14,  1.71s/it]  1%|          | 95/9822 [02:38<4:35:39,  1.70s/it]  1%|          | 96/9822 [02:39<4:34:40,  1.69s/it]  1%|          | 97/9822 [02:41<4:33:48,  1.69s/it]  1%|          | 98/9822 [02:43<4:33:19,  1.69s/it]  1%|          | 99/9822 [02:44<4:32:58,  1.68s/it]  1%|          | 100/9822 [02:46<4:32:42,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(1.8886e-07, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(6.2565e-07, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(1.1509e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(2.2694e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(3.2743e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(4.7495e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(6.3654e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(7.4895e-06, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(1.0158e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(1.2978e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(1.2871e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(1.7032e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(2.1008e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(2.2882e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(2.7261e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(3.2730e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(3.1786e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(3.8579e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(4.3959e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(5.2405e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(5.3585e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(5.8996e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(7.5253e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(7.5434e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(7.5332e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(8.7931e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(9.3218e-05, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:43:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:43:33 - INFO - __main__ - ***** test Results*****
04/29/2024 11:43:33 - INFO - __main__ -   Training step = 100
04/29/2024 11:43:33 - INFO - __main__ -  test_accuracy:0.7423133235724744 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:43:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:43:37 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:43:37 - INFO - __main__ -   Training step = 100
04/29/2024 11:43:37 - INFO - __main__ -  eval_accuracy:0.7283046503112413 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 11:43:37,849 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 11:43:37,849 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 11:43:37,890 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 11:43:39,692 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.7283046503112413}
test:
{'accuracy': 0.7423133235724744}
04/29/2024 11:43:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:43:48 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:43:48 - INFO - __main__ -   Training step = 100
04/29/2024 11:43:48 - INFO - __main__ -  eval_accuracy:0.9099231050897107 
  1%|          | 101/9822 [03:07<20:10:06,  7.47s/it]  1%|          | 102/9822 [03:09<15:27:28,  5.73s/it]  1%|          | 103/9822 [03:10<12:09:17,  4.50s/it]  1%|          | 104/9822 [03:12<9:50:29,  3.65s/it]   1%|          | 105/9822 [03:14<8:13:34,  3.05s/it]  1%|          | 106/9822 [03:15<7:06:11,  2.63s/it]  1%|          | 107/9822 [03:17<6:18:16,  2.34s/it]  1%|          | 108/9822 [03:19<5:45:17,  2.13s/it]  1%|          | 109/9822 [03:20<5:22:21,  1.99s/it]  1%|          | 110/9822 [03:22<5:06:09,  1.89s/it]  1%|          | 111/9822 [03:24<4:55:14,  1.82s/it]  1%|          | 112/9822 [03:25<4:48:02,  1.78s/it]  1%|          | 113/9822 [03:27<4:42:26,  1.75s/it]  1%|          | 114/9822 [03:29<4:37:58,  1.72s/it]  1%|          | 115/9822 [03:30<4:35:04,  1.70s/it]  1%|          | 116/9822 [03:32<4:32:48,  1.69s/it]  1%|          | 117/9822 [03:34<4:31:20,  1.68s/it]  1%|          | 118/9822 [03:35<4:30:47,  1.67s/it]  1%|          | 119/9822 [03:37<4:30:26,  1.67s/it]  1%|          | 120/9822 [03:39<4:29:35,  1.67s/it]  1%|          | 121/9822 [03:40<4:34:41,  1.70s/it]  1%|          | 122/9822 [03:42<4:32:44,  1.69s/it]  1%|▏         | 123/9822 [03:44<4:31:19,  1.68s/it]  1%|▏         | 124/9822 [03:45<4:30:24,  1.67s/it]  1%|▏         | 125/9822 [03:47<4:29:35,  1.67s/it]  1%|▏         | 126/9822 [03:49<4:30:02,  1.67s/it]  1%|▏         | 127/9822 [03:50<4:30:25,  1.67s/it]  1%|▏         | 128/9822 [03:52<4:30:45,  1.68s/it]  1%|▏         | 129/9822 [03:54<4:30:43,  1.68s/it]  1%|▏         | 130/9822 [03:55<4:30:38,  1.68s/it]  1%|▏         | 131/9822 [03:57<4:31:00,  1.68s/it]  1%|▏         | 132/9822 [03:59<4:31:08,  1.68s/it]  1%|▏         | 133/9822 [04:00<4:30:43,  1.68s/it]  1%|▏         | 134/9822 [04:02<4:29:38,  1.67s/it]  1%|▏         | 135/9822 [04:04<4:28:53,  1.67s/it]  1%|▏         | 136/9822 [04:05<4:28:32,  1.66s/it]  1%|▏         | 137/9822 [04:07<4:27:44,  1.66s/it]  1%|▏         | 138/9822 [04:09<4:27:28,  1.66s/it]  1%|▏         | 139/9822 [04:10<4:27:29,  1.66s/it]  1%|▏         | 140/9822 [04:12<4:27:06,  1.66s/it]  1%|▏         | 141/9822 [04:14<4:27:14,  1.66s/it]  1%|▏         | 142/9822 [04:15<4:27:28,  1.66s/it]  1%|▏         | 143/9822 [04:17<4:27:28,  1.66s/it]  1%|▏         | 144/9822 [04:19<4:27:33,  1.66s/it]  1%|▏         | 145/9822 [04:20<4:27:34,  1.66s/it]  1%|▏         | 146/9822 [04:22<4:28:01,  1.66s/it]  1%|▏         | 147/9822 [04:24<4:27:59,  1.66s/it]  2%|▏         | 148/9822 [04:25<4:31:37,  1.68s/it]  2%|▏         | 149/9822 [04:27<4:34:04,  1.70s/it]  2%|▏         | 150/9822 [04:29<4:32:03,  1.69s/it]  2%|▏         | 151/9822 [04:30<4:30:49,  1.68s/it]  2%|▏         | 152/9822 [04:32<4:30:04,  1.68s/it]  2%|▏         | 153/9822 [04:34<4:30:08,  1.68s/it]  2%|▏         | 154/9822 [04:36<4:34:24,  1.70s/it]  2%|▏         | 155/9822 [04:37<4:32:41,  1.69s/it]  2%|▏         | 156/9822 [04:39<4:30:59,  1.68s/it]  2%|▏         | 157/9822 [04:41<4:29:27,  1.67s/it]  2%|▏         | 158/9822 [04:42<4:28:53,  1.67s/it]  2%|▏         | 159/9822 [04:44<4:28:24,  1.67s/it]  2%|▏         | 160/9822 [04:46<4:27:41,  1.66s/it]  2%|▏         | 161/9822 [04:47<4:27:12,  1.66s/it]  2%|▏         | 162/9822 [04:49<4:27:40,  1.66s/it]  2%|▏         | 163/9822 [04:51<4:27:44,  1.66s/it]  2%|▏         | 164/9822 [04:52<4:27:57,  1.66s/it]  2%|▏         | 165/9822 [04:54<4:27:35,  1.66s/it]  2%|▏         | 166/9822 [04:56<4:27:21,  1.66s/it]  2%|▏         | 167/9822 [04:57<4:27:48,  1.66s/it]  2%|▏         | 168/9822 [04:59<4:27:35,  1.66s/it]  2%|▏         | 169/9822 [05:01<4:27:41,  1.66s/it]  2%|▏         | 170/9822 [05:02<4:32:52,  1.70s/it]  2%|▏         | 171/9822 [05:04<4:34:01,  1.70s/it]  2%|▏         | 172/9822 [05:06<4:29:10,  1.67s/it]  2%|▏         | 173/9822 [05:07<4:27:57,  1.67s/it]  2%|▏         | 174/9822 [05:09<4:27:23,  1.66s/it]  2%|▏         | 175/9822 [05:11<4:26:52,  1.66s/it]  2%|▏         | 176/9822 [05:12<4:31:33,  1.69s/it]  2%|▏         | 177/9822 [05:14<4:29:45,  1.68s/it]  2%|▏         | 178/9822 [05:16<4:28:52,  1.67s/it]  2%|▏         | 179/9822 [05:17<4:27:46,  1.67s/it]  2%|▏         | 180/9822 [05:19<4:27:12,  1.66s/it]  2%|▏         | 181/9822 [05:21<4:27:07,  1.66s/it]  2%|▏         | 182/9822 [05:22<4:27:09,  1.66s/it]  2%|▏         | 183/9822 [05:24<4:27:32,  1.67s/it]  2%|▏         | 184/9822 [05:26<4:27:24,  1.66s/it]  2%|▏         | 185/9822 [05:27<4:26:49,  1.66s/it]  2%|▏         | 186/9822 [05:29<4:26:15,  1.66s/it]  2%|▏         | 187/9822 [05:31<4:27:03,  1.66s/it]  2%|▏         | 188/9822 [05:32<4:27:04,  1.66s/it]  2%|▏         | 189/9822 [05:34<4:26:52,  1.66s/it]  2%|▏         | 190/9822 [05:36<4:26:53,  1.66s/it]  2%|▏         | 191/9822 [05:37<4:26:27,  1.66s/it]  2%|▏         | 192/9822 [05:39<4:27:16,  1.67s/it]  2%|▏         | 193/9822 [05:41<4:27:02,  1.66s/it]  2%|▏         | 194/9822 [05:42<4:26:29,  1.66s/it]  2%|▏         | 195/9822 [05:44<4:26:54,  1.66s/it]  2%|▏         | 196/9822 [05:46<4:26:35,  1.66s/it]  2%|▏         | 197/9822 [05:47<4:26:25,  1.66s/it]  2%|▏         | 198/9822 [05:49<4:26:08,  1.66s/it]  2%|▏         | 199/9822 [05:51<4:25:54,  1.66s/it]  2%|▏         | 200/9822 [05:52<4:25:34,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:46:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:46:39 - INFO - __main__ - ***** test Results*****
04/29/2024 11:46:39 - INFO - __main__ -   Training step = 200
04/29/2024 11:46:39 - INFO - __main__ -  test_accuracy:0.7994143484626647 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:46:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:46:43 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:46:43 - INFO - __main__ -   Training step = 200
04/29/2024 11:46:43 - INFO - __main__ -  eval_accuracy:0.7927499084584402 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 11:46:43,850 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 11:46:43,850 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 11:46:43,891 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 11:46:45,021 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.7927499084584402}
test:
{'accuracy': 0.7994143484626647}
04/29/2024 11:46:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:46:53 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:46:53 - INFO - __main__ -   Training step = 200
04/29/2024 11:46:53 - INFO - __main__ -  eval_accuracy:0.9080922738923471 
  2%|▏         | 201/9822 [06:13<19:25:25,  7.27s/it]  2%|▏         | 202/9822 [06:14<14:56:04,  5.59s/it]  2%|▏         | 203/9822 [06:16<11:52:28,  4.44s/it]  2%|▏         | 204/9822 [06:18<9:38:17,  3.61s/it]   2%|▏         | 205/9822 [06:19<8:04:34,  3.02s/it]  2%|▏         | 206/9822 [06:21<6:58:30,  2.61s/it]  2%|▏         | 207/9822 [06:23<6:12:16,  2.32s/it]  2%|▏         | 208/9822 [06:24<5:40:32,  2.13s/it]  2%|▏         | 209/9822 [06:26<5:17:55,  1.98s/it]  2%|▏         | 210/9822 [06:28<5:01:50,  1.88s/it]  2%|▏         | 211/9822 [06:29<4:51:03,  1.82s/it]  2%|▏         | 212/9822 [06:31<4:43:08,  1.77s/it]  2%|▏         | 213/9822 [06:33<4:37:24,  1.73s/it]  2%|▏         | 214/9822 [06:34<4:33:27,  1.71s/it]  2%|▏         | 215/9822 [06:36<4:30:59,  1.69s/it]  2%|▏         | 216/9822 [06:37<4:28:54,  1.68s/it]  2%|▏         | 217/9822 [06:39<4:28:37,  1.68s/it]  2%|▏         | 218/9822 [06:41<4:27:59,  1.67s/it]  2%|▏         | 219/9822 [06:42<4:27:18,  1.67s/it]  2%|▏         | 220/9822 [06:44<4:26:19,  1.66s/it]  2%|▏         | 221/9822 [06:46<4:25:44,  1.66s/it]  2%|▏         | 222/9822 [06:47<4:25:31,  1.66s/it]  2%|▏         | 223/9822 [06:49<4:25:34,  1.66s/it]  2%|▏         | 224/9822 [06:51<4:25:46,  1.66s/it]  2%|▏         | 225/9822 [06:53<4:30:33,  1.69s/it]  2%|▏         | 226/9822 [06:54<4:28:56,  1.68s/it]  2%|▏         | 227/9822 [06:56<4:28:11,  1.68s/it]  2%|▏         | 228/9822 [06:58<4:27:30,  1.67s/it]  2%|▏         | 229/9822 [06:59<4:27:19,  1.67s/it]  2%|▏         | 230/9822 [07:01<4:27:13,  1.67s/it]  2%|▏         | 231/9822 [07:03<4:26:47,  1.67s/it]  2%|▏         | 232/9822 [07:04<4:26:07,  1.67s/it]  2%|▏         | 233/9822 [07:06<4:25:20,  1.66s/it]  2%|▏         | 234/9822 [07:07<4:25:20,  1.66s/it]  2%|▏         | 235/9822 [07:09<4:25:03,  1.66s/it]  2%|▏         | 236/9822 [07:11<4:25:23,  1.66s/it]  2%|▏         | 237/9822 [07:12<4:26:25,  1.67s/it]  2%|▏         | 238/9822 [07:14<4:26:08,  1.67s/it]  2%|▏         | 239/9822 [07:16<4:26:18,  1.67s/it]  2%|▏         | 240/9822 [07:17<4:25:52,  1.66s/it]  2%|▏         | 241/9822 [07:19<4:25:23,  1.66s/it]  2%|▏         | 242/9822 [07:21<4:25:31,  1.66s/it]  2%|▏         | 243/9822 [07:22<4:25:57,  1.67s/it]  2%|▏         | 244/9822 [07:24<4:26:22,  1.67s/it]  2%|▏         | 245/9822 [07:26<4:26:44,  1.67s/it]  3%|▎         | 246/9822 [07:27<4:26:34,  1.67s/it]  3%|▎         | 247/9822 [07:29<4:26:16,  1.67s/it]  3%|▎         | 248/9822 [07:31<4:26:00,  1.67s/it]  3%|▎         | 249/9822 [07:32<4:26:12,  1.67s/it]  3%|▎         | 250/9822 [07:34<4:26:06,  1.67s/it]  3%|▎         | 251/9822 [07:36<4:25:38,  1.67s/it]  3%|▎         | 252/9822 [07:38<4:31:23,  1.70s/it]  3%|▎         | 253/9822 [07:39<4:29:59,  1.69s/it]  3%|▎         | 254/9822 [07:41<4:28:31,  1.68s/it]  3%|▎         | 255/9822 [07:43<4:27:27,  1.68s/it]  3%|▎         | 256/9822 [07:44<4:26:54,  1.67s/it]  3%|▎         | 257/9822 [07:46<4:26:28,  1.67s/it]  3%|▎         | 258/9822 [07:48<4:23:37,  1.65s/it]  3%|▎         | 259/9822 [07:49<4:23:52,  1.66s/it]  3%|▎         | 260/9822 [07:51<4:24:02,  1.66s/it]  3%|▎         | 261/9822 [07:53<4:23:40,  1.65s/it]  3%|▎         | 262/9822 [07:54<4:23:26,  1.65s/it]  3%|▎         | 263/9822 [07:56<4:23:19,  1.65s/it]  3%|▎         | 264/9822 [07:57<4:23:15,  1.65s/it]  3%|▎         | 265/9822 [07:59<4:23:21,  1.65s/it]  3%|▎         | 266/9822 [08:01<4:23:32,  1.65s/it]  3%|▎         | 267/9822 [08:02<4:23:52,  1.66s/it]  3%|▎         | 268/9822 [08:04<4:25:23,  1.67s/it]  3%|▎         | 269/9822 [08:06<4:25:17,  1.67s/it]  3%|▎         | 270/9822 [08:07<4:25:11,  1.67s/it]  3%|▎         | 271/9822 [08:09<4:24:37,  1.66s/it]  3%|▎         | 272/9822 [08:11<4:24:31,  1.66s/it]  3%|▎         | 273/9822 [08:12<4:24:22,  1.66s/it]  3%|▎         | 274/9822 [08:14<4:24:18,  1.66s/it]  3%|▎         | 275/9822 [08:16<4:24:17,  1.66s/it]  3%|▎         | 276/9822 [08:17<4:24:05,  1.66s/it]  3%|▎         | 277/9822 [08:19<4:24:06,  1.66s/it]  3%|▎         | 278/9822 [08:21<4:23:43,  1.66s/it]  3%|▎         | 279/9822 [08:22<4:23:55,  1.66s/it]  3%|▎         | 280/9822 [08:24<4:24:21,  1.66s/it]  3%|▎         | 281/9822 [08:26<4:24:33,  1.66s/it]  3%|▎         | 282/9822 [08:27<4:25:06,  1.67s/it]  3%|▎         | 283/9822 [08:29<4:25:35,  1.67s/it]  3%|▎         | 284/9822 [08:31<4:26:04,  1.67s/it]  3%|▎         | 285/9822 [08:33<4:31:32,  1.71s/it]  3%|▎         | 286/9822 [08:34<4:30:17,  1.70s/it]  3%|▎         | 287/9822 [08:36<4:29:00,  1.69s/it]  3%|▎         | 288/9822 [08:38<4:26:55,  1.68s/it]  3%|▎         | 289/9822 [08:39<4:26:18,  1.68s/it]  3%|▎         | 290/9822 [08:41<4:25:20,  1.67s/it]  3%|▎         | 291/9822 [08:43<4:25:08,  1.67s/it]  3%|▎         | 292/9822 [08:44<4:25:36,  1.67s/it]  3%|▎         | 293/9822 [08:46<4:25:15,  1.67s/it]  3%|▎         | 294/9822 [08:48<4:24:34,  1.67s/it]  3%|▎         | 295/9822 [08:49<4:25:05,  1.67s/it]  3%|▎         | 296/9822 [08:51<4:25:24,  1.67s/it]  3%|▎         | 297/9822 [08:53<4:25:23,  1.67s/it]  3%|▎         | 298/9822 [08:54<4:26:55,  1.68s/it]  3%|▎         | 299/9822 [08:56<4:27:48,  1.69s/it]  3%|▎         | 300/9822 [08:58<4:26:07,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:49:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:49:44 - INFO - __main__ - ***** test Results*****
04/29/2024 11:49:44 - INFO - __main__ -   Training step = 300
04/29/2024 11:49:44 - INFO - __main__ -  test_accuracy:0.8243045387994143 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:49:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:49:49 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:49:49 - INFO - __main__ -   Training step = 300
04/29/2024 11:49:49 - INFO - __main__ -  eval_accuracy:0.8161845477846943 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 11:49:49,341 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 11:49:49,341 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 11:49:49,382 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 11:49:50,513 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8161845477846943}
test:
{'accuracy': 0.8243045387994143}
04/29/2024 11:49:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:49:59 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:49:59 - INFO - __main__ -   Training step = 300
04/29/2024 11:49:59 - INFO - __main__ -  eval_accuracy:0.9025997803002563 
  3%|▎         | 301/9822 [09:18<19:18:23,  7.30s/it]  3%|▎         | 302/9822 [09:20<14:50:08,  5.61s/it]  3%|▎         | 303/9822 [09:21<11:41:37,  4.42s/it]  3%|▎         | 304/9822 [09:23<9:30:05,  3.59s/it]   3%|▎         | 305/9822 [09:25<7:57:31,  3.01s/it]  3%|▎         | 306/9822 [09:26<6:52:48,  2.60s/it]  3%|▎         | 307/9822 [09:28<6:12:43,  2.35s/it]  3%|▎         | 308/9822 [09:30<5:40:04,  2.14s/it]  3%|▎         | 309/9822 [09:31<5:16:34,  2.00s/it]  3%|▎         | 310/9822 [09:33<5:00:09,  1.89s/it]  3%|▎         | 311/9822 [09:35<4:48:41,  1.82s/it]  3%|▎         | 312/9822 [09:36<4:40:50,  1.77s/it]  3%|▎         | 313/9822 [09:38<4:35:05,  1.74s/it]  3%|▎         | 314/9822 [09:40<4:31:20,  1.71s/it]  3%|▎         | 315/9822 [09:41<4:29:28,  1.70s/it]  3%|▎         | 316/9822 [09:43<4:27:56,  1.69s/it]  3%|▎         | 317/9822 [09:45<4:26:24,  1.68s/it]  3%|▎         | 318/9822 [09:46<4:25:55,  1.68s/it]  3%|▎         | 319/9822 [09:48<4:24:47,  1.67s/it]  3%|▎         | 320/9822 [09:50<4:24:02,  1.67s/it]  3%|▎         | 321/9822 [09:51<4:23:39,  1.67s/it]  3%|▎         | 322/9822 [09:53<4:23:23,  1.66s/it]  3%|▎         | 323/9822 [09:55<4:22:51,  1.66s/it]  3%|▎         | 324/9822 [09:56<4:23:32,  1.66s/it]  3%|▎         | 325/9822 [09:58<4:23:04,  1.66s/it]  3%|▎         | 326/9822 [10:00<4:23:27,  1.66s/it]  3%|▎         | 327/9822 [10:01<4:22:57,  1.66s/it]  3%|▎         | 328/9822 [10:03<4:23:02,  1.66s/it]  3%|▎         | 329/9822 [10:05<4:23:01,  1.66s/it]  3%|▎         | 330/9822 [10:06<4:23:21,  1.66s/it]  3%|▎         | 331/9822 [10:08<4:23:24,  1.67s/it]  3%|▎         | 332/9822 [10:10<4:23:17,  1.66s/it]  3%|▎         | 333/9822 [10:11<4:23:28,  1.67s/it]  3%|▎         | 334/9822 [10:13<4:23:14,  1.66s/it]  3%|▎         | 335/9822 [10:15<4:23:15,  1.67s/it]  3%|▎         | 336/9822 [10:16<4:23:33,  1.67s/it]  3%|▎         | 337/9822 [10:18<4:23:42,  1.67s/it]  3%|▎         | 338/9822 [10:20<4:23:46,  1.67s/it]  3%|▎         | 339/9822 [10:21<4:23:48,  1.67s/it]  3%|▎         | 340/9822 [10:23<4:28:50,  1.70s/it]  3%|▎         | 341/9822 [10:25<4:27:03,  1.69s/it]  3%|▎         | 342/9822 [10:26<4:26:12,  1.68s/it]  3%|▎         | 343/9822 [10:28<4:25:44,  1.68s/it]  4%|▎         | 344/9822 [10:30<4:23:14,  1.67s/it]  4%|▎         | 345/9822 [10:31<4:24:12,  1.67s/it]  4%|▎         | 346/9822 [10:33<4:24:33,  1.68s/it]  4%|▎         | 347/9822 [10:35<4:23:41,  1.67s/it]  4%|▎         | 348/9822 [10:36<4:22:45,  1.66s/it]  4%|▎         | 349/9822 [10:38<4:22:22,  1.66s/it]  4%|▎         | 350/9822 [10:40<4:22:28,  1.66s/it]  4%|▎         | 351/9822 [10:41<4:22:24,  1.66s/it]  4%|▎         | 352/9822 [10:43<4:22:15,  1.66s/it]  4%|▎         | 353/9822 [10:45<4:22:51,  1.67s/it]  4%|▎         | 354/9822 [10:46<4:22:59,  1.67s/it]  4%|▎         | 355/9822 [10:48<4:23:20,  1.67s/it]  4%|▎         | 356/9822 [10:50<4:23:10,  1.67s/it]  4%|▎         | 357/9822 [10:51<4:22:25,  1.66s/it]  4%|▎         | 358/9822 [10:53<4:22:17,  1.66s/it]  4%|▎         | 359/9822 [10:55<4:21:58,  1.66s/it]  4%|▎         | 360/9822 [10:56<4:22:01,  1.66s/it]  4%|▎         | 361/9822 [10:58<4:21:54,  1.66s/it]  4%|▎         | 362/9822 [11:00<4:27:19,  1.70s/it]  4%|▎         | 363/9822 [11:01<4:25:27,  1.68s/it]  4%|▎         | 364/9822 [11:03<4:24:32,  1.68s/it]  4%|▎         | 365/9822 [11:05<4:23:08,  1.67s/it]  4%|▎         | 366/9822 [11:06<4:22:47,  1.67s/it]  4%|▎         | 367/9822 [11:08<4:22:49,  1.67s/it]  4%|▎         | 368/9822 [11:10<4:22:45,  1.67s/it]  4%|▍         | 369/9822 [11:11<4:22:20,  1.67s/it]  4%|▍         | 370/9822 [11:13<4:22:04,  1.66s/it]  4%|▍         | 371/9822 [11:15<4:21:49,  1.66s/it]  4%|▍         | 372/9822 [11:16<4:21:58,  1.66s/it]  4%|▍         | 373/9822 [11:18<4:22:10,  1.66s/it]  4%|▍         | 374/9822 [11:20<4:21:58,  1.66s/it]  4%|▍         | 375/9822 [11:21<4:21:21,  1.66s/it]  4%|▍         | 376/9822 [11:23<4:21:27,  1.66s/it]  4%|▍         | 377/9822 [11:25<4:21:31,  1.66s/it]  4%|▍         | 378/9822 [11:26<4:21:24,  1.66s/it]  4%|▍         | 379/9822 [11:28<4:21:32,  1.66s/it]  4%|▍         | 380/9822 [11:30<4:21:34,  1.66s/it]  4%|▍         | 381/9822 [11:31<4:21:36,  1.66s/it]  4%|▍         | 382/9822 [11:33<4:21:43,  1.66s/it]  4%|▍         | 383/9822 [11:35<4:21:43,  1.66s/it]  4%|▍         | 384/9822 [11:36<4:21:20,  1.66s/it]  4%|▍         | 385/9822 [11:38<4:21:28,  1.66s/it]  4%|▍         | 386/9822 [11:40<4:21:23,  1.66s/it]  4%|▍         | 387/9822 [11:41<4:21:09,  1.66s/it]  4%|▍         | 388/9822 [11:43<4:20:56,  1.66s/it]  4%|▍         | 389/9822 [11:45<4:27:02,  1.70s/it]  4%|▍         | 390/9822 [11:46<4:25:11,  1.69s/it]  4%|▍         | 391/9822 [11:48<4:24:36,  1.68s/it]  4%|▍         | 392/9822 [11:50<4:23:19,  1.68s/it]  4%|▍         | 393/9822 [11:51<4:22:14,  1.67s/it]  4%|▍         | 394/9822 [11:53<4:22:01,  1.67s/it]  4%|▍         | 395/9822 [11:55<4:22:06,  1.67s/it]  4%|▍         | 396/9822 [11:56<4:22:02,  1.67s/it]  4%|▍         | 397/9822 [11:58<4:21:31,  1.66s/it]  4%|▍         | 398/9822 [12:00<4:21:06,  1.66s/it]  4%|▍         | 399/9822 [12:01<4:21:14,  1.66s/it]  4%|▍         | 400/9822 [12:03<4:20:42,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0202, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:52:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:52:50 - INFO - __main__ - ***** test Results*****
04/29/2024 11:52:50 - INFO - __main__ -   Training step = 400
04/29/2024 11:52:50 - INFO - __main__ -  test_accuracy:0.8297950219619327 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:52:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:52:54 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:52:54 - INFO - __main__ -   Training step = 400
04/29/2024 11:52:54 - INFO - __main__ -  eval_accuracy:0.8191138777004761 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 11:52:54,861 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 11:52:54,861 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 11:52:54,902 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 11:52:56,024 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8191138777004761}
test:
{'accuracy': 0.8297950219619327}
04/29/2024 11:53:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:53:04 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:53:04 - INFO - __main__ -   Training step = 400
04/29/2024 11:53:04 - INFO - __main__ -  eval_accuracy:0.9044306114976199 
  4%|▍         | 401/9822 [12:23<19:05:31,  7.30s/it]  4%|▍         | 402/9822 [12:25<14:39:43,  5.60s/it]  4%|▍         | 403/9822 [12:27<11:33:38,  4.42s/it]  4%|▍         | 404/9822 [12:28<9:23:15,  3.59s/it]   4%|▍         | 405/9822 [12:30<7:52:06,  3.01s/it]  4%|▍         | 406/9822 [12:32<6:48:23,  2.60s/it]  4%|▍         | 407/9822 [12:33<6:04:06,  2.32s/it]  4%|▍         | 408/9822 [12:35<5:32:50,  2.12s/it]  4%|▍         | 409/9822 [12:37<5:11:00,  1.98s/it]  4%|▍         | 410/9822 [12:38<4:55:35,  1.88s/it]  4%|▍         | 411/9822 [12:40<4:44:49,  1.82s/it]  4%|▍         | 412/9822 [12:42<4:37:38,  1.77s/it]  4%|▍         | 413/9822 [12:43<4:32:19,  1.74s/it]  4%|▍         | 414/9822 [12:45<4:28:52,  1.71s/it]  4%|▍         | 415/9822 [12:47<4:26:12,  1.70s/it]  4%|▍         | 416/9822 [12:48<4:24:01,  1.68s/it]  4%|▍         | 417/9822 [12:50<4:23:02,  1.68s/it]  4%|▍         | 418/9822 [12:52<4:22:18,  1.67s/it]  4%|▍         | 419/9822 [12:53<4:22:00,  1.67s/it]  4%|▍         | 420/9822 [12:55<4:21:09,  1.67s/it]  4%|▍         | 421/9822 [12:57<4:20:59,  1.67s/it]  4%|▍         | 422/9822 [12:58<4:21:14,  1.67s/it]  4%|▍         | 423/9822 [13:00<4:20:31,  1.66s/it]  4%|▍         | 424/9822 [13:02<4:20:29,  1.66s/it]  4%|▍         | 425/9822 [13:03<4:20:31,  1.66s/it]  4%|▍         | 426/9822 [13:05<4:20:37,  1.66s/it]  4%|▍         | 427/9822 [13:07<4:25:30,  1.70s/it]  4%|▍         | 428/9822 [13:08<4:23:38,  1.68s/it]  4%|▍         | 429/9822 [13:10<4:22:20,  1.68s/it]  4%|▍         | 430/9822 [13:12<4:19:39,  1.66s/it]  4%|▍         | 431/9822 [13:13<4:19:20,  1.66s/it]  4%|▍         | 432/9822 [13:15<4:19:39,  1.66s/it]  4%|▍         | 433/9822 [13:17<4:19:45,  1.66s/it]  4%|▍         | 434/9822 [13:18<4:20:18,  1.66s/it]  4%|▍         | 435/9822 [13:20<4:20:35,  1.67s/it]  4%|▍         | 436/9822 [13:22<4:20:17,  1.66s/it]  4%|▍         | 437/9822 [13:23<4:20:06,  1.66s/it]  4%|▍         | 438/9822 [13:25<4:19:34,  1.66s/it]  4%|▍         | 439/9822 [13:27<4:19:24,  1.66s/it]  4%|▍         | 440/9822 [13:28<4:19:14,  1.66s/it]  4%|▍         | 441/9822 [13:30<4:19:46,  1.66s/it]  5%|▍         | 442/9822 [13:32<4:19:38,  1.66s/it]  5%|▍         | 443/9822 [13:33<4:19:58,  1.66s/it]  5%|▍         | 444/9822 [13:35<4:20:04,  1.66s/it]  5%|▍         | 445/9822 [13:37<4:19:53,  1.66s/it]  5%|▍         | 446/9822 [13:38<4:19:19,  1.66s/it]  5%|▍         | 447/9822 [13:40<4:19:25,  1.66s/it]  5%|▍         | 448/9822 [13:42<4:19:07,  1.66s/it]  5%|▍         | 449/9822 [13:43<4:19:00,  1.66s/it]  5%|▍         | 450/9822 [13:45<4:18:42,  1.66s/it]  5%|▍         | 451/9822 [13:47<4:18:36,  1.66s/it]  5%|▍         | 452/9822 [13:48<4:18:25,  1.65s/it]  5%|▍         | 453/9822 [13:50<4:18:19,  1.65s/it]  5%|▍         | 454/9822 [13:52<4:23:05,  1.69s/it]  5%|▍         | 455/9822 [13:53<4:21:37,  1.68s/it]  5%|▍         | 456/9822 [13:55<4:20:36,  1.67s/it]  5%|▍         | 457/9822 [13:57<4:19:52,  1.66s/it]  5%|▍         | 458/9822 [13:58<4:19:06,  1.66s/it]  5%|▍         | 459/9822 [14:00<4:18:43,  1.66s/it]  5%|▍         | 460/9822 [14:02<4:18:43,  1.66s/it]  5%|▍         | 461/9822 [14:03<4:18:22,  1.66s/it]  5%|▍         | 462/9822 [14:05<4:18:03,  1.65s/it]  5%|▍         | 463/9822 [14:07<4:18:40,  1.66s/it]  5%|▍         | 464/9822 [14:08<4:18:55,  1.66s/it]  5%|▍         | 465/9822 [14:10<4:18:36,  1.66s/it]  5%|▍         | 466/9822 [14:11<4:18:20,  1.66s/it]  5%|▍         | 467/9822 [14:13<4:18:26,  1.66s/it]  5%|▍         | 468/9822 [14:15<4:18:40,  1.66s/it]  5%|▍         | 469/9822 [14:16<4:19:16,  1.66s/it]  5%|▍         | 470/9822 [14:18<4:19:09,  1.66s/it]  5%|▍         | 471/9822 [14:20<4:19:39,  1.67s/it]  5%|▍         | 472/9822 [14:21<4:19:41,  1.67s/it]  5%|▍         | 473/9822 [14:23<4:19:35,  1.67s/it]  5%|▍         | 474/9822 [14:25<4:19:10,  1.66s/it]  5%|▍         | 475/9822 [14:26<4:19:18,  1.66s/it]  5%|▍         | 476/9822 [14:28<4:19:06,  1.66s/it]  5%|▍         | 477/9822 [14:30<4:19:35,  1.67s/it]  5%|▍         | 478/9822 [14:31<4:19:56,  1.67s/it]  5%|▍         | 479/9822 [14:33<4:20:13,  1.67s/it]  5%|▍         | 480/9822 [14:35<4:20:21,  1.67s/it]  5%|▍         | 481/9822 [14:37<4:25:16,  1.70s/it]  5%|▍         | 482/9822 [14:38<4:22:57,  1.69s/it]  5%|▍         | 483/9822 [14:40<4:22:01,  1.68s/it]  5%|▍         | 484/9822 [14:42<4:21:06,  1.68s/it]  5%|▍         | 485/9822 [14:43<4:20:20,  1.67s/it]  5%|▍         | 486/9822 [14:45<4:20:17,  1.67s/it]  5%|▍         | 487/9822 [14:47<4:19:35,  1.67s/it]  5%|▍         | 488/9822 [14:48<4:19:27,  1.67s/it]  5%|▍         | 489/9822 [14:50<4:19:13,  1.67s/it]  5%|▍         | 490/9822 [14:52<4:19:28,  1.67s/it]  5%|▍         | 491/9822 [14:53<4:19:26,  1.67s/it]  5%|▌         | 492/9822 [14:55<4:19:39,  1.67s/it]  5%|▌         | 493/9822 [14:57<4:19:39,  1.67s/it]  5%|▌         | 494/9822 [14:58<4:20:14,  1.67s/it]  5%|▌         | 495/9822 [15:00<4:20:20,  1.67s/it]  5%|▌         | 496/9822 [15:02<4:20:14,  1.67s/it]  5%|▌         | 497/9822 [15:03<4:19:50,  1.67s/it]  5%|▌         | 498/9822 [15:05<4:19:49,  1.67s/it]  5%|▌         | 499/9822 [15:07<4:19:18,  1.67s/it]  5%|▌         | 500/9822 [15:08<4:19:22,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:55:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:55:55 - INFO - __main__ - ***** test Results*****
04/29/2024 11:55:55 - INFO - __main__ -   Training step = 500
04/29/2024 11:55:55 - INFO - __main__ -  test_accuracy:0.8400439238653001 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:55:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:55:59 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:55:59 - INFO - __main__ -   Training step = 500
04/29/2024 11:55:59 - INFO - __main__ -  eval_accuracy:0.8183815452215306 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8191138777004761}
test:
{'accuracy': 0.8297950219619327}
04/29/2024 11:56:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:56:08 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:56:08 - INFO - __main__ -   Training step = 500
04/29/2024 11:56:08 - INFO - __main__ -  eval_accuracy:0.9040644452581472 
  5%|▌         | 501/9822 [15:27<17:54:55,  6.92s/it]  5%|▌         | 502/9822 [15:29<13:49:31,  5.34s/it]  5%|▌         | 503/9822 [15:31<10:58:08,  4.24s/it]  5%|▌         | 504/9822 [15:32<8:58:46,  3.47s/it]   5%|▌         | 505/9822 [15:34<7:34:53,  2.93s/it]  5%|▌         | 506/9822 [15:36<6:36:28,  2.55s/it]  5%|▌         | 507/9822 [15:37<5:55:02,  2.29s/it]  5%|▌         | 508/9822 [15:39<5:26:15,  2.10s/it]  5%|▌         | 509/9822 [15:41<5:05:39,  1.97s/it]  5%|▌         | 510/9822 [15:43<4:56:45,  1.91s/it]  5%|▌         | 511/9822 [15:44<4:45:42,  1.84s/it]  5%|▌         | 512/9822 [15:46<4:37:38,  1.79s/it]  5%|▌         | 513/9822 [15:48<4:32:06,  1.75s/it]  5%|▌         | 514/9822 [15:49<4:28:03,  1.73s/it]  5%|▌         | 515/9822 [15:51<4:24:58,  1.71s/it]  5%|▌         | 516/9822 [15:53<4:20:24,  1.68s/it]  5%|▌         | 517/9822 [15:54<4:19:08,  1.67s/it]  5%|▌         | 518/9822 [15:56<4:18:58,  1.67s/it]  5%|▌         | 519/9822 [15:58<4:18:30,  1.67s/it]  5%|▌         | 520/9822 [15:59<4:18:15,  1.67s/it]  5%|▌         | 521/9822 [16:01<4:17:52,  1.66s/it]  5%|▌         | 522/9822 [16:02<4:17:18,  1.66s/it]  5%|▌         | 523/9822 [16:04<4:17:39,  1.66s/it]  5%|▌         | 524/9822 [16:06<4:17:44,  1.66s/it]  5%|▌         | 525/9822 [16:07<4:17:38,  1.66s/it]  5%|▌         | 526/9822 [16:09<4:17:30,  1.66s/it]  5%|▌         | 527/9822 [16:11<4:17:38,  1.66s/it]  5%|▌         | 528/9822 [16:13<4:18:29,  1.67s/it]  5%|▌         | 529/9822 [16:14<4:18:20,  1.67s/it]  5%|▌         | 530/9822 [16:16<4:18:28,  1.67s/it]  5%|▌         | 531/9822 [16:18<4:18:27,  1.67s/it]  5%|▌         | 532/9822 [16:19<4:18:28,  1.67s/it]  5%|▌         | 533/9822 [16:21<4:18:28,  1.67s/it]  5%|▌         | 534/9822 [16:23<4:18:10,  1.67s/it]  5%|▌         | 535/9822 [16:24<4:18:11,  1.67s/it]  5%|▌         | 536/9822 [16:26<4:18:10,  1.67s/it]  5%|▌         | 537/9822 [16:27<4:17:17,  1.66s/it]  5%|▌         | 538/9822 [16:29<4:16:47,  1.66s/it]  5%|▌         | 539/9822 [16:31<4:17:03,  1.66s/it]  5%|▌         | 540/9822 [16:32<4:16:56,  1.66s/it]  6%|▌         | 541/9822 [16:34<4:17:08,  1.66s/it]  6%|▌         | 542/9822 [16:36<4:16:41,  1.66s/it]  6%|▌         | 543/9822 [16:38<4:21:33,  1.69s/it]  6%|▌         | 544/9822 [16:39<4:20:45,  1.69s/it]  6%|▌         | 545/9822 [16:41<4:20:38,  1.69s/it]  6%|▌         | 546/9822 [16:43<4:20:01,  1.68s/it]  6%|▌         | 547/9822 [16:44<4:19:36,  1.68s/it]  6%|▌         | 548/9822 [16:46<4:18:59,  1.68s/it]  6%|▌         | 549/9822 [16:48<4:17:55,  1.67s/it]  6%|▌         | 550/9822 [16:49<4:17:08,  1.66s/it]  6%|▌         | 551/9822 [16:51<4:17:06,  1.66s/it]  6%|▌         | 552/9822 [16:53<4:16:37,  1.66s/it]  6%|▌         | 553/9822 [16:54<4:16:34,  1.66s/it]  6%|▌         | 554/9822 [16:56<4:16:15,  1.66s/it]  6%|▌         | 555/9822 [16:58<4:17:00,  1.66s/it]  6%|▌         | 556/9822 [16:59<4:17:32,  1.67s/it]  6%|▌         | 557/9822 [17:01<4:17:24,  1.67s/it]  6%|▌         | 558/9822 [17:03<4:17:22,  1.67s/it]  6%|▌         | 559/9822 [17:04<4:17:30,  1.67s/it]  6%|▌         | 560/9822 [17:06<4:17:49,  1.67s/it]  6%|▌         | 561/9822 [17:08<4:17:54,  1.67s/it]  6%|▌         | 562/9822 [17:09<4:17:42,  1.67s/it]  6%|▌         | 563/9822 [17:11<4:17:54,  1.67s/it]  6%|▌         | 564/9822 [17:13<4:18:04,  1.67s/it]  6%|▌         | 565/9822 [17:14<4:22:46,  1.70s/it]  6%|▌         | 566/9822 [17:16<4:21:33,  1.70s/it]  6%|▌         | 567/9822 [17:18<4:20:33,  1.69s/it]  6%|▌         | 568/9822 [17:19<4:19:42,  1.68s/it]  6%|▌         | 569/9822 [17:21<4:19:36,  1.68s/it]  6%|▌         | 570/9822 [17:23<4:18:39,  1.68s/it]  6%|▌         | 571/9822 [17:24<4:17:57,  1.67s/it]  6%|▌         | 572/9822 [17:26<4:17:43,  1.67s/it]  6%|▌         | 573/9822 [17:28<4:17:32,  1.67s/it]  6%|▌         | 574/9822 [17:29<4:17:23,  1.67s/it]  6%|▌         | 575/9822 [17:31<4:17:08,  1.67s/it]  6%|▌         | 576/9822 [17:33<4:16:51,  1.67s/it]  6%|▌         | 577/9822 [17:34<4:16:42,  1.67s/it]  6%|▌         | 578/9822 [17:36<4:16:24,  1.66s/it]  6%|▌         | 579/9822 [17:38<4:16:10,  1.66s/it]  6%|▌         | 580/9822 [17:39<4:16:22,  1.66s/it]  6%|▌         | 581/9822 [17:41<4:16:24,  1.66s/it]  6%|▌         | 582/9822 [17:43<4:16:20,  1.66s/it]  6%|▌         | 583/9822 [17:44<4:16:44,  1.67s/it]  6%|▌         | 584/9822 [17:46<4:16:27,  1.67s/it]  6%|▌         | 585/9822 [17:48<4:16:30,  1.67s/it]  6%|▌         | 586/9822 [17:49<4:16:42,  1.67s/it]  6%|▌         | 587/9822 [17:51<4:17:03,  1.67s/it]  6%|▌         | 588/9822 [17:53<4:16:54,  1.67s/it]  6%|▌         | 589/9822 [17:54<4:17:09,  1.67s/it]  6%|▌         | 590/9822 [17:56<4:16:56,  1.67s/it]  6%|▌         | 591/9822 [17:58<4:17:05,  1.67s/it]  6%|▌         | 592/9822 [18:00<4:22:11,  1.70s/it]  6%|▌         | 593/9822 [18:01<4:20:34,  1.69s/it]  6%|▌         | 594/9822 [18:03<4:19:42,  1.69s/it]  6%|▌         | 595/9822 [18:05<4:19:20,  1.69s/it]  6%|▌         | 596/9822 [18:06<4:18:15,  1.68s/it]  6%|▌         | 597/9822 [18:08<4:18:23,  1.68s/it]  6%|▌         | 598/9822 [18:10<4:18:01,  1.68s/it]  6%|▌         | 599/9822 [18:11<4:17:32,  1.68s/it]  6%|▌         | 600/9822 [18:13<4:17:09,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 11:59:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:59:00 - INFO - __main__ - ***** test Results*****
04/29/2024 11:59:00 - INFO - __main__ -   Training step = 600
04/29/2024 11:59:00 - INFO - __main__ -  test_accuracy:0.8466325036603221 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:59:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:59:04 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 11:59:04 - INFO - __main__ -   Training step = 600
04/29/2024 11:59:04 - INFO - __main__ -  eval_accuracy:0.8319296960820213 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 11:59:04,616 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 11:59:04,617 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 11:59:04,657 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 11:59:05,808 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8319296960820213}
test:
{'accuracy': 0.8466325036603221}
04/29/2024 11:59:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 11:59:14 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 11:59:14 - INFO - __main__ -   Training step = 600
04/29/2024 11:59:14 - INFO - __main__ -  eval_accuracy:0.9025997803002563 
  6%|▌         | 601/9822 [18:33<18:39:01,  7.28s/it]  6%|▌         | 602/9822 [18:35<14:18:26,  5.59s/it]  6%|▌         | 603/9822 [18:37<11:17:43,  4.41s/it]  6%|▌         | 604/9822 [18:38<9:11:27,  3.59s/it]   6%|▌         | 605/9822 [18:40<7:43:23,  3.02s/it]  6%|▌         | 606/9822 [18:42<6:41:19,  2.61s/it]  6%|▌         | 607/9822 [18:43<5:57:17,  2.33s/it]  6%|▌         | 608/9822 [18:45<5:26:05,  2.12s/it]  6%|▌         | 609/9822 [18:47<5:04:36,  1.98s/it]  6%|▌         | 610/9822 [18:48<4:49:30,  1.89s/it]  6%|▌         | 611/9822 [18:50<4:38:58,  1.82s/it]  6%|▌         | 612/9822 [18:52<4:31:30,  1.77s/it]  6%|▌         | 613/9822 [18:53<4:26:59,  1.74s/it]  6%|▋         | 614/9822 [18:55<4:22:59,  1.71s/it]  6%|▋         | 615/9822 [18:57<4:20:06,  1.70s/it]  6%|▋         | 616/9822 [18:58<4:18:06,  1.68s/it]  6%|▋         | 617/9822 [19:00<4:17:15,  1.68s/it]  6%|▋         | 618/9822 [19:01<4:16:05,  1.67s/it]  6%|▋         | 619/9822 [19:03<4:15:28,  1.67s/it]  6%|▋         | 620/9822 [19:05<4:14:54,  1.66s/it]  6%|▋         | 621/9822 [19:06<4:14:25,  1.66s/it]  6%|▋         | 622/9822 [19:08<4:14:13,  1.66s/it]  6%|▋         | 623/9822 [19:10<4:14:34,  1.66s/it]  6%|▋         | 624/9822 [19:11<4:14:09,  1.66s/it]  6%|▋         | 625/9822 [19:13<4:15:06,  1.66s/it]  6%|▋         | 626/9822 [19:15<4:14:56,  1.66s/it]  6%|▋         | 627/9822 [19:16<4:14:39,  1.66s/it]  6%|▋         | 628/9822 [19:18<4:14:13,  1.66s/it]  6%|▋         | 629/9822 [19:20<4:13:50,  1.66s/it]  6%|▋         | 630/9822 [19:21<4:13:42,  1.66s/it]  6%|▋         | 631/9822 [19:23<4:13:31,  1.66s/it]  6%|▋         | 632/9822 [19:25<4:18:34,  1.69s/it]  6%|▋         | 633/9822 [19:26<4:16:52,  1.68s/it]  6%|▋         | 634/9822 [19:28<4:15:42,  1.67s/it]  6%|▋         | 635/9822 [19:30<4:15:08,  1.67s/it]  6%|▋         | 636/9822 [19:31<4:14:49,  1.66s/it]  6%|▋         | 637/9822 [19:33<4:14:39,  1.66s/it]  6%|▋         | 638/9822 [19:35<4:14:19,  1.66s/it]  7%|▋         | 639/9822 [19:36<4:13:54,  1.66s/it]  7%|▋         | 640/9822 [19:38<4:13:31,  1.66s/it]  7%|▋         | 641/9822 [19:40<4:13:18,  1.66s/it]  7%|▋         | 642/9822 [19:41<4:13:04,  1.65s/it]  7%|▋         | 643/9822 [19:43<4:13:20,  1.66s/it]  7%|▋         | 644/9822 [19:45<4:13:07,  1.65s/it]  7%|▋         | 645/9822 [19:46<4:13:40,  1.66s/it]  7%|▋         | 646/9822 [19:48<4:13:18,  1.66s/it]  7%|▋         | 647/9822 [19:50<4:13:42,  1.66s/it]  7%|▋         | 648/9822 [19:51<4:13:37,  1.66s/it]  7%|▋         | 649/9822 [19:53<4:13:56,  1.66s/it]  7%|▋         | 650/9822 [19:55<4:13:58,  1.66s/it]  7%|▋         | 651/9822 [19:56<4:13:32,  1.66s/it]  7%|▋         | 652/9822 [19:58<4:13:43,  1.66s/it]  7%|▋         | 653/9822 [20:00<4:13:50,  1.66s/it]  7%|▋         | 654/9822 [20:01<4:13:52,  1.66s/it]  7%|▋         | 655/9822 [20:03<4:14:19,  1.66s/it]  7%|▋         | 656/9822 [20:05<4:14:22,  1.67s/it]  7%|▋         | 657/9822 [20:06<4:14:33,  1.67s/it]  7%|▋         | 658/9822 [20:08<4:19:09,  1.70s/it]  7%|▋         | 659/9822 [20:10<4:17:43,  1.69s/it]  7%|▋         | 660/9822 [20:11<4:16:33,  1.68s/it]  7%|▋         | 661/9822 [20:13<4:16:05,  1.68s/it]  7%|▋         | 662/9822 [20:15<4:15:25,  1.67s/it]  7%|▋         | 663/9822 [20:16<4:15:02,  1.67s/it]  7%|▋         | 664/9822 [20:18<4:15:25,  1.67s/it]  7%|▋         | 665/9822 [20:20<4:14:29,  1.67s/it]  7%|▋         | 666/9822 [20:21<4:14:32,  1.67s/it]  7%|▋         | 667/9822 [20:23<4:14:27,  1.67s/it]  7%|▋         | 668/9822 [20:25<4:15:03,  1.67s/it]  7%|▋         | 669/9822 [20:26<4:15:11,  1.67s/it]  7%|▋         | 670/9822 [20:28<4:15:45,  1.68s/it]  7%|▋         | 671/9822 [20:30<4:15:08,  1.67s/it]  7%|▋         | 672/9822 [20:31<4:14:44,  1.67s/it]  7%|▋         | 673/9822 [20:33<4:14:02,  1.67s/it]  7%|▋         | 674/9822 [20:35<4:13:18,  1.66s/it]  7%|▋         | 675/9822 [20:36<4:13:26,  1.66s/it]  7%|▋         | 676/9822 [20:38<4:13:18,  1.66s/it]  7%|▋         | 677/9822 [20:40<4:13:35,  1.66s/it]  7%|▋         | 678/9822 [20:41<4:13:37,  1.66s/it]  7%|▋         | 679/9822 [20:43<4:13:30,  1.66s/it]  7%|▋         | 680/9822 [20:45<4:14:16,  1.67s/it]  7%|▋         | 681/9822 [20:46<4:14:30,  1.67s/it]  7%|▋         | 682/9822 [20:48<4:14:11,  1.67s/it]  7%|▋         | 683/9822 [20:50<4:13:47,  1.67s/it]  7%|▋         | 684/9822 [20:51<4:13:40,  1.67s/it]  7%|▋         | 685/9822 [20:53<4:13:41,  1.67s/it]  7%|▋         | 686/9822 [20:55<4:13:26,  1.66s/it]  7%|▋         | 687/9822 [20:56<4:13:43,  1.67s/it]  7%|▋         | 688/9822 [20:58<4:11:27,  1.65s/it]  7%|▋         | 689/9822 [21:00<4:11:57,  1.66s/it]  7%|▋         | 690/9822 [21:01<4:12:25,  1.66s/it]  7%|▋         | 691/9822 [21:03<4:12:50,  1.66s/it]  7%|▋         | 692/9822 [21:05<4:12:52,  1.66s/it]  7%|▋         | 693/9822 [21:06<4:13:37,  1.67s/it]  7%|▋         | 694/9822 [21:08<4:13:36,  1.67s/it]  7%|▋         | 695/9822 [21:10<4:14:02,  1.67s/it]  7%|▋         | 696/9822 [21:11<4:14:03,  1.67s/it]  7%|▋         | 697/9822 [21:13<4:14:17,  1.67s/it]  7%|▋         | 698/9822 [21:15<4:14:39,  1.67s/it]  7%|▋         | 699/9822 [21:16<4:19:19,  1.71s/it]  7%|▋         | 700/9822 [21:18<4:18:03,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:02:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:02:05 - INFO - __main__ - ***** test Results*****
04/29/2024 12:02:05 - INFO - __main__ -   Training step = 700
04/29/2024 12:02:05 - INFO - __main__ -  test_accuracy:0.8473645680819912 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:02:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:02:09 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:02:09 - INFO - __main__ -   Training step = 700
04/29/2024 12:02:09 - INFO - __main__ -  eval_accuracy:0.8249725375320396 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8319296960820213}
test:
{'accuracy': 0.8466325036603221}
04/29/2024 12:02:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:02:18 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:02:18 - INFO - __main__ -   Training step = 700
04/29/2024 12:02:18 - INFO - __main__ -  eval_accuracy:0.90040278286342 
  7%|▋         | 701/9822 [21:37<17:35:37,  6.94s/it]  7%|▋         | 702/9822 [21:39<13:34:33,  5.36s/it]  7%|▋         | 703/9822 [21:41<10:46:25,  4.25s/it]  7%|▋         | 704/9822 [21:42<8:49:02,  3.48s/it]   7%|▋         | 705/9822 [21:44<7:26:59,  2.94s/it]  7%|▋         | 706/9822 [21:46<6:29:02,  2.56s/it]  7%|▋         | 707/9822 [21:47<5:48:53,  2.30s/it]  7%|▋         | 708/9822 [21:49<5:20:11,  2.11s/it]  7%|▋         | 709/9822 [21:51<4:59:45,  1.97s/it]  7%|▋         | 710/9822 [21:52<4:45:19,  1.88s/it]  7%|▋         | 711/9822 [21:54<4:35:00,  1.81s/it]  7%|▋         | 712/9822 [21:56<4:27:41,  1.76s/it]  7%|▋         | 713/9822 [21:57<4:23:30,  1.74s/it]  7%|▋         | 714/9822 [21:59<4:19:53,  1.71s/it]  7%|▋         | 715/9822 [22:01<4:17:41,  1.70s/it]  7%|▋         | 716/9822 [22:02<4:16:11,  1.69s/it]  7%|▋         | 717/9822 [22:04<4:14:33,  1.68s/it]  7%|▋         | 718/9822 [22:06<4:13:22,  1.67s/it]  7%|▋         | 719/9822 [22:07<4:12:53,  1.67s/it]  7%|▋         | 720/9822 [22:09<4:12:09,  1.66s/it]  7%|▋         | 721/9822 [22:11<4:12:06,  1.66s/it]  7%|▋         | 722/9822 [22:12<4:12:25,  1.66s/it]  7%|▋         | 723/9822 [22:14<4:12:04,  1.66s/it]  7%|▋         | 724/9822 [22:16<4:11:48,  1.66s/it]  7%|▋         | 725/9822 [22:17<4:11:47,  1.66s/it]  7%|▋         | 726/9822 [22:19<4:11:36,  1.66s/it]  7%|▋         | 727/9822 [22:21<4:11:47,  1.66s/it]  7%|▋         | 728/9822 [22:22<4:11:31,  1.66s/it]  7%|▋         | 729/9822 [22:24<4:12:14,  1.66s/it]  7%|▋         | 730/9822 [22:26<4:11:58,  1.66s/it]  7%|▋         | 731/9822 [22:27<4:11:39,  1.66s/it]  7%|▋         | 732/9822 [22:29<4:11:59,  1.66s/it]  7%|▋         | 733/9822 [22:31<4:11:53,  1.66s/it]  7%|▋         | 734/9822 [22:32<4:11:40,  1.66s/it]  7%|▋         | 735/9822 [22:34<4:11:33,  1.66s/it]  7%|▋         | 736/9822 [22:36<4:11:19,  1.66s/it]  8%|▊         | 737/9822 [22:37<4:11:11,  1.66s/it]  8%|▊         | 738/9822 [22:39<4:11:30,  1.66s/it]  8%|▊         | 739/9822 [22:41<4:11:27,  1.66s/it]  8%|▊         | 740/9822 [22:42<4:11:45,  1.66s/it]  8%|▊         | 741/9822 [22:44<4:11:54,  1.66s/it]  8%|▊         | 742/9822 [22:46<4:12:02,  1.67s/it]  8%|▊         | 743/9822 [22:47<4:16:37,  1.70s/it]  8%|▊         | 744/9822 [22:49<4:15:20,  1.69s/it]  8%|▊         | 745/9822 [22:51<4:14:25,  1.68s/it]  8%|▊         | 746/9822 [22:52<4:14:00,  1.68s/it]  8%|▊         | 747/9822 [22:54<4:13:26,  1.68s/it]  8%|▊         | 748/9822 [22:56<4:12:45,  1.67s/it]  8%|▊         | 749/9822 [22:57<4:12:33,  1.67s/it]  8%|▊         | 750/9822 [22:59<4:12:49,  1.67s/it]  8%|▊         | 751/9822 [23:01<4:12:29,  1.67s/it]  8%|▊         | 752/9822 [23:02<4:14:19,  1.68s/it]  8%|▊         | 753/9822 [23:04<4:13:59,  1.68s/it]  8%|▊         | 754/9822 [23:06<4:13:26,  1.68s/it]  8%|▊         | 755/9822 [23:07<4:12:37,  1.67s/it]  8%|▊         | 756/9822 [23:09<4:12:31,  1.67s/it]  8%|▊         | 757/9822 [23:11<4:12:12,  1.67s/it]  8%|▊         | 758/9822 [23:12<4:12:04,  1.67s/it]  8%|▊         | 759/9822 [23:14<4:12:29,  1.67s/it]  8%|▊         | 760/9822 [23:16<4:12:45,  1.67s/it]  8%|▊         | 761/9822 [23:17<4:13:01,  1.68s/it]  8%|▊         | 762/9822 [23:19<4:12:50,  1.67s/it]  8%|▊         | 763/9822 [23:21<4:12:17,  1.67s/it]  8%|▊         | 764/9822 [23:22<4:11:53,  1.67s/it]  8%|▊         | 765/9822 [23:24<4:11:52,  1.67s/it]  8%|▊         | 766/9822 [23:26<4:12:10,  1.67s/it]  8%|▊         | 767/9822 [23:27<4:11:54,  1.67s/it]  8%|▊         | 768/9822 [23:29<4:11:58,  1.67s/it]  8%|▊         | 769/9822 [23:31<4:11:42,  1.67s/it]  8%|▊         | 770/9822 [23:33<4:16:09,  1.70s/it]  8%|▊         | 771/9822 [23:34<4:14:28,  1.69s/it]  8%|▊         | 772/9822 [23:36<4:13:41,  1.68s/it]  8%|▊         | 773/9822 [23:38<4:13:46,  1.68s/it]  8%|▊         | 774/9822 [23:39<4:11:27,  1.67s/it]  8%|▊         | 775/9822 [23:41<4:12:12,  1.67s/it]  8%|▊         | 776/9822 [23:43<4:11:55,  1.67s/it]  8%|▊         | 777/9822 [23:44<4:12:29,  1.67s/it]  8%|▊         | 778/9822 [23:46<4:12:42,  1.68s/it]  8%|▊         | 779/9822 [23:48<4:12:57,  1.68s/it]  8%|▊         | 780/9822 [23:49<4:12:25,  1.68s/it]  8%|▊         | 781/9822 [23:51<4:12:44,  1.68s/it]  8%|▊         | 782/9822 [23:53<4:12:58,  1.68s/it]  8%|▊         | 783/9822 [23:54<4:12:32,  1.68s/it]  8%|▊         | 784/9822 [23:56<4:12:06,  1.67s/it]  8%|▊         | 785/9822 [23:58<4:11:47,  1.67s/it]  8%|▊         | 786/9822 [23:59<4:11:25,  1.67s/it]  8%|▊         | 787/9822 [24:01<4:11:21,  1.67s/it]  8%|▊         | 788/9822 [24:03<4:11:45,  1.67s/it]  8%|▊         | 789/9822 [24:04<4:11:33,  1.67s/it]  8%|▊         | 790/9822 [24:06<4:11:30,  1.67s/it]  8%|▊         | 791/9822 [24:08<4:11:00,  1.67s/it]  8%|▊         | 792/9822 [24:09<4:11:07,  1.67s/it]  8%|▊         | 793/9822 [24:11<4:11:21,  1.67s/it]  8%|▊         | 794/9822 [24:13<4:11:35,  1.67s/it]  8%|▊         | 795/9822 [24:14<4:11:26,  1.67s/it]  8%|▊         | 796/9822 [24:16<4:11:32,  1.67s/it]  8%|▊         | 797/9822 [24:18<4:16:20,  1.70s/it]  8%|▊         | 798/9822 [24:19<4:15:07,  1.70s/it]  8%|▊         | 799/9822 [24:21<4:14:08,  1.69s/it]  8%|▊         | 800/9822 [24:23<4:12:44,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:05:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:05:09 - INFO - __main__ - ***** test Results*****
04/29/2024 12:05:09 - INFO - __main__ -   Training step = 800
04/29/2024 12:05:09 - INFO - __main__ -  test_accuracy:0.8473645680819912 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:05:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:05:14 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:05:14 - INFO - __main__ -   Training step = 800
04/29/2024 12:05:14 - INFO - __main__ -  eval_accuracy:0.829732698645185 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8319296960820213}
test:
{'accuracy': 0.8466325036603221}
04/29/2024 12:05:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:05:22 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:05:22 - INFO - __main__ -   Training step = 800
04/29/2024 12:05:22 - INFO - __main__ -  eval_accuracy:0.9084584401318199 
  8%|▊         | 801/9822 [24:42<17:23:29,  6.94s/it]  8%|▊         | 802/9822 [24:44<13:26:18,  5.36s/it]  8%|▊         | 803/9822 [24:45<10:40:08,  4.26s/it]  8%|▊         | 804/9822 [24:47<8:44:06,  3.49s/it]   8%|▊         | 805/9822 [24:49<7:22:46,  2.95s/it]  8%|▊         | 806/9822 [24:50<6:25:49,  2.57s/it]  8%|▊         | 807/9822 [24:52<5:45:48,  2.30s/it]  8%|▊         | 808/9822 [24:54<5:17:46,  2.12s/it]  8%|▊         | 809/9822 [24:55<4:58:28,  1.99s/it]  8%|▊         | 810/9822 [24:57<4:44:55,  1.90s/it]  8%|▊         | 811/9822 [24:59<4:35:17,  1.83s/it]  8%|▊         | 812/9822 [25:00<4:28:24,  1.79s/it]  8%|▊         | 813/9822 [25:02<4:23:33,  1.76s/it]  8%|▊         | 814/9822 [25:04<4:20:16,  1.73s/it]  8%|▊         | 815/9822 [25:06<4:18:00,  1.72s/it]  8%|▊         | 816/9822 [25:07<4:16:34,  1.71s/it]  8%|▊         | 817/9822 [25:09<4:15:17,  1.70s/it]  8%|▊         | 818/9822 [25:11<4:14:18,  1.69s/it]  8%|▊         | 819/9822 [25:12<4:13:44,  1.69s/it]  8%|▊         | 820/9822 [25:14<4:13:24,  1.69s/it]  8%|▊         | 821/9822 [25:16<4:13:09,  1.69s/it]  8%|▊         | 822/9822 [25:17<4:12:59,  1.69s/it]  8%|▊         | 823/9822 [25:19<4:17:24,  1.72s/it]  8%|▊         | 824/9822 [25:21<4:16:01,  1.71s/it]  8%|▊         | 825/9822 [25:22<4:14:57,  1.70s/it]  8%|▊         | 826/9822 [25:24<4:14:22,  1.70s/it]  8%|▊         | 827/9822 [25:26<4:13:49,  1.69s/it]  8%|▊         | 828/9822 [25:28<4:13:25,  1.69s/it]  8%|▊         | 829/9822 [25:29<4:13:21,  1.69s/it]  8%|▊         | 830/9822 [25:31<4:13:15,  1.69s/it]  8%|▊         | 831/9822 [25:33<4:12:56,  1.69s/it]  8%|▊         | 832/9822 [25:34<4:12:44,  1.69s/it]  8%|▊         | 833/9822 [25:36<4:12:36,  1.69s/it]  8%|▊         | 834/9822 [25:38<4:11:58,  1.68s/it]  9%|▊         | 835/9822 [25:39<4:12:10,  1.68s/it]  9%|▊         | 836/9822 [25:41<4:12:09,  1.68s/it]  9%|▊         | 837/9822 [25:43<4:12:07,  1.68s/it]  9%|▊         | 838/9822 [25:44<4:12:06,  1.68s/it]  9%|▊         | 839/9822 [25:46<4:11:59,  1.68s/it]  9%|▊         | 840/9822 [25:48<4:11:57,  1.68s/it]  9%|▊         | 841/9822 [25:49<4:11:54,  1.68s/it]  9%|▊         | 842/9822 [25:51<4:11:22,  1.68s/it]  9%|▊         | 843/9822 [25:53<4:10:50,  1.68s/it]  9%|▊         | 844/9822 [25:54<4:11:17,  1.68s/it]  9%|▊         | 845/9822 [25:56<4:11:19,  1.68s/it]  9%|▊         | 846/9822 [25:58<4:11:03,  1.68s/it]  9%|▊         | 847/9822 [26:00<4:11:59,  1.68s/it]  9%|▊         | 848/9822 [26:01<4:11:36,  1.68s/it]  9%|▊         | 849/9822 [26:03<4:11:15,  1.68s/it]  9%|▊         | 850/9822 [26:05<4:11:25,  1.68s/it]  9%|▊         | 851/9822 [26:06<4:10:37,  1.68s/it]  9%|▊         | 852/9822 [26:08<4:10:13,  1.67s/it]  9%|▊         | 853/9822 [26:10<4:09:54,  1.67s/it]  9%|▊         | 854/9822 [26:11<4:10:03,  1.67s/it]  9%|▊         | 855/9822 [26:13<4:10:35,  1.68s/it]  9%|▊         | 856/9822 [26:15<4:15:02,  1.71s/it]  9%|▊         | 857/9822 [26:16<4:13:10,  1.69s/it]  9%|▊         | 858/9822 [26:18<4:11:53,  1.69s/it]  9%|▊         | 859/9822 [26:20<4:10:59,  1.68s/it]  9%|▉         | 860/9822 [26:21<4:08:09,  1.66s/it]  9%|▉         | 861/9822 [26:23<4:08:17,  1.66s/it]  9%|▉         | 862/9822 [26:25<4:08:37,  1.66s/it]  9%|▉         | 863/9822 [26:26<4:08:34,  1.66s/it]  9%|▉         | 864/9822 [26:28<4:08:32,  1.66s/it]  9%|▉         | 865/9822 [26:30<4:08:36,  1.67s/it]  9%|▉         | 866/9822 [26:31<4:08:37,  1.67s/it]  9%|▉         | 867/9822 [26:33<4:08:55,  1.67s/it]  9%|▉         | 868/9822 [26:35<4:08:57,  1.67s/it]  9%|▉         | 869/9822 [26:36<4:08:57,  1.67s/it]  9%|▉         | 870/9822 [26:38<4:08:45,  1.67s/it]  9%|▉         | 871/9822 [26:40<4:08:36,  1.67s/it]  9%|▉         | 872/9822 [26:41<4:08:28,  1.67s/it]  9%|▉         | 873/9822 [26:43<4:08:49,  1.67s/it]  9%|▉         | 874/9822 [26:45<4:09:57,  1.68s/it]  9%|▉         | 875/9822 [26:46<4:10:19,  1.68s/it]  9%|▉         | 876/9822 [26:48<4:10:11,  1.68s/it]  9%|▉         | 877/9822 [26:50<4:09:34,  1.67s/it]  9%|▉         | 878/9822 [26:51<4:14:34,  1.71s/it]  9%|▉         | 879/9822 [26:53<4:12:36,  1.69s/it]  9%|▉         | 880/9822 [26:55<4:11:13,  1.69s/it]  9%|▉         | 881/9822 [26:57<4:11:09,  1.69s/it]  9%|▉         | 882/9822 [26:58<4:11:32,  1.69s/it]  9%|▉         | 883/9822 [27:00<4:11:18,  1.69s/it]  9%|▉         | 884/9822 [27:02<4:11:12,  1.69s/it]  9%|▉         | 885/9822 [27:03<4:10:16,  1.68s/it]  9%|▉         | 886/9822 [27:05<4:10:15,  1.68s/it]  9%|▉         | 887/9822 [27:07<4:09:39,  1.68s/it]  9%|▉         | 888/9822 [27:08<4:09:17,  1.67s/it]  9%|▉         | 889/9822 [27:10<4:09:17,  1.67s/it]  9%|▉         | 890/9822 [27:12<4:08:53,  1.67s/it]  9%|▉         | 891/9822 [27:13<4:08:30,  1.67s/it]  9%|▉         | 892/9822 [27:15<4:08:33,  1.67s/it]  9%|▉         | 893/9822 [27:17<4:09:10,  1.67s/it]  9%|▉         | 894/9822 [27:18<4:09:24,  1.68s/it]  9%|▉         | 895/9822 [27:20<4:09:03,  1.67s/it]  9%|▉         | 896/9822 [27:22<4:09:07,  1.67s/it]  9%|▉         | 897/9822 [27:23<4:09:11,  1.68s/it]  9%|▉         | 898/9822 [27:25<4:09:31,  1.68s/it]  9%|▉         | 899/9822 [27:27<4:09:34,  1.68s/it]  9%|▉         | 900/9822 [27:28<4:09:05,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:08:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:08:15 - INFO - __main__ - ***** test Results*****
04/29/2024 12:08:15 - INFO - __main__ -   Training step = 900
04/29/2024 12:08:15 - INFO - __main__ -  test_accuracy:0.8448023426061494 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:08:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:08:20 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:08:20 - INFO - __main__ -   Training step = 900
04/29/2024 12:08:20 - INFO - __main__ -  eval_accuracy:0.8286341999267668 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8319296960820213}
test:
{'accuracy': 0.8466325036603221}
04/29/2024 12:08:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:08:28 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:08:28 - INFO - __main__ -   Training step = 900
04/29/2024 12:08:28 - INFO - __main__ -  eval_accuracy:0.906993775173929 
  9%|▉         | 901/9822 [27:48<17:14:22,  6.96s/it]  9%|▉         | 902/9822 [27:49<13:18:08,  5.37s/it]  9%|▉         | 903/9822 [27:51<10:33:00,  4.26s/it]  9%|▉         | 904/9822 [27:53<8:39:06,  3.49s/it]   9%|▉         | 905/9822 [27:54<7:22:08,  2.98s/it]  9%|▉         | 906/9822 [27:56<6:23:53,  2.58s/it]  9%|▉         | 907/9822 [27:58<5:42:37,  2.31s/it]  9%|▉         | 908/9822 [27:59<5:14:28,  2.12s/it]  9%|▉         | 909/9822 [28:01<4:54:52,  1.98s/it]  9%|▉         | 910/9822 [28:03<4:40:28,  1.89s/it]  9%|▉         | 911/9822 [28:04<4:30:51,  1.82s/it]  9%|▉         | 912/9822 [28:06<4:23:42,  1.78s/it]  9%|▉         | 913/9822 [28:08<4:18:51,  1.74s/it]  9%|▉         | 914/9822 [28:09<4:14:55,  1.72s/it]  9%|▉         | 915/9822 [28:11<4:12:47,  1.70s/it]  9%|▉         | 916/9822 [28:13<4:11:02,  1.69s/it]  9%|▉         | 917/9822 [28:14<4:10:01,  1.68s/it]  9%|▉         | 918/9822 [28:16<4:09:06,  1.68s/it]  9%|▉         | 919/9822 [28:18<4:08:30,  1.67s/it]  9%|▉         | 920/9822 [28:19<4:07:33,  1.67s/it]  9%|▉         | 921/9822 [28:21<4:07:43,  1.67s/it]  9%|▉         | 922/9822 [28:23<4:07:47,  1.67s/it]  9%|▉         | 923/9822 [28:24<4:07:41,  1.67s/it]  9%|▉         | 924/9822 [28:26<4:07:53,  1.67s/it]  9%|▉         | 925/9822 [28:28<4:07:43,  1.67s/it]  9%|▉         | 926/9822 [28:29<4:07:56,  1.67s/it]  9%|▉         | 927/9822 [28:31<4:07:42,  1.67s/it]  9%|▉         | 928/9822 [28:33<4:07:18,  1.67s/it]  9%|▉         | 929/9822 [28:34<4:07:18,  1.67s/it]  9%|▉         | 930/9822 [28:36<4:07:11,  1.67s/it]  9%|▉         | 931/9822 [28:38<4:12:04,  1.70s/it]  9%|▉         | 932/9822 [28:40<4:10:43,  1.69s/it]  9%|▉         | 933/9822 [28:41<4:09:49,  1.69s/it] 10%|▉         | 934/9822 [28:43<4:09:09,  1.68s/it] 10%|▉         | 935/9822 [28:45<4:08:29,  1.68s/it] 10%|▉         | 936/9822 [28:46<4:08:33,  1.68s/it] 10%|▉         | 937/9822 [28:48<4:08:21,  1.68s/it] 10%|▉         | 938/9822 [28:50<4:07:42,  1.67s/it] 10%|▉         | 939/9822 [28:51<4:07:12,  1.67s/it] 10%|▉         | 940/9822 [28:53<4:06:52,  1.67s/it] 10%|▉         | 941/9822 [28:55<4:07:04,  1.67s/it] 10%|▉         | 942/9822 [28:56<4:06:40,  1.67s/it] 10%|▉         | 943/9822 [28:58<4:07:01,  1.67s/it] 10%|▉         | 944/9822 [29:00<4:06:55,  1.67s/it] 10%|▉         | 945/9822 [29:01<4:07:05,  1.67s/it] 10%|▉         | 946/9822 [29:03<4:05:08,  1.66s/it] 10%|▉         | 947/9822 [29:05<4:06:12,  1.66s/it] 10%|▉         | 948/9822 [29:06<4:06:48,  1.67s/it] 10%|▉         | 949/9822 [29:08<4:06:45,  1.67s/it] 10%|▉         | 950/9822 [29:10<4:07:05,  1.67s/it] 10%|▉         | 951/9822 [29:11<4:06:48,  1.67s/it] 10%|▉         | 952/9822 [29:13<4:06:34,  1.67s/it] 10%|▉         | 953/9822 [29:15<4:06:06,  1.66s/it] 10%|▉         | 954/9822 [29:16<4:06:38,  1.67s/it] 10%|▉         | 955/9822 [29:18<4:06:10,  1.67s/it] 10%|▉         | 956/9822 [29:20<4:06:53,  1.67s/it] 10%|▉         | 957/9822 [29:21<4:06:46,  1.67s/it] 10%|▉         | 958/9822 [29:23<4:06:47,  1.67s/it] 10%|▉         | 959/9822 [29:25<4:06:27,  1.67s/it] 10%|▉         | 960/9822 [29:26<4:06:35,  1.67s/it] 10%|▉         | 961/9822 [29:28<4:06:39,  1.67s/it] 10%|▉         | 962/9822 [29:30<4:06:30,  1.67s/it] 10%|▉         | 963/9822 [29:31<4:06:55,  1.67s/it] 10%|▉         | 964/9822 [29:33<4:11:52,  1.71s/it] 10%|▉         | 965/9822 [29:35<4:10:34,  1.70s/it] 10%|▉         | 966/9822 [29:36<4:09:54,  1.69s/it] 10%|▉         | 967/9822 [29:38<4:09:21,  1.69s/it] 10%|▉         | 968/9822 [29:40<4:09:13,  1.69s/it] 10%|▉         | 969/9822 [29:41<4:08:47,  1.69s/it] 10%|▉         | 970/9822 [29:43<4:08:39,  1.69s/it] 10%|▉         | 971/9822 [29:45<4:08:36,  1.69s/it] 10%|▉         | 972/9822 [29:47<4:08:39,  1.69s/it] 10%|▉         | 973/9822 [29:48<4:08:24,  1.68s/it] 10%|▉         | 974/9822 [29:50<4:08:20,  1.68s/it] 10%|▉         | 975/9822 [29:52<4:08:04,  1.68s/it] 10%|▉         | 976/9822 [29:53<4:07:19,  1.68s/it] 10%|▉         | 977/9822 [29:55<4:06:08,  1.67s/it] 10%|▉         | 978/9822 [29:57<4:06:23,  1.67s/it] 10%|▉         | 979/9822 [29:58<4:06:15,  1.67s/it] 10%|▉         | 980/9822 [30:00<4:05:21,  1.66s/it] 10%|▉         | 981/9822 [30:02<4:05:25,  1.67s/it] 10%|▉         | 982/9822 [30:03<4:04:59,  1.66s/it] 10%|█         | 983/9822 [30:05<4:04:45,  1.66s/it] 10%|█         | 984/9822 [30:07<4:04:37,  1.66s/it] 10%|█         | 985/9822 [30:08<4:04:51,  1.66s/it] 10%|█         | 986/9822 [30:10<4:08:56,  1.69s/it] 10%|█         | 987/9822 [30:12<4:07:14,  1.68s/it] 10%|█         | 988/9822 [30:13<4:06:23,  1.67s/it] 10%|█         | 989/9822 [30:15<4:05:45,  1.67s/it] 10%|█         | 990/9822 [30:17<4:05:13,  1.67s/it] 10%|█         | 991/9822 [30:18<4:05:01,  1.66s/it] 10%|█         | 992/9822 [30:20<4:05:40,  1.67s/it] 10%|█         | 993/9822 [30:22<4:05:00,  1.67s/it] 10%|█         | 994/9822 [30:23<4:04:36,  1.66s/it] 10%|█         | 995/9822 [30:25<4:04:19,  1.66s/it] 10%|█         | 996/9822 [30:27<4:04:24,  1.66s/it] 10%|█         | 997/9822 [30:28<4:04:20,  1.66s/it] 10%|█         | 998/9822 [30:30<4:05:18,  1.67s/it] 10%|█         | 999/9822 [30:32<4:05:37,  1.67s/it] 10%|█         | 1000/9822 [30:33<4:05:41,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:11:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:11:20 - INFO - __main__ - ***** test Results*****
04/29/2024 12:11:20 - INFO - __main__ -   Training step = 1000
04/29/2024 12:11:20 - INFO - __main__ -  test_accuracy:0.8513909224011713 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:11:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:11:24 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:11:24 - INFO - __main__ -   Training step = 1000
04/29/2024 12:11:24 - INFO - __main__ -  eval_accuracy:0.8341266935188576 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:11:24,981 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:11:24,981 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:11:25,022 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:11:26,950 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8341266935188576}
test:
{'accuracy': 0.8513909224011713}
04/29/2024 12:11:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:11:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:11:35 - INFO - __main__ -   Training step = 1000
04/29/2024 12:11:35 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 10%|█         | 1001/9822 [30:54<18:25:05,  7.52s/it] 10%|█         | 1002/9822 [30:56<14:06:30,  5.76s/it] 10%|█         | 1003/9822 [30:58<11:05:57,  4.53s/it] 10%|█         | 1004/9822 [30:59<8:59:48,  3.67s/it]  10%|█         | 1005/9822 [31:01<7:31:15,  3.07s/it] 10%|█         | 1006/9822 [31:03<6:29:15,  2.65s/it] 10%|█         | 1007/9822 [31:04<5:45:59,  2.35s/it] 10%|█         | 1008/9822 [31:06<5:15:50,  2.15s/it] 10%|█         | 1009/9822 [31:08<4:54:40,  2.01s/it] 10%|█         | 1010/9822 [31:09<4:39:24,  1.90s/it] 10%|█         | 1011/9822 [31:11<4:28:58,  1.83s/it] 10%|█         | 1012/9822 [31:13<4:21:57,  1.78s/it] 10%|█         | 1013/9822 [31:14<4:16:47,  1.75s/it] 10%|█         | 1014/9822 [31:16<4:12:57,  1.72s/it] 10%|█         | 1015/9822 [31:18<4:10:44,  1.71s/it] 10%|█         | 1016/9822 [31:19<4:09:08,  1.70s/it] 10%|█         | 1017/9822 [31:21<4:12:13,  1.72s/it] 10%|█         | 1018/9822 [31:23<4:10:16,  1.71s/it] 10%|█         | 1019/9822 [31:25<4:08:57,  1.70s/it] 10%|█         | 1020/9822 [31:26<4:07:17,  1.69s/it] 10%|█         | 1021/9822 [31:28<4:05:46,  1.68s/it] 10%|█         | 1022/9822 [31:30<4:05:46,  1.68s/it] 10%|█         | 1023/9822 [31:31<4:05:58,  1.68s/it] 10%|█         | 1024/9822 [31:33<4:06:03,  1.68s/it] 10%|█         | 1025/9822 [31:35<4:06:05,  1.68s/it] 10%|█         | 1026/9822 [31:36<4:05:40,  1.68s/it] 10%|█         | 1027/9822 [31:38<4:05:05,  1.67s/it] 10%|█         | 1028/9822 [31:40<4:04:36,  1.67s/it] 10%|█         | 1029/9822 [31:41<4:05:13,  1.67s/it] 10%|█         | 1030/9822 [31:43<4:05:06,  1.67s/it] 10%|█         | 1031/9822 [31:45<4:05:09,  1.67s/it] 11%|█         | 1032/9822 [31:46<4:02:48,  1.66s/it] 11%|█         | 1033/9822 [31:48<4:03:25,  1.66s/it] 11%|█         | 1034/9822 [31:50<4:03:04,  1.66s/it] 11%|█         | 1035/9822 [31:51<4:03:55,  1.67s/it] 11%|█         | 1036/9822 [31:53<4:04:14,  1.67s/it] 11%|█         | 1037/9822 [31:55<4:04:13,  1.67s/it] 11%|█         | 1038/9822 [31:56<4:03:42,  1.66s/it] 11%|█         | 1039/9822 [31:58<4:03:22,  1.66s/it] 11%|█         | 1040/9822 [32:00<4:03:20,  1.66s/it] 11%|█         | 1041/9822 [32:01<4:02:53,  1.66s/it] 11%|█         | 1042/9822 [32:03<4:02:54,  1.66s/it] 11%|█         | 1043/9822 [32:05<4:03:43,  1.67s/it] 11%|█         | 1044/9822 [32:06<4:03:51,  1.67s/it] 11%|█         | 1045/9822 [32:08<4:03:57,  1.67s/it] 11%|█         | 1046/9822 [32:10<4:04:23,  1.67s/it] 11%|█         | 1047/9822 [32:11<4:04:16,  1.67s/it] 11%|█         | 1048/9822 [32:13<4:03:46,  1.67s/it] 11%|█         | 1049/9822 [32:15<4:03:58,  1.67s/it] 11%|█         | 1050/9822 [32:16<4:04:03,  1.67s/it] 11%|█         | 1051/9822 [32:18<4:03:39,  1.67s/it] 11%|█         | 1052/9822 [32:20<4:03:17,  1.66s/it] 11%|█         | 1053/9822 [32:21<4:03:36,  1.67s/it] 11%|█         | 1054/9822 [32:23<4:03:27,  1.67s/it] 11%|█         | 1055/9822 [32:25<4:03:21,  1.67s/it] 11%|█         | 1056/9822 [32:26<4:03:35,  1.67s/it] 11%|█         | 1057/9822 [32:28<4:07:51,  1.70s/it] 11%|█         | 1058/9822 [32:30<4:06:37,  1.69s/it] 11%|█         | 1059/9822 [32:31<4:05:50,  1.68s/it] 11%|█         | 1060/9822 [32:33<4:05:19,  1.68s/it] 11%|█         | 1061/9822 [32:35<4:04:07,  1.67s/it] 11%|█         | 1062/9822 [32:36<4:03:51,  1.67s/it] 11%|█         | 1063/9822 [32:38<4:03:19,  1.67s/it] 11%|█         | 1064/9822 [32:40<4:02:39,  1.66s/it] 11%|█         | 1065/9822 [32:41<4:02:42,  1.66s/it] 11%|█         | 1066/9822 [32:43<4:03:04,  1.67s/it] 11%|█         | 1067/9822 [32:45<4:02:49,  1.66s/it] 11%|█         | 1068/9822 [32:46<4:02:44,  1.66s/it] 11%|█         | 1069/9822 [32:48<4:02:53,  1.66s/it] 11%|█         | 1070/9822 [32:50<4:02:47,  1.66s/it] 11%|█         | 1071/9822 [32:51<4:02:34,  1.66s/it] 11%|█         | 1072/9822 [32:53<4:03:01,  1.67s/it] 11%|█         | 1073/9822 [32:55<4:02:33,  1.66s/it] 11%|█         | 1074/9822 [32:56<4:02:31,  1.66s/it] 11%|█         | 1075/9822 [32:58<4:02:37,  1.66s/it] 11%|█         | 1076/9822 [33:00<4:03:31,  1.67s/it] 11%|█         | 1077/9822 [33:01<4:05:34,  1.68s/it] 11%|█         | 1078/9822 [33:03<4:05:31,  1.68s/it] 11%|█         | 1079/9822 [33:05<4:05:23,  1.68s/it] 11%|█         | 1080/9822 [33:06<4:05:18,  1.68s/it] 11%|█         | 1081/9822 [33:08<4:05:18,  1.68s/it] 11%|█         | 1082/9822 [33:10<4:05:05,  1.68s/it] 11%|█         | 1083/9822 [33:11<4:04:39,  1.68s/it] 11%|█         | 1084/9822 [33:13<4:08:12,  1.70s/it] 11%|█         | 1085/9822 [33:15<4:06:22,  1.69s/it] 11%|█         | 1086/9822 [33:17<4:05:44,  1.69s/it] 11%|█         | 1087/9822 [33:18<4:05:36,  1.69s/it] 11%|█         | 1088/9822 [33:20<4:04:52,  1.68s/it] 11%|█         | 1089/9822 [33:22<4:04:11,  1.68s/it] 11%|█         | 1090/9822 [33:23<4:03:42,  1.67s/it] 11%|█         | 1091/9822 [33:25<4:02:47,  1.67s/it] 11%|█         | 1092/9822 [33:27<4:02:15,  1.66s/it] 11%|█         | 1093/9822 [33:28<4:02:37,  1.67s/it] 11%|█         | 1094/9822 [33:30<4:02:59,  1.67s/it] 11%|█         | 1095/9822 [33:32<4:03:02,  1.67s/it] 11%|█         | 1096/9822 [33:33<4:03:21,  1.67s/it] 11%|█         | 1097/9822 [33:35<4:02:54,  1.67s/it] 11%|█         | 1098/9822 [33:37<4:02:49,  1.67s/it] 11%|█         | 1099/9822 [33:38<4:02:43,  1.67s/it] 11%|█         | 1100/9822 [33:40<4:02:48,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:14:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:14:27 - INFO - __main__ - ***** test Results*****
04/29/2024 12:14:27 - INFO - __main__ -   Training step = 1100
04/29/2024 12:14:27 - INFO - __main__ -  test_accuracy:0.8448023426061494 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:14:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:14:31 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:14:31 - INFO - __main__ -   Training step = 1100
04/29/2024 12:14:31 - INFO - __main__ -  eval_accuracy:0.8352251922372758 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:14:31,631 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:14:31,631 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:14:31,673 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:14:32,822 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8352251922372758}
test:
{'accuracy': 0.8448023426061494}
04/29/2024 12:14:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:14:41 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:14:41 - INFO - __main__ -   Training step = 1100
04/29/2024 12:14:41 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 11%|█         | 1101/9822 [34:00<17:40:57,  7.30s/it] 11%|█         | 1102/9822 [34:02<13:36:06,  5.62s/it] 11%|█         | 1103/9822 [34:04<10:44:45,  4.44s/it] 11%|█         | 1104/9822 [34:05<8:43:53,  3.61s/it]  11%|█▏        | 1105/9822 [34:07<7:19:48,  3.03s/it] 11%|█▏        | 1106/9822 [34:09<6:20:33,  2.62s/it] 11%|█▏        | 1107/9822 [34:10<5:38:50,  2.33s/it] 11%|█▏        | 1108/9822 [34:12<5:09:57,  2.13s/it] 11%|█▏        | 1109/9822 [34:14<4:50:08,  2.00s/it] 11%|█▏        | 1110/9822 [34:15<4:35:45,  1.90s/it] 11%|█▏        | 1111/9822 [34:17<4:25:40,  1.83s/it] 11%|█▏        | 1112/9822 [34:19<4:18:39,  1.78s/it] 11%|█▏        | 1113/9822 [34:20<4:14:06,  1.75s/it] 11%|█▏        | 1114/9822 [34:22<4:10:11,  1.72s/it] 11%|█▏        | 1115/9822 [34:24<4:12:02,  1.74s/it] 11%|█▏        | 1116/9822 [34:26<4:09:13,  1.72s/it] 11%|█▏        | 1117/9822 [34:27<4:07:12,  1.70s/it] 11%|█▏        | 1118/9822 [34:29<4:03:42,  1.68s/it] 11%|█▏        | 1119/9822 [34:31<4:03:30,  1.68s/it] 11%|█▏        | 1120/9822 [34:32<4:03:06,  1.68s/it] 11%|█▏        | 1121/9822 [34:34<4:02:58,  1.68s/it] 11%|█▏        | 1122/9822 [34:36<4:02:45,  1.67s/it] 11%|█▏        | 1123/9822 [34:37<4:02:39,  1.67s/it] 11%|█▏        | 1124/9822 [34:39<4:02:45,  1.67s/it] 11%|█▏        | 1125/9822 [34:41<4:02:36,  1.67s/it] 11%|█▏        | 1126/9822 [34:42<4:02:28,  1.67s/it] 11%|█▏        | 1127/9822 [34:44<4:02:14,  1.67s/it] 11%|█▏        | 1128/9822 [34:46<4:01:59,  1.67s/it] 11%|█▏        | 1129/9822 [34:47<4:02:17,  1.67s/it] 12%|█▏        | 1130/9822 [34:49<4:02:12,  1.67s/it] 12%|█▏        | 1131/9822 [34:51<4:02:30,  1.67s/it] 12%|█▏        | 1132/9822 [34:52<4:02:49,  1.68s/it] 12%|█▏        | 1133/9822 [34:54<4:02:40,  1.68s/it] 12%|█▏        | 1134/9822 [34:56<4:02:25,  1.67s/it] 12%|█▏        | 1135/9822 [34:57<4:02:12,  1.67s/it] 12%|█▏        | 1136/9822 [34:59<4:02:21,  1.67s/it] 12%|█▏        | 1137/9822 [35:01<4:02:00,  1.67s/it] 12%|█▏        | 1138/9822 [35:02<4:01:49,  1.67s/it] 12%|█▏        | 1139/9822 [35:04<4:01:29,  1.67s/it] 12%|█▏        | 1140/9822 [35:06<4:01:58,  1.67s/it] 12%|█▏        | 1141/9822 [35:07<4:01:29,  1.67s/it] 12%|█▏        | 1142/9822 [35:09<4:01:31,  1.67s/it] 12%|█▏        | 1143/9822 [35:11<4:01:19,  1.67s/it] 12%|█▏        | 1144/9822 [35:12<4:00:43,  1.66s/it] 12%|█▏        | 1145/9822 [35:14<4:00:29,  1.66s/it] 12%|█▏        | 1146/9822 [35:16<4:00:58,  1.67s/it] 12%|█▏        | 1147/9822 [35:17<4:00:44,  1.67s/it] 12%|█▏        | 1148/9822 [35:19<4:01:14,  1.67s/it] 12%|█▏        | 1149/9822 [35:21<4:01:29,  1.67s/it] 12%|█▏        | 1150/9822 [35:22<4:01:28,  1.67s/it] 12%|█▏        | 1151/9822 [35:24<4:01:21,  1.67s/it] 12%|█▏        | 1152/9822 [35:26<4:01:33,  1.67s/it] 12%|█▏        | 1153/9822 [35:27<4:01:51,  1.67s/it] 12%|█▏        | 1154/9822 [35:29<4:01:43,  1.67s/it] 12%|█▏        | 1155/9822 [35:31<4:06:01,  1.70s/it] 12%|█▏        | 1156/9822 [35:32<4:04:13,  1.69s/it] 12%|█▏        | 1157/9822 [35:34<4:03:12,  1.68s/it] 12%|█▏        | 1158/9822 [35:36<4:02:23,  1.68s/it] 12%|█▏        | 1159/9822 [35:37<4:01:36,  1.67s/it] 12%|█▏        | 1160/9822 [35:39<4:01:38,  1.67s/it] 12%|█▏        | 1161/9822 [35:41<4:01:00,  1.67s/it] 12%|█▏        | 1162/9822 [35:42<4:01:16,  1.67s/it] 12%|█▏        | 1163/9822 [35:44<4:01:05,  1.67s/it] 12%|█▏        | 1164/9822 [35:46<4:00:28,  1.67s/it] 12%|█▏        | 1165/9822 [35:47<4:00:27,  1.67s/it] 12%|█▏        | 1166/9822 [35:49<4:00:20,  1.67s/it] 12%|█▏        | 1167/9822 [35:51<4:00:11,  1.67s/it] 12%|█▏        | 1168/9822 [35:52<4:00:59,  1.67s/it] 12%|█▏        | 1169/9822 [35:54<4:00:32,  1.67s/it] 12%|█▏        | 1170/9822 [35:56<4:00:17,  1.67s/it] 12%|█▏        | 1171/9822 [35:57<3:59:55,  1.66s/it] 12%|█▏        | 1172/9822 [35:59<3:59:49,  1.66s/it] 12%|█▏        | 1173/9822 [36:01<4:00:08,  1.67s/it] 12%|█▏        | 1174/9822 [36:02<3:59:46,  1.66s/it] 12%|█▏        | 1175/9822 [36:04<3:59:44,  1.66s/it] 12%|█▏        | 1176/9822 [36:06<3:59:52,  1.66s/it] 12%|█▏        | 1177/9822 [36:07<3:59:57,  1.67s/it] 12%|█▏        | 1178/9822 [36:09<4:00:07,  1.67s/it] 12%|█▏        | 1179/9822 [36:11<4:00:56,  1.67s/it] 12%|█▏        | 1180/9822 [36:12<4:00:54,  1.67s/it] 12%|█▏        | 1181/9822 [36:14<4:00:34,  1.67s/it] 12%|█▏        | 1182/9822 [36:16<4:04:38,  1.70s/it] 12%|█▏        | 1183/9822 [36:18<4:03:12,  1.69s/it] 12%|█▏        | 1184/9822 [36:19<4:02:03,  1.68s/it] 12%|█▏        | 1185/9822 [36:21<4:01:21,  1.68s/it] 12%|█▏        | 1186/9822 [36:23<4:00:53,  1.67s/it] 12%|█▏        | 1187/9822 [36:24<4:00:31,  1.67s/it] 12%|█▏        | 1188/9822 [36:26<3:59:59,  1.67s/it] 12%|█▏        | 1189/9822 [36:28<3:59:20,  1.66s/it] 12%|█▏        | 1190/9822 [36:29<3:59:27,  1.66s/it] 12%|█▏        | 1191/9822 [36:31<3:59:11,  1.66s/it] 12%|█▏        | 1192/9822 [36:33<3:59:31,  1.67s/it] 12%|█▏        | 1193/9822 [36:34<3:59:27,  1.67s/it] 12%|█▏        | 1194/9822 [36:36<3:59:16,  1.66s/it] 12%|█▏        | 1195/9822 [36:38<3:59:52,  1.67s/it] 12%|█▏        | 1196/9822 [36:39<4:00:28,  1.67s/it] 12%|█▏        | 1197/9822 [36:41<4:00:16,  1.67s/it] 12%|█▏        | 1198/9822 [36:43<4:00:04,  1.67s/it] 12%|█▏        | 1199/9822 [36:44<3:59:38,  1.67s/it] 12%|█▏        | 1200/9822 [36:46<3:59:49,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:17:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:17:33 - INFO - __main__ - ***** test Results*****
04/29/2024 12:17:33 - INFO - __main__ -   Training step = 1200
04/29/2024 12:17:33 - INFO - __main__ -  test_accuracy:0.8495607613469985 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:17:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:17:37 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:17:37 - INFO - __main__ -   Training step = 1200
04/29/2024 12:17:37 - INFO - __main__ -  eval_accuracy:0.832295862321494 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8352251922372758}
test:
{'accuracy': 0.8448023426061494}
04/29/2024 12:17:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:17:46 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:17:46 - INFO - __main__ -   Training step = 1200
04/29/2024 12:17:46 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 12%|█▏        | 1201/9822 [37:05<16:36:28,  6.94s/it] 12%|█▏        | 1202/9822 [37:07<12:48:44,  5.35s/it] 12%|█▏        | 1203/9822 [37:08<10:10:00,  4.25s/it] 12%|█▏        | 1204/9822 [37:10<8:16:19,  3.46s/it]  12%|█▏        | 1205/9822 [37:12<6:59:00,  2.92s/it] 12%|█▏        | 1206/9822 [37:13<6:04:59,  2.54s/it] 12%|█▏        | 1207/9822 [37:15<5:27:38,  2.28s/it] 12%|█▏        | 1208/9822 [37:17<5:05:53,  2.13s/it] 12%|█▏        | 1209/9822 [37:18<4:45:36,  1.99s/it] 12%|█▏        | 1210/9822 [37:20<4:31:20,  1.89s/it] 12%|█▏        | 1211/9822 [37:22<4:22:21,  1.83s/it] 12%|█▏        | 1212/9822 [37:23<4:15:55,  1.78s/it] 12%|█▏        | 1213/9822 [37:25<4:11:36,  1.75s/it] 12%|█▏        | 1214/9822 [37:27<4:07:50,  1.73s/it] 12%|█▏        | 1215/9822 [37:29<4:05:21,  1.71s/it] 12%|█▏        | 1216/9822 [37:30<4:04:06,  1.70s/it] 12%|█▏        | 1217/9822 [37:32<4:02:10,  1.69s/it] 12%|█▏        | 1218/9822 [37:34<4:01:24,  1.68s/it] 12%|█▏        | 1219/9822 [37:35<4:00:31,  1.68s/it] 12%|█▏        | 1220/9822 [37:37<4:00:27,  1.68s/it] 12%|█▏        | 1221/9822 [37:39<4:00:19,  1.68s/it] 12%|█▏        | 1222/9822 [37:40<4:00:20,  1.68s/it] 12%|█▏        | 1223/9822 [37:42<3:59:23,  1.67s/it] 12%|█▏        | 1224/9822 [37:44<3:58:34,  1.66s/it] 12%|█▏        | 1225/9822 [37:45<3:58:06,  1.66s/it] 12%|█▏        | 1226/9822 [37:47<3:57:58,  1.66s/it] 12%|█▏        | 1227/9822 [37:48<3:57:41,  1.66s/it] 13%|█▎        | 1228/9822 [37:50<3:58:07,  1.66s/it] 13%|█▎        | 1229/9822 [37:52<3:57:58,  1.66s/it] 13%|█▎        | 1230/9822 [37:53<3:57:33,  1.66s/it] 13%|█▎        | 1231/9822 [37:55<3:57:32,  1.66s/it] 13%|█▎        | 1232/9822 [37:57<3:57:23,  1.66s/it] 13%|█▎        | 1233/9822 [37:58<3:58:02,  1.66s/it] 13%|█▎        | 1234/9822 [38:00<3:57:50,  1.66s/it] 13%|█▎        | 1235/9822 [38:02<4:01:58,  1.69s/it] 13%|█▎        | 1236/9822 [38:04<4:00:21,  1.68s/it] 13%|█▎        | 1237/9822 [38:05<3:59:49,  1.68s/it] 13%|█▎        | 1238/9822 [38:07<4:00:03,  1.68s/it] 13%|█▎        | 1239/9822 [38:09<3:59:55,  1.68s/it] 13%|█▎        | 1240/9822 [38:10<3:59:36,  1.68s/it] 13%|█▎        | 1241/9822 [38:12<3:58:51,  1.67s/it] 13%|█▎        | 1242/9822 [38:14<3:59:01,  1.67s/it] 13%|█▎        | 1243/9822 [38:15<3:58:30,  1.67s/it] 13%|█▎        | 1244/9822 [38:17<3:58:02,  1.67s/it] 13%|█▎        | 1245/9822 [38:19<3:58:03,  1.67s/it] 13%|█▎        | 1246/9822 [38:20<3:58:57,  1.67s/it] 13%|█▎        | 1247/9822 [38:22<3:58:59,  1.67s/it] 13%|█▎        | 1248/9822 [38:24<3:58:56,  1.67s/it] 13%|█▎        | 1249/9822 [38:25<3:58:51,  1.67s/it] 13%|█▎        | 1250/9822 [38:27<3:58:11,  1.67s/it] 13%|█▎        | 1251/9822 [38:29<3:57:49,  1.66s/it] 13%|█▎        | 1252/9822 [38:30<3:57:47,  1.66s/it] 13%|█▎        | 1253/9822 [38:32<3:57:15,  1.66s/it] 13%|█▎        | 1254/9822 [38:34<3:56:59,  1.66s/it] 13%|█▎        | 1255/9822 [38:35<3:56:50,  1.66s/it] 13%|█▎        | 1256/9822 [38:37<3:56:36,  1.66s/it] 13%|█▎        | 1257/9822 [38:39<3:56:52,  1.66s/it] 13%|█▎        | 1258/9822 [38:40<3:56:37,  1.66s/it] 13%|█▎        | 1259/9822 [38:42<3:56:55,  1.66s/it] 13%|█▎        | 1260/9822 [38:43<3:57:01,  1.66s/it] 13%|█▎        | 1261/9822 [38:45<3:57:06,  1.66s/it] 13%|█▎        | 1262/9822 [38:47<3:56:57,  1.66s/it] 13%|█▎        | 1263/9822 [38:48<3:56:55,  1.66s/it] 13%|█▎        | 1264/9822 [38:50<3:56:39,  1.66s/it] 13%|█▎        | 1265/9822 [38:52<3:56:30,  1.66s/it] 13%|█▎        | 1266/9822 [38:53<3:56:37,  1.66s/it] 13%|█▎        | 1267/9822 [38:55<3:56:59,  1.66s/it] 13%|█▎        | 1268/9822 [38:57<4:01:14,  1.69s/it] 13%|█▎        | 1269/9822 [38:59<4:00:00,  1.68s/it] 13%|█▎        | 1270/9822 [39:00<3:59:47,  1.68s/it] 13%|█▎        | 1271/9822 [39:02<3:58:48,  1.68s/it] 13%|█▎        | 1272/9822 [39:04<3:58:21,  1.67s/it] 13%|█▎        | 1273/9822 [39:05<3:57:43,  1.67s/it] 13%|█▎        | 1274/9822 [39:07<3:57:14,  1.67s/it] 13%|█▎        | 1275/9822 [39:09<3:57:06,  1.66s/it] 13%|█▎        | 1276/9822 [39:10<3:57:07,  1.66s/it] 13%|█▎        | 1277/9822 [39:12<3:56:58,  1.66s/it] 13%|█▎        | 1278/9822 [39:14<3:57:03,  1.66s/it] 13%|█▎        | 1279/9822 [39:15<3:57:11,  1.67s/it] 13%|█▎        | 1280/9822 [39:17<3:56:32,  1.66s/it] 13%|█▎        | 1281/9822 [39:19<3:56:53,  1.66s/it] 13%|█▎        | 1282/9822 [39:20<3:57:00,  1.67s/it] 13%|█▎        | 1283/9822 [39:22<3:56:52,  1.66s/it] 13%|█▎        | 1284/9822 [39:24<3:56:32,  1.66s/it] 13%|█▎        | 1285/9822 [39:25<3:56:26,  1.66s/it] 13%|█▎        | 1286/9822 [39:27<3:56:59,  1.67s/it] 13%|█▎        | 1287/9822 [39:28<3:56:35,  1.66s/it] 13%|█▎        | 1288/9822 [39:30<3:56:47,  1.66s/it] 13%|█▎        | 1289/9822 [39:32<3:57:27,  1.67s/it] 13%|█▎        | 1290/9822 [39:34<4:00:09,  1.69s/it] 13%|█▎        | 1291/9822 [39:35<3:59:28,  1.68s/it] 13%|█▎        | 1292/9822 [39:37<3:58:09,  1.68s/it] 13%|█▎        | 1293/9822 [39:39<3:57:19,  1.67s/it] 13%|█▎        | 1294/9822 [39:40<3:57:10,  1.67s/it] 13%|█▎        | 1295/9822 [39:42<3:57:19,  1.67s/it] 13%|█▎        | 1296/9822 [39:44<3:58:01,  1.68s/it] 13%|█▎        | 1297/9822 [39:45<3:57:36,  1.67s/it] 13%|█▎        | 1298/9822 [39:47<3:57:12,  1.67s/it] 13%|█▎        | 1299/9822 [39:49<3:56:27,  1.66s/it] 13%|█▎        | 1300/9822 [39:50<3:56:31,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:20:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:20:37 - INFO - __main__ - ***** test Results*****
04/29/2024 12:20:37 - INFO - __main__ -   Training step = 1300
04/29/2024 12:20:37 - INFO - __main__ -  test_accuracy:0.8521229868228404 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:20:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:20:41 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:20:41 - INFO - __main__ -   Training step = 1300
04/29/2024 12:20:41 - INFO - __main__ -  eval_accuracy:0.8381545221530575 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:20:41,951 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:20:41,951 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:20:41,992 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:20:43,120 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8381545221530575}
test:
{'accuracy': 0.8521229868228404}
04/29/2024 12:20:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:20:51 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:20:51 - INFO - __main__ -   Training step = 1300
04/29/2024 12:20:51 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 13%|█▎        | 1301/9822 [40:11<17:13:21,  7.28s/it] 13%|█▎        | 1302/9822 [40:12<13:14:24,  5.59s/it] 13%|█▎        | 1303/9822 [40:14<10:27:17,  4.42s/it] 13%|█▎        | 1304/9822 [40:16<8:30:01,  3.59s/it]  13%|█▎        | 1305/9822 [40:17<7:07:56,  3.01s/it] 13%|█▎        | 1306/9822 [40:19<6:10:35,  2.61s/it] 13%|█▎        | 1307/9822 [40:21<5:30:00,  2.33s/it] 13%|█▎        | 1308/9822 [40:22<5:02:13,  2.13s/it] 13%|█▎        | 1309/9822 [40:24<4:42:52,  1.99s/it] 13%|█▎        | 1310/9822 [40:26<4:28:47,  1.89s/it] 13%|█▎        | 1311/9822 [40:27<4:19:09,  1.83s/it] 13%|█▎        | 1312/9822 [40:29<4:12:12,  1.78s/it] 13%|█▎        | 1313/9822 [40:31<4:07:48,  1.75s/it] 13%|█▎        | 1314/9822 [40:32<4:04:33,  1.72s/it] 13%|█▎        | 1315/9822 [40:34<4:01:39,  1.70s/it] 13%|█▎        | 1316/9822 [40:36<4:00:32,  1.70s/it] 13%|█▎        | 1317/9822 [40:37<3:59:43,  1.69s/it] 13%|█▎        | 1318/9822 [40:39<3:59:07,  1.69s/it] 13%|█▎        | 1319/9822 [40:41<3:58:30,  1.68s/it] 13%|█▎        | 1320/9822 [40:42<3:57:46,  1.68s/it] 13%|█▎        | 1321/9822 [40:44<3:56:59,  1.67s/it] 13%|█▎        | 1322/9822 [40:46<3:57:05,  1.67s/it] 13%|█▎        | 1323/9822 [40:47<4:01:02,  1.70s/it] 13%|█▎        | 1324/9822 [40:49<3:59:36,  1.69s/it] 13%|█▎        | 1325/9822 [40:51<3:57:54,  1.68s/it] 14%|█▎        | 1326/9822 [40:52<3:57:24,  1.68s/it] 14%|█▎        | 1327/9822 [40:54<3:56:38,  1.67s/it] 14%|█▎        | 1328/9822 [40:56<3:56:38,  1.67s/it] 14%|█▎        | 1329/9822 [40:57<3:56:00,  1.67s/it] 14%|█▎        | 1330/9822 [40:59<3:56:12,  1.67s/it] 14%|█▎        | 1331/9822 [41:01<3:55:53,  1.67s/it] 14%|█▎        | 1332/9822 [41:02<3:55:24,  1.66s/it] 14%|█▎        | 1333/9822 [41:04<3:55:11,  1.66s/it] 14%|█▎        | 1334/9822 [41:06<3:55:34,  1.67s/it] 14%|█▎        | 1335/9822 [41:07<3:54:58,  1.66s/it] 14%|█▎        | 1336/9822 [41:09<3:54:55,  1.66s/it] 14%|█▎        | 1337/9822 [41:11<3:54:46,  1.66s/it] 14%|█▎        | 1338/9822 [41:12<3:54:39,  1.66s/it] 14%|█▎        | 1339/9822 [41:14<3:54:40,  1.66s/it] 14%|█▎        | 1340/9822 [41:16<3:54:47,  1.66s/it] 14%|█▎        | 1341/9822 [41:17<3:55:10,  1.66s/it] 14%|█▎        | 1342/9822 [41:19<3:55:04,  1.66s/it] 14%|█▎        | 1343/9822 [41:21<3:55:23,  1.67s/it] 14%|█▎        | 1344/9822 [41:22<3:54:58,  1.66s/it] 14%|█▎        | 1345/9822 [41:24<3:58:49,  1.69s/it] 14%|█▎        | 1346/9822 [41:26<3:57:28,  1.68s/it] 14%|█▎        | 1347/9822 [41:27<3:56:18,  1.67s/it] 14%|█▎        | 1348/9822 [41:29<3:55:21,  1.67s/it] 14%|█▎        | 1349/9822 [41:31<3:55:18,  1.67s/it] 14%|█▎        | 1350/9822 [41:32<3:54:53,  1.66s/it] 14%|█▍        | 1351/9822 [41:34<3:54:59,  1.66s/it] 14%|█▍        | 1352/9822 [41:36<3:54:44,  1.66s/it] 14%|█▍        | 1353/9822 [41:37<3:54:31,  1.66s/it] 14%|█▍        | 1354/9822 [41:39<3:54:09,  1.66s/it] 14%|█▍        | 1355/9822 [41:41<3:54:39,  1.66s/it] 14%|█▍        | 1356/9822 [41:42<3:54:58,  1.67s/it] 14%|█▍        | 1357/9822 [41:44<3:55:37,  1.67s/it] 14%|█▍        | 1358/9822 [41:46<3:56:08,  1.67s/it] 14%|█▍        | 1359/9822 [41:47<3:56:27,  1.68s/it] 14%|█▍        | 1360/9822 [41:49<3:56:42,  1.68s/it] 14%|█▍        | 1361/9822 [41:51<3:56:53,  1.68s/it] 14%|█▍        | 1362/9822 [41:52<3:57:12,  1.68s/it] 14%|█▍        | 1363/9822 [41:54<3:57:08,  1.68s/it] 14%|█▍        | 1364/9822 [41:56<3:57:05,  1.68s/it] 14%|█▍        | 1365/9822 [41:58<3:56:29,  1.68s/it] 14%|█▍        | 1366/9822 [41:59<3:56:32,  1.68s/it] 14%|█▍        | 1367/9822 [42:01<3:56:30,  1.68s/it] 14%|█▍        | 1368/9822 [42:03<3:56:09,  1.68s/it] 14%|█▍        | 1369/9822 [42:04<3:56:17,  1.68s/it] 14%|█▍        | 1370/9822 [42:06<3:55:58,  1.68s/it] 14%|█▍        | 1371/9822 [42:08<3:56:09,  1.68s/it] 14%|█▍        | 1372/9822 [42:09<4:00:31,  1.71s/it] 14%|█▍        | 1373/9822 [42:11<3:58:16,  1.69s/it] 14%|█▍        | 1374/9822 [42:13<3:57:02,  1.68s/it] 14%|█▍        | 1375/9822 [42:14<3:56:25,  1.68s/it] 14%|█▍        | 1376/9822 [42:16<3:54:09,  1.66s/it] 14%|█▍        | 1377/9822 [42:18<3:54:20,  1.66s/it] 14%|█▍        | 1378/9822 [42:19<3:54:51,  1.67s/it] 14%|█▍        | 1379/9822 [42:21<3:54:48,  1.67s/it] 14%|█▍        | 1380/9822 [42:23<3:54:37,  1.67s/it] 14%|█▍        | 1381/9822 [42:24<3:55:14,  1.67s/it] 14%|█▍        | 1382/9822 [42:26<3:55:09,  1.67s/it] 14%|█▍        | 1383/9822 [42:28<3:55:40,  1.68s/it] 14%|█▍        | 1384/9822 [42:29<3:55:32,  1.67s/it] 14%|█▍        | 1385/9822 [42:31<3:56:45,  1.68s/it] 14%|█▍        | 1386/9822 [42:33<3:57:32,  1.69s/it] 14%|█▍        | 1387/9822 [42:34<3:56:42,  1.68s/it] 14%|█▍        | 1388/9822 [42:36<3:56:01,  1.68s/it] 14%|█▍        | 1389/9822 [42:38<3:55:20,  1.67s/it] 14%|█▍        | 1390/9822 [42:39<3:55:17,  1.67s/it] 14%|█▍        | 1391/9822 [42:41<3:54:53,  1.67s/it] 14%|█▍        | 1392/9822 [42:43<3:54:35,  1.67s/it] 14%|█▍        | 1393/9822 [42:44<3:54:09,  1.67s/it] 14%|█▍        | 1394/9822 [42:46<3:54:07,  1.67s/it] 14%|█▍        | 1395/9822 [42:48<3:53:50,  1.66s/it] 14%|█▍        | 1396/9822 [42:49<3:54:23,  1.67s/it] 14%|█▍        | 1397/9822 [42:51<3:54:50,  1.67s/it] 14%|█▍        | 1398/9822 [42:53<3:54:36,  1.67s/it] 14%|█▍        | 1399/9822 [42:54<3:54:53,  1.67s/it] 14%|█▍        | 1400/9822 [42:56<3:54:16,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:23:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:23:43 - INFO - __main__ - ***** test Results*****
04/29/2024 12:23:43 - INFO - __main__ -   Training step = 1400
04/29/2024 12:23:43 - INFO - __main__ -  test_accuracy:0.8539531478770132 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:23:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:23:47 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:23:47 - INFO - __main__ -   Training step = 1400
04/29/2024 12:23:47 - INFO - __main__ -  eval_accuracy:0.8355913584767485 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8381545221530575}
test:
{'accuracy': 0.8521229868228404}
04/29/2024 12:23:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:23:56 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:23:56 - INFO - __main__ -   Training step = 1400
04/29/2024 12:23:56 - INFO - __main__ -  eval_accuracy:0.9058952764555108 
 14%|█▍        | 1401/9822 [43:15<16:12:11,  6.93s/it] 14%|█▍        | 1402/9822 [43:17<12:30:15,  5.35s/it] 14%|█▍        | 1403/9822 [43:19<9:59:07,  4.27s/it]  14%|█▍        | 1404/9822 [43:20<8:09:26,  3.49s/it] 14%|█▍        | 1405/9822 [43:22<6:53:16,  2.95s/it] 14%|█▍        | 1406/9822 [43:24<5:59:55,  2.57s/it] 14%|█▍        | 1407/9822 [43:25<5:22:36,  2.30s/it] 14%|█▍        | 1408/9822 [43:27<4:56:22,  2.11s/it] 14%|█▍        | 1409/9822 [43:29<4:38:06,  1.98s/it] 14%|█▍        | 1410/9822 [43:30<4:25:02,  1.89s/it] 14%|█▍        | 1411/9822 [43:32<4:16:15,  1.83s/it] 14%|█▍        | 1412/9822 [43:34<4:10:08,  1.78s/it] 14%|█▍        | 1413/9822 [43:36<4:05:44,  1.75s/it] 14%|█▍        | 1414/9822 [43:37<4:02:56,  1.73s/it] 14%|█▍        | 1415/9822 [43:39<4:00:47,  1.72s/it] 14%|█▍        | 1416/9822 [43:41<3:59:28,  1.71s/it] 14%|█▍        | 1417/9822 [43:42<3:58:35,  1.70s/it] 14%|█▍        | 1418/9822 [43:44<3:57:46,  1.70s/it] 14%|█▍        | 1419/9822 [43:46<3:57:02,  1.69s/it] 14%|█▍        | 1420/9822 [43:47<3:56:36,  1.69s/it] 14%|█▍        | 1421/9822 [43:49<3:56:13,  1.69s/it] 14%|█▍        | 1422/9822 [43:51<3:56:10,  1.69s/it] 14%|█▍        | 1423/9822 [43:52<3:55:47,  1.68s/it] 14%|█▍        | 1424/9822 [43:54<3:55:00,  1.68s/it] 15%|█▍        | 1425/9822 [43:56<3:54:29,  1.68s/it] 15%|█▍        | 1426/9822 [43:57<3:53:55,  1.67s/it] 15%|█▍        | 1427/9822 [43:59<3:53:39,  1.67s/it] 15%|█▍        | 1428/9822 [44:01<3:53:59,  1.67s/it] 15%|█▍        | 1429/9822 [44:02<3:54:06,  1.67s/it] 15%|█▍        | 1430/9822 [44:04<3:53:39,  1.67s/it] 15%|█▍        | 1431/9822 [44:06<3:53:51,  1.67s/it] 15%|█▍        | 1432/9822 [44:07<3:53:32,  1.67s/it] 15%|█▍        | 1433/9822 [44:09<3:53:27,  1.67s/it] 15%|█▍        | 1434/9822 [44:11<3:53:38,  1.67s/it] 15%|█▍        | 1435/9822 [44:12<3:53:08,  1.67s/it] 15%|█▍        | 1436/9822 [44:14<3:53:08,  1.67s/it] 15%|█▍        | 1437/9822 [44:16<3:53:58,  1.67s/it] 15%|█▍        | 1438/9822 [44:17<3:54:43,  1.68s/it] 15%|█▍        | 1439/9822 [44:19<3:54:20,  1.68s/it] 15%|█▍        | 1440/9822 [44:21<3:53:40,  1.67s/it] 15%|█▍        | 1441/9822 [44:22<3:53:07,  1.67s/it] 15%|█▍        | 1442/9822 [44:24<3:53:26,  1.67s/it] 15%|█▍        | 1443/9822 [44:26<3:53:03,  1.67s/it] 15%|█▍        | 1444/9822 [44:28<3:56:57,  1.70s/it] 15%|█▍        | 1445/9822 [44:29<3:55:46,  1.69s/it] 15%|█▍        | 1446/9822 [44:31<3:54:25,  1.68s/it] 15%|█▍        | 1447/9822 [44:33<3:53:29,  1.67s/it] 15%|█▍        | 1448/9822 [44:34<3:52:57,  1.67s/it] 15%|█▍        | 1449/9822 [44:36<3:52:52,  1.67s/it] 15%|█▍        | 1450/9822 [44:38<3:52:24,  1.67s/it] 15%|█▍        | 1451/9822 [44:39<3:52:13,  1.66s/it] 15%|█▍        | 1452/9822 [44:41<3:52:03,  1.66s/it] 15%|█▍        | 1453/9822 [44:42<3:51:39,  1.66s/it] 15%|█▍        | 1454/9822 [44:44<3:51:34,  1.66s/it] 15%|█▍        | 1455/9822 [44:46<3:51:36,  1.66s/it] 15%|█▍        | 1456/9822 [44:47<3:51:18,  1.66s/it] 15%|█▍        | 1457/9822 [44:49<3:51:50,  1.66s/it] 15%|█▍        | 1458/9822 [44:51<3:51:45,  1.66s/it] 15%|█▍        | 1459/9822 [44:52<3:52:26,  1.67s/it] 15%|█▍        | 1460/9822 [44:54<3:52:44,  1.67s/it] 15%|█▍        | 1461/9822 [44:56<3:52:32,  1.67s/it] 15%|█▍        | 1462/9822 [44:57<3:50:38,  1.66s/it] 15%|█▍        | 1463/9822 [44:59<3:51:15,  1.66s/it] 15%|█▍        | 1464/9822 [45:01<3:51:42,  1.66s/it] 15%|█▍        | 1465/9822 [45:02<3:52:20,  1.67s/it] 15%|█▍        | 1466/9822 [45:04<3:52:31,  1.67s/it] 15%|█▍        | 1467/9822 [45:06<3:52:17,  1.67s/it] 15%|█▍        | 1468/9822 [45:07<3:52:36,  1.67s/it] 15%|█▍        | 1469/9822 [45:09<3:52:14,  1.67s/it] 15%|█▍        | 1470/9822 [45:11<3:52:26,  1.67s/it] 15%|█▍        | 1471/9822 [45:13<3:56:27,  1.70s/it] 15%|█▍        | 1472/9822 [45:14<3:54:44,  1.69s/it] 15%|█▍        | 1473/9822 [45:16<3:53:42,  1.68s/it] 15%|█▌        | 1474/9822 [45:18<3:52:56,  1.67s/it] 15%|█▌        | 1475/9822 [45:19<3:52:19,  1.67s/it] 15%|█▌        | 1476/9822 [45:21<3:52:05,  1.67s/it] 15%|█▌        | 1477/9822 [45:23<3:51:27,  1.66s/it] 15%|█▌        | 1478/9822 [45:24<3:51:30,  1.66s/it] 15%|█▌        | 1479/9822 [45:26<3:51:27,  1.66s/it] 15%|█▌        | 1480/9822 [45:28<3:51:13,  1.66s/it] 15%|█▌        | 1481/9822 [45:29<3:50:55,  1.66s/it] 15%|█▌        | 1482/9822 [45:31<3:50:52,  1.66s/it] 15%|█▌        | 1483/9822 [45:33<3:51:04,  1.66s/it] 15%|█▌        | 1484/9822 [45:34<3:50:59,  1.66s/it] 15%|█▌        | 1485/9822 [45:36<3:50:44,  1.66s/it] 15%|█▌        | 1486/9822 [45:37<3:51:16,  1.66s/it] 15%|█▌        | 1487/9822 [45:39<3:51:19,  1.67s/it] 15%|█▌        | 1488/9822 [45:41<3:51:48,  1.67s/it] 15%|█▌        | 1489/9822 [45:43<3:52:24,  1.67s/it] 15%|█▌        | 1490/9822 [45:44<3:52:37,  1.68s/it] 15%|█▌        | 1491/9822 [45:46<3:52:38,  1.68s/it] 15%|█▌        | 1492/9822 [45:48<3:52:39,  1.68s/it] 15%|█▌        | 1493/9822 [45:49<3:52:44,  1.68s/it] 15%|█▌        | 1494/9822 [45:51<3:52:35,  1.68s/it] 15%|█▌        | 1495/9822 [45:53<3:52:25,  1.67s/it] 15%|█▌        | 1496/9822 [45:54<3:51:57,  1.67s/it] 15%|█▌        | 1497/9822 [45:56<3:51:33,  1.67s/it] 15%|█▌        | 1498/9822 [45:58<3:55:50,  1.70s/it] 15%|█▌        | 1499/9822 [45:59<3:53:57,  1.69s/it] 15%|█▌        | 1500/9822 [46:01<3:52:49,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:26:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:26:48 - INFO - __main__ - ***** test Results*****
04/29/2024 12:26:48 - INFO - __main__ -   Training step = 1500
04/29/2024 12:26:48 - INFO - __main__ -  test_accuracy:0.8554172767203514 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:26:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:26:52 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:26:52 - INFO - __main__ -   Training step = 1500
04/29/2024 12:26:52 - INFO - __main__ -  eval_accuracy:0.8410838520688393 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:26:52,711 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:26:52,711 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:26:52,752 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:26:56,615 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8554172767203514}
04/29/2024 12:27:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:27:05 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:27:05 - INFO - __main__ -   Training step = 1500
04/29/2024 12:27:05 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 15%|█▌        | 1501/9822 [46:24<18:54:59,  8.18s/it] 15%|█▌        | 1502/9822 [46:26<14:23:25,  6.23s/it] 15%|█▌        | 1503/9822 [46:28<11:13:44,  4.86s/it] 15%|█▌        | 1504/9822 [46:29<9:01:14,  3.90s/it]  15%|█▌        | 1505/9822 [46:31<7:27:40,  3.23s/it] 15%|█▌        | 1506/9822 [46:33<6:22:34,  2.76s/it] 15%|█▌        | 1507/9822 [46:34<5:37:07,  2.43s/it] 15%|█▌        | 1508/9822 [46:36<5:04:27,  2.20s/it] 15%|█▌        | 1509/9822 [46:38<4:42:36,  2.04s/it] 15%|█▌        | 1510/9822 [46:39<4:26:34,  1.92s/it] 15%|█▌        | 1511/9822 [46:41<4:15:10,  1.84s/it] 15%|█▌        | 1512/9822 [46:43<4:07:44,  1.79s/it] 15%|█▌        | 1513/9822 [46:44<4:02:33,  1.75s/it] 15%|█▌        | 1514/9822 [46:46<3:59:06,  1.73s/it] 15%|█▌        | 1515/9822 [46:48<3:56:12,  1.71s/it] 15%|█▌        | 1516/9822 [46:49<3:54:10,  1.69s/it] 15%|█▌        | 1517/9822 [46:51<3:53:22,  1.69s/it] 15%|█▌        | 1518/9822 [46:53<3:53:05,  1.68s/it] 15%|█▌        | 1519/9822 [46:54<3:53:03,  1.68s/it] 15%|█▌        | 1520/9822 [46:56<3:52:19,  1.68s/it] 15%|█▌        | 1521/9822 [46:58<3:51:33,  1.67s/it] 15%|█▌        | 1522/9822 [46:59<3:51:10,  1.67s/it] 16%|█▌        | 1523/9822 [47:01<3:51:16,  1.67s/it] 16%|█▌        | 1524/9822 [47:03<3:50:54,  1.67s/it] 16%|█▌        | 1525/9822 [47:04<3:50:48,  1.67s/it] 16%|█▌        | 1526/9822 [47:06<3:50:21,  1.67s/it] 16%|█▌        | 1527/9822 [47:08<3:50:51,  1.67s/it] 16%|█▌        | 1528/9822 [47:09<3:54:58,  1.70s/it] 16%|█▌        | 1529/9822 [47:11<3:56:29,  1.71s/it] 16%|█▌        | 1530/9822 [47:13<3:54:53,  1.70s/it] 16%|█▌        | 1531/9822 [47:15<3:53:24,  1.69s/it] 16%|█▌        | 1532/9822 [47:16<3:52:54,  1.69s/it] 16%|█▌        | 1533/9822 [47:18<3:53:02,  1.69s/it] 16%|█▌        | 1534/9822 [47:20<3:51:59,  1.68s/it] 16%|█▌        | 1535/9822 [47:21<3:51:49,  1.68s/it] 16%|█▌        | 1536/9822 [47:23<3:50:45,  1.67s/it] 16%|█▌        | 1537/9822 [47:25<3:49:58,  1.67s/it] 16%|█▌        | 1538/9822 [47:26<3:49:35,  1.66s/it] 16%|█▌        | 1539/9822 [47:28<3:53:34,  1.69s/it] 16%|█▌        | 1540/9822 [47:30<3:52:32,  1.68s/it] 16%|█▌        | 1541/9822 [47:31<3:51:55,  1.68s/it] 16%|█▌        | 1542/9822 [47:33<3:51:24,  1.68s/it] 16%|█▌        | 1543/9822 [47:35<3:50:53,  1.67s/it] 16%|█▌        | 1544/9822 [47:36<3:50:44,  1.67s/it] 16%|█▌        | 1545/9822 [47:38<3:50:17,  1.67s/it] 16%|█▌        | 1546/9822 [47:40<3:49:50,  1.67s/it] 16%|█▌        | 1547/9822 [47:41<3:50:06,  1.67s/it] 16%|█▌        | 1548/9822 [47:43<3:48:09,  1.65s/it] 16%|█▌        | 1549/9822 [47:45<3:48:29,  1.66s/it] 16%|█▌        | 1550/9822 [47:46<3:49:17,  1.66s/it] 16%|█▌        | 1551/9822 [47:48<3:50:02,  1.67s/it] 16%|█▌        | 1552/9822 [47:50<3:50:08,  1.67s/it] 16%|█▌        | 1553/9822 [47:51<3:50:24,  1.67s/it] 16%|█▌        | 1554/9822 [47:53<3:49:51,  1.67s/it] 16%|█▌        | 1555/9822 [47:55<3:49:33,  1.67s/it] 16%|█▌        | 1556/9822 [47:56<3:49:43,  1.67s/it] 16%|█▌        | 1557/9822 [47:58<3:49:58,  1.67s/it] 16%|█▌        | 1558/9822 [48:00<3:49:42,  1.67s/it] 16%|█▌        | 1559/9822 [48:01<3:49:48,  1.67s/it] 16%|█▌        | 1560/9822 [48:03<3:49:50,  1.67s/it] 16%|█▌        | 1561/9822 [48:05<3:49:56,  1.67s/it] 16%|█▌        | 1562/9822 [48:06<3:49:17,  1.67s/it] 16%|█▌        | 1563/9822 [48:08<3:48:52,  1.66s/it] 16%|█▌        | 1564/9822 [48:10<3:48:57,  1.66s/it] 16%|█▌        | 1565/9822 [48:11<3:49:02,  1.66s/it] 16%|█▌        | 1566/9822 [48:13<3:53:52,  1.70s/it] 16%|█▌        | 1567/9822 [48:15<3:52:52,  1.69s/it] 16%|█▌        | 1568/9822 [48:16<3:51:26,  1.68s/it] 16%|█▌        | 1569/9822 [48:18<3:50:34,  1.68s/it] 16%|█▌        | 1570/9822 [48:20<3:50:10,  1.67s/it] 16%|█▌        | 1571/9822 [48:21<3:49:48,  1.67s/it] 16%|█▌        | 1572/9822 [48:23<3:49:27,  1.67s/it] 16%|█▌        | 1573/9822 [48:25<3:48:47,  1.66s/it] 16%|█▌        | 1574/9822 [48:26<3:48:36,  1.66s/it] 16%|█▌        | 1575/9822 [48:28<3:48:11,  1.66s/it] 16%|█▌        | 1576/9822 [48:30<3:48:12,  1.66s/it] 16%|█▌        | 1577/9822 [48:31<3:47:52,  1.66s/it] 16%|█▌        | 1578/9822 [48:33<3:48:21,  1.66s/it] 16%|█▌        | 1579/9822 [48:35<3:48:29,  1.66s/it] 16%|█▌        | 1580/9822 [48:36<3:48:23,  1.66s/it] 16%|█▌        | 1581/9822 [48:38<3:48:33,  1.66s/it] 16%|█▌        | 1582/9822 [48:40<3:49:04,  1.67s/it] 16%|█▌        | 1583/9822 [48:41<3:49:19,  1.67s/it] 16%|█▌        | 1584/9822 [48:43<3:49:14,  1.67s/it] 16%|█▌        | 1585/9822 [48:45<3:49:00,  1.67s/it] 16%|█▌        | 1586/9822 [48:46<3:49:01,  1.67s/it] 16%|█▌        | 1587/9822 [48:48<3:48:43,  1.67s/it] 16%|█▌        | 1588/9822 [48:50<3:48:41,  1.67s/it] 16%|█▌        | 1589/9822 [48:51<3:49:16,  1.67s/it] 16%|█▌        | 1590/9822 [48:53<3:49:18,  1.67s/it] 16%|█▌        | 1591/9822 [48:55<3:49:15,  1.67s/it] 16%|█▌        | 1592/9822 [48:56<3:49:37,  1.67s/it] 16%|█▌        | 1593/9822 [48:58<3:53:48,  1.70s/it] 16%|█▌        | 1594/9822 [49:00<3:52:03,  1.69s/it] 16%|█▌        | 1595/9822 [49:01<3:50:34,  1.68s/it] 16%|█▌        | 1596/9822 [49:03<3:49:30,  1.67s/it] 16%|█▋        | 1597/9822 [49:05<3:49:15,  1.67s/it] 16%|█▋        | 1598/9822 [49:06<3:48:58,  1.67s/it] 16%|█▋        | 1599/9822 [49:08<3:48:32,  1.67s/it] 16%|█▋        | 1600/9822 [49:10<3:48:10,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:29:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:29:56 - INFO - __main__ - ***** test Results*****
04/29/2024 12:29:56 - INFO - __main__ -   Training step = 1600
04/29/2024 12:29:56 - INFO - __main__ -  test_accuracy:0.8506588579795022 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:30:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:30:01 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:30:01 - INFO - __main__ -   Training step = 1600
04/29/2024 12:30:01 - INFO - __main__ -  eval_accuracy:0.8344928597583303 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8554172767203514}
04/29/2024 12:30:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:30:10 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:30:10 - INFO - __main__ -   Training step = 1600
04/29/2024 12:30:10 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 16%|█▋        | 1601/9822 [49:29<15:48:45,  6.92s/it] 16%|█▋        | 1602/9822 [49:31<12:12:17,  5.35s/it] 16%|█▋        | 1603/9822 [49:32<9:41:16,  4.24s/it]  16%|█▋        | 1604/9822 [49:34<7:55:10,  3.47s/it] 16%|█▋        | 1605/9822 [49:36<6:40:54,  2.93s/it] 16%|█▋        | 1606/9822 [49:37<5:49:32,  2.55s/it] 16%|█▋        | 1607/9822 [49:39<5:13:19,  2.29s/it] 16%|█▋        | 1608/9822 [49:41<4:47:38,  2.10s/it] 16%|█▋        | 1609/9822 [49:42<4:29:49,  1.97s/it] 16%|█▋        | 1610/9822 [49:44<4:16:53,  1.88s/it] 16%|█▋        | 1611/9822 [49:46<4:08:41,  1.82s/it] 16%|█▋        | 1612/9822 [49:47<4:02:53,  1.78s/it] 16%|█▋        | 1613/9822 [49:49<3:58:11,  1.74s/it] 16%|█▋        | 1614/9822 [49:51<3:55:15,  1.72s/it] 16%|█▋        | 1615/9822 [49:52<3:53:08,  1.70s/it] 16%|█▋        | 1616/9822 [49:54<3:51:33,  1.69s/it] 16%|█▋        | 1617/9822 [49:56<3:50:25,  1.69s/it] 16%|█▋        | 1618/9822 [49:57<3:49:39,  1.68s/it] 16%|█▋        | 1619/9822 [49:59<3:53:04,  1.70s/it] 16%|█▋        | 1620/9822 [50:01<3:51:34,  1.69s/it] 17%|█▋        | 1621/9822 [50:02<3:50:33,  1.69s/it] 17%|█▋        | 1622/9822 [50:04<3:49:30,  1.68s/it] 17%|█▋        | 1623/9822 [50:06<3:48:32,  1.67s/it] 17%|█▋        | 1624/9822 [50:07<3:47:49,  1.67s/it] 17%|█▋        | 1625/9822 [50:09<3:47:30,  1.67s/it] 17%|█▋        | 1626/9822 [50:11<3:47:40,  1.67s/it] 17%|█▋        | 1627/9822 [50:12<3:47:34,  1.67s/it] 17%|█▋        | 1628/9822 [50:14<3:48:13,  1.67s/it] 17%|█▋        | 1629/9822 [50:16<3:47:47,  1.67s/it] 17%|█▋        | 1630/9822 [50:17<3:48:01,  1.67s/it] 17%|█▋        | 1631/9822 [50:19<3:47:24,  1.67s/it] 17%|█▋        | 1632/9822 [50:21<3:47:05,  1.66s/it] 17%|█▋        | 1633/9822 [50:22<3:48:02,  1.67s/it] 17%|█▋        | 1634/9822 [50:24<3:45:26,  1.65s/it] 17%|█▋        | 1635/9822 [50:26<3:45:49,  1.65s/it] 17%|█▋        | 1636/9822 [50:27<3:46:08,  1.66s/it] 17%|█▋        | 1637/9822 [50:29<3:47:16,  1.67s/it] 17%|█▋        | 1638/9822 [50:31<3:47:50,  1.67s/it] 17%|█▋        | 1639/9822 [50:32<3:47:25,  1.67s/it] 17%|█▋        | 1640/9822 [50:34<3:48:07,  1.67s/it] 17%|█▋        | 1641/9822 [50:36<3:48:36,  1.68s/it] 17%|█▋        | 1642/9822 [50:37<3:48:52,  1.68s/it] 17%|█▋        | 1643/9822 [50:39<3:48:37,  1.68s/it] 17%|█▋        | 1644/9822 [50:41<3:48:28,  1.68s/it] 17%|█▋        | 1645/9822 [50:42<3:47:48,  1.67s/it] 17%|█▋        | 1646/9822 [50:44<3:47:27,  1.67s/it] 17%|█▋        | 1647/9822 [50:46<3:47:23,  1.67s/it] 17%|█▋        | 1648/9822 [50:47<3:47:30,  1.67s/it] 17%|█▋        | 1649/9822 [50:49<3:47:14,  1.67s/it] 17%|█▋        | 1650/9822 [50:51<3:47:09,  1.67s/it] 17%|█▋        | 1651/9822 [50:52<3:47:01,  1.67s/it] 17%|█▋        | 1652/9822 [50:54<3:51:48,  1.70s/it] 17%|█▋        | 1653/9822 [50:56<3:50:38,  1.69s/it] 17%|█▋        | 1654/9822 [50:58<3:49:59,  1.69s/it] 17%|█▋        | 1655/9822 [50:59<3:49:03,  1.68s/it] 17%|█▋        | 1656/9822 [51:01<3:48:23,  1.68s/it] 17%|█▋        | 1657/9822 [51:03<3:47:51,  1.67s/it] 17%|█▋        | 1658/9822 [51:04<3:47:07,  1.67s/it] 17%|█▋        | 1659/9822 [51:06<3:47:57,  1.68s/it] 17%|█▋        | 1660/9822 [51:08<3:47:36,  1.67s/it] 17%|█▋        | 1661/9822 [51:09<3:47:05,  1.67s/it] 17%|█▋        | 1662/9822 [51:11<3:46:37,  1.67s/it] 17%|█▋        | 1663/9822 [51:13<3:47:23,  1.67s/it] 17%|█▋        | 1664/9822 [51:14<3:47:15,  1.67s/it] 17%|█▋        | 1665/9822 [51:16<3:46:50,  1.67s/it] 17%|█▋        | 1666/9822 [51:18<3:46:31,  1.67s/it] 17%|█▋        | 1667/9822 [51:19<3:46:36,  1.67s/it] 17%|█▋        | 1668/9822 [51:21<3:46:24,  1.67s/it] 17%|█▋        | 1669/9822 [51:23<3:46:42,  1.67s/it] 17%|█▋        | 1670/9822 [51:24<3:47:12,  1.67s/it] 17%|█▋        | 1671/9822 [51:26<3:47:18,  1.67s/it] 17%|█▋        | 1672/9822 [51:28<3:47:04,  1.67s/it] 17%|█▋        | 1673/9822 [51:29<3:47:26,  1.67s/it] 17%|█▋        | 1674/9822 [51:31<3:51:47,  1.71s/it] 17%|█▋        | 1675/9822 [51:33<3:50:29,  1.70s/it] 17%|█▋        | 1676/9822 [51:34<3:49:21,  1.69s/it] 17%|█▋        | 1677/9822 [51:36<3:48:29,  1.68s/it] 17%|█▋        | 1678/9822 [51:38<3:47:43,  1.68s/it] 17%|█▋        | 1679/9822 [51:39<3:47:37,  1.68s/it] 17%|█▋        | 1680/9822 [51:41<3:47:12,  1.67s/it] 17%|█▋        | 1681/9822 [51:43<3:46:58,  1.67s/it] 17%|█▋        | 1682/9822 [51:44<3:46:41,  1.67s/it] 17%|█▋        | 1683/9822 [51:46<3:47:01,  1.67s/it] 17%|█▋        | 1684/9822 [51:48<3:47:17,  1.68s/it] 17%|█▋        | 1685/9822 [51:49<3:47:35,  1.68s/it] 17%|█▋        | 1686/9822 [51:51<3:47:01,  1.67s/it] 17%|█▋        | 1687/9822 [51:53<3:47:01,  1.67s/it] 17%|█▋        | 1688/9822 [51:54<3:46:43,  1.67s/it] 17%|█▋        | 1689/9822 [51:56<3:46:33,  1.67s/it] 17%|█▋        | 1690/9822 [51:58<3:46:48,  1.67s/it] 17%|█▋        | 1691/9822 [52:00<3:46:54,  1.67s/it] 17%|█▋        | 1692/9822 [52:01<3:46:42,  1.67s/it] 17%|█▋        | 1693/9822 [52:03<3:46:37,  1.67s/it] 17%|█▋        | 1694/9822 [52:05<3:47:10,  1.68s/it] 17%|█▋        | 1695/9822 [52:06<3:46:54,  1.68s/it] 17%|█▋        | 1696/9822 [52:08<3:47:06,  1.68s/it] 17%|█▋        | 1697/9822 [52:10<3:46:43,  1.67s/it] 17%|█▋        | 1698/9822 [52:11<3:46:37,  1.67s/it] 17%|█▋        | 1699/9822 [52:13<3:46:17,  1.67s/it] 17%|█▋        | 1700/9822 [52:15<3:46:09,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:33:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:33:01 - INFO - __main__ - ***** test Results*****
04/29/2024 12:33:01 - INFO - __main__ -   Training step = 1700
04/29/2024 12:33:01 - INFO - __main__ -  test_accuracy:0.8532210834553441 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:33:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:33:06 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:33:06 - INFO - __main__ -   Training step = 1700
04/29/2024 12:33:06 - INFO - __main__ -  eval_accuracy:0.8425485170267302 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:33:06,291 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:33:06,291 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:33:06,332 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:33:07,448 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8425485170267302}
test:
{'accuracy': 0.8532210834553441}
04/29/2024 12:33:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:33:15 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:33:15 - INFO - __main__ -   Training step = 1700
04/29/2024 12:33:15 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 17%|█▋        | 1701/9822 [52:35<16:24:10,  7.27s/it] 17%|█▋        | 1702/9822 [52:37<12:40:42,  5.62s/it] 17%|█▋        | 1703/9822 [52:38<10:00:07,  4.43s/it] 17%|█▋        | 1704/9822 [52:40<8:07:21,  3.60s/it]  17%|█▋        | 1705/9822 [52:42<6:50:07,  3.03s/it] 17%|█▋        | 1706/9822 [52:43<5:55:20,  2.63s/it] 17%|█▋        | 1707/9822 [52:45<5:16:44,  2.34s/it] 17%|█▋        | 1708/9822 [52:47<4:49:23,  2.14s/it] 17%|█▋        | 1709/9822 [52:48<4:30:33,  2.00s/it] 17%|█▋        | 1710/9822 [52:50<4:16:54,  1.90s/it] 17%|█▋        | 1711/9822 [52:52<4:07:10,  1.83s/it] 17%|█▋        | 1712/9822 [52:53<4:00:03,  1.78s/it] 17%|█▋        | 1713/9822 [52:55<3:55:46,  1.74s/it] 17%|█▋        | 1714/9822 [52:57<3:52:44,  1.72s/it] 17%|█▋        | 1715/9822 [52:58<3:50:12,  1.70s/it] 17%|█▋        | 1716/9822 [53:00<3:48:22,  1.69s/it] 17%|█▋        | 1717/9822 [53:02<3:48:33,  1.69s/it] 17%|█▋        | 1718/9822 [53:03<3:47:50,  1.69s/it] 18%|█▊        | 1719/9822 [53:05<3:46:46,  1.68s/it] 18%|█▊        | 1720/9822 [53:07<3:43:57,  1.66s/it] 18%|█▊        | 1721/9822 [53:08<3:44:23,  1.66s/it] 18%|█▊        | 1722/9822 [53:10<3:45:03,  1.67s/it] 18%|█▊        | 1723/9822 [53:12<3:45:11,  1.67s/it] 18%|█▊        | 1724/9822 [53:13<3:45:16,  1.67s/it] 18%|█▊        | 1725/9822 [53:15<3:45:52,  1.67s/it] 18%|█▊        | 1726/9822 [53:17<3:46:13,  1.68s/it] 18%|█▊        | 1727/9822 [53:18<3:46:21,  1.68s/it] 18%|█▊        | 1728/9822 [53:20<3:46:00,  1.68s/it] 18%|█▊        | 1729/9822 [53:22<3:46:24,  1.68s/it] 18%|█▊        | 1730/9822 [53:23<3:46:17,  1.68s/it] 18%|█▊        | 1731/9822 [53:25<3:46:21,  1.68s/it] 18%|█▊        | 1732/9822 [53:27<3:46:08,  1.68s/it] 18%|█▊        | 1733/9822 [53:28<3:45:38,  1.67s/it] 18%|█▊        | 1734/9822 [53:30<3:45:13,  1.67s/it] 18%|█▊        | 1735/9822 [53:32<3:44:58,  1.67s/it] 18%|█▊        | 1736/9822 [53:33<3:45:00,  1.67s/it] 18%|█▊        | 1737/9822 [53:35<3:44:55,  1.67s/it] 18%|█▊        | 1738/9822 [53:37<3:44:30,  1.67s/it] 18%|█▊        | 1739/9822 [53:38<3:44:52,  1.67s/it] 18%|█▊        | 1740/9822 [53:40<3:44:53,  1.67s/it] 18%|█▊        | 1741/9822 [53:42<3:45:13,  1.67s/it] 18%|█▊        | 1742/9822 [53:44<3:49:42,  1.71s/it] 18%|█▊        | 1743/9822 [53:45<3:47:29,  1.69s/it] 18%|█▊        | 1744/9822 [53:47<3:46:21,  1.68s/it] 18%|█▊        | 1745/9822 [53:49<3:45:40,  1.68s/it] 18%|█▊        | 1746/9822 [53:50<3:44:42,  1.67s/it] 18%|█▊        | 1747/9822 [53:52<3:44:24,  1.67s/it] 18%|█▊        | 1748/9822 [53:54<3:44:20,  1.67s/it] 18%|█▊        | 1749/9822 [53:55<3:43:40,  1.66s/it] 18%|█▊        | 1750/9822 [53:57<3:43:40,  1.66s/it] 18%|█▊        | 1751/9822 [53:59<3:44:03,  1.67s/it] 18%|█▊        | 1752/9822 [54:00<3:44:05,  1.67s/it] 18%|█▊        | 1753/9822 [54:02<3:43:40,  1.66s/it] 18%|█▊        | 1754/9822 [54:04<3:43:49,  1.66s/it] 18%|█▊        | 1755/9822 [54:05<3:43:46,  1.66s/it] 18%|█▊        | 1756/9822 [54:07<3:43:45,  1.66s/it] 18%|█▊        | 1757/9822 [54:09<3:43:59,  1.67s/it] 18%|█▊        | 1758/9822 [54:10<3:44:23,  1.67s/it] 18%|█▊        | 1759/9822 [54:12<3:44:00,  1.67s/it] 18%|█▊        | 1760/9822 [54:14<3:44:04,  1.67s/it] 18%|█▊        | 1761/9822 [54:15<3:43:34,  1.66s/it] 18%|█▊        | 1762/9822 [54:17<3:43:31,  1.66s/it] 18%|█▊        | 1763/9822 [54:19<3:43:28,  1.66s/it] 18%|█▊        | 1764/9822 [54:20<3:43:30,  1.66s/it] 18%|█▊        | 1765/9822 [54:22<3:43:24,  1.66s/it] 18%|█▊        | 1766/9822 [54:24<3:43:50,  1.67s/it] 18%|█▊        | 1767/9822 [54:25<3:44:08,  1.67s/it] 18%|█▊        | 1768/9822 [54:27<3:43:59,  1.67s/it] 18%|█▊        | 1769/9822 [54:29<3:47:46,  1.70s/it] 18%|█▊        | 1770/9822 [54:30<3:46:27,  1.69s/it] 18%|█▊        | 1771/9822 [54:32<3:45:07,  1.68s/it] 18%|█▊        | 1772/9822 [54:34<3:44:27,  1.67s/it] 18%|█▊        | 1773/9822 [54:35<3:44:08,  1.67s/it] 18%|█▊        | 1774/9822 [54:37<3:43:25,  1.67s/it] 18%|█▊        | 1775/9822 [54:39<3:43:48,  1.67s/it] 18%|█▊        | 1776/9822 [54:40<3:43:37,  1.67s/it] 18%|█▊        | 1777/9822 [54:42<3:42:56,  1.66s/it] 18%|█▊        | 1778/9822 [54:44<3:42:51,  1.66s/it] 18%|█▊        | 1779/9822 [54:45<3:43:13,  1.67s/it] 18%|█▊        | 1780/9822 [54:47<3:43:34,  1.67s/it] 18%|█▊        | 1781/9822 [54:49<3:43:36,  1.67s/it] 18%|█▊        | 1782/9822 [54:50<3:43:28,  1.67s/it] 18%|█▊        | 1783/9822 [54:52<3:44:33,  1.68s/it] 18%|█▊        | 1784/9822 [54:54<3:48:30,  1.71s/it] 18%|█▊        | 1785/9822 [54:55<3:49:00,  1.71s/it] 18%|█▊        | 1786/9822 [54:57<3:47:56,  1.70s/it] 18%|█▊        | 1787/9822 [54:59<3:47:21,  1.70s/it] 18%|█▊        | 1788/9822 [55:01<3:46:38,  1.69s/it] 18%|█▊        | 1789/9822 [55:02<3:45:37,  1.69s/it] 18%|█▊        | 1790/9822 [55:04<3:45:12,  1.68s/it] 18%|█▊        | 1791/9822 [55:06<3:44:27,  1.68s/it] 18%|█▊        | 1792/9822 [55:07<3:44:42,  1.68s/it] 18%|█▊        | 1793/9822 [55:09<3:44:42,  1.68s/it] 18%|█▊        | 1794/9822 [55:11<3:44:49,  1.68s/it] 18%|█▊        | 1795/9822 [55:12<3:44:33,  1.68s/it] 18%|█▊        | 1796/9822 [55:14<3:48:20,  1.71s/it] 18%|█▊        | 1797/9822 [55:16<3:46:29,  1.69s/it] 18%|█▊        | 1798/9822 [55:17<3:46:02,  1.69s/it] 18%|█▊        | 1799/9822 [55:19<3:45:31,  1.69s/it] 18%|█▊        | 1800/9822 [55:21<3:45:08,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:36:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:36:07 - INFO - __main__ - ***** test Results*****
04/29/2024 12:36:07 - INFO - __main__ -   Training step = 1800
04/29/2024 12:36:07 - INFO - __main__ -  test_accuracy:0.8568814055636896 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:36:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:36:12 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:36:12 - INFO - __main__ -   Training step = 1800
04/29/2024 12:36:12 - INFO - __main__ -  eval_accuracy:0.8366898571951666 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8425485170267302}
test:
{'accuracy': 0.8532210834553441}
04/29/2024 12:36:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:36:20 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:36:20 - INFO - __main__ -   Training step = 1800
04/29/2024 12:36:20 - INFO - __main__ -  eval_accuracy:0.9044306114976199 
 18%|█▊        | 1801/9822 [55:40<15:28:26,  6.95s/it] 18%|█▊        | 1802/9822 [55:42<11:57:11,  5.37s/it] 18%|█▊        | 1803/9822 [55:43<9:28:44,  4.26s/it]  18%|█▊        | 1804/9822 [55:45<7:45:41,  3.48s/it] 18%|█▊        | 1805/9822 [55:47<6:33:35,  2.95s/it] 18%|█▊        | 1806/9822 [55:48<5:40:44,  2.55s/it] 18%|█▊        | 1807/9822 [55:50<5:05:51,  2.29s/it] 18%|█▊        | 1808/9822 [55:52<4:41:27,  2.11s/it] 18%|█▊        | 1809/9822 [55:53<4:24:23,  1.98s/it] 18%|█▊        | 1810/9822 [55:55<4:12:19,  1.89s/it] 18%|█▊        | 1811/9822 [55:57<4:03:08,  1.82s/it] 18%|█▊        | 1812/9822 [55:58<3:56:54,  1.77s/it] 18%|█▊        | 1813/9822 [56:00<3:52:46,  1.74s/it] 18%|█▊        | 1814/9822 [56:02<3:49:25,  1.72s/it] 18%|█▊        | 1815/9822 [56:03<3:47:44,  1.71s/it] 18%|█▊        | 1816/9822 [56:05<3:46:25,  1.70s/it] 18%|█▊        | 1817/9822 [56:07<3:46:01,  1.69s/it] 19%|█▊        | 1818/9822 [56:08<3:45:36,  1.69s/it] 19%|█▊        | 1819/9822 [56:10<3:44:58,  1.69s/it] 19%|█▊        | 1820/9822 [56:12<3:43:37,  1.68s/it] 19%|█▊        | 1821/9822 [56:13<3:43:03,  1.67s/it] 19%|█▊        | 1822/9822 [56:15<3:47:17,  1.70s/it] 19%|█▊        | 1823/9822 [56:17<3:46:02,  1.70s/it] 19%|█▊        | 1824/9822 [56:19<3:44:57,  1.69s/it] 19%|█▊        | 1825/9822 [56:20<3:44:10,  1.68s/it] 19%|█▊        | 1826/9822 [56:22<3:43:43,  1.68s/it] 19%|█▊        | 1827/9822 [56:24<3:43:18,  1.68s/it] 19%|█▊        | 1828/9822 [56:25<3:43:42,  1.68s/it] 19%|█▊        | 1829/9822 [56:27<3:43:16,  1.68s/it] 19%|█▊        | 1830/9822 [56:29<3:42:56,  1.67s/it] 19%|█▊        | 1831/9822 [56:30<3:42:14,  1.67s/it] 19%|█▊        | 1832/9822 [56:32<3:43:53,  1.68s/it] 19%|█▊        | 1833/9822 [56:34<3:43:10,  1.68s/it] 19%|█▊        | 1834/9822 [56:35<3:43:08,  1.68s/it] 19%|█▊        | 1835/9822 [56:37<3:42:40,  1.67s/it] 19%|█▊        | 1836/9822 [56:39<3:42:59,  1.68s/it] 19%|█▊        | 1837/9822 [56:40<3:43:03,  1.68s/it] 19%|█▊        | 1838/9822 [56:42<3:43:09,  1.68s/it] 19%|█▊        | 1839/9822 [56:44<3:42:35,  1.67s/it] 19%|█▊        | 1840/9822 [56:45<3:42:38,  1.67s/it] 19%|█▊        | 1841/9822 [56:47<3:45:43,  1.70s/it] 19%|█▉        | 1842/9822 [56:49<3:44:57,  1.69s/it] 19%|█▉        | 1843/9822 [56:50<3:43:51,  1.68s/it] 19%|█▉        | 1844/9822 [56:52<3:43:09,  1.68s/it] 19%|█▉        | 1845/9822 [56:54<3:42:30,  1.67s/it] 19%|█▉        | 1846/9822 [56:55<3:42:07,  1.67s/it] 19%|█▉        | 1847/9822 [56:57<3:41:50,  1.67s/it] 19%|█▉        | 1848/9822 [56:59<3:41:31,  1.67s/it] 19%|█▉        | 1849/9822 [57:00<3:41:47,  1.67s/it] 19%|█▉        | 1850/9822 [57:02<3:41:07,  1.66s/it] 19%|█▉        | 1851/9822 [57:04<3:41:07,  1.66s/it] 19%|█▉        | 1852/9822 [57:05<3:41:11,  1.67s/it] 19%|█▉        | 1853/9822 [57:07<3:42:37,  1.68s/it] 19%|█▉        | 1854/9822 [57:09<3:42:04,  1.67s/it] 19%|█▉        | 1855/9822 [57:11<3:46:15,  1.70s/it] 19%|█▉        | 1856/9822 [57:12<3:44:41,  1.69s/it] 19%|█▉        | 1857/9822 [57:14<3:43:02,  1.68s/it] 19%|█▉        | 1858/9822 [57:16<3:42:49,  1.68s/it] 19%|█▉        | 1859/9822 [57:17<3:42:21,  1.68s/it] 19%|█▉        | 1860/9822 [57:19<3:42:02,  1.67s/it] 19%|█▉        | 1861/9822 [57:21<3:41:40,  1.67s/it] 19%|█▉        | 1862/9822 [57:22<3:41:12,  1.67s/it] 19%|█▉        | 1863/9822 [57:24<3:40:59,  1.67s/it] 19%|█▉        | 1864/9822 [57:26<3:40:58,  1.67s/it] 19%|█▉        | 1865/9822 [57:27<3:40:58,  1.67s/it] 19%|█▉        | 1866/9822 [57:29<3:40:49,  1.67s/it] 19%|█▉        | 1867/9822 [57:31<3:40:36,  1.66s/it] 19%|█▉        | 1868/9822 [57:32<3:40:46,  1.67s/it] 19%|█▉        | 1869/9822 [57:34<3:40:41,  1.67s/it] 19%|█▉        | 1870/9822 [57:36<3:40:41,  1.67s/it] 19%|█▉        | 1871/9822 [57:37<3:40:41,  1.67s/it] 19%|█▉        | 1872/9822 [57:39<3:40:23,  1.66s/it] 19%|█▉        | 1873/9822 [57:41<3:40:43,  1.67s/it] 19%|█▉        | 1874/9822 [57:42<3:40:39,  1.67s/it] 19%|█▉        | 1875/9822 [57:44<3:40:45,  1.67s/it] 19%|█▉        | 1876/9822 [57:46<3:40:43,  1.67s/it] 19%|█▉        | 1877/9822 [57:47<3:44:44,  1.70s/it] 19%|█▉        | 1878/9822 [57:49<3:43:42,  1.69s/it] 19%|█▉        | 1879/9822 [57:51<3:42:44,  1.68s/it] 19%|█▉        | 1880/9822 [57:52<3:41:41,  1.67s/it] 19%|█▉        | 1881/9822 [57:54<3:41:36,  1.67s/it] 19%|█▉        | 1882/9822 [57:56<3:40:55,  1.67s/it] 19%|█▉        | 1883/9822 [57:57<3:40:34,  1.67s/it] 19%|█▉        | 1884/9822 [57:59<3:40:39,  1.67s/it] 19%|█▉        | 1885/9822 [58:01<3:40:18,  1.67s/it] 19%|█▉        | 1886/9822 [58:02<3:40:22,  1.67s/it] 19%|█▉        | 1887/9822 [58:04<3:41:08,  1.67s/it] 19%|█▉        | 1888/9822 [58:06<3:41:06,  1.67s/it] 19%|█▉        | 1889/9822 [58:07<3:40:50,  1.67s/it] 19%|█▉        | 1890/9822 [58:09<3:40:40,  1.67s/it] 19%|█▉        | 1891/9822 [58:11<3:40:41,  1.67s/it] 19%|█▉        | 1892/9822 [58:12<3:39:03,  1.66s/it] 19%|█▉        | 1893/9822 [58:14<3:39:38,  1.66s/it] 19%|█▉        | 1894/9822 [58:16<3:40:18,  1.67s/it] 19%|█▉        | 1895/9822 [58:17<3:40:36,  1.67s/it] 19%|█▉        | 1896/9822 [58:19<3:40:08,  1.67s/it] 19%|█▉        | 1897/9822 [58:21<3:41:00,  1.67s/it] 19%|█▉        | 1898/9822 [58:22<3:40:43,  1.67s/it] 19%|█▉        | 1899/9822 [58:24<3:40:43,  1.67s/it] 19%|█▉        | 1900/9822 [58:26<3:41:05,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:39:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:39:12 - INFO - __main__ - ***** test Results*****
04/29/2024 12:39:12 - INFO - __main__ -   Training step = 1900
04/29/2024 12:39:12 - INFO - __main__ -  test_accuracy:0.8565153733528551 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:39:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:39:17 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:39:17 - INFO - __main__ -   Training step = 1900
04/29/2024 12:39:17 - INFO - __main__ -  eval_accuracy:0.8396191871109484 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8425485170267302}
test:
{'accuracy': 0.8532210834553441}
04/29/2024 12:39:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:39:25 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:39:25 - INFO - __main__ -   Training step = 1900
04/29/2024 12:39:25 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 19%|█▉        | 1901/9822 [58:45<15:17:13,  6.95s/it] 19%|█▉        | 1902/9822 [58:47<11:48:49,  5.37s/it] 19%|█▉        | 1903/9822 [58:48<9:23:05,  4.27s/it]  19%|█▉        | 1904/9822 [58:50<7:44:52,  3.52s/it] 19%|█▉        | 1905/9822 [58:52<6:32:03,  2.97s/it] 19%|█▉        | 1906/9822 [58:53<5:41:13,  2.59s/it] 19%|█▉        | 1907/9822 [58:55<5:05:29,  2.32s/it] 19%|█▉        | 1908/9822 [58:57<4:40:35,  2.13s/it] 19%|█▉        | 1909/9822 [58:58<4:22:32,  1.99s/it] 19%|█▉        | 1910/9822 [59:00<4:10:22,  1.90s/it] 19%|█▉        | 1911/9822 [59:02<4:01:36,  1.83s/it] 19%|█▉        | 1912/9822 [59:04<3:55:46,  1.79s/it] 19%|█▉        | 1913/9822 [59:05<3:51:47,  1.76s/it] 19%|█▉        | 1914/9822 [59:07<3:48:31,  1.73s/it] 19%|█▉        | 1915/9822 [59:09<3:46:29,  1.72s/it] 20%|█▉        | 1916/9822 [59:10<3:45:05,  1.71s/it] 20%|█▉        | 1917/9822 [59:12<3:44:25,  1.70s/it] 20%|█▉        | 1918/9822 [59:14<3:43:38,  1.70s/it] 20%|█▉        | 1919/9822 [59:15<3:42:44,  1.69s/it] 20%|█▉        | 1920/9822 [59:17<3:42:28,  1.69s/it] 20%|█▉        | 1921/9822 [59:19<3:42:03,  1.69s/it] 20%|█▉        | 1922/9822 [59:20<3:41:47,  1.68s/it] 20%|█▉        | 1923/9822 [59:22<3:41:26,  1.68s/it] 20%|█▉        | 1924/9822 [59:24<3:40:58,  1.68s/it] 20%|█▉        | 1925/9822 [59:25<3:41:09,  1.68s/it] 20%|█▉        | 1926/9822 [59:27<3:41:17,  1.68s/it] 20%|█▉        | 1927/9822 [59:29<3:41:03,  1.68s/it] 20%|█▉        | 1928/9822 [59:30<3:41:10,  1.68s/it] 20%|█▉        | 1929/9822 [59:32<3:41:22,  1.68s/it] 20%|█▉        | 1930/9822 [59:34<3:45:33,  1.71s/it] 20%|█▉        | 1931/9822 [59:36<3:44:25,  1.71s/it] 20%|█▉        | 1932/9822 [59:37<3:43:28,  1.70s/it] 20%|█▉        | 1933/9822 [59:39<3:42:51,  1.69s/it] 20%|█▉        | 1934/9822 [59:41<3:42:26,  1.69s/it] 20%|█▉        | 1935/9822 [59:42<3:41:56,  1.69s/it] 20%|█▉        | 1936/9822 [59:44<3:41:36,  1.69s/it] 20%|█▉        | 1937/9822 [59:46<3:41:09,  1.68s/it] 20%|█▉        | 1938/9822 [59:47<3:41:14,  1.68s/it] 20%|█▉        | 1939/9822 [59:49<3:41:10,  1.68s/it] 20%|█▉        | 1940/9822 [59:51<3:41:07,  1.68s/it] 20%|█▉        | 1941/9822 [59:52<3:41:19,  1.68s/it] 20%|█▉        | 1942/9822 [59:54<3:40:47,  1.68s/it] 20%|█▉        | 1943/9822 [59:56<3:40:50,  1.68s/it] 20%|█▉        | 1944/9822 [59:57<3:40:56,  1.68s/it] 20%|█▉        | 1945/9822 [59:59<3:40:49,  1.68s/it] 20%|█▉        | 1946/9822 [1:00:01<3:40:52,  1.68s/it] 20%|█▉        | 1947/9822 [1:00:03<3:40:57,  1.68s/it] 20%|█▉        | 1948/9822 [1:00:04<3:40:59,  1.68s/it] 20%|█▉        | 1949/9822 [1:00:06<3:40:26,  1.68s/it] 20%|█▉        | 1950/9822 [1:00:08<3:39:59,  1.68s/it] 20%|█▉        | 1951/9822 [1:00:09<3:39:54,  1.68s/it] 20%|█▉        | 1952/9822 [1:00:11<3:39:31,  1.67s/it] 20%|█▉        | 1953/9822 [1:00:13<3:39:04,  1.67s/it] 20%|█▉        | 1954/9822 [1:00:14<3:39:01,  1.67s/it] 20%|█▉        | 1955/9822 [1:00:16<3:38:32,  1.67s/it] 20%|█▉        | 1956/9822 [1:00:18<3:38:47,  1.67s/it] 20%|█▉        | 1957/9822 [1:00:19<3:38:52,  1.67s/it] 20%|█▉        | 1958/9822 [1:00:21<3:39:07,  1.67s/it] 20%|█▉        | 1959/9822 [1:00:23<3:39:22,  1.67s/it] 20%|█▉        | 1960/9822 [1:00:24<3:38:39,  1.67s/it] 20%|█▉        | 1961/9822 [1:00:26<3:38:02,  1.66s/it] 20%|█▉        | 1962/9822 [1:00:28<3:38:14,  1.67s/it] 20%|█▉        | 1963/9822 [1:00:29<3:42:21,  1.70s/it] 20%|█▉        | 1964/9822 [1:00:31<3:41:17,  1.69s/it] 20%|██        | 1965/9822 [1:00:33<3:40:41,  1.69s/it] 20%|██        | 1966/9822 [1:00:34<3:39:51,  1.68s/it] 20%|██        | 1967/9822 [1:00:36<3:39:20,  1.68s/it] 20%|██        | 1968/9822 [1:00:38<3:39:31,  1.68s/it] 20%|██        | 1969/9822 [1:00:39<3:39:27,  1.68s/it] 20%|██        | 1970/9822 [1:00:41<3:39:29,  1.68s/it] 20%|██        | 1971/9822 [1:00:43<3:39:25,  1.68s/it] 20%|██        | 1972/9822 [1:00:44<3:39:22,  1.68s/it] 20%|██        | 1973/9822 [1:00:46<3:39:05,  1.67s/it] 20%|██        | 1974/9822 [1:00:48<3:38:34,  1.67s/it] 20%|██        | 1975/9822 [1:00:49<3:38:26,  1.67s/it] 20%|██        | 1976/9822 [1:00:51<3:38:44,  1.67s/it] 20%|██        | 1977/9822 [1:00:53<3:38:29,  1.67s/it] 20%|██        | 1978/9822 [1:00:54<3:36:37,  1.66s/it] 20%|██        | 1979/9822 [1:00:56<3:37:06,  1.66s/it] 20%|██        | 1980/9822 [1:00:58<3:37:25,  1.66s/it] 20%|██        | 1981/9822 [1:00:59<3:36:59,  1.66s/it] 20%|██        | 1982/9822 [1:01:01<3:37:03,  1.66s/it] 20%|██        | 1983/9822 [1:01:03<3:37:11,  1.66s/it] 20%|██        | 1984/9822 [1:01:04<3:37:50,  1.67s/it] 20%|██        | 1985/9822 [1:01:06<3:42:03,  1.70s/it] 20%|██        | 1986/9822 [1:01:08<3:40:27,  1.69s/it] 20%|██        | 1987/9822 [1:01:09<3:39:09,  1.68s/it] 20%|██        | 1988/9822 [1:01:11<3:38:12,  1.67s/it] 20%|██        | 1989/9822 [1:01:13<3:37:56,  1.67s/it] 20%|██        | 1990/9822 [1:01:14<3:38:01,  1.67s/it] 20%|██        | 1991/9822 [1:01:16<3:38:00,  1.67s/it] 20%|██        | 1992/9822 [1:01:18<3:37:57,  1.67s/it] 20%|██        | 1993/9822 [1:01:19<3:38:00,  1.67s/it] 20%|██        | 1994/9822 [1:01:21<3:38:08,  1.67s/it] 20%|██        | 1995/9822 [1:01:23<3:38:25,  1.67s/it] 20%|██        | 1996/9822 [1:01:24<3:38:13,  1.67s/it] 20%|██        | 1997/9822 [1:01:26<3:38:34,  1.68s/it] 20%|██        | 1998/9822 [1:01:28<3:38:26,  1.68s/it] 20%|██        | 1999/9822 [1:01:30<3:40:54,  1.69s/it] 20%|██        | 2000/9822 [1:01:31<3:41:57,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:42:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:42:18 - INFO - __main__ - ***** test Results*****
04/29/2024 12:42:18 - INFO - __main__ -   Training step = 2000
04/29/2024 12:42:18 - INFO - __main__ -  test_accuracy:0.8546852122986823 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:42:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:42:23 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:42:23 - INFO - __main__ -   Training step = 2000
04/29/2024 12:42:23 - INFO - __main__ -  eval_accuracy:0.841450018308312 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8425485170267302}
test:
{'accuracy': 0.8532210834553441}
04/29/2024 12:42:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:42:31 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:42:31 - INFO - __main__ -   Training step = 2000
04/29/2024 12:42:31 - INFO - __main__ -  eval_accuracy:0.9036982790186745 
 20%|██        | 2001/9822 [1:01:51<15:06:20,  6.95s/it] 20%|██        | 2002/9822 [1:01:52<11:40:24,  5.37s/it] 20%|██        | 2003/9822 [1:01:54<9:15:34,  4.26s/it]  20%|██        | 2004/9822 [1:01:56<7:34:32,  3.49s/it] 20%|██        | 2005/9822 [1:01:57<6:23:37,  2.94s/it] 20%|██        | 2006/9822 [1:01:59<5:34:19,  2.57s/it] 20%|██        | 2007/9822 [1:02:01<4:59:39,  2.30s/it] 20%|██        | 2008/9822 [1:02:02<4:35:27,  2.12s/it] 20%|██        | 2009/9822 [1:02:04<4:18:24,  1.98s/it] 20%|██        | 2010/9822 [1:02:06<4:06:32,  1.89s/it] 20%|██        | 2011/9822 [1:02:07<4:01:12,  1.85s/it] 20%|██        | 2012/9822 [1:02:09<3:54:16,  1.80s/it] 20%|██        | 2013/9822 [1:02:11<3:49:12,  1.76s/it] 21%|██        | 2014/9822 [1:02:12<3:45:33,  1.73s/it] 21%|██        | 2015/9822 [1:02:14<3:43:19,  1.72s/it] 21%|██        | 2016/9822 [1:02:16<3:41:52,  1.71s/it] 21%|██        | 2017/9822 [1:02:17<3:40:25,  1.69s/it] 21%|██        | 2018/9822 [1:02:19<3:39:01,  1.68s/it] 21%|██        | 2019/9822 [1:02:21<3:38:28,  1.68s/it] 21%|██        | 2020/9822 [1:02:22<3:38:13,  1.68s/it] 21%|██        | 2021/9822 [1:02:24<3:38:03,  1.68s/it] 21%|██        | 2022/9822 [1:02:26<3:37:17,  1.67s/it] 21%|██        | 2023/9822 [1:02:27<3:36:36,  1.67s/it] 21%|██        | 2024/9822 [1:02:29<3:36:43,  1.67s/it] 21%|██        | 2025/9822 [1:02:31<3:36:54,  1.67s/it] 21%|██        | 2026/9822 [1:02:32<3:36:36,  1.67s/it] 21%|██        | 2027/9822 [1:02:34<3:36:45,  1.67s/it] 21%|██        | 2028/9822 [1:02:36<3:36:22,  1.67s/it] 21%|██        | 2029/9822 [1:02:37<3:36:42,  1.67s/it] 21%|██        | 2030/9822 [1:02:39<3:37:22,  1.67s/it] 21%|██        | 2031/9822 [1:02:41<3:37:35,  1.68s/it] 21%|██        | 2032/9822 [1:02:42<3:37:56,  1.68s/it] 21%|██        | 2033/9822 [1:02:44<3:37:48,  1.68s/it] 21%|██        | 2034/9822 [1:02:46<3:37:10,  1.67s/it] 21%|██        | 2035/9822 [1:02:48<3:37:26,  1.68s/it] 21%|██        | 2036/9822 [1:02:49<3:37:10,  1.67s/it] 21%|██        | 2037/9822 [1:02:51<3:36:59,  1.67s/it] 21%|██        | 2038/9822 [1:02:53<3:40:28,  1.70s/it] 21%|██        | 2039/9822 [1:02:54<3:39:01,  1.69s/it] 21%|██        | 2040/9822 [1:02:56<3:38:21,  1.68s/it] 21%|██        | 2041/9822 [1:02:58<3:38:22,  1.68s/it] 21%|██        | 2042/9822 [1:02:59<3:38:19,  1.68s/it] 21%|██        | 2043/9822 [1:03:01<3:38:14,  1.68s/it] 21%|██        | 2044/9822 [1:03:03<3:38:09,  1.68s/it] 21%|██        | 2045/9822 [1:03:04<3:38:13,  1.68s/it] 21%|██        | 2046/9822 [1:03:06<3:38:11,  1.68s/it] 21%|██        | 2047/9822 [1:03:08<3:38:17,  1.68s/it] 21%|██        | 2048/9822 [1:03:09<3:38:06,  1.68s/it] 21%|██        | 2049/9822 [1:03:11<3:38:04,  1.68s/it] 21%|██        | 2050/9822 [1:03:13<3:38:05,  1.68s/it] 21%|██        | 2051/9822 [1:03:14<3:37:58,  1.68s/it] 21%|██        | 2052/9822 [1:03:16<3:38:01,  1.68s/it] 21%|██        | 2053/9822 [1:03:18<3:37:47,  1.68s/it] 21%|██        | 2054/9822 [1:03:20<3:37:50,  1.68s/it] 21%|██        | 2055/9822 [1:03:21<3:37:39,  1.68s/it] 21%|██        | 2056/9822 [1:03:23<3:37:14,  1.68s/it] 21%|██        | 2057/9822 [1:03:25<3:36:41,  1.67s/it] 21%|██        | 2058/9822 [1:03:26<3:36:25,  1.67s/it] 21%|██        | 2059/9822 [1:03:28<3:36:31,  1.67s/it] 21%|██        | 2060/9822 [1:03:30<3:36:34,  1.67s/it] 21%|██        | 2061/9822 [1:03:31<3:36:10,  1.67s/it] 21%|██        | 2062/9822 [1:03:33<3:35:51,  1.67s/it] 21%|██        | 2063/9822 [1:03:35<3:36:15,  1.67s/it] 21%|██        | 2064/9822 [1:03:36<3:34:00,  1.66s/it] 21%|██        | 2065/9822 [1:03:38<3:34:10,  1.66s/it] 21%|██        | 2066/9822 [1:03:39<3:34:33,  1.66s/it] 21%|██        | 2067/9822 [1:03:41<3:34:16,  1.66s/it] 21%|██        | 2068/9822 [1:03:43<3:34:13,  1.66s/it] 21%|██        | 2069/9822 [1:03:44<3:34:19,  1.66s/it] 21%|██        | 2070/9822 [1:03:46<3:34:18,  1.66s/it] 21%|██        | 2071/9822 [1:03:48<3:38:19,  1.69s/it] 21%|██        | 2072/9822 [1:03:50<3:37:47,  1.69s/it] 21%|██        | 2073/9822 [1:03:51<3:37:37,  1.69s/it] 21%|██        | 2074/9822 [1:03:53<3:37:15,  1.68s/it] 21%|██        | 2075/9822 [1:03:55<3:36:41,  1.68s/it] 21%|██        | 2076/9822 [1:03:56<3:36:10,  1.67s/it] 21%|██        | 2077/9822 [1:03:58<3:36:06,  1.67s/it] 21%|██        | 2078/9822 [1:04:00<3:36:01,  1.67s/it] 21%|██        | 2079/9822 [1:04:01<3:35:36,  1.67s/it] 21%|██        | 2080/9822 [1:04:03<3:35:47,  1.67s/it] 21%|██        | 2081/9822 [1:04:05<3:36:11,  1.68s/it] 21%|██        | 2082/9822 [1:04:06<3:35:56,  1.67s/it] 21%|██        | 2083/9822 [1:04:08<3:35:49,  1.67s/it] 21%|██        | 2084/9822 [1:04:10<3:35:59,  1.67s/it] 21%|██        | 2085/9822 [1:04:11<3:35:36,  1.67s/it] 21%|██        | 2086/9822 [1:04:13<3:35:33,  1.67s/it] 21%|██        | 2087/9822 [1:04:15<3:35:04,  1.67s/it] 21%|██▏       | 2088/9822 [1:04:16<3:35:00,  1.67s/it] 21%|██▏       | 2089/9822 [1:04:18<3:35:20,  1.67s/it] 21%|██▏       | 2090/9822 [1:04:20<3:35:55,  1.68s/it] 21%|██▏       | 2091/9822 [1:04:21<3:35:22,  1.67s/it] 21%|██▏       | 2092/9822 [1:04:23<3:35:08,  1.67s/it] 21%|██▏       | 2093/9822 [1:04:25<3:39:09,  1.70s/it] 21%|██▏       | 2094/9822 [1:04:26<3:38:05,  1.69s/it] 21%|██▏       | 2095/9822 [1:04:28<3:37:31,  1.69s/it] 21%|██▏       | 2096/9822 [1:04:30<3:36:30,  1.68s/it] 21%|██▏       | 2097/9822 [1:04:31<3:36:32,  1.68s/it] 21%|██▏       | 2098/9822 [1:04:33<3:36:26,  1.68s/it] 21%|██▏       | 2099/9822 [1:04:35<3:36:14,  1.68s/it] 21%|██▏       | 2100/9822 [1:04:37<3:36:12,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:45:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:45:23 - INFO - __main__ - ***** test Results*****
04/29/2024 12:45:23 - INFO - __main__ -   Training step = 2100
04/29/2024 12:45:23 - INFO - __main__ -  test_accuracy:0.8565153733528551 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:45:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:45:28 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:45:28 - INFO - __main__ -   Training step = 2100
04/29/2024 12:45:28 - INFO - __main__ -  eval_accuracy:0.8429146832662029 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:45:28,233 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:45:28,233 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:45:28,274 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:45:29,384 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8429146832662029}
test:
{'accuracy': 0.8565153733528551}
04/29/2024 12:45:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:45:37 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:45:37 - INFO - __main__ -   Training step = 2100
04/29/2024 12:45:37 - INFO - __main__ -  eval_accuracy:0.9062614426949835 
 21%|██▏       | 2101/9822 [1:04:57<15:37:56,  7.29s/it] 21%|██▏       | 2102/9822 [1:04:59<12:01:38,  5.61s/it] 21%|██▏       | 2103/9822 [1:05:00<9:29:55,  4.43s/it]  21%|██▏       | 2104/9822 [1:05:02<7:43:16,  3.60s/it] 21%|██▏       | 2105/9822 [1:05:04<6:28:30,  3.02s/it] 21%|██▏       | 2106/9822 [1:05:05<5:36:26,  2.62s/it] 21%|██▏       | 2107/9822 [1:05:07<4:59:52,  2.33s/it] 21%|██▏       | 2108/9822 [1:05:09<4:33:58,  2.13s/it] 21%|██▏       | 2109/9822 [1:05:10<4:16:33,  2.00s/it] 21%|██▏       | 2110/9822 [1:05:12<4:04:12,  1.90s/it] 21%|██▏       | 2111/9822 [1:05:14<3:55:55,  1.84s/it] 22%|██▏       | 2112/9822 [1:05:15<3:49:16,  1.78s/it] 22%|██▏       | 2113/9822 [1:05:17<3:44:56,  1.75s/it] 22%|██▏       | 2114/9822 [1:05:19<3:42:20,  1.73s/it] 22%|██▏       | 2115/9822 [1:05:20<3:39:55,  1.71s/it] 22%|██▏       | 2116/9822 [1:05:22<3:38:42,  1.70s/it] 22%|██▏       | 2117/9822 [1:05:24<3:37:57,  1.70s/it] 22%|██▏       | 2118/9822 [1:05:25<3:37:02,  1.69s/it] 22%|██▏       | 2119/9822 [1:05:27<3:36:24,  1.69s/it] 22%|██▏       | 2120/9822 [1:05:29<3:35:58,  1.68s/it] 22%|██▏       | 2121/9822 [1:05:30<3:35:14,  1.68s/it] 22%|██▏       | 2122/9822 [1:05:32<3:35:23,  1.68s/it] 22%|██▏       | 2123/9822 [1:05:34<3:35:05,  1.68s/it] 22%|██▏       | 2124/9822 [1:05:35<3:34:50,  1.67s/it] 22%|██▏       | 2125/9822 [1:05:37<3:34:25,  1.67s/it] 22%|██▏       | 2126/9822 [1:05:39<3:38:56,  1.71s/it] 22%|██▏       | 2127/9822 [1:05:41<3:38:08,  1.70s/it] 22%|██▏       | 2128/9822 [1:05:42<3:36:44,  1.69s/it] 22%|██▏       | 2129/9822 [1:05:44<3:35:59,  1.68s/it] 22%|██▏       | 2130/9822 [1:05:46<3:35:26,  1.68s/it] 22%|██▏       | 2131/9822 [1:05:47<3:35:09,  1.68s/it] 22%|██▏       | 2132/9822 [1:05:49<3:34:59,  1.68s/it] 22%|██▏       | 2133/9822 [1:05:51<3:34:29,  1.67s/it] 22%|██▏       | 2134/9822 [1:05:52<3:34:04,  1.67s/it] 22%|██▏       | 2135/9822 [1:05:54<3:34:22,  1.67s/it] 22%|██▏       | 2136/9822 [1:05:56<3:33:57,  1.67s/it] 22%|██▏       | 2137/9822 [1:05:57<3:33:52,  1.67s/it] 22%|██▏       | 2138/9822 [1:05:59<3:34:24,  1.67s/it] 22%|██▏       | 2139/9822 [1:06:01<3:34:35,  1.68s/it] 22%|██▏       | 2140/9822 [1:06:02<3:34:17,  1.67s/it] 22%|██▏       | 2141/9822 [1:06:04<3:34:34,  1.68s/it] 22%|██▏       | 2142/9822 [1:06:06<3:35:03,  1.68s/it] 22%|██▏       | 2143/9822 [1:06:07<3:34:51,  1.68s/it] 22%|██▏       | 2144/9822 [1:06:09<3:34:30,  1.68s/it] 22%|██▏       | 2145/9822 [1:06:11<3:34:50,  1.68s/it] 22%|██▏       | 2146/9822 [1:06:12<3:34:32,  1.68s/it] 22%|██▏       | 2147/9822 [1:06:14<3:34:13,  1.67s/it] 22%|██▏       | 2148/9822 [1:06:16<3:37:55,  1.70s/it] 22%|██▏       | 2149/9822 [1:06:17<3:37:04,  1.70s/it] 22%|██▏       | 2150/9822 [1:06:19<3:34:05,  1.67s/it] 22%|██▏       | 2151/9822 [1:06:21<3:33:53,  1.67s/it] 22%|██▏       | 2152/9822 [1:06:22<3:34:00,  1.67s/it] 22%|██▏       | 2153/9822 [1:06:24<3:33:50,  1.67s/it] 22%|██▏       | 2154/9822 [1:06:26<3:33:45,  1.67s/it] 22%|██▏       | 2155/9822 [1:06:27<3:33:22,  1.67s/it] 22%|██▏       | 2156/9822 [1:06:29<3:33:25,  1.67s/it] 22%|██▏       | 2157/9822 [1:06:31<3:34:26,  1.68s/it] 22%|██▏       | 2158/9822 [1:06:33<3:35:20,  1.69s/it] 22%|██▏       | 2159/9822 [1:06:34<3:35:21,  1.69s/it] 22%|██▏       | 2160/9822 [1:06:36<3:34:51,  1.68s/it] 22%|██▏       | 2161/9822 [1:06:38<3:34:25,  1.68s/it] 22%|██▏       | 2162/9822 [1:06:39<3:34:39,  1.68s/it] 22%|██▏       | 2163/9822 [1:06:41<3:34:07,  1.68s/it] 22%|██▏       | 2164/9822 [1:06:43<3:34:10,  1.68s/it] 22%|██▏       | 2165/9822 [1:06:44<3:34:15,  1.68s/it] 22%|██▏       | 2166/9822 [1:06:46<3:34:06,  1.68s/it] 22%|██▏       | 2167/9822 [1:06:48<3:33:58,  1.68s/it] 22%|██▏       | 2168/9822 [1:06:49<3:33:54,  1.68s/it] 22%|██▏       | 2169/9822 [1:06:51<3:33:44,  1.68s/it] 22%|██▏       | 2170/9822 [1:06:53<3:33:21,  1.67s/it] 22%|██▏       | 2171/9822 [1:06:54<3:33:20,  1.67s/it] 22%|██▏       | 2172/9822 [1:06:56<3:33:52,  1.68s/it] 22%|██▏       | 2173/9822 [1:06:58<3:33:43,  1.68s/it] 22%|██▏       | 2174/9822 [1:06:59<3:33:44,  1.68s/it] 22%|██▏       | 2175/9822 [1:07:01<3:37:50,  1.71s/it] 22%|██▏       | 2176/9822 [1:07:03<3:36:34,  1.70s/it] 22%|██▏       | 2177/9822 [1:07:04<3:35:30,  1.69s/it] 22%|██▏       | 2178/9822 [1:07:06<3:34:32,  1.68s/it] 22%|██▏       | 2179/9822 [1:07:08<3:34:02,  1.68s/it] 22%|██▏       | 2180/9822 [1:07:09<3:33:41,  1.68s/it] 22%|██▏       | 2181/9822 [1:07:11<3:33:23,  1.68s/it] 22%|██▏       | 2182/9822 [1:07:13<3:33:26,  1.68s/it] 22%|██▏       | 2183/9822 [1:07:15<3:33:22,  1.68s/it] 22%|██▏       | 2184/9822 [1:07:16<3:33:27,  1.68s/it] 22%|██▏       | 2185/9822 [1:07:18<3:33:30,  1.68s/it] 22%|██▏       | 2186/9822 [1:07:20<3:33:26,  1.68s/it] 22%|██▏       | 2187/9822 [1:07:21<3:33:42,  1.68s/it] 22%|██▏       | 2188/9822 [1:07:23<3:33:54,  1.68s/it] 22%|██▏       | 2189/9822 [1:07:25<3:33:51,  1.68s/it] 22%|██▏       | 2190/9822 [1:07:26<3:34:03,  1.68s/it] 22%|██▏       | 2191/9822 [1:07:28<3:34:04,  1.68s/it] 22%|██▏       | 2192/9822 [1:07:30<3:33:27,  1.68s/it] 22%|██▏       | 2193/9822 [1:07:31<3:33:45,  1.68s/it] 22%|██▏       | 2194/9822 [1:07:33<3:33:34,  1.68s/it] 22%|██▏       | 2195/9822 [1:07:35<3:33:07,  1.68s/it] 22%|██▏       | 2196/9822 [1:07:36<3:33:17,  1.68s/it] 22%|██▏       | 2197/9822 [1:07:38<3:33:30,  1.68s/it] 22%|██▏       | 2198/9822 [1:07:40<3:33:25,  1.68s/it] 22%|██▏       | 2199/9822 [1:07:41<3:33:23,  1.68s/it] 22%|██▏       | 2200/9822 [1:07:43<3:32:56,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:48:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:48:30 - INFO - __main__ - ***** test Results*****
04/29/2024 12:48:30 - INFO - __main__ -   Training step = 2200
04/29/2024 12:48:30 - INFO - __main__ -  test_accuracy:0.8554172767203514 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:48:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:48:34 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:48:34 - INFO - __main__ -   Training step = 2200
04/29/2024 12:48:34 - INFO - __main__ -  eval_accuracy:0.8381545221530575 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8429146832662029}
test:
{'accuracy': 0.8565153733528551}
04/29/2024 12:48:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:48:43 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:48:43 - INFO - __main__ -   Training step = 2200
04/29/2024 12:48:43 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 22%|██▏       | 2201/9822 [1:08:02<14:40:55,  6.94s/it] 22%|██▏       | 2202/9822 [1:08:04<11:20:22,  5.36s/it] 22%|██▏       | 2203/9822 [1:08:06<8:59:56,  4.25s/it]  22%|██▏       | 2204/9822 [1:08:07<7:21:25,  3.48s/it] 22%|██▏       | 2205/9822 [1:08:09<6:12:38,  2.94s/it] 22%|██▏       | 2206/9822 [1:08:11<5:28:43,  2.59s/it] 22%|██▏       | 2207/9822 [1:08:12<4:54:21,  2.32s/it] 22%|██▏       | 2208/9822 [1:08:14<4:29:52,  2.13s/it] 22%|██▏       | 2209/9822 [1:08:16<4:12:39,  1.99s/it] 23%|██▎       | 2210/9822 [1:08:17<4:00:36,  1.90s/it] 23%|██▎       | 2211/9822 [1:08:19<3:52:06,  1.83s/it] 23%|██▎       | 2212/9822 [1:08:21<3:46:11,  1.78s/it] 23%|██▎       | 2213/9822 [1:08:22<3:42:01,  1.75s/it] 23%|██▎       | 2214/9822 [1:08:24<3:39:00,  1.73s/it] 23%|██▎       | 2215/9822 [1:08:26<3:36:35,  1.71s/it] 23%|██▎       | 2216/9822 [1:08:27<3:35:03,  1.70s/it] 23%|██▎       | 2217/9822 [1:08:29<3:33:33,  1.68s/it] 23%|██▎       | 2218/9822 [1:08:31<3:33:00,  1.68s/it] 23%|██▎       | 2219/9822 [1:08:32<3:32:35,  1.68s/it] 23%|██▎       | 2220/9822 [1:08:34<3:32:21,  1.68s/it] 23%|██▎       | 2221/9822 [1:08:36<3:32:19,  1.68s/it] 23%|██▎       | 2222/9822 [1:08:38<3:32:24,  1.68s/it] 23%|██▎       | 2223/9822 [1:08:39<3:32:29,  1.68s/it] 23%|██▎       | 2224/9822 [1:08:41<3:32:34,  1.68s/it] 23%|██▎       | 2225/9822 [1:08:43<3:32:43,  1.68s/it] 23%|██▎       | 2226/9822 [1:08:44<3:32:31,  1.68s/it] 23%|██▎       | 2227/9822 [1:08:46<3:32:19,  1.68s/it] 23%|██▎       | 2228/9822 [1:08:48<3:32:36,  1.68s/it] 23%|██▎       | 2229/9822 [1:08:49<3:32:45,  1.68s/it] 23%|██▎       | 2230/9822 [1:08:51<3:32:23,  1.68s/it] 23%|██▎       | 2231/9822 [1:08:53<3:32:25,  1.68s/it] 23%|██▎       | 2232/9822 [1:08:54<3:32:28,  1.68s/it] 23%|██▎       | 2233/9822 [1:08:56<3:32:26,  1.68s/it] 23%|██▎       | 2234/9822 [1:08:58<3:32:15,  1.68s/it] 23%|██▎       | 2235/9822 [1:08:59<3:31:40,  1.67s/it] 23%|██▎       | 2236/9822 [1:09:01<3:29:44,  1.66s/it] 23%|██▎       | 2237/9822 [1:09:03<3:30:01,  1.66s/it] 23%|██▎       | 2238/9822 [1:09:04<3:30:28,  1.67s/it] 23%|██▎       | 2239/9822 [1:09:06<3:30:31,  1.67s/it] 23%|██▎       | 2240/9822 [1:09:08<3:31:03,  1.67s/it] 23%|██▎       | 2241/9822 [1:09:09<3:30:59,  1.67s/it] 23%|██▎       | 2242/9822 [1:09:11<3:30:57,  1.67s/it] 23%|██▎       | 2243/9822 [1:09:13<3:31:19,  1.67s/it] 23%|██▎       | 2244/9822 [1:09:14<3:31:16,  1.67s/it] 23%|██▎       | 2245/9822 [1:09:16<3:31:26,  1.67s/it] 23%|██▎       | 2246/9822 [1:09:18<3:31:43,  1.68s/it] 23%|██▎       | 2247/9822 [1:09:19<3:35:14,  1.70s/it] 23%|██▎       | 2248/9822 [1:09:21<3:33:33,  1.69s/it] 23%|██▎       | 2249/9822 [1:09:23<3:32:30,  1.68s/it] 23%|██▎       | 2250/9822 [1:09:24<3:32:08,  1.68s/it] 23%|██▎       | 2251/9822 [1:09:26<3:31:30,  1.68s/it] 23%|██▎       | 2252/9822 [1:09:28<3:31:26,  1.68s/it] 23%|██▎       | 2253/9822 [1:09:29<3:31:22,  1.68s/it] 23%|██▎       | 2254/9822 [1:09:31<3:30:49,  1.67s/it] 23%|██▎       | 2255/9822 [1:09:33<3:30:57,  1.67s/it] 23%|██▎       | 2256/9822 [1:09:34<3:30:38,  1.67s/it] 23%|██▎       | 2257/9822 [1:09:36<3:30:14,  1.67s/it] 23%|██▎       | 2258/9822 [1:09:38<3:30:28,  1.67s/it] 23%|██▎       | 2259/9822 [1:09:40<3:30:57,  1.67s/it] 23%|██▎       | 2260/9822 [1:09:41<3:31:02,  1.67s/it] 23%|██▎       | 2261/9822 [1:09:43<3:31:05,  1.68s/it] 23%|██▎       | 2262/9822 [1:09:45<3:30:39,  1.67s/it] 23%|██▎       | 2263/9822 [1:09:46<3:30:37,  1.67s/it] 23%|██▎       | 2264/9822 [1:09:48<3:30:27,  1.67s/it] 23%|██▎       | 2265/9822 [1:09:50<3:30:17,  1.67s/it] 23%|██▎       | 2266/9822 [1:09:51<3:30:34,  1.67s/it] 23%|██▎       | 2267/9822 [1:09:53<3:30:11,  1.67s/it] 23%|██▎       | 2268/9822 [1:09:55<3:30:41,  1.67s/it] 23%|██▎       | 2269/9822 [1:09:56<3:30:48,  1.67s/it] 23%|██▎       | 2270/9822 [1:09:58<3:30:49,  1.68s/it] 23%|██▎       | 2271/9822 [1:10:00<3:30:38,  1.67s/it] 23%|██▎       | 2272/9822 [1:10:01<3:30:49,  1.68s/it] 23%|██▎       | 2273/9822 [1:10:03<3:31:12,  1.68s/it] 23%|██▎       | 2274/9822 [1:10:05<3:35:15,  1.71s/it] 23%|██▎       | 2275/9822 [1:10:06<3:34:12,  1.70s/it] 23%|██▎       | 2276/9822 [1:10:08<3:33:19,  1.70s/it] 23%|██▎       | 2277/9822 [1:10:10<3:32:55,  1.69s/it] 23%|██▎       | 2278/9822 [1:10:11<3:32:36,  1.69s/it] 23%|██▎       | 2279/9822 [1:10:13<3:32:14,  1.69s/it] 23%|██▎       | 2280/9822 [1:10:15<3:32:06,  1.69s/it] 23%|██▎       | 2281/9822 [1:10:17<3:31:57,  1.69s/it] 23%|██▎       | 2282/9822 [1:10:18<3:31:50,  1.69s/it] 23%|██▎       | 2283/9822 [1:10:20<3:31:31,  1.68s/it] 23%|██▎       | 2284/9822 [1:10:22<3:31:35,  1.68s/it] 23%|██▎       | 2285/9822 [1:10:23<3:31:32,  1.68s/it] 23%|██▎       | 2286/9822 [1:10:25<3:31:24,  1.68s/it] 23%|██▎       | 2287/9822 [1:10:27<3:31:28,  1.68s/it] 23%|██▎       | 2288/9822 [1:10:28<3:31:31,  1.68s/it] 23%|██▎       | 2289/9822 [1:10:30<3:31:26,  1.68s/it] 23%|██▎       | 2290/9822 [1:10:32<3:31:24,  1.68s/it] 23%|██▎       | 2291/9822 [1:10:33<3:31:22,  1.68s/it] 23%|██▎       | 2292/9822 [1:10:35<3:31:30,  1.69s/it] 23%|██▎       | 2293/9822 [1:10:37<3:31:33,  1.69s/it] 23%|██▎       | 2294/9822 [1:10:38<3:31:28,  1.69s/it] 23%|██▎       | 2295/9822 [1:10:40<3:31:32,  1.69s/it] 23%|██▎       | 2296/9822 [1:10:42<3:31:34,  1.69s/it] 23%|██▎       | 2297/9822 [1:10:43<3:31:26,  1.69s/it] 23%|██▎       | 2298/9822 [1:10:45<3:31:22,  1.69s/it] 23%|██▎       | 2299/9822 [1:10:47<3:31:08,  1.68s/it] 23%|██▎       | 2300/9822 [1:10:49<3:31:22,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:51:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:51:35 - INFO - __main__ - ***** test Results*****
04/29/2024 12:51:35 - INFO - __main__ -   Training step = 2300
04/29/2024 12:51:35 - INFO - __main__ -  test_accuracy:0.859809663250366 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:51:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:51:40 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:51:40 - INFO - __main__ -   Training step = 2300
04/29/2024 12:51:40 - INFO - __main__ -  eval_accuracy:0.8425485170267302 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8429146832662029}
test:
{'accuracy': 0.8565153733528551}
04/29/2024 12:51:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:51:48 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:51:48 - INFO - __main__ -   Training step = 2300
04/29/2024 12:51:48 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 23%|██▎       | 2301/9822 [1:11:08<14:34:18,  6.97s/it] 23%|██▎       | 2302/9822 [1:11:10<11:14:19,  5.38s/it] 23%|██▎       | 2303/9822 [1:11:11<8:55:04,  4.27s/it]  23%|██▎       | 2304/9822 [1:11:13<7:17:12,  3.49s/it] 23%|██▎       | 2305/9822 [1:11:15<6:08:49,  2.94s/it] 23%|██▎       | 2306/9822 [1:11:16<5:21:00,  2.56s/it] 23%|██▎       | 2307/9822 [1:11:18<4:47:03,  2.29s/it] 23%|██▎       | 2308/9822 [1:11:20<4:23:49,  2.11s/it] 24%|██▎       | 2309/9822 [1:11:21<4:07:46,  1.98s/it] 24%|██▎       | 2310/9822 [1:11:23<3:55:53,  1.88s/it] 24%|██▎       | 2311/9822 [1:11:25<3:47:33,  1.82s/it] 24%|██▎       | 2312/9822 [1:11:26<3:41:52,  1.77s/it] 24%|██▎       | 2313/9822 [1:11:28<3:38:23,  1.74s/it] 24%|██▎       | 2314/9822 [1:11:30<3:35:57,  1.73s/it] 24%|██▎       | 2315/9822 [1:11:31<3:33:53,  1.71s/it] 24%|██▎       | 2316/9822 [1:11:33<3:32:56,  1.70s/it] 24%|██▎       | 2317/9822 [1:11:35<3:32:05,  1.70s/it] 24%|██▎       | 2318/9822 [1:11:36<3:31:44,  1.69s/it] 24%|██▎       | 2319/9822 [1:11:38<3:31:21,  1.69s/it] 24%|██▎       | 2320/9822 [1:11:40<3:30:57,  1.69s/it] 24%|██▎       | 2321/9822 [1:11:41<3:30:19,  1.68s/it] 24%|██▎       | 2322/9822 [1:11:43<3:27:56,  1.66s/it] 24%|██▎       | 2323/9822 [1:11:45<3:28:36,  1.67s/it] 24%|██▎       | 2324/9822 [1:11:46<3:28:37,  1.67s/it] 24%|██▎       | 2325/9822 [1:11:48<3:28:23,  1.67s/it] 24%|██▎       | 2326/9822 [1:11:50<3:28:05,  1.67s/it] 24%|██▎       | 2327/9822 [1:11:51<3:32:27,  1.70s/it] 24%|██▎       | 2328/9822 [1:11:53<3:30:47,  1.69s/it] 24%|██▎       | 2329/9822 [1:11:55<3:29:44,  1.68s/it] 24%|██▎       | 2330/9822 [1:11:56<3:29:12,  1.68s/it] 24%|██▎       | 2331/9822 [1:11:58<3:28:38,  1.67s/it] 24%|██▎       | 2332/9822 [1:12:00<3:28:08,  1.67s/it] 24%|██▍       | 2333/9822 [1:12:01<3:27:46,  1.66s/it] 24%|██▍       | 2334/9822 [1:12:03<3:27:20,  1.66s/it] 24%|██▍       | 2335/9822 [1:12:05<3:27:11,  1.66s/it] 24%|██▍       | 2336/9822 [1:12:06<3:27:09,  1.66s/it] 24%|██▍       | 2337/9822 [1:12:08<3:27:18,  1.66s/it] 24%|██▍       | 2338/9822 [1:12:10<3:27:23,  1.66s/it] 24%|██▍       | 2339/9822 [1:12:11<3:27:39,  1.66s/it] 24%|██▍       | 2340/9822 [1:12:13<3:27:53,  1.67s/it] 24%|██▍       | 2341/9822 [1:12:15<3:27:54,  1.67s/it] 24%|██▍       | 2342/9822 [1:12:16<3:27:53,  1.67s/it] 24%|██▍       | 2343/9822 [1:12:18<3:27:33,  1.67s/it] 24%|██▍       | 2344/9822 [1:12:20<3:27:48,  1.67s/it] 24%|██▍       | 2345/9822 [1:12:21<3:28:06,  1.67s/it] 24%|██▍       | 2346/9822 [1:12:23<3:27:44,  1.67s/it] 24%|██▍       | 2347/9822 [1:12:25<3:28:01,  1.67s/it] 24%|██▍       | 2348/9822 [1:12:26<3:27:53,  1.67s/it] 24%|██▍       | 2349/9822 [1:12:28<3:27:36,  1.67s/it] 24%|██▍       | 2350/9822 [1:12:30<3:28:18,  1.67s/it] 24%|██▍       | 2351/9822 [1:12:31<3:28:21,  1.67s/it] 24%|██▍       | 2352/9822 [1:12:33<3:28:41,  1.68s/it] 24%|██▍       | 2353/9822 [1:12:35<3:28:38,  1.68s/it] 24%|██▍       | 2354/9822 [1:12:36<3:28:33,  1.68s/it] 24%|██▍       | 2355/9822 [1:12:38<3:28:02,  1.67s/it] 24%|██▍       | 2356/9822 [1:12:40<3:27:30,  1.67s/it] 24%|██▍       | 2357/9822 [1:12:41<3:27:03,  1.66s/it] 24%|██▍       | 2358/9822 [1:12:43<3:26:49,  1.66s/it] 24%|██▍       | 2359/9822 [1:12:45<3:26:58,  1.66s/it] 24%|██▍       | 2360/9822 [1:12:47<3:31:26,  1.70s/it] 24%|██▍       | 2361/9822 [1:12:48<3:30:40,  1.69s/it] 24%|██▍       | 2362/9822 [1:12:50<3:29:48,  1.69s/it] 24%|██▍       | 2363/9822 [1:12:52<3:28:52,  1.68s/it] 24%|██▍       | 2364/9822 [1:12:53<3:28:16,  1.68s/it] 24%|██▍       | 2365/9822 [1:12:55<3:27:52,  1.67s/it] 24%|██▍       | 2366/9822 [1:12:57<3:27:21,  1.67s/it] 24%|██▍       | 2367/9822 [1:12:58<3:27:45,  1.67s/it] 24%|██▍       | 2368/9822 [1:13:00<3:27:30,  1.67s/it] 24%|██▍       | 2369/9822 [1:13:02<3:27:22,  1.67s/it] 24%|██▍       | 2370/9822 [1:13:03<3:27:29,  1.67s/it] 24%|██▍       | 2371/9822 [1:13:05<3:28:06,  1.68s/it] 24%|██▍       | 2372/9822 [1:13:07<3:27:56,  1.67s/it] 24%|██▍       | 2373/9822 [1:13:08<3:27:47,  1.67s/it] 24%|██▍       | 2374/9822 [1:13:10<3:27:44,  1.67s/it] 24%|██▍       | 2375/9822 [1:13:12<3:27:32,  1.67s/it] 24%|██▍       | 2376/9822 [1:13:13<3:27:19,  1.67s/it] 24%|██▍       | 2377/9822 [1:13:15<3:27:30,  1.67s/it] 24%|██▍       | 2378/9822 [1:13:17<3:27:43,  1.67s/it] 24%|██▍       | 2379/9822 [1:13:18<3:27:54,  1.68s/it] 24%|██▍       | 2380/9822 [1:13:20<3:28:01,  1.68s/it] 24%|██▍       | 2381/9822 [1:13:22<3:28:24,  1.68s/it] 24%|██▍       | 2382/9822 [1:13:23<3:31:38,  1.71s/it] 24%|██▍       | 2383/9822 [1:13:25<3:30:37,  1.70s/it] 24%|██▍       | 2384/9822 [1:13:27<3:29:29,  1.69s/it] 24%|██▍       | 2385/9822 [1:13:28<3:28:25,  1.68s/it] 24%|██▍       | 2386/9822 [1:13:30<3:28:06,  1.68s/it] 24%|██▍       | 2387/9822 [1:13:32<3:27:33,  1.67s/it] 24%|██▍       | 2388/9822 [1:13:33<3:27:40,  1.68s/it] 24%|██▍       | 2389/9822 [1:13:35<3:27:15,  1.67s/it] 24%|██▍       | 2390/9822 [1:13:37<3:26:47,  1.67s/it] 24%|██▍       | 2391/9822 [1:13:38<3:26:13,  1.67s/it] 24%|██▍       | 2392/9822 [1:13:40<3:26:31,  1.67s/it] 24%|██▍       | 2393/9822 [1:13:42<3:26:46,  1.67s/it] 24%|██▍       | 2394/9822 [1:13:43<3:26:33,  1.67s/it] 24%|██▍       | 2395/9822 [1:13:45<3:26:47,  1.67s/it] 24%|██▍       | 2396/9822 [1:13:47<3:26:12,  1.67s/it] 24%|██▍       | 2397/9822 [1:13:48<3:26:06,  1.67s/it] 24%|██▍       | 2398/9822 [1:13:50<3:26:10,  1.67s/it] 24%|██▍       | 2399/9822 [1:13:52<3:26:23,  1.67s/it] 24%|██▍       | 2400/9822 [1:13:53<3:26:52,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:54:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:54:40 - INFO - __main__ - ***** test Results*****
04/29/2024 12:54:40 - INFO - __main__ -   Training step = 2400
04/29/2024 12:54:40 - INFO - __main__ -  test_accuracy:0.8601756954612005 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:54:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:54:45 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:54:45 - INFO - __main__ -   Training step = 2400
04/29/2024 12:54:45 - INFO - __main__ -  eval_accuracy:0.8458440131819847 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 12:54:45,169 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 12:54:45,169 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 12:54:45,210 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 12:54:46,326 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8458440131819847}
test:
{'accuracy': 0.8601756954612005}
04/29/2024 12:54:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:54:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:54:54 - INFO - __main__ -   Training step = 2400
04/29/2024 12:54:54 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 24%|██▍       | 2401/9822 [1:14:14<15:01:05,  7.29s/it] 24%|██▍       | 2402/9822 [1:14:16<11:32:56,  5.60s/it] 24%|██▍       | 2403/9822 [1:14:17<9:06:37,  4.42s/it]  24%|██▍       | 2404/9822 [1:14:19<7:24:14,  3.59s/it] 24%|██▍       | 2405/9822 [1:14:21<6:13:04,  3.02s/it] 24%|██▍       | 2406/9822 [1:14:22<5:22:45,  2.61s/it] 25%|██▍       | 2407/9822 [1:14:24<4:48:05,  2.33s/it] 25%|██▍       | 2408/9822 [1:14:25<4:21:50,  2.12s/it] 25%|██▍       | 2409/9822 [1:14:27<4:05:23,  1.99s/it] 25%|██▍       | 2410/9822 [1:14:29<3:57:43,  1.92s/it] 25%|██▍       | 2411/9822 [1:14:31<3:48:34,  1.85s/it] 25%|██▍       | 2412/9822 [1:14:32<3:42:16,  1.80s/it] 25%|██▍       | 2413/9822 [1:14:34<3:37:36,  1.76s/it] 25%|██▍       | 2414/9822 [1:14:36<3:34:06,  1.73s/it] 25%|██▍       | 2415/9822 [1:14:37<3:31:22,  1.71s/it] 25%|██▍       | 2416/9822 [1:14:39<3:29:39,  1.70s/it] 25%|██▍       | 2417/9822 [1:14:41<3:28:51,  1.69s/it] 25%|██▍       | 2418/9822 [1:14:42<3:28:29,  1.69s/it] 25%|██▍       | 2419/9822 [1:14:44<3:27:50,  1.68s/it] 25%|██▍       | 2420/9822 [1:14:46<3:27:17,  1.68s/it] 25%|██▍       | 2421/9822 [1:14:47<3:27:07,  1.68s/it] 25%|██▍       | 2422/9822 [1:14:49<3:26:40,  1.68s/it] 25%|██▍       | 2423/9822 [1:14:51<3:26:29,  1.67s/it] 25%|██▍       | 2424/9822 [1:14:52<3:26:14,  1.67s/it] 25%|██▍       | 2425/9822 [1:14:54<3:26:13,  1.67s/it] 25%|██▍       | 2426/9822 [1:14:56<3:26:27,  1.67s/it] 25%|██▍       | 2427/9822 [1:14:57<3:26:25,  1.67s/it] 25%|██▍       | 2428/9822 [1:14:59<3:26:05,  1.67s/it] 25%|██▍       | 2429/9822 [1:15:01<3:25:49,  1.67s/it] 25%|██▍       | 2430/9822 [1:15:02<3:26:09,  1.67s/it] 25%|██▍       | 2431/9822 [1:15:04<3:26:34,  1.68s/it] 25%|██▍       | 2432/9822 [1:15:06<3:26:22,  1.68s/it] 25%|██▍       | 2433/9822 [1:15:07<3:25:57,  1.67s/it] 25%|██▍       | 2434/9822 [1:15:09<3:26:02,  1.67s/it] 25%|██▍       | 2435/9822 [1:15:11<3:26:14,  1.68s/it] 25%|██▍       | 2436/9822 [1:15:12<3:26:17,  1.68s/it] 25%|██▍       | 2437/9822 [1:15:14<3:25:51,  1.67s/it] 25%|██▍       | 2438/9822 [1:15:16<3:25:49,  1.67s/it] 25%|██▍       | 2439/9822 [1:15:17<3:26:01,  1.67s/it] 25%|██▍       | 2440/9822 [1:15:19<3:25:52,  1.67s/it] 25%|██▍       | 2441/9822 [1:15:21<3:26:10,  1.68s/it] 25%|██▍       | 2442/9822 [1:15:22<3:25:52,  1.67s/it] 25%|██▍       | 2443/9822 [1:15:24<3:25:54,  1.67s/it] 25%|██▍       | 2444/9822 [1:15:26<3:26:05,  1.68s/it] 25%|██▍       | 2445/9822 [1:15:28<3:25:53,  1.67s/it] 25%|██▍       | 2446/9822 [1:15:29<3:25:40,  1.67s/it] 25%|██▍       | 2447/9822 [1:15:31<3:26:00,  1.68s/it] 25%|██▍       | 2448/9822 [1:15:33<3:25:53,  1.68s/it] 25%|██▍       | 2449/9822 [1:15:34<3:25:26,  1.67s/it] 25%|██▍       | 2450/9822 [1:15:36<3:29:22,  1.70s/it] 25%|██▍       | 2451/9822 [1:15:38<3:28:01,  1.69s/it] 25%|██▍       | 2452/9822 [1:15:39<3:26:54,  1.68s/it] 25%|██▍       | 2453/9822 [1:15:41<3:26:11,  1.68s/it] 25%|██▍       | 2454/9822 [1:15:43<3:25:25,  1.67s/it] 25%|██▍       | 2455/9822 [1:15:44<3:25:08,  1.67s/it] 25%|██▌       | 2456/9822 [1:15:46<3:25:14,  1.67s/it] 25%|██▌       | 2457/9822 [1:15:48<3:25:39,  1.68s/it] 25%|██▌       | 2458/9822 [1:15:49<3:25:25,  1.67s/it] 25%|██▌       | 2459/9822 [1:15:51<3:25:24,  1.67s/it] 25%|██▌       | 2460/9822 [1:15:53<3:25:43,  1.68s/it] 25%|██▌       | 2461/9822 [1:15:54<3:25:21,  1.67s/it] 25%|██▌       | 2462/9822 [1:15:56<3:25:10,  1.67s/it] 25%|██▌       | 2463/9822 [1:15:58<3:25:07,  1.67s/it] 25%|██▌       | 2464/9822 [1:15:59<3:25:13,  1.67s/it] 25%|██▌       | 2465/9822 [1:16:01<3:25:04,  1.67s/it] 25%|██▌       | 2466/9822 [1:16:03<3:25:04,  1.67s/it] 25%|██▌       | 2467/9822 [1:16:04<3:24:42,  1.67s/it] 25%|██▌       | 2468/9822 [1:16:06<3:24:17,  1.67s/it] 25%|██▌       | 2469/9822 [1:16:08<3:24:48,  1.67s/it] 25%|██▌       | 2470/9822 [1:16:09<3:24:54,  1.67s/it] 25%|██▌       | 2471/9822 [1:16:11<3:24:35,  1.67s/it] 25%|██▌       | 2472/9822 [1:16:13<3:24:35,  1.67s/it] 25%|██▌       | 2473/9822 [1:16:14<3:24:17,  1.67s/it] 25%|██▌       | 2474/9822 [1:16:16<3:24:28,  1.67s/it] 25%|██▌       | 2475/9822 [1:16:18<3:24:42,  1.67s/it] 25%|██▌       | 2476/9822 [1:16:19<3:24:34,  1.67s/it] 25%|██▌       | 2477/9822 [1:16:21<3:28:12,  1.70s/it] 25%|██▌       | 2478/9822 [1:16:23<3:27:26,  1.69s/it] 25%|██▌       | 2479/9822 [1:16:25<3:26:11,  1.68s/it] 25%|██▌       | 2480/9822 [1:16:26<3:25:38,  1.68s/it] 25%|██▌       | 2481/9822 [1:16:28<3:25:10,  1.68s/it] 25%|██▌       | 2482/9822 [1:16:30<3:24:31,  1.67s/it] 25%|██▌       | 2483/9822 [1:16:31<3:24:23,  1.67s/it] 25%|██▌       | 2484/9822 [1:16:33<3:24:29,  1.67s/it] 25%|██▌       | 2485/9822 [1:16:35<3:24:37,  1.67s/it] 25%|██▌       | 2486/9822 [1:16:36<3:24:43,  1.67s/it] 25%|██▌       | 2487/9822 [1:16:38<3:24:53,  1.68s/it] 25%|██▌       | 2488/9822 [1:16:40<3:24:29,  1.67s/it] 25%|██▌       | 2489/9822 [1:16:41<3:24:53,  1.68s/it] 25%|██▌       | 2490/9822 [1:16:43<3:24:53,  1.68s/it] 25%|██▌       | 2491/9822 [1:16:45<3:25:15,  1.68s/it] 25%|██▌       | 2492/9822 [1:16:46<3:24:59,  1.68s/it] 25%|██▌       | 2493/9822 [1:16:48<3:24:48,  1.68s/it] 25%|██▌       | 2494/9822 [1:16:50<3:22:37,  1.66s/it] 25%|██▌       | 2495/9822 [1:16:51<3:23:14,  1.66s/it] 25%|██▌       | 2496/9822 [1:16:53<3:23:27,  1.67s/it] 25%|██▌       | 2497/9822 [1:16:55<3:23:44,  1.67s/it] 25%|██▌       | 2498/9822 [1:16:56<3:24:25,  1.67s/it] 25%|██▌       | 2499/9822 [1:16:58<3:24:18,  1.67s/it] 25%|██▌       | 2500/9822 [1:17:00<3:24:22,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 12:57:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:57:46 - INFO - __main__ - ***** test Results*****
04/29/2024 12:57:46 - INFO - __main__ -   Training step = 2500
04/29/2024 12:57:46 - INFO - __main__ -  test_accuracy:0.8451683748169839 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:57:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:57:51 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 12:57:51 - INFO - __main__ -   Training step = 2500
04/29/2024 12:57:51 - INFO - __main__ -  eval_accuracy:0.8352251922372758 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8458440131819847}
test:
{'accuracy': 0.8601756954612005}
04/29/2024 12:57:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 12:57:59 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 12:57:59 - INFO - __main__ -   Training step = 2500
04/29/2024 12:57:59 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 25%|██▌       | 2501/9822 [1:17:19<14:07:09,  6.94s/it] 25%|██▌       | 2502/9822 [1:17:21<10:54:24,  5.36s/it] 25%|██▌       | 2503/9822 [1:17:22<8:39:40,  4.26s/it]  25%|██▌       | 2504/9822 [1:17:24<7:09:07,  3.52s/it] 26%|██▌       | 2505/9822 [1:17:26<6:01:30,  2.96s/it] 26%|██▌       | 2506/9822 [1:17:27<5:14:04,  2.58s/it] 26%|██▌       | 2507/9822 [1:17:29<4:40:58,  2.30s/it] 26%|██▌       | 2508/9822 [1:17:31<4:17:36,  2.11s/it] 26%|██▌       | 2509/9822 [1:17:32<4:01:28,  1.98s/it] 26%|██▌       | 2510/9822 [1:17:34<3:49:47,  1.89s/it] 26%|██▌       | 2511/9822 [1:17:36<3:41:49,  1.82s/it] 26%|██▌       | 2512/9822 [1:17:37<3:35:49,  1.77s/it] 26%|██▌       | 2513/9822 [1:17:39<3:32:10,  1.74s/it] 26%|██▌       | 2514/9822 [1:17:41<3:29:15,  1.72s/it] 26%|██▌       | 2515/9822 [1:17:42<3:27:42,  1.71s/it] 26%|██▌       | 2516/9822 [1:17:44<3:26:14,  1.69s/it] 26%|██▌       | 2517/9822 [1:17:46<3:25:27,  1.69s/it] 26%|██▌       | 2518/9822 [1:17:47<3:25:07,  1.69s/it] 26%|██▌       | 2519/9822 [1:17:49<3:24:23,  1.68s/it] 26%|██▌       | 2520/9822 [1:17:51<3:24:30,  1.68s/it] 26%|██▌       | 2521/9822 [1:17:52<3:24:04,  1.68s/it] 26%|██▌       | 2522/9822 [1:17:54<3:23:34,  1.67s/it] 26%|██▌       | 2523/9822 [1:17:56<3:23:36,  1.67s/it] 26%|██▌       | 2524/9822 [1:17:57<3:23:32,  1.67s/it] 26%|██▌       | 2525/9822 [1:17:59<3:23:22,  1.67s/it] 26%|██▌       | 2526/9822 [1:18:01<3:23:16,  1.67s/it] 26%|██▌       | 2527/9822 [1:18:02<3:23:25,  1.67s/it] 26%|██▌       | 2528/9822 [1:18:04<3:23:35,  1.67s/it] 26%|██▌       | 2529/9822 [1:18:06<3:23:28,  1.67s/it] 26%|██▌       | 2530/9822 [1:18:08<3:27:45,  1.71s/it] 26%|██▌       | 2531/9822 [1:18:09<3:26:09,  1.70s/it] 26%|██▌       | 2532/9822 [1:18:11<3:25:25,  1.69s/it] 26%|██▌       | 2533/9822 [1:18:13<3:25:09,  1.69s/it] 26%|██▌       | 2534/9822 [1:18:14<3:24:35,  1.68s/it] 26%|██▌       | 2535/9822 [1:18:16<3:24:02,  1.68s/it] 26%|██▌       | 2536/9822 [1:18:18<3:23:35,  1.68s/it] 26%|██▌       | 2537/9822 [1:18:19<3:23:06,  1.67s/it] 26%|██▌       | 2538/9822 [1:18:21<3:23:06,  1.67s/it] 26%|██▌       | 2539/9822 [1:18:23<3:22:37,  1.67s/it] 26%|██▌       | 2540/9822 [1:18:24<3:22:47,  1.67s/it] 26%|██▌       | 2541/9822 [1:18:26<3:22:43,  1.67s/it] 26%|██▌       | 2542/9822 [1:18:28<3:22:45,  1.67s/it] 26%|██▌       | 2543/9822 [1:18:29<3:22:34,  1.67s/it] 26%|██▌       | 2544/9822 [1:18:31<3:22:39,  1.67s/it] 26%|██▌       | 2545/9822 [1:18:33<3:22:51,  1.67s/it] 26%|██▌       | 2546/9822 [1:18:34<3:22:47,  1.67s/it] 26%|██▌       | 2547/9822 [1:18:36<3:22:39,  1.67s/it] 26%|██▌       | 2548/9822 [1:18:38<3:23:01,  1.67s/it] 26%|██▌       | 2549/9822 [1:18:39<3:22:48,  1.67s/it] 26%|██▌       | 2550/9822 [1:18:41<3:22:43,  1.67s/it] 26%|██▌       | 2551/9822 [1:18:43<3:23:06,  1.68s/it] 26%|██▌       | 2552/9822 [1:18:44<3:22:59,  1.68s/it] 26%|██▌       | 2553/9822 [1:18:46<3:22:54,  1.67s/it] 26%|██▌       | 2554/9822 [1:18:48<3:22:45,  1.67s/it] 26%|██▌       | 2555/9822 [1:18:49<3:22:25,  1.67s/it] 26%|██▌       | 2556/9822 [1:18:51<3:22:33,  1.67s/it] 26%|██▌       | 2557/9822 [1:18:53<3:22:55,  1.68s/it] 26%|██▌       | 2558/9822 [1:18:54<3:22:30,  1.67s/it] 26%|██▌       | 2559/9822 [1:18:56<3:22:34,  1.67s/it] 26%|██▌       | 2560/9822 [1:18:58<3:22:24,  1.67s/it] 26%|██▌       | 2561/9822 [1:18:59<3:22:36,  1.67s/it] 26%|██▌       | 2562/9822 [1:19:01<3:22:47,  1.68s/it] 26%|██▌       | 2563/9822 [1:19:03<3:26:17,  1.71s/it] 26%|██▌       | 2564/9822 [1:19:05<3:24:55,  1.69s/it] 26%|██▌       | 2565/9822 [1:19:06<3:23:40,  1.68s/it] 26%|██▌       | 2566/9822 [1:19:08<3:23:14,  1.68s/it] 26%|██▌       | 2567/9822 [1:19:10<3:23:04,  1.68s/it] 26%|██▌       | 2568/9822 [1:19:11<3:22:56,  1.68s/it] 26%|██▌       | 2569/9822 [1:19:13<3:22:38,  1.68s/it] 26%|██▌       | 2570/9822 [1:19:15<3:21:54,  1.67s/it] 26%|██▌       | 2571/9822 [1:19:16<3:22:15,  1.67s/it] 26%|██▌       | 2572/9822 [1:19:18<3:21:56,  1.67s/it] 26%|██▌       | 2573/9822 [1:19:20<3:21:39,  1.67s/it] 26%|██▌       | 2574/9822 [1:19:21<3:21:32,  1.67s/it] 26%|██▌       | 2575/9822 [1:19:23<3:21:46,  1.67s/it] 26%|██▌       | 2576/9822 [1:19:25<3:21:17,  1.67s/it] 26%|██▌       | 2577/9822 [1:19:26<3:21:45,  1.67s/it] 26%|██▌       | 2578/9822 [1:19:28<3:22:00,  1.67s/it] 26%|██▋       | 2579/9822 [1:19:30<3:21:19,  1.67s/it] 26%|██▋       | 2580/9822 [1:19:31<3:19:20,  1.65s/it] 26%|██▋       | 2581/9822 [1:19:33<3:19:26,  1.65s/it] 26%|██▋       | 2582/9822 [1:19:35<3:19:52,  1.66s/it] 26%|██▋       | 2583/9822 [1:19:36<3:20:02,  1.66s/it] 26%|██▋       | 2584/9822 [1:19:38<3:20:28,  1.66s/it] 26%|██▋       | 2585/9822 [1:19:40<3:24:59,  1.70s/it] 26%|██▋       | 2586/9822 [1:19:41<3:24:23,  1.69s/it] 26%|██▋       | 2587/9822 [1:19:43<3:23:37,  1.69s/it] 26%|██▋       | 2588/9822 [1:19:45<3:22:43,  1.68s/it] 26%|██▋       | 2589/9822 [1:19:46<3:21:56,  1.68s/it] 26%|██▋       | 2590/9822 [1:19:48<3:21:41,  1.67s/it] 26%|██▋       | 2591/9822 [1:19:50<3:21:32,  1.67s/it] 26%|██▋       | 2592/9822 [1:19:51<3:21:24,  1.67s/it] 26%|██▋       | 2593/9822 [1:19:53<3:21:06,  1.67s/it] 26%|██▋       | 2594/9822 [1:19:55<3:20:58,  1.67s/it] 26%|██▋       | 2595/9822 [1:19:56<3:20:29,  1.66s/it] 26%|██▋       | 2596/9822 [1:19:58<3:20:42,  1.67s/it] 26%|██▋       | 2597/9822 [1:20:00<3:20:41,  1.67s/it] 26%|██▋       | 2598/9822 [1:20:01<3:20:34,  1.67s/it] 26%|██▋       | 2599/9822 [1:20:03<3:20:50,  1.67s/it] 26%|██▋       | 2600/9822 [1:20:05<3:21:04,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:00:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:00:51 - INFO - __main__ - ***** test Results*****
04/29/2024 13:00:51 - INFO - __main__ -   Training step = 2600
04/29/2024 13:00:51 - INFO - __main__ -  test_accuracy:0.8634699853587116 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:00:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:00:56 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:00:56 - INFO - __main__ -   Training step = 2600
04/29/2024 13:00:56 - INFO - __main__ -  eval_accuracy:0.844013181984621 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8458440131819847}
test:
{'accuracy': 0.8601756954612005}
04/29/2024 13:01:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:01:04 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:01:04 - INFO - __main__ -   Training step = 2600
04/29/2024 13:01:04 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 26%|██▋       | 2601/9822 [1:20:24<13:54:06,  6.93s/it] 26%|██▋       | 2602/9822 [1:20:26<10:44:10,  5.35s/it] 27%|██▋       | 2603/9822 [1:20:27<8:31:07,  4.25s/it]  27%|██▋       | 2604/9822 [1:20:29<6:58:02,  3.47s/it] 27%|██▋       | 2605/9822 [1:20:31<5:52:58,  2.93s/it] 27%|██▋       | 2606/9822 [1:20:32<5:07:33,  2.56s/it] 27%|██▋       | 2607/9822 [1:20:34<4:35:35,  2.29s/it] 27%|██▋       | 2608/9822 [1:20:36<4:13:45,  2.11s/it] 27%|██▋       | 2609/9822 [1:20:37<3:58:11,  1.98s/it] 27%|██▋       | 2610/9822 [1:20:39<3:47:07,  1.89s/it] 27%|██▋       | 2611/9822 [1:20:41<3:43:00,  1.86s/it] 27%|██▋       | 2612/9822 [1:20:42<3:36:37,  1.80s/it] 27%|██▋       | 2613/9822 [1:20:44<3:32:29,  1.77s/it] 27%|██▋       | 2614/9822 [1:20:46<3:28:55,  1.74s/it] 27%|██▋       | 2615/9822 [1:20:47<3:26:31,  1.72s/it] 27%|██▋       | 2616/9822 [1:20:49<3:24:36,  1.70s/it] 27%|██▋       | 2617/9822 [1:20:51<3:23:17,  1.69s/it] 27%|██▋       | 2618/9822 [1:20:52<3:22:26,  1.69s/it] 27%|██▋       | 2619/9822 [1:20:54<3:22:03,  1.68s/it] 27%|██▋       | 2620/9822 [1:20:56<3:21:16,  1.68s/it] 27%|██▋       | 2621/9822 [1:20:57<3:21:04,  1.68s/it] 27%|██▋       | 2622/9822 [1:20:59<3:20:36,  1.67s/it] 27%|██▋       | 2623/9822 [1:21:01<3:20:47,  1.67s/it] 27%|██▋       | 2624/9822 [1:21:02<3:20:58,  1.68s/it] 27%|██▋       | 2625/9822 [1:21:04<3:20:50,  1.67s/it] 27%|██▋       | 2626/9822 [1:21:06<3:20:41,  1.67s/it] 27%|██▋       | 2627/9822 [1:21:08<3:20:39,  1.67s/it] 27%|██▋       | 2628/9822 [1:21:09<3:20:52,  1.68s/it] 27%|██▋       | 2629/9822 [1:21:11<3:20:58,  1.68s/it] 27%|██▋       | 2630/9822 [1:21:13<3:21:13,  1.68s/it] 27%|██▋       | 2631/9822 [1:21:14<3:21:00,  1.68s/it] 27%|██▋       | 2632/9822 [1:21:16<3:20:29,  1.67s/it] 27%|██▋       | 2633/9822 [1:21:18<3:20:49,  1.68s/it] 27%|██▋       | 2634/9822 [1:21:19<3:20:38,  1.67s/it] 27%|██▋       | 2635/9822 [1:21:21<3:20:51,  1.68s/it] 27%|██▋       | 2636/9822 [1:21:23<3:20:42,  1.68s/it] 27%|██▋       | 2637/9822 [1:21:24<3:20:34,  1.67s/it] 27%|██▋       | 2638/9822 [1:21:26<3:24:13,  1.71s/it] 27%|██▋       | 2639/9822 [1:21:28<3:23:06,  1.70s/it] 27%|██▋       | 2640/9822 [1:21:29<3:22:49,  1.69s/it] 27%|██▋       | 2641/9822 [1:21:31<3:21:57,  1.69s/it] 27%|██▋       | 2642/9822 [1:21:33<3:21:53,  1.69s/it] 27%|██▋       | 2643/9822 [1:21:34<3:21:53,  1.69s/it] 27%|██▋       | 2644/9822 [1:21:36<3:20:56,  1.68s/it] 27%|██▋       | 2645/9822 [1:21:38<3:21:08,  1.68s/it] 27%|██▋       | 2646/9822 [1:21:39<3:20:22,  1.68s/it] 27%|██▋       | 2647/9822 [1:21:41<3:20:27,  1.68s/it] 27%|██▋       | 2648/9822 [1:21:43<3:20:33,  1.68s/it] 27%|██▋       | 2649/9822 [1:21:45<3:20:39,  1.68s/it] 27%|██▋       | 2650/9822 [1:21:46<3:20:19,  1.68s/it] 27%|██▋       | 2651/9822 [1:21:48<3:20:18,  1.68s/it] 27%|██▋       | 2652/9822 [1:21:50<3:20:04,  1.67s/it] 27%|██▋       | 2653/9822 [1:21:51<3:20:15,  1.68s/it] 27%|██▋       | 2654/9822 [1:21:53<3:20:13,  1.68s/it] 27%|██▋       | 2655/9822 [1:21:55<3:20:10,  1.68s/it] 27%|██▋       | 2656/9822 [1:21:56<3:19:48,  1.67s/it] 27%|██▋       | 2657/9822 [1:21:58<3:19:33,  1.67s/it] 27%|██▋       | 2658/9822 [1:22:00<3:19:40,  1.67s/it] 27%|██▋       | 2659/9822 [1:22:01<3:20:02,  1.68s/it] 27%|██▋       | 2660/9822 [1:22:03<3:20:22,  1.68s/it] 27%|██▋       | 2661/9822 [1:22:05<3:20:02,  1.68s/it] 27%|██▋       | 2662/9822 [1:22:06<3:19:39,  1.67s/it] 27%|██▋       | 2663/9822 [1:22:08<3:19:16,  1.67s/it] 27%|██▋       | 2664/9822 [1:22:10<3:19:18,  1.67s/it] 27%|██▋       | 2665/9822 [1:22:11<3:19:27,  1.67s/it] 27%|██▋       | 2666/9822 [1:22:13<3:17:21,  1.65s/it] 27%|██▋       | 2667/9822 [1:22:15<3:18:09,  1.66s/it] 27%|██▋       | 2668/9822 [1:22:16<3:18:46,  1.67s/it] 27%|██▋       | 2669/9822 [1:22:18<3:19:12,  1.67s/it] 27%|██▋       | 2670/9822 [1:22:20<3:19:25,  1.67s/it] 27%|██▋       | 2671/9822 [1:22:21<3:23:10,  1.70s/it] 27%|██▋       | 2672/9822 [1:22:23<3:21:41,  1.69s/it] 27%|██▋       | 2673/9822 [1:22:25<3:21:13,  1.69s/it] 27%|██▋       | 2674/9822 [1:22:26<3:20:51,  1.69s/it] 27%|██▋       | 2675/9822 [1:22:28<3:20:24,  1.68s/it] 27%|██▋       | 2676/9822 [1:22:30<3:20:03,  1.68s/it] 27%|██▋       | 2677/9822 [1:22:31<3:19:57,  1.68s/it] 27%|██▋       | 2678/9822 [1:22:33<3:20:02,  1.68s/it] 27%|██▋       | 2679/9822 [1:22:35<3:19:35,  1.68s/it] 27%|██▋       | 2680/9822 [1:22:36<3:19:36,  1.68s/it] 27%|██▋       | 2681/9822 [1:22:38<3:19:30,  1.68s/it] 27%|██▋       | 2682/9822 [1:22:40<3:19:09,  1.67s/it] 27%|██▋       | 2683/9822 [1:22:41<3:19:15,  1.67s/it] 27%|██▋       | 2684/9822 [1:22:43<3:19:23,  1.68s/it] 27%|██▋       | 2685/9822 [1:22:45<3:19:05,  1.67s/it] 27%|██▋       | 2686/9822 [1:22:46<3:18:48,  1.67s/it] 27%|██▋       | 2687/9822 [1:22:48<3:18:41,  1.67s/it] 27%|██▋       | 2688/9822 [1:22:50<3:18:52,  1.67s/it] 27%|██▋       | 2689/9822 [1:22:52<3:18:56,  1.67s/it] 27%|██▋       | 2690/9822 [1:22:53<3:19:01,  1.67s/it] 27%|██▋       | 2691/9822 [1:22:55<3:18:53,  1.67s/it] 27%|██▋       | 2692/9822 [1:22:57<3:19:00,  1.67s/it] 27%|██▋       | 2693/9822 [1:22:58<3:22:32,  1.70s/it] 27%|██▋       | 2694/9822 [1:23:00<3:21:22,  1.70s/it] 27%|██▋       | 2695/9822 [1:23:02<3:20:32,  1.69s/it] 27%|██▋       | 2696/9822 [1:23:03<3:20:02,  1.68s/it] 27%|██▋       | 2697/9822 [1:23:05<3:19:24,  1.68s/it] 27%|██▋       | 2698/9822 [1:23:07<3:19:04,  1.68s/it] 27%|██▋       | 2699/9822 [1:23:08<3:18:40,  1.67s/it] 27%|██▋       | 2700/9822 [1:23:10<3:18:43,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:03:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:03:57 - INFO - __main__ - ***** test Results*****
04/29/2024 13:03:57 - INFO - __main__ -   Training step = 2700
04/29/2024 13:03:57 - INFO - __main__ -  test_accuracy:0.8645680819912153 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:04:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:04:01 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:04:01 - INFO - __main__ -   Training step = 2700
04/29/2024 13:04:01 - INFO - __main__ -  eval_accuracy:0.848041010618821 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 13:04:01,738 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 13:04:01,738 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 13:04:01,779 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 13:04:03,745 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.848041010618821}
test:
{'accuracy': 0.8645680819912153}
04/29/2024 13:04:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:04:12 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:04:12 - INFO - __main__ -   Training step = 2700
04/29/2024 13:04:12 - INFO - __main__ -  eval_accuracy:0.9146832662028561 
 27%|██▋       | 2701/9822 [1:23:31<14:54:54,  7.54s/it] 28%|██▊       | 2702/9822 [1:23:33<11:26:19,  5.78s/it] 28%|██▊       | 2703/9822 [1:23:35<9:00:02,  4.55s/it]  28%|██▊       | 2704/9822 [1:23:36<7:17:41,  3.69s/it] 28%|██▊       | 2705/9822 [1:23:38<6:06:24,  3.09s/it] 28%|██▊       | 2706/9822 [1:23:40<5:16:20,  2.67s/it] 28%|██▊       | 2707/9822 [1:23:41<4:41:13,  2.37s/it] 28%|██▊       | 2708/9822 [1:23:43<4:16:15,  2.16s/it] 28%|██▊       | 2709/9822 [1:23:45<3:59:01,  2.02s/it] 28%|██▊       | 2710/9822 [1:23:46<3:46:52,  1.91s/it] 28%|██▊       | 2711/9822 [1:23:48<3:37:53,  1.84s/it] 28%|██▊       | 2712/9822 [1:23:50<3:31:42,  1.79s/it] 28%|██▊       | 2713/9822 [1:23:51<3:27:54,  1.75s/it] 28%|██▊       | 2714/9822 [1:23:53<3:25:03,  1.73s/it] 28%|██▊       | 2715/9822 [1:23:55<3:22:48,  1.71s/it] 28%|██▊       | 2716/9822 [1:23:56<3:21:05,  1.70s/it] 28%|██▊       | 2717/9822 [1:23:58<3:20:09,  1.69s/it] 28%|██▊       | 2718/9822 [1:24:00<3:19:20,  1.68s/it] 28%|██▊       | 2719/9822 [1:24:01<3:18:27,  1.68s/it] 28%|██▊       | 2720/9822 [1:24:03<3:17:42,  1.67s/it] 28%|██▊       | 2721/9822 [1:24:05<3:17:31,  1.67s/it] 28%|██▊       | 2722/9822 [1:24:06<3:17:36,  1.67s/it] 28%|██▊       | 2723/9822 [1:24:08<3:17:38,  1.67s/it] 28%|██▊       | 2724/9822 [1:24:10<3:18:00,  1.67s/it] 28%|██▊       | 2725/9822 [1:24:11<3:17:38,  1.67s/it] 28%|██▊       | 2726/9822 [1:24:13<3:21:21,  1.70s/it] 28%|██▊       | 2727/9822 [1:24:15<3:20:04,  1.69s/it] 28%|██▊       | 2728/9822 [1:24:17<3:19:12,  1.68s/it] 28%|██▊       | 2729/9822 [1:24:18<3:18:59,  1.68s/it] 28%|██▊       | 2730/9822 [1:24:20<3:18:19,  1.68s/it] 28%|██▊       | 2731/9822 [1:24:22<3:18:13,  1.68s/it] 28%|██▊       | 2732/9822 [1:24:23<3:17:39,  1.67s/it] 28%|██▊       | 2733/9822 [1:24:25<3:17:39,  1.67s/it] 28%|██▊       | 2734/9822 [1:24:27<3:17:34,  1.67s/it] 28%|██▊       | 2735/9822 [1:24:28<3:17:57,  1.68s/it] 28%|██▊       | 2736/9822 [1:24:30<3:18:09,  1.68s/it] 28%|██▊       | 2737/9822 [1:24:32<3:17:56,  1.68s/it] 28%|██▊       | 2738/9822 [1:24:33<3:17:35,  1.67s/it] 28%|██▊       | 2739/9822 [1:24:35<3:17:25,  1.67s/it] 28%|██▊       | 2740/9822 [1:24:37<3:17:32,  1.67s/it] 28%|██▊       | 2741/9822 [1:24:38<3:17:34,  1.67s/it] 28%|██▊       | 2742/9822 [1:24:40<3:17:37,  1.67s/it] 28%|██▊       | 2743/9822 [1:24:42<3:17:36,  1.67s/it] 28%|██▊       | 2744/9822 [1:24:43<3:17:48,  1.68s/it] 28%|██▊       | 2745/9822 [1:24:45<3:17:42,  1.68s/it] 28%|██▊       | 2746/9822 [1:24:47<3:17:23,  1.67s/it] 28%|██▊       | 2747/9822 [1:24:48<3:17:28,  1.67s/it] 28%|██▊       | 2748/9822 [1:24:50<3:21:11,  1.71s/it] 28%|██▊       | 2749/9822 [1:24:52<3:19:42,  1.69s/it] 28%|██▊       | 2750/9822 [1:24:53<3:18:40,  1.69s/it] 28%|██▊       | 2751/9822 [1:24:55<3:17:45,  1.68s/it] 28%|██▊       | 2752/9822 [1:24:57<3:15:24,  1.66s/it] 28%|██▊       | 2753/9822 [1:24:58<3:15:58,  1.66s/it] 28%|██▊       | 2754/9822 [1:25:00<3:16:39,  1.67s/it] 28%|██▊       | 2755/9822 [1:25:02<3:16:28,  1.67s/it] 28%|██▊       | 2756/9822 [1:25:03<3:16:38,  1.67s/it] 28%|██▊       | 2757/9822 [1:25:05<3:16:58,  1.67s/it] 28%|██▊       | 2758/9822 [1:25:07<3:16:49,  1.67s/it] 28%|██▊       | 2759/9822 [1:25:08<3:16:54,  1.67s/it] 28%|██▊       | 2760/9822 [1:25:10<3:16:52,  1.67s/it] 28%|██▊       | 2761/9822 [1:25:12<3:16:53,  1.67s/it] 28%|██▊       | 2762/9822 [1:25:13<3:16:58,  1.67s/it] 28%|██▊       | 2763/9822 [1:25:15<3:16:59,  1.67s/it] 28%|██▊       | 2764/9822 [1:25:17<3:17:03,  1.68s/it] 28%|██▊       | 2765/9822 [1:25:18<3:16:56,  1.67s/it] 28%|██▊       | 2766/9822 [1:25:20<3:17:29,  1.68s/it] 28%|██▊       | 2767/9822 [1:25:22<3:17:34,  1.68s/it] 28%|██▊       | 2768/9822 [1:25:24<3:17:41,  1.68s/it] 28%|██▊       | 2769/9822 [1:25:25<3:17:15,  1.68s/it] 28%|██▊       | 2770/9822 [1:25:27<3:17:21,  1.68s/it] 28%|██▊       | 2771/9822 [1:25:29<3:17:08,  1.68s/it] 28%|██▊       | 2772/9822 [1:25:30<3:16:52,  1.68s/it] 28%|██▊       | 2773/9822 [1:25:32<3:16:36,  1.67s/it] 28%|██▊       | 2774/9822 [1:25:34<3:16:31,  1.67s/it] 28%|██▊       | 2775/9822 [1:25:35<3:20:23,  1.71s/it] 28%|██▊       | 2776/9822 [1:25:37<3:18:57,  1.69s/it] 28%|██▊       | 2777/9822 [1:25:39<3:18:06,  1.69s/it] 28%|██▊       | 2778/9822 [1:25:40<3:17:38,  1.68s/it] 28%|██▊       | 2779/9822 [1:25:42<3:17:11,  1.68s/it] 28%|██▊       | 2780/9822 [1:25:44<3:17:08,  1.68s/it] 28%|██▊       | 2781/9822 [1:25:45<3:17:00,  1.68s/it] 28%|██▊       | 2782/9822 [1:25:47<3:16:51,  1.68s/it] 28%|██▊       | 2783/9822 [1:25:49<3:16:39,  1.68s/it] 28%|██▊       | 2784/9822 [1:25:50<3:16:44,  1.68s/it] 28%|██▊       | 2785/9822 [1:25:52<3:16:20,  1.67s/it] 28%|██▊       | 2786/9822 [1:25:54<3:16:19,  1.67s/it] 28%|██▊       | 2787/9822 [1:25:55<3:16:16,  1.67s/it] 28%|██▊       | 2788/9822 [1:25:57<3:16:39,  1.68s/it] 28%|██▊       | 2789/9822 [1:25:59<3:16:25,  1.68s/it] 28%|██▊       | 2790/9822 [1:26:00<3:16:13,  1.67s/it] 28%|██▊       | 2791/9822 [1:26:02<3:16:19,  1.68s/it] 28%|██▊       | 2792/9822 [1:26:04<3:15:55,  1.67s/it] 28%|██▊       | 2793/9822 [1:26:05<3:15:51,  1.67s/it] 28%|██▊       | 2794/9822 [1:26:07<3:16:42,  1.68s/it] 28%|██▊       | 2795/9822 [1:26:09<3:16:25,  1.68s/it] 28%|██▊       | 2796/9822 [1:26:11<3:16:06,  1.67s/it] 28%|██▊       | 2797/9822 [1:26:12<3:15:27,  1.67s/it] 28%|██▊       | 2798/9822 [1:26:14<3:15:42,  1.67s/it] 28%|██▊       | 2799/9822 [1:26:16<3:15:36,  1.67s/it] 29%|██▊       | 2800/9822 [1:26:17<3:14:57,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:07:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:07:04 - INFO - __main__ - ***** test Results*****
04/29/2024 13:07:04 - INFO - __main__ -   Training step = 2800
04/29/2024 13:07:04 - INFO - __main__ -  test_accuracy:0.866398243045388 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:07:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:07:08 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:07:08 - INFO - __main__ -   Training step = 2800
04/29/2024 13:07:08 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 13:07:08,877 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 13:07:08,877 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 13:07:08,918 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 13:07:10,047 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8491395093372391}
test:
{'accuracy': 0.866398243045388}
04/29/2024 13:07:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:07:18 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:07:18 - INFO - __main__ -   Training step = 2800
04/29/2024 13:07:18 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 29%|██▊       | 2801/9822 [1:26:38<14:11:26,  7.28s/it] 29%|██▊       | 2802/9822 [1:26:39<10:55:03,  5.60s/it] 29%|██▊       | 2803/9822 [1:26:41<8:37:27,  4.42s/it]  29%|██▊       | 2804/9822 [1:26:43<7:01:10,  3.60s/it] 29%|██▊       | 2805/9822 [1:26:44<5:53:46,  3.02s/it] 29%|██▊       | 2806/9822 [1:26:46<5:06:36,  2.62s/it] 29%|██▊       | 2807/9822 [1:26:48<4:37:18,  2.37s/it] 29%|██▊       | 2808/9822 [1:26:49<4:13:09,  2.17s/it] 29%|██▊       | 2809/9822 [1:26:51<3:56:02,  2.02s/it] 29%|██▊       | 2810/9822 [1:26:53<3:43:56,  1.92s/it] 29%|██▊       | 2811/9822 [1:26:54<3:35:12,  1.84s/it] 29%|██▊       | 2812/9822 [1:26:56<3:28:33,  1.79s/it] 29%|██▊       | 2813/9822 [1:26:58<3:24:21,  1.75s/it] 29%|██▊       | 2814/9822 [1:26:59<3:20:55,  1.72s/it] 29%|██▊       | 2815/9822 [1:27:01<3:19:07,  1.71s/it] 29%|██▊       | 2816/9822 [1:27:03<3:17:20,  1.69s/it] 29%|██▊       | 2817/9822 [1:27:04<3:16:03,  1.68s/it] 29%|██▊       | 2818/9822 [1:27:06<3:15:31,  1.68s/it] 29%|██▊       | 2819/9822 [1:27:08<3:15:15,  1.67s/it] 29%|██▊       | 2820/9822 [1:27:09<3:15:22,  1.67s/it] 29%|██▊       | 2821/9822 [1:27:11<3:15:06,  1.67s/it] 29%|██▊       | 2822/9822 [1:27:13<3:15:21,  1.67s/it] 29%|██▊       | 2823/9822 [1:27:14<3:14:42,  1.67s/it] 29%|██▉       | 2824/9822 [1:27:16<3:14:33,  1.67s/it] 29%|██▉       | 2825/9822 [1:27:18<3:14:40,  1.67s/it] 29%|██▉       | 2826/9822 [1:27:19<3:14:08,  1.67s/it] 29%|██▉       | 2827/9822 [1:27:21<3:14:23,  1.67s/it] 29%|██▉       | 2828/9822 [1:27:23<3:13:54,  1.66s/it] 29%|██▉       | 2829/9822 [1:27:24<3:13:33,  1.66s/it] 29%|██▉       | 2830/9822 [1:27:26<3:13:26,  1.66s/it] 29%|██▉       | 2831/9822 [1:27:28<3:13:41,  1.66s/it] 29%|██▉       | 2832/9822 [1:27:29<3:13:58,  1.66s/it] 29%|██▉       | 2833/9822 [1:27:31<3:13:48,  1.66s/it] 29%|██▉       | 2834/9822 [1:27:33<3:17:33,  1.70s/it] 29%|██▉       | 2835/9822 [1:27:34<3:16:30,  1.69s/it] 29%|██▉       | 2836/9822 [1:27:36<3:15:50,  1.68s/it] 29%|██▉       | 2837/9822 [1:27:38<3:15:27,  1.68s/it] 29%|██▉       | 2838/9822 [1:27:39<3:13:14,  1.66s/it] 29%|██▉       | 2839/9822 [1:27:41<3:13:43,  1.66s/it] 29%|██▉       | 2840/9822 [1:27:43<3:13:56,  1.67s/it] 29%|██▉       | 2841/9822 [1:27:44<3:14:20,  1.67s/it] 29%|██▉       | 2842/9822 [1:27:46<3:13:53,  1.67s/it] 29%|██▉       | 2843/9822 [1:27:48<3:13:37,  1.66s/it] 29%|██▉       | 2844/9822 [1:27:49<3:13:39,  1.67s/it] 29%|██▉       | 2845/9822 [1:27:51<3:13:54,  1.67s/it] 29%|██▉       | 2846/9822 [1:27:53<3:14:01,  1.67s/it] 29%|██▉       | 2847/9822 [1:27:54<3:13:28,  1.66s/it] 29%|██▉       | 2848/9822 [1:27:56<3:13:29,  1.66s/it] 29%|██▉       | 2849/9822 [1:27:58<3:13:45,  1.67s/it] 29%|██▉       | 2850/9822 [1:27:59<3:13:34,  1.67s/it] 29%|██▉       | 2851/9822 [1:28:01<3:13:39,  1.67s/it] 29%|██▉       | 2852/9822 [1:28:03<3:14:14,  1.67s/it] 29%|██▉       | 2853/9822 [1:28:04<3:14:44,  1.68s/it] 29%|██▉       | 2854/9822 [1:28:06<3:14:58,  1.68s/it] 29%|██▉       | 2855/9822 [1:28:08<3:14:45,  1.68s/it] 29%|██▉       | 2856/9822 [1:28:10<3:15:03,  1.68s/it] 29%|██▉       | 2857/9822 [1:28:11<3:15:05,  1.68s/it] 29%|██▉       | 2858/9822 [1:28:13<3:14:49,  1.68s/it] 29%|██▉       | 2859/9822 [1:28:15<3:14:58,  1.68s/it] 29%|██▉       | 2860/9822 [1:28:16<3:14:47,  1.68s/it] 29%|██▉       | 2861/9822 [1:28:18<3:18:36,  1.71s/it] 29%|██▉       | 2862/9822 [1:28:20<3:17:05,  1.70s/it] 29%|██▉       | 2863/9822 [1:28:21<3:16:09,  1.69s/it] 29%|██▉       | 2864/9822 [1:28:23<3:15:34,  1.69s/it] 29%|██▉       | 2865/9822 [1:28:25<3:15:23,  1.69s/it] 29%|██▉       | 2866/9822 [1:28:26<3:14:38,  1.68s/it] 29%|██▉       | 2867/9822 [1:28:28<3:14:47,  1.68s/it] 29%|██▉       | 2868/9822 [1:28:30<3:14:28,  1.68s/it] 29%|██▉       | 2869/9822 [1:28:31<3:14:06,  1.68s/it] 29%|██▉       | 2870/9822 [1:28:33<3:13:50,  1.67s/it] 29%|██▉       | 2871/9822 [1:28:35<3:13:44,  1.67s/it] 29%|██▉       | 2872/9822 [1:28:36<3:13:46,  1.67s/it] 29%|██▉       | 2873/9822 [1:28:38<3:13:48,  1.67s/it] 29%|██▉       | 2874/9822 [1:28:40<3:13:33,  1.67s/it] 29%|██▉       | 2875/9822 [1:28:41<3:13:35,  1.67s/it] 29%|██▉       | 2876/9822 [1:28:43<3:13:31,  1.67s/it] 29%|██▉       | 2877/9822 [1:28:45<3:13:17,  1.67s/it] 29%|██▉       | 2878/9822 [1:28:46<3:13:03,  1.67s/it] 29%|██▉       | 2879/9822 [1:28:48<3:12:43,  1.67s/it] 29%|██▉       | 2880/9822 [1:28:50<3:12:30,  1.66s/it] 29%|██▉       | 2881/9822 [1:28:51<3:12:46,  1.67s/it] 29%|██▉       | 2882/9822 [1:28:53<3:12:52,  1.67s/it] 29%|██▉       | 2883/9822 [1:28:55<3:12:48,  1.67s/it] 29%|██▉       | 2884/9822 [1:28:56<3:12:58,  1.67s/it] 29%|██▉       | 2885/9822 [1:28:58<3:12:58,  1.67s/it] 29%|██▉       | 2886/9822 [1:29:00<3:12:47,  1.67s/it] 29%|██▉       | 2887/9822 [1:29:01<3:12:29,  1.67s/it] 29%|██▉       | 2888/9822 [1:29:03<3:13:14,  1.67s/it] 29%|██▉       | 2889/9822 [1:29:05<3:13:26,  1.67s/it] 29%|██▉       | 2890/9822 [1:29:06<3:13:02,  1.67s/it] 29%|██▉       | 2891/9822 [1:29:08<3:12:45,  1.67s/it] 29%|██▉       | 2892/9822 [1:29:10<3:12:28,  1.67s/it] 29%|██▉       | 2893/9822 [1:29:11<3:12:31,  1.67s/it] 29%|██▉       | 2894/9822 [1:29:13<3:16:23,  1.70s/it] 29%|██▉       | 2895/9822 [1:29:15<3:15:47,  1.70s/it] 29%|██▉       | 2896/9822 [1:29:17<3:15:03,  1.69s/it] 29%|██▉       | 2897/9822 [1:29:18<3:14:42,  1.69s/it] 30%|██▉       | 2898/9822 [1:29:20<3:13:43,  1.68s/it] 30%|██▉       | 2899/9822 [1:29:22<3:13:40,  1.68s/it] 30%|██▉       | 2900/9822 [1:29:23<3:13:31,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:10:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:10:10 - INFO - __main__ - ***** test Results*****
04/29/2024 13:10:10 - INFO - __main__ -   Training step = 2900
04/29/2024 13:10:10 - INFO - __main__ -  test_accuracy:0.8689604685212299 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:10:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:10:15 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:10:15 - INFO - __main__ -   Training step = 2900
04/29/2024 13:10:15 - INFO - __main__ -  eval_accuracy:0.8469425119004028 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8491395093372391}
test:
{'accuracy': 0.866398243045388}
04/29/2024 13:10:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:10:23 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:10:23 - INFO - __main__ -   Training step = 2900
04/29/2024 13:10:23 - INFO - __main__ -  eval_accuracy:0.9154155986818016 
 30%|██▉       | 2901/9822 [1:29:43<13:21:06,  6.95s/it] 30%|██▉       | 2902/9822 [1:29:44<10:19:03,  5.37s/it] 30%|██▉       | 2903/9822 [1:29:46<8:11:29,  4.26s/it]  30%|██▉       | 2904/9822 [1:29:48<6:42:20,  3.49s/it] 30%|██▉       | 2905/9822 [1:29:49<5:39:57,  2.95s/it] 30%|██▉       | 2906/9822 [1:29:51<4:56:19,  2.57s/it] 30%|██▉       | 2907/9822 [1:29:53<4:25:37,  2.30s/it] 30%|██▉       | 2908/9822 [1:29:54<4:03:56,  2.12s/it] 30%|██▉       | 2909/9822 [1:29:56<3:48:58,  1.99s/it] 30%|██▉       | 2910/9822 [1:29:58<3:38:29,  1.90s/it] 30%|██▉       | 2911/9822 [1:29:59<3:31:04,  1.83s/it] 30%|██▉       | 2912/9822 [1:30:01<3:25:59,  1.79s/it] 30%|██▉       | 2913/9822 [1:30:03<3:22:22,  1.76s/it] 30%|██▉       | 2914/9822 [1:30:04<3:19:58,  1.74s/it] 30%|██▉       | 2915/9822 [1:30:06<3:18:09,  1.72s/it] 30%|██▉       | 2916/9822 [1:30:08<3:16:56,  1.71s/it] 30%|██▉       | 2917/9822 [1:30:09<3:15:58,  1.70s/it] 30%|██▉       | 2918/9822 [1:30:11<3:15:17,  1.70s/it] 30%|██▉       | 2919/9822 [1:30:13<3:14:49,  1.69s/it] 30%|██▉       | 2920/9822 [1:30:15<3:14:26,  1.69s/it] 30%|██▉       | 2921/9822 [1:30:16<3:14:13,  1.69s/it] 30%|██▉       | 2922/9822 [1:30:18<3:14:10,  1.69s/it] 30%|██▉       | 2923/9822 [1:30:20<3:13:59,  1.69s/it] 30%|██▉       | 2924/9822 [1:30:21<3:12:11,  1.67s/it] 30%|██▉       | 2925/9822 [1:30:23<3:12:43,  1.68s/it] 30%|██▉       | 2926/9822 [1:30:25<3:12:57,  1.68s/it] 30%|██▉       | 2927/9822 [1:30:26<3:13:02,  1.68s/it] 30%|██▉       | 2928/9822 [1:30:28<3:13:04,  1.68s/it] 30%|██▉       | 2929/9822 [1:30:30<3:12:49,  1.68s/it] 30%|██▉       | 2930/9822 [1:30:31<3:16:21,  1.71s/it] 30%|██▉       | 2931/9822 [1:30:33<3:15:20,  1.70s/it] 30%|██▉       | 2932/9822 [1:30:35<3:14:34,  1.69s/it] 30%|██▉       | 2933/9822 [1:30:36<3:13:22,  1.68s/it] 30%|██▉       | 2934/9822 [1:30:38<3:13:12,  1.68s/it] 30%|██▉       | 2935/9822 [1:30:40<3:12:29,  1.68s/it] 30%|██▉       | 2936/9822 [1:30:41<3:12:26,  1.68s/it] 30%|██▉       | 2937/9822 [1:30:43<3:11:58,  1.67s/it] 30%|██▉       | 2938/9822 [1:30:45<3:12:28,  1.68s/it] 30%|██▉       | 2939/9822 [1:30:46<3:12:17,  1.68s/it] 30%|██▉       | 2940/9822 [1:30:48<3:12:11,  1.68s/it] 30%|██▉       | 2941/9822 [1:30:50<3:13:13,  1.68s/it] 30%|██▉       | 2942/9822 [1:30:52<3:13:07,  1.68s/it] 30%|██▉       | 2943/9822 [1:30:53<3:12:39,  1.68s/it] 30%|██▉       | 2944/9822 [1:30:55<3:12:16,  1.68s/it] 30%|██▉       | 2945/9822 [1:30:57<3:12:37,  1.68s/it] 30%|██▉       | 2946/9822 [1:30:58<3:12:37,  1.68s/it] 30%|███       | 2947/9822 [1:31:00<3:12:34,  1.68s/it] 30%|███       | 2948/9822 [1:31:02<3:12:31,  1.68s/it] 30%|███       | 2949/9822 [1:31:03<3:12:27,  1.68s/it] 30%|███       | 2950/9822 [1:31:05<3:12:10,  1.68s/it] 30%|███       | 2951/9822 [1:31:07<3:12:13,  1.68s/it] 30%|███       | 2952/9822 [1:31:08<3:12:21,  1.68s/it] 30%|███       | 2953/9822 [1:31:10<3:11:59,  1.68s/it] 30%|███       | 2954/9822 [1:31:12<3:12:00,  1.68s/it] 30%|███       | 2955/9822 [1:31:13<3:12:10,  1.68s/it] 30%|███       | 2956/9822 [1:31:15<3:12:12,  1.68s/it] 30%|███       | 2957/9822 [1:31:17<3:16:01,  1.71s/it] 30%|███       | 2958/9822 [1:31:19<3:14:35,  1.70s/it] 30%|███       | 2959/9822 [1:31:20<3:13:55,  1.70s/it] 30%|███       | 2960/9822 [1:31:22<3:13:15,  1.69s/it] 30%|███       | 2961/9822 [1:31:24<3:12:53,  1.69s/it] 30%|███       | 2962/9822 [1:31:25<3:12:31,  1.68s/it] 30%|███       | 2963/9822 [1:31:27<3:12:28,  1.68s/it] 30%|███       | 2964/9822 [1:31:29<3:12:21,  1.68s/it] 30%|███       | 2965/9822 [1:31:30<3:12:05,  1.68s/it] 30%|███       | 2966/9822 [1:31:32<3:12:16,  1.68s/it] 30%|███       | 2967/9822 [1:31:34<3:12:26,  1.68s/it] 30%|███       | 2968/9822 [1:31:35<3:12:05,  1.68s/it] 30%|███       | 2969/9822 [1:31:37<3:11:26,  1.68s/it] 30%|███       | 2970/9822 [1:31:39<3:11:08,  1.67s/it] 30%|███       | 2971/9822 [1:31:40<3:10:51,  1.67s/it] 30%|███       | 2972/9822 [1:31:42<3:10:24,  1.67s/it] 30%|███       | 2973/9822 [1:31:44<3:10:03,  1.67s/it] 30%|███       | 2974/9822 [1:31:45<3:10:13,  1.67s/it] 30%|███       | 2975/9822 [1:31:47<3:10:19,  1.67s/it] 30%|███       | 2976/9822 [1:31:49<3:10:06,  1.67s/it] 30%|███       | 2977/9822 [1:31:50<3:10:22,  1.67s/it] 30%|███       | 2978/9822 [1:31:52<3:10:27,  1.67s/it] 30%|███       | 2979/9822 [1:31:54<3:10:20,  1.67s/it] 30%|███       | 2980/9822 [1:31:55<3:10:24,  1.67s/it] 30%|███       | 2981/9822 [1:31:57<3:10:35,  1.67s/it] 30%|███       | 2982/9822 [1:31:59<3:10:40,  1.67s/it] 30%|███       | 2983/9822 [1:32:00<3:10:45,  1.67s/it] 30%|███       | 2984/9822 [1:32:02<3:14:10,  1.70s/it] 30%|███       | 2985/9822 [1:32:04<3:13:27,  1.70s/it] 30%|███       | 2986/9822 [1:32:05<3:12:31,  1.69s/it] 30%|███       | 2987/9822 [1:32:07<3:11:45,  1.68s/it] 30%|███       | 2988/9822 [1:32:09<3:11:29,  1.68s/it] 30%|███       | 2989/9822 [1:32:11<3:10:59,  1.68s/it] 30%|███       | 2990/9822 [1:32:12<3:10:50,  1.68s/it] 30%|███       | 2991/9822 [1:32:14<3:11:01,  1.68s/it] 30%|███       | 2992/9822 [1:32:16<3:11:03,  1.68s/it] 30%|███       | 2993/9822 [1:32:17<3:11:14,  1.68s/it] 30%|███       | 2994/9822 [1:32:19<3:11:12,  1.68s/it] 30%|███       | 2995/9822 [1:32:21<3:11:18,  1.68s/it] 31%|███       | 2996/9822 [1:32:22<3:11:07,  1.68s/it] 31%|███       | 2997/9822 [1:32:24<3:10:40,  1.68s/it] 31%|███       | 2998/9822 [1:32:26<3:10:54,  1.68s/it] 31%|███       | 2999/9822 [1:32:27<3:11:04,  1.68s/it] 31%|███       | 3000/9822 [1:32:29<3:11:08,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:13:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:13:16 - INFO - __main__ - ***** test Results*****
04/29/2024 13:13:16 - INFO - __main__ -   Training step = 3000
04/29/2024 13:13:16 - INFO - __main__ -  test_accuracy:0.8667642752562226 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:13:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:13:20 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:13:20 - INFO - __main__ -   Training step = 3000
04/29/2024 13:13:20 - INFO - __main__ -  eval_accuracy:0.8498718418161846 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 13:13:20,715 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 13:13:20,715 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 13:13:20,756 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 13:13:21,894 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:13:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:13:30 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:13:30 - INFO - __main__ -   Training step = 3000
04/29/2024 13:13:30 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 31%|███       | 3001/9822 [1:32:49<13:50:16,  7.30s/it] 31%|███       | 3002/9822 [1:32:51<10:38:26,  5.62s/it] 31%|███       | 3003/9822 [1:32:53<8:23:35,  4.43s/it]  31%|███       | 3004/9822 [1:32:54<6:49:21,  3.60s/it] 31%|███       | 3005/9822 [1:32:56<5:43:05,  3.02s/it] 31%|███       | 3006/9822 [1:32:58<4:56:58,  2.61s/it] 31%|███       | 3007/9822 [1:32:59<4:24:33,  2.33s/it] 31%|███       | 3008/9822 [1:33:01<4:02:12,  2.13s/it] 31%|███       | 3009/9822 [1:33:03<3:46:16,  1.99s/it] 31%|███       | 3010/9822 [1:33:04<3:33:33,  1.88s/it] 31%|███       | 3011/9822 [1:33:06<3:26:39,  1.82s/it] 31%|███       | 3012/9822 [1:33:08<3:21:34,  1.78s/it] 31%|███       | 3013/9822 [1:33:09<3:18:31,  1.75s/it] 31%|███       | 3014/9822 [1:33:11<3:15:43,  1.72s/it] 31%|███       | 3015/9822 [1:33:13<3:13:56,  1.71s/it] 31%|███       | 3016/9822 [1:33:14<3:12:56,  1.70s/it] 31%|███       | 3017/9822 [1:33:16<3:12:25,  1.70s/it] 31%|███       | 3018/9822 [1:33:18<3:12:04,  1.69s/it] 31%|███       | 3019/9822 [1:33:20<3:15:08,  1.72s/it] 31%|███       | 3020/9822 [1:33:21<3:13:45,  1.71s/it] 31%|███       | 3021/9822 [1:33:23<3:12:56,  1.70s/it] 31%|███       | 3022/9822 [1:33:25<3:12:03,  1.69s/it] 31%|███       | 3023/9822 [1:33:26<3:11:23,  1.69s/it] 31%|███       | 3024/9822 [1:33:28<3:11:17,  1.69s/it] 31%|███       | 3025/9822 [1:33:30<3:11:06,  1.69s/it] 31%|███       | 3026/9822 [1:33:31<3:10:20,  1.68s/it] 31%|███       | 3027/9822 [1:33:33<3:10:25,  1.68s/it] 31%|███       | 3028/9822 [1:33:35<3:10:19,  1.68s/it] 31%|███       | 3029/9822 [1:33:36<3:09:44,  1.68s/it] 31%|███       | 3030/9822 [1:33:38<3:09:32,  1.67s/it] 31%|███       | 3031/9822 [1:33:40<3:09:10,  1.67s/it] 31%|███       | 3032/9822 [1:33:41<3:09:00,  1.67s/it] 31%|███       | 3033/9822 [1:33:43<3:09:34,  1.68s/it] 31%|███       | 3034/9822 [1:33:45<3:09:46,  1.68s/it] 31%|███       | 3035/9822 [1:33:46<3:09:44,  1.68s/it] 31%|███       | 3036/9822 [1:33:48<3:09:41,  1.68s/it] 31%|███       | 3037/9822 [1:33:50<3:09:55,  1.68s/it] 31%|███       | 3038/9822 [1:33:51<3:10:08,  1.68s/it] 31%|███       | 3039/9822 [1:33:53<3:09:58,  1.68s/it] 31%|███       | 3040/9822 [1:33:55<3:10:03,  1.68s/it] 31%|███       | 3041/9822 [1:33:57<3:10:07,  1.68s/it] 31%|███       | 3042/9822 [1:33:58<3:10:00,  1.68s/it] 31%|███       | 3043/9822 [1:34:00<3:09:31,  1.68s/it] 31%|███       | 3044/9822 [1:34:02<3:09:36,  1.68s/it] 31%|███       | 3045/9822 [1:34:03<3:09:19,  1.68s/it] 31%|███       | 3046/9822 [1:34:05<3:12:58,  1.71s/it] 31%|███       | 3047/9822 [1:34:07<3:12:06,  1.70s/it] 31%|███       | 3048/9822 [1:34:08<3:11:14,  1.69s/it] 31%|███       | 3049/9822 [1:34:10<3:10:50,  1.69s/it] 31%|███       | 3050/9822 [1:34:12<3:10:16,  1.69s/it] 31%|███       | 3051/9822 [1:34:13<3:09:40,  1.68s/it] 31%|███       | 3052/9822 [1:34:15<3:09:47,  1.68s/it] 31%|███       | 3053/9822 [1:34:17<3:09:14,  1.68s/it] 31%|███       | 3054/9822 [1:34:18<3:09:15,  1.68s/it] 31%|███       | 3055/9822 [1:34:20<3:09:22,  1.68s/it] 31%|███       | 3056/9822 [1:34:22<3:09:01,  1.68s/it] 31%|███       | 3057/9822 [1:34:23<3:08:56,  1.68s/it] 31%|███       | 3058/9822 [1:34:25<3:08:47,  1.67s/it] 31%|███       | 3059/9822 [1:34:27<3:08:56,  1.68s/it] 31%|███       | 3060/9822 [1:34:28<3:09:10,  1.68s/it] 31%|███       | 3061/9822 [1:34:30<3:09:09,  1.68s/it] 31%|███       | 3062/9822 [1:34:32<3:08:41,  1.67s/it] 31%|███       | 3063/9822 [1:34:33<3:08:23,  1.67s/it] 31%|███       | 3064/9822 [1:34:35<3:08:25,  1.67s/it] 31%|███       | 3065/9822 [1:34:37<3:08:29,  1.67s/it] 31%|███       | 3066/9822 [1:34:39<3:08:30,  1.67s/it] 31%|███       | 3067/9822 [1:34:40<3:08:38,  1.68s/it] 31%|███       | 3068/9822 [1:34:42<3:08:54,  1.68s/it] 31%|███       | 3069/9822 [1:34:44<3:09:12,  1.68s/it] 31%|███▏      | 3070/9822 [1:34:45<3:08:50,  1.68s/it] 31%|███▏      | 3071/9822 [1:34:47<3:08:39,  1.68s/it] 31%|███▏      | 3072/9822 [1:34:49<3:08:51,  1.68s/it] 31%|███▏      | 3073/9822 [1:34:50<3:12:14,  1.71s/it] 31%|███▏      | 3074/9822 [1:34:52<3:11:11,  1.70s/it] 31%|███▏      | 3075/9822 [1:34:54<3:10:25,  1.69s/it] 31%|███▏      | 3076/9822 [1:34:55<3:09:32,  1.69s/it] 31%|███▏      | 3077/9822 [1:34:57<3:09:34,  1.69s/it] 31%|███▏      | 3078/9822 [1:34:59<3:09:21,  1.68s/it] 31%|███▏      | 3079/9822 [1:35:00<3:08:54,  1.68s/it] 31%|███▏      | 3080/9822 [1:35:02<3:09:04,  1.68s/it] 31%|███▏      | 3081/9822 [1:35:04<3:08:48,  1.68s/it] 31%|███▏      | 3082/9822 [1:35:05<3:08:38,  1.68s/it] 31%|███▏      | 3083/9822 [1:35:07<3:08:23,  1.68s/it] 31%|███▏      | 3084/9822 [1:35:09<3:08:08,  1.68s/it] 31%|███▏      | 3085/9822 [1:35:11<3:08:25,  1.68s/it] 31%|███▏      | 3086/9822 [1:35:12<3:08:07,  1.68s/it] 31%|███▏      | 3087/9822 [1:35:14<3:08:11,  1.68s/it] 31%|███▏      | 3088/9822 [1:35:16<3:08:01,  1.68s/it] 31%|███▏      | 3089/9822 [1:35:17<3:08:00,  1.68s/it] 31%|███▏      | 3090/9822 [1:35:19<3:08:00,  1.68s/it] 31%|███▏      | 3091/9822 [1:35:21<3:08:21,  1.68s/it] 31%|███▏      | 3092/9822 [1:35:22<3:08:22,  1.68s/it] 31%|███▏      | 3093/9822 [1:35:24<3:08:06,  1.68s/it] 32%|███▏      | 3094/9822 [1:35:26<3:08:25,  1.68s/it] 32%|███▏      | 3095/9822 [1:35:27<3:08:23,  1.68s/it] 32%|███▏      | 3096/9822 [1:35:29<3:06:21,  1.66s/it] 32%|███▏      | 3097/9822 [1:35:31<3:06:28,  1.66s/it] 32%|███▏      | 3098/9822 [1:35:32<3:06:32,  1.66s/it] 32%|███▏      | 3099/9822 [1:35:34<3:06:45,  1.67s/it] 32%|███▏      | 3100/9822 [1:35:36<3:06:59,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:16:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:16:22 - INFO - __main__ - ***** test Results*****
04/29/2024 13:16:22 - INFO - __main__ -   Training step = 3100
04/29/2024 13:16:22 - INFO - __main__ -  test_accuracy:0.8576134699853587 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:16:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:16:27 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:16:27 - INFO - __main__ -   Training step = 3100
04/29/2024 13:16:27 - INFO - __main__ -  eval_accuracy:0.8432808495056756 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:16:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:16:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:16:35 - INFO - __main__ -   Training step = 3100
04/29/2024 13:16:35 - INFO - __main__ -  eval_accuracy:0.9139509337239107 
 32%|███▏      | 3101/9822 [1:35:55<12:57:46,  6.94s/it] 32%|███▏      | 3102/9822 [1:35:57<10:00:13,  5.36s/it] 32%|███▏      | 3103/9822 [1:35:58<7:56:14,  4.25s/it]  32%|███▏      | 3104/9822 [1:36:00<6:29:23,  3.48s/it] 32%|███▏      | 3105/9822 [1:36:02<5:32:35,  2.97s/it] 32%|███▏      | 3106/9822 [1:36:03<4:49:00,  2.58s/it] 32%|███▏      | 3107/9822 [1:36:05<4:18:41,  2.31s/it] 32%|███▏      | 3108/9822 [1:36:07<3:56:48,  2.12s/it] 32%|███▏      | 3109/9822 [1:36:08<3:41:46,  1.98s/it] 32%|███▏      | 3110/9822 [1:36:10<3:31:06,  1.89s/it] 32%|███▏      | 3111/9822 [1:36:12<3:24:00,  1.82s/it] 32%|███▏      | 3112/9822 [1:36:13<3:19:23,  1.78s/it] 32%|███▏      | 3113/9822 [1:36:15<3:16:09,  1.75s/it] 32%|███▏      | 3114/9822 [1:36:17<3:14:00,  1.74s/it] 32%|███▏      | 3115/9822 [1:36:18<3:12:10,  1.72s/it] 32%|███▏      | 3116/9822 [1:36:20<3:10:45,  1.71s/it] 32%|███▏      | 3117/9822 [1:36:22<3:09:46,  1.70s/it] 32%|███▏      | 3118/9822 [1:36:23<3:09:09,  1.69s/it] 32%|███▏      | 3119/9822 [1:36:25<3:08:32,  1.69s/it] 32%|███▏      | 3120/9822 [1:36:27<3:07:42,  1.68s/it] 32%|███▏      | 3121/9822 [1:36:28<3:07:08,  1.68s/it] 32%|███▏      | 3122/9822 [1:36:30<3:07:17,  1.68s/it] 32%|███▏      | 3123/9822 [1:36:32<3:07:10,  1.68s/it] 32%|███▏      | 3124/9822 [1:36:33<3:07:19,  1.68s/it] 32%|███▏      | 3125/9822 [1:36:35<3:07:02,  1.68s/it] 32%|███▏      | 3126/9822 [1:36:37<3:07:24,  1.68s/it] 32%|███▏      | 3127/9822 [1:36:39<3:07:15,  1.68s/it] 32%|███▏      | 3128/9822 [1:36:40<3:07:01,  1.68s/it] 32%|███▏      | 3129/9822 [1:36:42<3:07:08,  1.68s/it] 32%|███▏      | 3130/9822 [1:36:44<3:06:44,  1.67s/it] 32%|███▏      | 3131/9822 [1:36:45<3:06:33,  1.67s/it] 32%|███▏      | 3132/9822 [1:36:47<3:06:29,  1.67s/it] 32%|███▏      | 3133/9822 [1:36:49<3:06:26,  1.67s/it] 32%|███▏      | 3134/9822 [1:36:50<3:06:17,  1.67s/it] 32%|███▏      | 3135/9822 [1:36:52<3:06:25,  1.67s/it] 32%|███▏      | 3136/9822 [1:36:54<3:06:16,  1.67s/it] 32%|███▏      | 3137/9822 [1:36:55<3:06:34,  1.67s/it] 32%|███▏      | 3138/9822 [1:36:57<3:10:07,  1.71s/it] 32%|███▏      | 3139/9822 [1:36:59<3:09:25,  1.70s/it] 32%|███▏      | 3140/9822 [1:37:00<3:08:16,  1.69s/it] 32%|███▏      | 3141/9822 [1:37:02<3:07:31,  1.68s/it] 32%|███▏      | 3142/9822 [1:37:04<3:07:15,  1.68s/it] 32%|███▏      | 3143/9822 [1:37:05<3:06:58,  1.68s/it] 32%|███▏      | 3144/9822 [1:37:07<3:07:07,  1.68s/it] 32%|███▏      | 3145/9822 [1:37:09<3:07:10,  1.68s/it] 32%|███▏      | 3146/9822 [1:37:10<3:06:48,  1.68s/it] 32%|███▏      | 3147/9822 [1:37:12<3:06:33,  1.68s/it] 32%|███▏      | 3148/9822 [1:37:14<3:06:32,  1.68s/it] 32%|███▏      | 3149/9822 [1:37:15<3:06:13,  1.67s/it] 32%|███▏      | 3150/9822 [1:37:17<3:06:06,  1.67s/it] 32%|███▏      | 3151/9822 [1:37:19<3:06:18,  1.68s/it] 32%|███▏      | 3152/9822 [1:37:20<3:06:14,  1.68s/it] 32%|███▏      | 3153/9822 [1:37:22<3:05:57,  1.67s/it] 32%|███▏      | 3154/9822 [1:37:24<3:06:17,  1.68s/it] 32%|███▏      | 3155/9822 [1:37:26<3:06:17,  1.68s/it] 32%|███▏      | 3156/9822 [1:37:27<3:06:22,  1.68s/it] 32%|███▏      | 3157/9822 [1:37:29<3:06:19,  1.68s/it] 32%|███▏      | 3158/9822 [1:37:31<3:06:12,  1.68s/it] 32%|███▏      | 3159/9822 [1:37:32<3:06:08,  1.68s/it] 32%|███▏      | 3160/9822 [1:37:34<3:09:50,  1.71s/it] 32%|███▏      | 3161/9822 [1:37:36<3:08:57,  1.70s/it] 32%|███▏      | 3162/9822 [1:37:37<3:08:17,  1.70s/it] 32%|███▏      | 3163/9822 [1:37:39<3:07:31,  1.69s/it] 32%|███▏      | 3164/9822 [1:37:41<3:07:24,  1.69s/it] 32%|███▏      | 3165/9822 [1:37:42<3:07:03,  1.69s/it] 32%|███▏      | 3166/9822 [1:37:44<3:06:52,  1.68s/it] 32%|███▏      | 3167/9822 [1:37:46<3:06:40,  1.68s/it] 32%|███▏      | 3168/9822 [1:37:47<3:06:48,  1.68s/it] 32%|███▏      | 3169/9822 [1:37:49<3:06:34,  1.68s/it] 32%|███▏      | 3170/9822 [1:37:51<3:06:30,  1.68s/it] 32%|███▏      | 3171/9822 [1:37:53<3:06:39,  1.68s/it] 32%|███▏      | 3172/9822 [1:37:54<3:06:04,  1.68s/it] 32%|███▏      | 3173/9822 [1:37:56<3:06:07,  1.68s/it] 32%|███▏      | 3174/9822 [1:37:58<3:05:36,  1.68s/it] 32%|███▏      | 3175/9822 [1:37:59<3:05:19,  1.67s/it] 32%|███▏      | 3176/9822 [1:38:01<3:05:26,  1.67s/it] 32%|███▏      | 3177/9822 [1:38:03<3:05:20,  1.67s/it] 32%|███▏      | 3178/9822 [1:38:04<3:05:04,  1.67s/it] 32%|███▏      | 3179/9822 [1:38:06<3:04:59,  1.67s/it] 32%|███▏      | 3180/9822 [1:38:08<3:04:44,  1.67s/it] 32%|███▏      | 3181/9822 [1:38:09<3:04:45,  1.67s/it] 32%|███▏      | 3182/9822 [1:38:11<3:03:37,  1.66s/it] 32%|███▏      | 3183/9822 [1:38:13<3:03:57,  1.66s/it] 32%|███▏      | 3184/9822 [1:38:14<3:04:17,  1.67s/it] 32%|███▏      | 3185/9822 [1:38:16<3:05:00,  1.67s/it] 32%|███▏      | 3186/9822 [1:38:18<3:05:03,  1.67s/it] 32%|███▏      | 3187/9822 [1:38:19<3:08:14,  1.70s/it] 32%|███▏      | 3188/9822 [1:38:21<3:07:15,  1.69s/it] 32%|███▏      | 3189/9822 [1:38:23<3:06:28,  1.69s/it] 32%|███▏      | 3190/9822 [1:38:24<3:06:03,  1.68s/it] 32%|███▏      | 3191/9822 [1:38:26<3:05:16,  1.68s/it] 32%|███▏      | 3192/9822 [1:38:28<3:05:05,  1.68s/it] 33%|███▎      | 3193/9822 [1:38:29<3:04:41,  1.67s/it] 33%|███▎      | 3194/9822 [1:38:31<3:04:08,  1.67s/it] 33%|███▎      | 3195/9822 [1:38:33<3:04:40,  1.67s/it] 33%|███▎      | 3196/9822 [1:38:34<3:05:01,  1.68s/it] 33%|███▎      | 3197/9822 [1:38:36<3:04:49,  1.67s/it] 33%|███▎      | 3198/9822 [1:38:38<3:04:34,  1.67s/it] 33%|███▎      | 3199/9822 [1:38:39<3:04:29,  1.67s/it] 33%|███▎      | 3200/9822 [1:38:41<3:04:14,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:19:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:19:28 - INFO - __main__ - ***** test Results*****
04/29/2024 13:19:28 - INFO - __main__ -   Training step = 3200
04/29/2024 13:19:28 - INFO - __main__ -  test_accuracy:0.8715226939970717 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:19:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:19:32 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:19:32 - INFO - __main__ -   Training step = 3200
04/29/2024 13:19:32 - INFO - __main__ -  eval_accuracy:0.8465763456609301 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:19:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:19:41 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:19:41 - INFO - __main__ -   Training step = 3200
04/29/2024 13:19:41 - INFO - __main__ -  eval_accuracy:0.9077261076528744 
 33%|███▎      | 3201/9822 [1:39:00<12:46:19,  6.94s/it] 33%|███▎      | 3202/9822 [1:39:02<9:51:32,  5.36s/it]  33%|███▎      | 3203/9822 [1:39:04<7:49:20,  4.25s/it] 33%|███▎      | 3204/9822 [1:39:05<6:23:36,  3.48s/it] 33%|███▎      | 3205/9822 [1:39:07<5:23:51,  2.94s/it] 33%|███▎      | 3206/9822 [1:39:09<4:41:44,  2.56s/it] 33%|███▎      | 3207/9822 [1:39:10<4:12:09,  2.29s/it] 33%|███▎      | 3208/9822 [1:39:12<3:51:28,  2.10s/it] 33%|███▎      | 3209/9822 [1:39:14<3:37:06,  1.97s/it] 33%|███▎      | 3210/9822 [1:39:15<3:27:13,  1.88s/it] 33%|███▎      | 3211/9822 [1:39:17<3:20:26,  1.82s/it] 33%|███▎      | 3212/9822 [1:39:19<3:15:30,  1.77s/it] 33%|███▎      | 3213/9822 [1:39:20<3:12:00,  1.74s/it] 33%|███▎      | 3214/9822 [1:39:22<3:09:17,  1.72s/it] 33%|███▎      | 3215/9822 [1:39:24<3:11:21,  1.74s/it] 33%|███▎      | 3216/9822 [1:39:25<3:09:32,  1.72s/it] 33%|███▎      | 3217/9822 [1:39:27<3:07:34,  1.70s/it] 33%|███▎      | 3218/9822 [1:39:29<3:06:19,  1.69s/it] 33%|███▎      | 3219/9822 [1:39:30<3:05:41,  1.69s/it] 33%|███▎      | 3220/9822 [1:39:32<3:05:08,  1.68s/it] 33%|███▎      | 3221/9822 [1:39:34<3:04:15,  1.67s/it] 33%|███▎      | 3222/9822 [1:39:35<3:04:32,  1.68s/it] 33%|███▎      | 3223/9822 [1:39:37<3:04:50,  1.68s/it] 33%|███▎      | 3224/9822 [1:39:39<3:04:44,  1.68s/it] 33%|███▎      | 3225/9822 [1:39:40<3:04:30,  1.68s/it] 33%|███▎      | 3226/9822 [1:39:42<3:04:47,  1.68s/it] 33%|███▎      | 3227/9822 [1:39:44<3:04:53,  1.68s/it] 33%|███▎      | 3228/9822 [1:39:46<3:05:08,  1.68s/it] 33%|███▎      | 3229/9822 [1:39:47<3:05:07,  1.68s/it] 33%|███▎      | 3230/9822 [1:39:49<3:05:12,  1.69s/it] 33%|███▎      | 3231/9822 [1:39:51<3:05:05,  1.68s/it] 33%|███▎      | 3232/9822 [1:39:52<3:05:02,  1.68s/it] 33%|███▎      | 3233/9822 [1:39:54<3:04:59,  1.68s/it] 33%|███▎      | 3234/9822 [1:39:56<3:05:08,  1.69s/it] 33%|███▎      | 3235/9822 [1:39:57<3:04:22,  1.68s/it] 33%|███▎      | 3236/9822 [1:39:59<3:03:46,  1.67s/it] 33%|███▎      | 3237/9822 [1:40:01<3:03:18,  1.67s/it] 33%|███▎      | 3238/9822 [1:40:02<3:02:54,  1.67s/it] 33%|███▎      | 3239/9822 [1:40:04<3:02:48,  1.67s/it] 33%|███▎      | 3240/9822 [1:40:06<3:02:34,  1.66s/it] 33%|███▎      | 3241/9822 [1:40:07<3:02:46,  1.67s/it] 33%|███▎      | 3242/9822 [1:40:09<3:02:42,  1.67s/it] 33%|███▎      | 3243/9822 [1:40:11<3:02:53,  1.67s/it] 33%|███▎      | 3244/9822 [1:40:12<3:02:42,  1.67s/it] 33%|███▎      | 3245/9822 [1:40:14<3:03:01,  1.67s/it] 33%|███▎      | 3246/9822 [1:40:16<3:02:54,  1.67s/it] 33%|███▎      | 3247/9822 [1:40:17<3:02:35,  1.67s/it] 33%|███▎      | 3248/9822 [1:40:19<3:02:53,  1.67s/it] 33%|███▎      | 3249/9822 [1:40:21<3:02:49,  1.67s/it] 33%|███▎      | 3250/9822 [1:40:22<3:02:41,  1.67s/it] 33%|███▎      | 3251/9822 [1:40:24<3:02:52,  1.67s/it] 33%|███▎      | 3252/9822 [1:40:26<3:02:25,  1.67s/it] 33%|███▎      | 3253/9822 [1:40:27<3:02:37,  1.67s/it] 33%|███▎      | 3254/9822 [1:40:29<3:02:23,  1.67s/it] 33%|███▎      | 3255/9822 [1:40:31<3:02:50,  1.67s/it] 33%|███▎      | 3256/9822 [1:40:32<3:06:07,  1.70s/it] 33%|███▎      | 3257/9822 [1:40:34<3:05:09,  1.69s/it] 33%|███▎      | 3258/9822 [1:40:36<3:04:25,  1.69s/it] 33%|███▎      | 3259/9822 [1:40:37<3:03:33,  1.68s/it] 33%|███▎      | 3260/9822 [1:40:39<3:03:21,  1.68s/it] 33%|███▎      | 3261/9822 [1:40:41<3:03:18,  1.68s/it] 33%|███▎      | 3262/9822 [1:40:42<3:02:54,  1.67s/it] 33%|███▎      | 3263/9822 [1:40:44<3:02:57,  1.67s/it] 33%|███▎      | 3264/9822 [1:40:46<3:03:01,  1.67s/it] 33%|███▎      | 3265/9822 [1:40:47<3:02:52,  1.67s/it] 33%|███▎      | 3266/9822 [1:40:49<3:02:59,  1.67s/it] 33%|███▎      | 3267/9822 [1:40:51<3:02:45,  1.67s/it] 33%|███▎      | 3268/9822 [1:40:52<3:00:56,  1.66s/it] 33%|███▎      | 3269/9822 [1:40:54<3:01:09,  1.66s/it] 33%|███▎      | 3270/9822 [1:40:56<3:01:43,  1.66s/it] 33%|███▎      | 3271/9822 [1:40:57<3:01:30,  1.66s/it] 33%|███▎      | 3272/9822 [1:40:59<3:01:17,  1.66s/it] 33%|███▎      | 3273/9822 [1:41:01<3:01:28,  1.66s/it] 33%|███▎      | 3274/9822 [1:41:02<2:44:56,  1.51s/it] 33%|███▎      | 3275/9822 [1:41:04<2:51:03,  1.57s/it] 33%|███▎      | 3276/9822 [1:41:05<2:54:24,  1.60s/it] 33%|███▎      | 3277/9822 [1:41:07<2:56:40,  1.62s/it] 33%|███▎      | 3278/9822 [1:41:09<2:58:13,  1.63s/it] 33%|███▎      | 3279/9822 [1:41:10<2:59:25,  1.65s/it] 33%|███▎      | 3280/9822 [1:41:12<3:00:02,  1.65s/it] 33%|███▎      | 3281/9822 [1:41:14<3:00:34,  1.66s/it] 33%|███▎      | 3282/9822 [1:41:15<3:00:56,  1.66s/it] 33%|███▎      | 3283/9822 [1:41:17<3:05:18,  1.70s/it] 33%|███▎      | 3284/9822 [1:41:19<3:04:20,  1.69s/it] 33%|███▎      | 3285/9822 [1:41:20<3:03:29,  1.68s/it] 33%|███▎      | 3286/9822 [1:41:22<3:02:58,  1.68s/it] 33%|███▎      | 3287/9822 [1:41:24<3:02:42,  1.68s/it] 33%|███▎      | 3288/9822 [1:41:25<3:02:22,  1.67s/it] 33%|███▎      | 3289/9822 [1:41:27<3:02:06,  1.67s/it] 33%|███▎      | 3290/9822 [1:41:29<3:02:11,  1.67s/it] 34%|███▎      | 3291/9822 [1:41:30<3:02:07,  1.67s/it] 34%|███▎      | 3292/9822 [1:41:32<3:01:55,  1.67s/it] 34%|███▎      | 3293/9822 [1:41:34<3:01:40,  1.67s/it] 34%|███▎      | 3294/9822 [1:41:35<3:01:24,  1.67s/it] 34%|███▎      | 3295/9822 [1:41:37<3:01:27,  1.67s/it] 34%|███▎      | 3296/9822 [1:41:39<3:01:40,  1.67s/it] 34%|███▎      | 3297/9822 [1:41:40<3:01:57,  1.67s/it] 34%|███▎      | 3298/9822 [1:41:42<3:02:10,  1.68s/it] 34%|███▎      | 3299/9822 [1:41:44<3:02:19,  1.68s/it] 34%|███▎      | 3300/9822 [1:41:46<3:02:14,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:22:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:22:32 - INFO - __main__ - ***** test Results*****
04/29/2024 13:22:32 - INFO - __main__ -   Training step = 3300
04/29/2024 13:22:32 - INFO - __main__ -  test_accuracy:0.8539531478770132 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:22:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:22:37 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:22:37 - INFO - __main__ -   Training step = 3300
04/29/2024 13:22:37 - INFO - __main__ -  eval_accuracy:0.8381545221530575 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:22:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:22:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:22:45 - INFO - __main__ -   Training step = 3300
04/29/2024 13:22:45 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 34%|███▎      | 3301/9822 [1:42:05<12:35:38,  6.95s/it] 34%|███▎      | 3302/9822 [1:42:06<9:43:07,  5.37s/it]  34%|███▎      | 3303/9822 [1:42:08<7:42:30,  4.26s/it] 34%|███▎      | 3304/9822 [1:42:10<6:17:59,  3.48s/it] 34%|███▎      | 3305/9822 [1:42:11<5:19:20,  2.94s/it] 34%|███▎      | 3306/9822 [1:42:13<4:37:39,  2.56s/it] 34%|███▎      | 3307/9822 [1:42:15<4:08:48,  2.29s/it] 34%|███▎      | 3308/9822 [1:42:16<3:48:15,  2.10s/it] 34%|███▎      | 3309/9822 [1:42:18<3:37:18,  2.00s/it] 34%|███▎      | 3310/9822 [1:42:20<3:26:19,  1.90s/it] 34%|███▎      | 3311/9822 [1:42:22<3:18:47,  1.83s/it] 34%|███▎      | 3312/9822 [1:42:23<3:13:41,  1.79s/it] 34%|███▎      | 3313/9822 [1:42:25<3:09:57,  1.75s/it] 34%|███▎      | 3314/9822 [1:42:27<3:07:01,  1.72s/it] 34%|███▍      | 3315/9822 [1:42:28<3:05:15,  1.71s/it] 34%|███▍      | 3316/9822 [1:42:30<3:03:56,  1.70s/it] 34%|███▍      | 3317/9822 [1:42:32<3:03:07,  1.69s/it] 34%|███▍      | 3318/9822 [1:42:33<3:02:44,  1.69s/it] 34%|███▍      | 3319/9822 [1:42:35<3:02:16,  1.68s/it] 34%|███▍      | 3320/9822 [1:42:37<3:01:50,  1.68s/it] 34%|███▍      | 3321/9822 [1:42:38<3:01:51,  1.68s/it] 34%|███▍      | 3322/9822 [1:42:40<3:02:00,  1.68s/it] 34%|███▍      | 3323/9822 [1:42:42<3:01:41,  1.68s/it] 34%|███▍      | 3324/9822 [1:42:43<3:01:38,  1.68s/it] 34%|███▍      | 3325/9822 [1:42:45<3:01:30,  1.68s/it] 34%|███▍      | 3326/9822 [1:42:47<3:01:12,  1.67s/it] 34%|███▍      | 3327/9822 [1:42:48<3:01:19,  1.68s/it] 34%|███▍      | 3328/9822 [1:42:50<3:01:02,  1.67s/it] 34%|███▍      | 3329/9822 [1:42:52<3:00:58,  1.67s/it] 34%|███▍      | 3330/9822 [1:42:53<3:00:57,  1.67s/it] 34%|███▍      | 3331/9822 [1:42:55<3:00:43,  1.67s/it] 34%|███▍      | 3332/9822 [1:42:57<3:00:48,  1.67s/it] 34%|███▍      | 3333/9822 [1:42:58<3:01:11,  1.68s/it] 34%|███▍      | 3334/9822 [1:43:00<3:01:19,  1.68s/it] 34%|███▍      | 3335/9822 [1:43:02<3:01:19,  1.68s/it] 34%|███▍      | 3336/9822 [1:43:04<3:04:57,  1.71s/it] 34%|███▍      | 3337/9822 [1:43:05<3:03:54,  1.70s/it] 34%|███▍      | 3338/9822 [1:43:07<3:03:04,  1.69s/it] 34%|███▍      | 3339/9822 [1:43:09<3:02:35,  1.69s/it] 34%|███▍      | 3340/9822 [1:43:10<3:02:29,  1.69s/it] 34%|███▍      | 3341/9822 [1:43:12<3:02:25,  1.69s/it] 34%|███▍      | 3342/9822 [1:43:14<3:01:45,  1.68s/it] 34%|███▍      | 3343/9822 [1:43:15<3:01:43,  1.68s/it] 34%|███▍      | 3344/9822 [1:43:17<3:01:52,  1.68s/it] 34%|███▍      | 3345/9822 [1:43:19<3:01:57,  1.69s/it] 34%|███▍      | 3346/9822 [1:43:20<3:01:51,  1.68s/it] 34%|███▍      | 3347/9822 [1:43:22<3:01:43,  1.68s/it] 34%|███▍      | 3348/9822 [1:43:24<3:01:47,  1.68s/it] 34%|███▍      | 3349/9822 [1:43:25<3:01:44,  1.68s/it] 34%|███▍      | 3350/9822 [1:43:27<3:01:49,  1.69s/it] 34%|███▍      | 3351/9822 [1:43:29<3:01:44,  1.69s/it] 34%|███▍      | 3352/9822 [1:43:30<3:01:41,  1.68s/it] 34%|███▍      | 3353/9822 [1:43:32<3:01:49,  1.69s/it] 34%|███▍      | 3354/9822 [1:43:34<3:00:04,  1.67s/it] 34%|███▍      | 3355/9822 [1:43:35<3:00:35,  1.68s/it] 34%|███▍      | 3356/9822 [1:43:37<3:00:59,  1.68s/it] 34%|███▍      | 3357/9822 [1:43:39<3:00:58,  1.68s/it] 34%|███▍      | 3358/9822 [1:43:41<3:01:12,  1.68s/it] 34%|███▍      | 3359/9822 [1:43:42<3:01:16,  1.68s/it] 34%|███▍      | 3360/9822 [1:43:44<3:01:33,  1.69s/it] 34%|███▍      | 3361/9822 [1:43:46<3:01:15,  1.68s/it] 34%|███▍      | 3362/9822 [1:43:47<3:01:19,  1.68s/it] 34%|███▍      | 3363/9822 [1:43:49<3:01:32,  1.69s/it] 34%|███▍      | 3364/9822 [1:43:51<3:01:35,  1.69s/it] 34%|███▍      | 3365/9822 [1:43:52<3:01:41,  1.69s/it] 34%|███▍      | 3366/9822 [1:43:54<3:01:32,  1.69s/it] 34%|███▍      | 3367/9822 [1:43:56<3:01:03,  1.68s/it] 34%|███▍      | 3368/9822 [1:43:57<3:00:57,  1.68s/it] 34%|███▍      | 3369/9822 [1:43:59<3:03:58,  1.71s/it] 34%|███▍      | 3370/9822 [1:44:01<3:02:33,  1.70s/it] 34%|███▍      | 3371/9822 [1:44:02<3:01:39,  1.69s/it] 34%|███▍      | 3372/9822 [1:44:04<3:01:00,  1.68s/it] 34%|███▍      | 3373/9822 [1:44:06<3:00:13,  1.68s/it] 34%|███▍      | 3374/9822 [1:44:07<3:00:04,  1.68s/it] 34%|███▍      | 3375/9822 [1:44:09<2:59:57,  1.67s/it] 34%|███▍      | 3376/9822 [1:44:11<2:59:34,  1.67s/it] 34%|███▍      | 3377/9822 [1:44:13<2:59:31,  1.67s/it] 34%|███▍      | 3378/9822 [1:44:14<2:59:38,  1.67s/it] 34%|███▍      | 3379/9822 [1:44:16<2:59:26,  1.67s/it] 34%|███▍      | 3380/9822 [1:44:18<2:59:30,  1.67s/it] 34%|███▍      | 3381/9822 [1:44:19<2:59:31,  1.67s/it] 34%|███▍      | 3382/9822 [1:44:21<2:59:43,  1.67s/it] 34%|███▍      | 3383/9822 [1:44:23<2:59:36,  1.67s/it] 34%|███▍      | 3384/9822 [1:44:24<2:59:39,  1.67s/it] 34%|███▍      | 3385/9822 [1:44:26<2:59:48,  1.68s/it] 34%|███▍      | 3386/9822 [1:44:28<2:59:35,  1.67s/it] 34%|███▍      | 3387/9822 [1:44:29<2:59:24,  1.67s/it] 34%|███▍      | 3388/9822 [1:44:31<2:58:59,  1.67s/it] 35%|███▍      | 3389/9822 [1:44:33<2:59:01,  1.67s/it] 35%|███▍      | 3390/9822 [1:44:34<2:59:06,  1.67s/it] 35%|███▍      | 3391/9822 [1:44:36<3:02:38,  1.70s/it] 35%|███▍      | 3392/9822 [1:44:38<3:01:28,  1.69s/it] 35%|███▍      | 3393/9822 [1:44:39<3:00:45,  1.69s/it] 35%|███▍      | 3394/9822 [1:44:41<3:00:45,  1.69s/it] 35%|███▍      | 3395/9822 [1:44:43<3:00:25,  1.68s/it] 35%|███▍      | 3396/9822 [1:44:44<2:59:56,  1.68s/it] 35%|███▍      | 3397/9822 [1:44:46<3:00:07,  1.68s/it] 35%|███▍      | 3398/9822 [1:44:48<2:59:58,  1.68s/it] 35%|███▍      | 3399/9822 [1:44:49<2:59:52,  1.68s/it] 35%|███▍      | 3400/9822 [1:44:51<3:00:10,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:25:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:25:38 - INFO - __main__ - ***** test Results*****
04/29/2024 13:25:38 - INFO - __main__ -   Training step = 3400
04/29/2024 13:25:38 - INFO - __main__ -  test_accuracy:0.8674963396778916 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:25:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:25:42 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:25:42 - INFO - __main__ -   Training step = 3400
04/29/2024 13:25:42 - INFO - __main__ -  eval_accuracy:0.8487733430977664 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:25:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:25:51 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:25:51 - INFO - __main__ -   Training step = 3400
04/29/2024 13:25:51 - INFO - __main__ -  eval_accuracy:0.9154155986818016 
 35%|███▍      | 3401/9822 [1:45:10<12:23:46,  6.95s/it] 35%|███▍      | 3402/9822 [1:45:12<9:33:56,  5.36s/it]  35%|███▍      | 3403/9822 [1:45:14<7:35:27,  4.26s/it] 35%|███▍      | 3404/9822 [1:45:15<6:12:10,  3.48s/it] 35%|███▍      | 3405/9822 [1:45:17<5:13:48,  2.93s/it] 35%|███▍      | 3406/9822 [1:45:19<4:33:01,  2.55s/it] 35%|███▍      | 3407/9822 [1:45:20<4:05:01,  2.29s/it] 35%|███▍      | 3408/9822 [1:45:22<3:45:22,  2.11s/it] 35%|███▍      | 3409/9822 [1:45:24<3:31:23,  1.98s/it] 35%|███▍      | 3410/9822 [1:45:25<3:21:10,  1.88s/it] 35%|███▍      | 3411/9822 [1:45:27<3:14:21,  1.82s/it] 35%|███▍      | 3412/9822 [1:45:29<3:09:47,  1.78s/it] 35%|███▍      | 3413/9822 [1:45:30<3:06:03,  1.74s/it] 35%|███▍      | 3414/9822 [1:45:32<3:04:47,  1.73s/it] 35%|███▍      | 3415/9822 [1:45:34<3:06:48,  1.75s/it] 35%|███▍      | 3416/9822 [1:45:36<3:04:49,  1.73s/it] 35%|███▍      | 3417/9822 [1:45:37<3:03:16,  1.72s/it] 35%|███▍      | 3418/9822 [1:45:39<3:02:20,  1.71s/it] 35%|███▍      | 3419/9822 [1:45:41<3:01:13,  1.70s/it] 35%|███▍      | 3420/9822 [1:45:42<3:00:49,  1.69s/it] 35%|███▍      | 3421/9822 [1:45:44<3:00:39,  1.69s/it] 35%|███▍      | 3422/9822 [1:45:46<3:00:28,  1.69s/it] 35%|███▍      | 3423/9822 [1:45:47<3:00:18,  1.69s/it] 35%|███▍      | 3424/9822 [1:45:49<2:59:55,  1.69s/it] 35%|███▍      | 3425/9822 [1:45:51<2:59:44,  1.69s/it] 35%|███▍      | 3426/9822 [1:45:52<2:59:43,  1.69s/it] 35%|███▍      | 3427/9822 [1:45:54<2:59:38,  1.69s/it] 35%|███▍      | 3428/9822 [1:45:56<2:59:33,  1.68s/it] 35%|███▍      | 3429/9822 [1:45:57<2:59:29,  1.68s/it] 35%|███▍      | 3430/9822 [1:45:59<2:59:27,  1.68s/it] 35%|███▍      | 3431/9822 [1:46:01<2:59:30,  1.69s/it] 35%|███▍      | 3432/9822 [1:46:03<2:59:32,  1.69s/it] 35%|███▍      | 3433/9822 [1:46:04<2:59:38,  1.69s/it] 35%|███▍      | 3434/9822 [1:46:06<2:59:45,  1.69s/it] 35%|███▍      | 3435/9822 [1:46:08<2:59:43,  1.69s/it] 35%|███▍      | 3436/9822 [1:46:09<2:59:18,  1.68s/it] 35%|███▍      | 3437/9822 [1:46:11<2:59:29,  1.69s/it] 35%|███▌      | 3438/9822 [1:46:13<2:59:24,  1.69s/it] 35%|███▌      | 3439/9822 [1:46:14<2:59:31,  1.69s/it] 35%|███▌      | 3440/9822 [1:46:16<2:57:52,  1.67s/it] 35%|███▌      | 3441/9822 [1:46:18<3:01:47,  1.71s/it] 35%|███▌      | 3442/9822 [1:46:19<3:00:57,  1.70s/it] 35%|███▌      | 3443/9822 [1:46:21<2:59:42,  1.69s/it] 35%|███▌      | 3444/9822 [1:46:23<2:58:52,  1.68s/it] 35%|███▌      | 3445/9822 [1:46:24<2:58:01,  1.67s/it] 35%|███▌      | 3446/9822 [1:46:26<2:58:01,  1.68s/it] 35%|███▌      | 3447/9822 [1:46:28<2:57:42,  1.67s/it] 35%|███▌      | 3448/9822 [1:46:29<2:57:07,  1.67s/it] 35%|███▌      | 3449/9822 [1:46:31<2:56:41,  1.66s/it] 35%|███▌      | 3450/9822 [1:46:33<2:56:26,  1.66s/it] 35%|███▌      | 3451/9822 [1:46:34<2:56:26,  1.66s/it] 35%|███▌      | 3452/9822 [1:46:36<2:56:21,  1.66s/it] 35%|███▌      | 3453/9822 [1:46:38<2:56:17,  1.66s/it] 35%|███▌      | 3454/9822 [1:46:39<2:56:59,  1.67s/it] 35%|███▌      | 3455/9822 [1:46:41<2:56:38,  1.66s/it] 35%|███▌      | 3456/9822 [1:46:43<2:57:16,  1.67s/it] 35%|███▌      | 3457/9822 [1:46:44<2:56:48,  1.67s/it] 35%|███▌      | 3458/9822 [1:46:46<2:56:42,  1.67s/it] 35%|███▌      | 3459/9822 [1:46:48<2:56:37,  1.67s/it] 35%|███▌      | 3460/9822 [1:46:49<2:57:17,  1.67s/it] 35%|███▌      | 3461/9822 [1:46:51<2:57:18,  1.67s/it] 35%|███▌      | 3462/9822 [1:46:53<2:57:09,  1.67s/it] 35%|███▌      | 3463/9822 [1:46:54<2:57:12,  1.67s/it] 35%|███▌      | 3464/9822 [1:46:56<2:57:16,  1.67s/it] 35%|███▌      | 3465/9822 [1:46:58<2:57:33,  1.68s/it] 35%|███▌      | 3466/9822 [1:47:00<2:57:30,  1.68s/it] 35%|███▌      | 3467/9822 [1:47:01<2:57:19,  1.67s/it] 35%|███▌      | 3468/9822 [1:47:03<2:57:30,  1.68s/it] 35%|███▌      | 3469/9822 [1:47:05<2:57:21,  1.67s/it] 35%|███▌      | 3470/9822 [1:47:06<2:57:00,  1.67s/it] 35%|███▌      | 3471/9822 [1:47:08<2:56:48,  1.67s/it] 35%|███▌      | 3472/9822 [1:47:10<2:57:04,  1.67s/it] 35%|███▌      | 3473/9822 [1:47:11<2:57:11,  1.67s/it] 35%|███▌      | 3474/9822 [1:47:13<3:00:41,  1.71s/it] 35%|███▌      | 3475/9822 [1:47:15<2:59:43,  1.70s/it] 35%|███▌      | 3476/9822 [1:47:16<2:58:50,  1.69s/it] 35%|███▌      | 3477/9822 [1:47:18<2:58:10,  1.68s/it] 35%|███▌      | 3478/9822 [1:47:20<2:57:21,  1.68s/it] 35%|███▌      | 3479/9822 [1:47:21<2:57:01,  1.67s/it] 35%|███▌      | 3480/9822 [1:47:23<2:56:34,  1.67s/it] 35%|███▌      | 3481/9822 [1:47:25<2:56:18,  1.67s/it] 35%|███▌      | 3482/9822 [1:47:26<2:56:00,  1.67s/it] 35%|███▌      | 3483/9822 [1:47:28<2:55:37,  1.66s/it] 35%|███▌      | 3484/9822 [1:47:30<2:56:07,  1.67s/it] 35%|███▌      | 3485/9822 [1:47:31<2:56:12,  1.67s/it] 35%|███▌      | 3486/9822 [1:47:33<2:56:27,  1.67s/it] 36%|███▌      | 3487/9822 [1:47:35<2:55:56,  1.67s/it] 36%|███▌      | 3488/9822 [1:47:36<2:56:01,  1.67s/it] 36%|███▌      | 3489/9822 [1:47:38<2:55:56,  1.67s/it] 36%|███▌      | 3490/9822 [1:47:40<2:56:14,  1.67s/it] 36%|███▌      | 3491/9822 [1:47:41<2:56:04,  1.67s/it] 36%|███▌      | 3492/9822 [1:47:43<2:56:20,  1.67s/it] 36%|███▌      | 3493/9822 [1:47:45<2:56:27,  1.67s/it] 36%|███▌      | 3494/9822 [1:47:46<2:56:51,  1.68s/it] 36%|███▌      | 3495/9822 [1:47:48<2:57:03,  1.68s/it] 36%|███▌      | 3496/9822 [1:47:50<3:00:30,  1.71s/it] 36%|███▌      | 3497/9822 [1:47:52<2:59:41,  1.70s/it] 36%|███▌      | 3498/9822 [1:47:53<2:59:00,  1.70s/it] 36%|███▌      | 3499/9822 [1:47:55<2:58:03,  1.69s/it] 36%|███▌      | 3500/9822 [1:47:57<2:57:29,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:28:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:28:43 - INFO - __main__ - ***** test Results*****
04/29/2024 13:28:43 - INFO - __main__ -   Training step = 3500
04/29/2024 13:28:43 - INFO - __main__ -  test_accuracy:0.866398243045388 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:28:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:28:48 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:28:48 - INFO - __main__ -   Training step = 3500
04/29/2024 13:28:48 - INFO - __main__ -  eval_accuracy:0.8462101794214574 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8498718418161846}
test:
{'accuracy': 0.8667642752562226}
04/29/2024 13:28:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:28:56 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:28:56 - INFO - __main__ -   Training step = 3500
04/29/2024 13:28:56 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 36%|███▌      | 3501/9822 [1:48:16<12:12:30,  6.95s/it] 36%|███▌      | 3502/9822 [1:48:18<9:25:37,  5.37s/it]  36%|███▌      | 3503/9822 [1:48:19<7:28:42,  4.26s/it] 36%|███▌      | 3504/9822 [1:48:21<6:07:10,  3.49s/it] 36%|███▌      | 3505/9822 [1:48:23<5:10:00,  2.94s/it] 36%|███▌      | 3506/9822 [1:48:24<4:30:02,  2.57s/it] 36%|███▌      | 3507/9822 [1:48:26<4:01:48,  2.30s/it] 36%|███▌      | 3508/9822 [1:48:28<3:42:21,  2.11s/it] 36%|███▌      | 3509/9822 [1:48:29<3:28:09,  1.98s/it] 36%|███▌      | 3510/9822 [1:48:31<3:18:35,  1.89s/it] 36%|███▌      | 3511/9822 [1:48:33<3:11:59,  1.83s/it] 36%|███▌      | 3512/9822 [1:48:34<3:07:18,  1.78s/it] 36%|███▌      | 3513/9822 [1:48:36<3:03:47,  1.75s/it] 36%|███▌      | 3514/9822 [1:48:38<3:01:21,  1.73s/it] 36%|███▌      | 3515/9822 [1:48:39<2:59:59,  1.71s/it] 36%|███▌      | 3516/9822 [1:48:41<2:58:44,  1.70s/it] 36%|███▌      | 3517/9822 [1:48:43<2:58:04,  1.69s/it] 36%|███▌      | 3518/9822 [1:48:44<2:57:41,  1.69s/it] 36%|███▌      | 3519/9822 [1:48:46<2:57:03,  1.69s/it] 36%|███▌      | 3520/9822 [1:48:48<3:00:00,  1.71s/it] 36%|███▌      | 3521/9822 [1:48:49<2:58:25,  1.70s/it] 36%|███▌      | 3522/9822 [1:48:51<2:57:48,  1.69s/it] 36%|███▌      | 3523/9822 [1:48:53<2:57:14,  1.69s/it] 36%|███▌      | 3524/9822 [1:48:54<2:56:54,  1.69s/it] 36%|███▌      | 3525/9822 [1:48:56<2:56:25,  1.68s/it] 36%|███▌      | 3526/9822 [1:48:58<2:54:41,  1.66s/it] 36%|███▌      | 3527/9822 [1:48:59<2:54:36,  1.66s/it] 36%|███▌      | 3528/9822 [1:49:01<2:55:04,  1.67s/it] 36%|███▌      | 3529/9822 [1:49:03<2:55:10,  1.67s/it] 36%|███▌      | 3530/9822 [1:49:04<2:55:09,  1.67s/it] 36%|███▌      | 3531/9822 [1:49:06<2:55:28,  1.67s/it] 36%|███▌      | 3532/9822 [1:49:08<2:55:38,  1.68s/it] 36%|███▌      | 3533/9822 [1:49:10<2:55:58,  1.68s/it] 36%|███▌      | 3534/9822 [1:49:11<2:56:06,  1.68s/it] 36%|███▌      | 3535/9822 [1:49:13<2:56:18,  1.68s/it] 36%|███▌      | 3536/9822 [1:49:15<2:56:12,  1.68s/it] 36%|███▌      | 3537/9822 [1:49:16<2:56:15,  1.68s/it] 36%|███▌      | 3538/9822 [1:49:18<2:55:36,  1.68s/it] 36%|███▌      | 3539/9822 [1:49:20<2:55:49,  1.68s/it] 36%|███▌      | 3540/9822 [1:49:21<2:55:36,  1.68s/it] 36%|███▌      | 3541/9822 [1:49:23<2:55:24,  1.68s/it] 36%|███▌      | 3542/9822 [1:49:25<2:55:29,  1.68s/it] 36%|███▌      | 3543/9822 [1:49:26<2:55:19,  1.68s/it] 36%|███▌      | 3544/9822 [1:49:28<2:55:26,  1.68s/it] 36%|███▌      | 3545/9822 [1:49:30<2:55:38,  1.68s/it] 36%|███▌      | 3546/9822 [1:49:31<2:59:03,  1.71s/it] 36%|███▌      | 3547/9822 [1:49:33<2:58:08,  1.70s/it] 36%|███▌      | 3548/9822 [1:49:35<2:57:36,  1.70s/it] 36%|███▌      | 3549/9822 [1:49:37<2:57:08,  1.69s/it] 36%|███▌      | 3550/9822 [1:49:38<2:56:48,  1.69s/it] 36%|███▌      | 3551/9822 [1:49:40<2:56:33,  1.69s/it] 36%|███▌      | 3552/9822 [1:49:42<2:56:24,  1.69s/it] 36%|███▌      | 3553/9822 [1:49:43<2:56:11,  1.69s/it] 36%|███▌      | 3554/9822 [1:49:45<2:56:04,  1.69s/it] 36%|███▌      | 3555/9822 [1:49:47<2:56:00,  1.69s/it] 36%|███▌      | 3556/9822 [1:49:48<2:55:32,  1.68s/it] 36%|███▌      | 3557/9822 [1:49:50<2:55:01,  1.68s/it] 36%|███▌      | 3558/9822 [1:49:52<2:55:06,  1.68s/it] 36%|███▌      | 3559/9822 [1:49:53<2:55:01,  1.68s/it] 36%|███▌      | 3560/9822 [1:49:55<2:54:50,  1.68s/it] 36%|███▋      | 3561/9822 [1:49:57<2:54:57,  1.68s/it] 36%|███▋      | 3562/9822 [1:49:58<2:54:43,  1.67s/it] 36%|███▋      | 3563/9822 [1:50:00<2:54:24,  1.67s/it] 36%|███▋      | 3564/9822 [1:50:02<2:54:54,  1.68s/it] 36%|███▋      | 3565/9822 [1:50:03<2:54:35,  1.67s/it] 36%|███▋      | 3566/9822 [1:50:05<2:54:45,  1.68s/it] 36%|███▋      | 3567/9822 [1:50:07<2:54:35,  1.67s/it] 36%|███▋      | 3568/9822 [1:50:08<2:54:39,  1.68s/it] 36%|███▋      | 3569/9822 [1:50:10<2:54:38,  1.68s/it] 36%|███▋      | 3570/9822 [1:50:12<2:54:42,  1.68s/it] 36%|███▋      | 3571/9822 [1:50:13<2:54:26,  1.67s/it] 36%|███▋      | 3572/9822 [1:50:15<2:54:16,  1.67s/it] 36%|███▋      | 3573/9822 [1:50:17<2:53:57,  1.67s/it] 36%|███▋      | 3574/9822 [1:50:18<2:53:54,  1.67s/it] 36%|███▋      | 3575/9822 [1:50:20<2:54:08,  1.67s/it] 36%|███▋      | 3576/9822 [1:50:22<2:54:00,  1.67s/it] 36%|███▋      | 3577/9822 [1:50:23<2:53:58,  1.67s/it] 36%|███▋      | 3578/9822 [1:50:25<2:53:51,  1.67s/it] 36%|███▋      | 3579/9822 [1:50:27<2:57:29,  1.71s/it] 36%|███▋      | 3580/9822 [1:50:29<2:56:38,  1.70s/it] 36%|███▋      | 3581/9822 [1:50:30<2:55:56,  1.69s/it] 36%|███▋      | 3582/9822 [1:50:32<2:55:22,  1.69s/it] 36%|███▋      | 3583/9822 [1:50:34<2:54:50,  1.68s/it] 36%|███▋      | 3584/9822 [1:50:35<2:54:46,  1.68s/it] 36%|███▋      | 3585/9822 [1:50:37<2:54:35,  1.68s/it] 37%|███▋      | 3586/9822 [1:50:39<2:54:20,  1.68s/it] 37%|███▋      | 3587/9822 [1:50:40<2:53:59,  1.67s/it] 37%|███▋      | 3588/9822 [1:50:42<2:53:50,  1.67s/it] 37%|███▋      | 3589/9822 [1:50:44<2:53:43,  1.67s/it] 37%|███▋      | 3590/9822 [1:50:45<2:53:40,  1.67s/it] 37%|███▋      | 3591/9822 [1:50:47<2:53:40,  1.67s/it] 37%|███▋      | 3592/9822 [1:50:49<2:53:35,  1.67s/it] 37%|███▋      | 3593/9822 [1:50:50<2:53:43,  1.67s/it] 37%|███▋      | 3594/9822 [1:50:52<2:53:59,  1.68s/it] 37%|███▋      | 3595/9822 [1:50:54<2:53:57,  1.68s/it] 37%|███▋      | 3596/9822 [1:50:55<2:53:39,  1.67s/it] 37%|███▋      | 3597/9822 [1:50:57<2:53:43,  1.67s/it] 37%|███▋      | 3598/9822 [1:50:59<2:53:43,  1.67s/it] 37%|███▋      | 3599/9822 [1:51:00<2:53:20,  1.67s/it] 37%|███▋      | 3600/9822 [1:51:02<2:53:36,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:31:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:31:49 - INFO - __main__ - ***** test Results*****
04/29/2024 13:31:49 - INFO - __main__ -   Training step = 3600
04/29/2024 13:31:49 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:31:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:31:53 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:31:53 - INFO - __main__ -   Training step = 3600
04/29/2024 13:31:53 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 13:31:53,875 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 13:31:53,875 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 13:31:53,917 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 13:31:55,058 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:32:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:32:03 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:32:03 - INFO - __main__ -   Training step = 3600
04/29/2024 13:32:03 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 37%|███▋      | 3601/9822 [1:51:23<12:40:17,  7.33s/it] 37%|███▋      | 3602/9822 [1:51:24<9:44:00,  5.63s/it]  37%|███▋      | 3603/9822 [1:51:26<7:40:20,  4.44s/it] 37%|███▋      | 3604/9822 [1:51:28<6:13:56,  3.61s/it] 37%|███▋      | 3605/9822 [1:51:29<5:13:42,  3.03s/it] 37%|███▋      | 3606/9822 [1:51:31<4:31:20,  2.62s/it] 37%|███▋      | 3607/9822 [1:51:33<4:01:46,  2.33s/it] 37%|███▋      | 3608/9822 [1:51:34<3:41:01,  2.13s/it] 37%|███▋      | 3609/9822 [1:51:36<3:26:26,  1.99s/it] 37%|███▋      | 3610/9822 [1:51:38<3:16:32,  1.90s/it] 37%|███▋      | 3611/9822 [1:51:39<3:09:47,  1.83s/it] 37%|███▋      | 3612/9822 [1:51:41<3:03:01,  1.77s/it] 37%|███▋      | 3613/9822 [1:51:43<2:59:36,  1.74s/it] 37%|███▋      | 3614/9822 [1:51:44<2:57:43,  1.72s/it] 37%|███▋      | 3615/9822 [1:51:46<2:55:58,  1.70s/it] 37%|███▋      | 3616/9822 [1:51:48<2:55:01,  1.69s/it] 37%|███▋      | 3617/9822 [1:51:49<2:54:49,  1.69s/it] 37%|███▋      | 3618/9822 [1:51:51<2:54:23,  1.69s/it] 37%|███▋      | 3619/9822 [1:51:53<2:53:51,  1.68s/it] 37%|███▋      | 3620/9822 [1:51:54<2:53:18,  1.68s/it] 37%|███▋      | 3621/9822 [1:51:56<2:53:37,  1.68s/it] 37%|███▋      | 3622/9822 [1:51:58<2:53:35,  1.68s/it] 37%|███▋      | 3623/9822 [1:51:59<2:53:20,  1.68s/it] 37%|███▋      | 3624/9822 [1:52:01<2:53:17,  1.68s/it] 37%|███▋      | 3625/9822 [1:52:03<2:53:00,  1.68s/it] 37%|███▋      | 3626/9822 [1:52:04<2:53:09,  1.68s/it] 37%|███▋      | 3627/9822 [1:52:06<2:53:12,  1.68s/it] 37%|███▋      | 3628/9822 [1:52:08<2:53:46,  1.68s/it] 37%|███▋      | 3629/9822 [1:52:09<2:52:57,  1.68s/it] 37%|███▋      | 3630/9822 [1:52:11<2:52:31,  1.67s/it] 37%|███▋      | 3631/9822 [1:52:13<2:52:04,  1.67s/it] 37%|███▋      | 3632/9822 [1:52:14<2:51:58,  1.67s/it] 37%|███▋      | 3633/9822 [1:52:16<2:52:02,  1.67s/it] 37%|███▋      | 3634/9822 [1:52:18<2:52:02,  1.67s/it] 37%|███▋      | 3635/9822 [1:52:19<2:52:32,  1.67s/it] 37%|███▋      | 3636/9822 [1:52:21<2:52:30,  1.67s/it] 37%|███▋      | 3637/9822 [1:52:23<2:52:20,  1.67s/it] 37%|███▋      | 3638/9822 [1:52:24<2:52:09,  1.67s/it] 37%|███▋      | 3639/9822 [1:52:26<2:52:32,  1.67s/it] 37%|███▋      | 3640/9822 [1:52:28<2:52:49,  1.68s/it] 37%|███▋      | 3641/9822 [1:52:30<2:56:13,  1.71s/it] 37%|███▋      | 3642/9822 [1:52:31<2:55:22,  1.70s/it] 37%|███▋      | 3643/9822 [1:52:33<2:54:40,  1.70s/it] 37%|███▋      | 3644/9822 [1:52:35<2:54:26,  1.69s/it] 37%|███▋      | 3645/9822 [1:52:36<2:54:00,  1.69s/it] 37%|███▋      | 3646/9822 [1:52:38<2:53:54,  1.69s/it] 37%|███▋      | 3647/9822 [1:52:40<2:53:35,  1.69s/it] 37%|███▋      | 3648/9822 [1:52:41<2:53:15,  1.68s/it] 37%|███▋      | 3649/9822 [1:52:43<2:53:26,  1.69s/it] 37%|███▋      | 3650/9822 [1:52:45<2:53:27,  1.69s/it] 37%|███▋      | 3651/9822 [1:52:46<2:53:25,  1.69s/it] 37%|███▋      | 3652/9822 [1:52:48<2:53:18,  1.69s/it] 37%|███▋      | 3653/9822 [1:52:50<2:53:14,  1.68s/it] 37%|███▋      | 3654/9822 [1:52:51<2:53:15,  1.69s/it] 37%|███▋      | 3655/9822 [1:52:53<2:53:13,  1.69s/it] 37%|███▋      | 3656/9822 [1:52:55<2:53:04,  1.68s/it] 37%|███▋      | 3657/9822 [1:52:56<2:53:00,  1.68s/it] 37%|███▋      | 3658/9822 [1:52:58<2:52:59,  1.68s/it] 37%|███▋      | 3659/9822 [1:53:00<2:52:48,  1.68s/it] 37%|███▋      | 3660/9822 [1:53:02<2:52:54,  1.68s/it] 37%|███▋      | 3661/9822 [1:53:03<2:53:42,  1.69s/it] 37%|███▋      | 3662/9822 [1:53:05<2:53:04,  1.69s/it] 37%|███▋      | 3663/9822 [1:53:07<2:52:30,  1.68s/it] 37%|███▋      | 3664/9822 [1:53:08<2:52:12,  1.68s/it] 37%|███▋      | 3665/9822 [1:53:10<2:51:49,  1.67s/it] 37%|███▋      | 3666/9822 [1:53:12<2:52:01,  1.68s/it] 37%|███▋      | 3667/9822 [1:53:13<2:51:57,  1.68s/it] 37%|███▋      | 3668/9822 [1:53:15<2:54:46,  1.70s/it] 37%|███▋      | 3669/9822 [1:53:17<2:53:53,  1.70s/it] 37%|███▋      | 3670/9822 [1:53:18<2:52:56,  1.69s/it] 37%|███▋      | 3671/9822 [1:53:20<2:52:04,  1.68s/it] 37%|███▋      | 3672/9822 [1:53:22<2:51:39,  1.67s/it] 37%|███▋      | 3673/9822 [1:53:23<2:51:44,  1.68s/it] 37%|███▋      | 3674/9822 [1:53:25<2:51:50,  1.68s/it] 37%|███▋      | 3675/9822 [1:53:27<2:51:36,  1.68s/it] 37%|███▋      | 3676/9822 [1:53:28<2:51:29,  1.67s/it] 37%|███▋      | 3677/9822 [1:53:30<2:51:29,  1.67s/it] 37%|███▋      | 3678/9822 [1:53:32<2:51:07,  1.67s/it] 37%|███▋      | 3679/9822 [1:53:33<2:51:04,  1.67s/it] 37%|███▋      | 3680/9822 [1:53:35<2:51:08,  1.67s/it] 37%|███▋      | 3681/9822 [1:53:37<2:50:35,  1.67s/it] 37%|███▋      | 3682/9822 [1:53:38<2:50:32,  1.67s/it] 37%|███▋      | 3683/9822 [1:53:40<2:50:39,  1.67s/it] 38%|███▊      | 3684/9822 [1:53:42<2:50:48,  1.67s/it] 38%|███▊      | 3685/9822 [1:53:43<2:50:41,  1.67s/it] 38%|███▊      | 3686/9822 [1:53:45<2:50:34,  1.67s/it] 38%|███▊      | 3687/9822 [1:53:47<2:50:36,  1.67s/it] 38%|███▊      | 3688/9822 [1:53:48<2:50:34,  1.67s/it] 38%|███▊      | 3689/9822 [1:53:50<2:51:08,  1.67s/it] 38%|███▊      | 3690/9822 [1:53:52<2:51:00,  1.67s/it] 38%|███▊      | 3691/9822 [1:53:53<2:50:58,  1.67s/it] 38%|███▊      | 3692/9822 [1:53:55<2:50:51,  1.67s/it] 38%|███▊      | 3693/9822 [1:53:57<2:50:36,  1.67s/it] 38%|███▊      | 3694/9822 [1:53:58<2:50:16,  1.67s/it] 38%|███▊      | 3695/9822 [1:54:00<2:53:40,  1.70s/it] 38%|███▊      | 3696/9822 [1:54:02<2:52:33,  1.69s/it] 38%|███▊      | 3697/9822 [1:54:04<2:51:54,  1.68s/it] 38%|███▊      | 3698/9822 [1:54:05<2:49:43,  1.66s/it] 38%|███▊      | 3699/9822 [1:54:07<2:49:53,  1.66s/it] 38%|███▊      | 3700/9822 [1:54:09<2:49:55,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:34:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:34:55 - INFO - __main__ - ***** test Results*****
04/29/2024 13:34:55 - INFO - __main__ -   Training step = 3700
04/29/2024 13:34:55 - INFO - __main__ -  test_accuracy:0.8660322108345534 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:35:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:35:00 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:35:00 - INFO - __main__ -   Training step = 3700
04/29/2024 13:35:00 - INFO - __main__ -  eval_accuracy:0.8454778469425119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:35:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:35:08 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:35:08 - INFO - __main__ -   Training step = 3700
04/29/2024 13:35:08 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 38%|███▊      | 3701/9822 [1:54:28<11:47:33,  6.94s/it] 38%|███▊      | 3702/9822 [1:54:29<9:06:07,  5.35s/it]  38%|███▊      | 3703/9822 [1:54:31<7:13:22,  4.25s/it] 38%|███▊      | 3704/9822 [1:54:33<5:54:31,  3.48s/it] 38%|███▊      | 3705/9822 [1:54:34<4:59:21,  2.94s/it] 38%|███▊      | 3706/9822 [1:54:36<4:20:54,  2.56s/it] 38%|███▊      | 3707/9822 [1:54:38<3:53:39,  2.29s/it] 38%|███▊      | 3708/9822 [1:54:39<3:34:48,  2.11s/it] 38%|███▊      | 3709/9822 [1:54:41<3:21:34,  1.98s/it] 38%|███▊      | 3710/9822 [1:54:43<3:12:26,  1.89s/it] 38%|███▊      | 3711/9822 [1:54:44<3:05:44,  1.82s/it] 38%|███▊      | 3712/9822 [1:54:46<3:01:24,  1.78s/it] 38%|███▊      | 3713/9822 [1:54:48<2:58:04,  1.75s/it] 38%|███▊      | 3714/9822 [1:54:50<2:55:39,  1.73s/it] 38%|███▊      | 3715/9822 [1:54:51<2:54:14,  1.71s/it] 38%|███▊      | 3716/9822 [1:54:53<2:52:45,  1.70s/it] 38%|███▊      | 3717/9822 [1:54:55<2:51:34,  1.69s/it] 38%|███▊      | 3718/9822 [1:54:56<2:51:28,  1.69s/it] 38%|███▊      | 3719/9822 [1:54:58<2:51:10,  1.68s/it] 38%|███▊      | 3720/9822 [1:55:00<2:50:48,  1.68s/it] 38%|███▊      | 3721/9822 [1:55:01<2:51:02,  1.68s/it] 38%|███▊      | 3722/9822 [1:55:03<2:50:49,  1.68s/it] 38%|███▊      | 3723/9822 [1:55:05<2:54:04,  1.71s/it] 38%|███▊      | 3724/9822 [1:55:06<2:53:08,  1.70s/it] 38%|███▊      | 3725/9822 [1:55:08<2:52:26,  1.70s/it] 38%|███▊      | 3726/9822 [1:55:10<2:51:37,  1.69s/it] 38%|███▊      | 3727/9822 [1:55:11<2:51:03,  1.68s/it] 38%|███▊      | 3728/9822 [1:55:13<2:50:43,  1.68s/it] 38%|███▊      | 3729/9822 [1:55:15<2:50:34,  1.68s/it] 38%|███▊      | 3730/9822 [1:55:16<2:50:16,  1.68s/it] 38%|███▊      | 3731/9822 [1:55:18<2:50:06,  1.68s/it] 38%|███▊      | 3732/9822 [1:55:20<2:49:48,  1.67s/it] 38%|███▊      | 3733/9822 [1:55:21<2:49:54,  1.67s/it] 38%|███▊      | 3734/9822 [1:55:23<2:50:05,  1.68s/it] 38%|███▊      | 3735/9822 [1:55:25<2:49:54,  1.67s/it] 38%|███▊      | 3736/9822 [1:55:26<2:49:52,  1.67s/it] 38%|███▊      | 3737/9822 [1:55:28<2:49:32,  1.67s/it] 38%|███▊      | 3738/9822 [1:55:30<2:50:21,  1.68s/it] 38%|███▊      | 3739/9822 [1:55:32<2:50:05,  1.68s/it] 38%|███▊      | 3740/9822 [1:55:33<2:50:13,  1.68s/it] 38%|███▊      | 3741/9822 [1:55:35<2:49:51,  1.68s/it] 38%|███▊      | 3742/9822 [1:55:37<2:49:59,  1.68s/it] 38%|███▊      | 3743/9822 [1:55:38<2:50:08,  1.68s/it] 38%|███▊      | 3744/9822 [1:55:40<2:49:41,  1.68s/it] 38%|███▊      | 3745/9822 [1:55:42<2:49:37,  1.67s/it] 38%|███▊      | 3746/9822 [1:55:43<2:49:37,  1.67s/it] 38%|███▊      | 3747/9822 [1:55:45<2:49:31,  1.67s/it] 38%|███▊      | 3748/9822 [1:55:47<2:49:26,  1.67s/it] 38%|███▊      | 3749/9822 [1:55:48<2:49:18,  1.67s/it] 38%|███▊      | 3750/9822 [1:55:50<2:49:24,  1.67s/it] 38%|███▊      | 3751/9822 [1:55:52<2:49:20,  1.67s/it] 38%|███▊      | 3752/9822 [1:55:53<2:49:02,  1.67s/it] 38%|███▊      | 3753/9822 [1:55:55<2:49:13,  1.67s/it] 38%|███▊      | 3754/9822 [1:55:57<2:49:11,  1.67s/it] 38%|███▊      | 3755/9822 [1:55:58<2:48:49,  1.67s/it] 38%|███▊      | 3756/9822 [1:56:00<2:48:46,  1.67s/it] 38%|███▊      | 3757/9822 [1:56:02<2:48:53,  1.67s/it] 38%|███▊      | 3758/9822 [1:56:03<2:48:33,  1.67s/it] 38%|███▊      | 3759/9822 [1:56:05<2:48:45,  1.67s/it] 38%|███▊      | 3760/9822 [1:56:07<2:48:43,  1.67s/it] 38%|███▊      | 3761/9822 [1:56:08<2:48:26,  1.67s/it] 38%|███▊      | 3762/9822 [1:56:10<2:48:38,  1.67s/it] 38%|███▊      | 3763/9822 [1:56:12<2:48:36,  1.67s/it] 38%|███▊      | 3764/9822 [1:56:13<2:51:29,  1.70s/it] 38%|███▊      | 3765/9822 [1:56:15<2:50:26,  1.69s/it] 38%|███▊      | 3766/9822 [1:56:17<2:50:02,  1.68s/it] 38%|███▊      | 3767/9822 [1:56:18<2:49:35,  1.68s/it] 38%|███▊      | 3768/9822 [1:56:20<2:49:34,  1.68s/it] 38%|███▊      | 3769/9822 [1:56:22<2:49:26,  1.68s/it] 38%|███▊      | 3770/9822 [1:56:23<2:49:09,  1.68s/it] 38%|███▊      | 3771/9822 [1:56:25<2:49:13,  1.68s/it] 38%|███▊      | 3772/9822 [1:56:27<2:49:09,  1.68s/it] 38%|███▊      | 3773/9822 [1:56:28<2:48:54,  1.68s/it] 38%|███▊      | 3774/9822 [1:56:30<2:48:57,  1.68s/it] 38%|███▊      | 3775/9822 [1:56:32<2:48:43,  1.67s/it] 38%|███▊      | 3776/9822 [1:56:33<2:48:25,  1.67s/it] 38%|███▊      | 3777/9822 [1:56:35<2:48:39,  1.67s/it] 38%|███▊      | 3778/9822 [1:56:37<2:48:39,  1.67s/it] 38%|███▊      | 3779/9822 [1:56:39<2:48:44,  1.68s/it] 38%|███▊      | 3780/9822 [1:56:40<2:48:49,  1.68s/it] 38%|███▊      | 3781/9822 [1:56:42<2:48:54,  1.68s/it] 39%|███▊      | 3782/9822 [1:56:44<2:48:44,  1.68s/it] 39%|███▊      | 3783/9822 [1:56:45<2:48:46,  1.68s/it] 39%|███▊      | 3784/9822 [1:56:47<2:47:04,  1.66s/it] 39%|███▊      | 3785/9822 [1:56:49<2:47:27,  1.66s/it] 39%|███▊      | 3786/9822 [1:56:50<2:48:03,  1.67s/it] 39%|███▊      | 3787/9822 [1:56:52<2:48:07,  1.67s/it] 39%|███▊      | 3788/9822 [1:56:54<2:48:11,  1.67s/it] 39%|███▊      | 3789/9822 [1:56:55<2:48:24,  1.67s/it] 39%|███▊      | 3790/9822 [1:56:57<2:48:14,  1.67s/it] 39%|███▊      | 3791/9822 [1:56:59<2:51:35,  1.71s/it] 39%|███▊      | 3792/9822 [1:57:00<2:50:55,  1.70s/it] 39%|███▊      | 3793/9822 [1:57:02<2:50:18,  1.69s/it] 39%|███▊      | 3794/9822 [1:57:04<2:49:50,  1.69s/it] 39%|███▊      | 3795/9822 [1:57:05<2:49:04,  1.68s/it] 39%|███▊      | 3796/9822 [1:57:07<2:48:53,  1.68s/it] 39%|███▊      | 3797/9822 [1:57:09<2:48:43,  1.68s/it] 39%|███▊      | 3798/9822 [1:57:10<2:48:18,  1.68s/it] 39%|███▊      | 3799/9822 [1:57:12<2:48:24,  1.68s/it] 39%|███▊      | 3800/9822 [1:57:14<2:48:20,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:38:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:38:00 - INFO - __main__ - ***** test Results*****
04/29/2024 13:38:00 - INFO - __main__ -   Training step = 3800
04/29/2024 13:38:00 - INFO - __main__ -  test_accuracy:0.8645680819912153 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:38:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:38:05 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:38:05 - INFO - __main__ -   Training step = 3800
04/29/2024 13:38:05 - INFO - __main__ -  eval_accuracy:0.8462101794214574 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:38:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:38:14 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:38:14 - INFO - __main__ -   Training step = 3800
04/29/2024 13:38:14 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 39%|███▊      | 3801/9822 [1:57:33<11:36:55,  6.94s/it] 39%|███▊      | 3802/9822 [1:57:35<8:58:24,  5.37s/it]  39%|███▊      | 3803/9822 [1:57:36<7:07:09,  4.26s/it] 39%|███▊      | 3804/9822 [1:57:38<5:49:24,  3.48s/it] 39%|███▊      | 3805/9822 [1:57:40<4:55:10,  2.94s/it] 39%|███▊      | 3806/9822 [1:57:41<4:16:48,  2.56s/it] 39%|███▉      | 3807/9822 [1:57:43<3:50:07,  2.30s/it] 39%|███▉      | 3808/9822 [1:57:45<3:31:53,  2.11s/it] 39%|███▉      | 3809/9822 [1:57:46<3:18:19,  1.98s/it] 39%|███▉      | 3810/9822 [1:57:48<3:09:09,  1.89s/it] 39%|███▉      | 3811/9822 [1:57:50<3:02:44,  1.82s/it] 39%|███▉      | 3812/9822 [1:57:51<2:57:58,  1.78s/it] 39%|███▉      | 3813/9822 [1:57:53<2:54:59,  1.75s/it] 39%|███▉      | 3814/9822 [1:57:55<2:52:51,  1.73s/it] 39%|███▉      | 3815/9822 [1:57:57<2:54:40,  1.74s/it] 39%|███▉      | 3816/9822 [1:57:58<2:52:36,  1.72s/it] 39%|███▉      | 3817/9822 [1:58:00<2:51:07,  1.71s/it] 39%|███▉      | 3818/9822 [1:58:02<2:49:51,  1.70s/it] 39%|███▉      | 3819/9822 [1:58:03<2:48:52,  1.69s/it] 39%|███▉      | 3820/9822 [1:58:05<2:48:16,  1.68s/it] 39%|███▉      | 3821/9822 [1:58:07<2:48:03,  1.68s/it] 39%|███▉      | 3822/9822 [1:58:08<2:47:28,  1.67s/it] 39%|███▉      | 3823/9822 [1:58:10<2:47:33,  1.68s/it] 39%|███▉      | 3824/9822 [1:58:12<2:47:17,  1.67s/it] 39%|███▉      | 3825/9822 [1:58:13<2:47:04,  1.67s/it] 39%|███▉      | 3826/9822 [1:58:15<2:46:50,  1.67s/it] 39%|███▉      | 3827/9822 [1:58:17<2:46:40,  1.67s/it] 39%|███▉      | 3828/9822 [1:58:18<2:46:39,  1.67s/it] 39%|███▉      | 3829/9822 [1:58:20<2:46:29,  1.67s/it] 39%|███▉      | 3830/9822 [1:58:22<2:47:05,  1.67s/it] 39%|███▉      | 3831/9822 [1:58:23<2:47:28,  1.68s/it] 39%|███▉      | 3832/9822 [1:58:25<2:47:18,  1.68s/it] 39%|███▉      | 3833/9822 [1:58:27<2:47:02,  1.67s/it] 39%|███▉      | 3834/9822 [1:58:28<2:46:45,  1.67s/it] 39%|███▉      | 3835/9822 [1:58:30<2:46:45,  1.67s/it] 39%|███▉      | 3836/9822 [1:58:32<2:46:43,  1.67s/it] 39%|███▉      | 3837/9822 [1:58:33<2:46:53,  1.67s/it] 39%|███▉      | 3838/9822 [1:58:35<2:46:48,  1.67s/it] 39%|███▉      | 3839/9822 [1:58:37<2:47:13,  1.68s/it] 39%|███▉      | 3840/9822 [1:58:38<2:47:30,  1.68s/it] 39%|███▉      | 3841/9822 [1:58:40<2:50:44,  1.71s/it] 39%|███▉      | 3842/9822 [1:58:42<2:49:24,  1.70s/it] 39%|███▉      | 3843/9822 [1:58:44<2:48:38,  1.69s/it] 39%|███▉      | 3844/9822 [1:58:45<2:48:19,  1.69s/it] 39%|███▉      | 3845/9822 [1:58:47<2:47:41,  1.68s/it] 39%|███▉      | 3846/9822 [1:58:49<2:47:26,  1.68s/it] 39%|███▉      | 3847/9822 [1:58:50<2:47:20,  1.68s/it] 39%|███▉      | 3848/9822 [1:58:52<2:47:14,  1.68s/it] 39%|███▉      | 3849/9822 [1:58:54<2:46:32,  1.67s/it] 39%|███▉      | 3850/9822 [1:58:55<2:46:18,  1.67s/it] 39%|███▉      | 3851/9822 [1:58:57<2:46:24,  1.67s/it] 39%|███▉      | 3852/9822 [1:58:59<2:45:58,  1.67s/it] 39%|███▉      | 3853/9822 [1:59:00<2:45:46,  1.67s/it] 39%|███▉      | 3854/9822 [1:59:02<2:45:48,  1.67s/it] 39%|███▉      | 3855/9822 [1:59:04<2:46:02,  1.67s/it] 39%|███▉      | 3856/9822 [1:59:05<2:46:24,  1.67s/it] 39%|███▉      | 3857/9822 [1:59:07<2:46:13,  1.67s/it] 39%|███▉      | 3858/9822 [1:59:09<2:46:25,  1.67s/it] 39%|███▉      | 3859/9822 [1:59:10<2:45:56,  1.67s/it] 39%|███▉      | 3860/9822 [1:59:12<2:45:51,  1.67s/it] 39%|███▉      | 3861/9822 [1:59:14<2:45:29,  1.67s/it] 39%|███▉      | 3862/9822 [1:59:15<2:45:16,  1.66s/it] 39%|███▉      | 3863/9822 [1:59:17<2:45:37,  1.67s/it] 39%|███▉      | 3864/9822 [1:59:19<2:45:37,  1.67s/it] 39%|███▉      | 3865/9822 [1:59:20<2:45:41,  1.67s/it] 39%|███▉      | 3866/9822 [1:59:22<2:45:26,  1.67s/it] 39%|███▉      | 3867/9822 [1:59:24<2:45:16,  1.67s/it] 39%|███▉      | 3868/9822 [1:59:25<2:45:05,  1.66s/it] 39%|███▉      | 3869/9822 [1:59:27<2:45:10,  1.66s/it] 39%|███▉      | 3870/9822 [1:59:29<2:43:36,  1.65s/it] 39%|███▉      | 3871/9822 [1:59:30<2:43:55,  1.65s/it] 39%|███▉      | 3872/9822 [1:59:32<2:44:27,  1.66s/it] 39%|███▉      | 3873/9822 [1:59:34<2:45:06,  1.67s/it] 39%|███▉      | 3874/9822 [1:59:35<2:48:34,  1.70s/it] 39%|███▉      | 3875/9822 [1:59:37<2:47:57,  1.69s/it] 39%|███▉      | 3876/9822 [1:59:39<2:47:36,  1.69s/it] 39%|███▉      | 3877/9822 [1:59:40<2:47:00,  1.69s/it] 39%|███▉      | 3878/9822 [1:59:42<2:46:35,  1.68s/it] 39%|███▉      | 3879/9822 [1:59:44<2:46:01,  1.68s/it] 40%|███▉      | 3880/9822 [1:59:45<2:45:38,  1.67s/it] 40%|███▉      | 3881/9822 [1:59:47<2:45:42,  1.67s/it] 40%|███▉      | 3882/9822 [1:59:49<2:45:44,  1.67s/it] 40%|███▉      | 3883/9822 [1:59:50<2:45:38,  1.67s/it] 40%|███▉      | 3884/9822 [1:59:52<2:45:33,  1.67s/it] 40%|███▉      | 3885/9822 [1:59:54<2:45:52,  1.68s/it] 40%|███▉      | 3886/9822 [1:59:55<2:47:02,  1.69s/it] 40%|███▉      | 3887/9822 [1:59:57<2:46:41,  1.69s/it] 40%|███▉      | 3888/9822 [1:59:59<2:46:27,  1.68s/it] 40%|███▉      | 3889/9822 [2:00:01<2:46:08,  1.68s/it] 40%|███▉      | 3890/9822 [2:00:02<2:45:49,  1.68s/it] 40%|███▉      | 3891/9822 [2:00:04<2:45:26,  1.67s/it] 40%|███▉      | 3892/9822 [2:00:06<2:45:09,  1.67s/it] 40%|███▉      | 3893/9822 [2:00:07<2:45:21,  1.67s/it] 40%|███▉      | 3894/9822 [2:00:09<2:45:01,  1.67s/it] 40%|███▉      | 3895/9822 [2:00:11<2:44:47,  1.67s/it] 40%|███▉      | 3896/9822 [2:00:12<2:48:06,  1.70s/it] 40%|███▉      | 3897/9822 [2:00:14<2:47:04,  1.69s/it] 40%|███▉      | 3898/9822 [2:00:16<2:46:32,  1.69s/it] 40%|███▉      | 3899/9822 [2:00:17<2:46:11,  1.68s/it] 40%|███▉      | 3900/9822 [2:00:19<2:45:45,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:41:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:41:06 - INFO - __main__ - ***** test Results*****
04/29/2024 13:41:06 - INFO - __main__ -   Training step = 3900
04/29/2024 13:41:06 - INFO - __main__ -  test_accuracy:0.8638360175695461 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:41:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:41:10 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:41:10 - INFO - __main__ -   Training step = 3900
04/29/2024 13:41:10 - INFO - __main__ -  eval_accuracy:0.8407176858293666 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:41:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:41:19 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:41:19 - INFO - __main__ -   Training step = 3900
04/29/2024 13:41:19 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 40%|███▉      | 3901/9822 [2:00:38<11:25:33,  6.95s/it] 40%|███▉      | 3902/9822 [2:00:40<8:49:26,  5.37s/it]  40%|███▉      | 3903/9822 [2:00:42<6:59:58,  4.26s/it] 40%|███▉      | 3904/9822 [2:00:43<5:43:31,  3.48s/it] 40%|███▉      | 3905/9822 [2:00:45<4:49:55,  2.94s/it] 40%|███▉      | 3906/9822 [2:00:47<4:12:12,  2.56s/it] 40%|███▉      | 3907/9822 [2:00:48<3:46:19,  2.30s/it] 40%|███▉      | 3908/9822 [2:00:50<3:27:40,  2.11s/it] 40%|███▉      | 3909/9822 [2:00:52<3:14:33,  1.97s/it] 40%|███▉      | 3910/9822 [2:00:53<3:05:41,  1.88s/it] 40%|███▉      | 3911/9822 [2:00:55<2:59:30,  1.82s/it] 40%|███▉      | 3912/9822 [2:00:57<2:54:47,  1.77s/it] 40%|███▉      | 3913/9822 [2:00:58<2:51:57,  1.75s/it] 40%|███▉      | 3914/9822 [2:01:00<2:49:28,  1.72s/it] 40%|███▉      | 3915/9822 [2:01:02<2:48:02,  1.71s/it] 40%|███▉      | 3916/9822 [2:01:03<2:47:12,  1.70s/it] 40%|███▉      | 3917/9822 [2:01:05<2:46:33,  1.69s/it] 40%|███▉      | 3918/9822 [2:01:07<2:45:53,  1.69s/it] 40%|███▉      | 3919/9822 [2:01:08<2:45:30,  1.68s/it] 40%|███▉      | 3920/9822 [2:01:10<2:48:13,  1.71s/it] 40%|███▉      | 3921/9822 [2:01:12<2:47:35,  1.70s/it] 40%|███▉      | 3922/9822 [2:01:13<2:46:38,  1.69s/it] 40%|███▉      | 3923/9822 [2:01:15<2:46:10,  1.69s/it] 40%|███▉      | 3924/9822 [2:01:17<2:45:29,  1.68s/it] 40%|███▉      | 3925/9822 [2:01:18<2:45:07,  1.68s/it] 40%|███▉      | 3926/9822 [2:01:20<2:44:57,  1.68s/it] 40%|███▉      | 3927/9822 [2:01:22<2:44:53,  1.68s/it] 40%|███▉      | 3928/9822 [2:01:24<2:44:31,  1.67s/it] 40%|████      | 3929/9822 [2:01:25<2:44:48,  1.68s/it] 40%|████      | 3930/9822 [2:01:27<2:44:34,  1.68s/it] 40%|████      | 3931/9822 [2:01:29<2:44:28,  1.68s/it] 40%|████      | 3932/9822 [2:01:30<2:44:19,  1.67s/it] 40%|████      | 3933/9822 [2:01:32<2:44:02,  1.67s/it] 40%|████      | 3934/9822 [2:01:34<2:44:03,  1.67s/it] 40%|████      | 3935/9822 [2:01:35<2:44:18,  1.67s/it] 40%|████      | 3936/9822 [2:01:37<2:44:39,  1.68s/it] 40%|████      | 3937/9822 [2:01:39<2:44:12,  1.67s/it] 40%|████      | 3938/9822 [2:01:40<2:43:46,  1.67s/it] 40%|████      | 3939/9822 [2:01:42<2:43:55,  1.67s/it] 40%|████      | 3940/9822 [2:01:44<2:43:57,  1.67s/it] 40%|████      | 3941/9822 [2:01:45<2:43:50,  1.67s/it] 40%|████      | 3942/9822 [2:01:47<2:44:11,  1.68s/it] 40%|████      | 3943/9822 [2:01:49<2:44:10,  1.68s/it] 40%|████      | 3944/9822 [2:01:50<2:43:53,  1.67s/it] 40%|████      | 3945/9822 [2:01:52<2:43:37,  1.67s/it] 40%|████      | 3946/9822 [2:01:54<2:46:24,  1.70s/it] 40%|████      | 3947/9822 [2:01:55<2:45:45,  1.69s/it] 40%|████      | 3948/9822 [2:01:57<2:44:54,  1.68s/it] 40%|████      | 3949/9822 [2:01:59<2:44:26,  1.68s/it] 40%|████      | 3950/9822 [2:02:00<2:44:03,  1.68s/it] 40%|████      | 3951/9822 [2:02:02<2:43:51,  1.67s/it] 40%|████      | 3952/9822 [2:02:04<2:43:30,  1.67s/it] 40%|████      | 3953/9822 [2:02:05<2:43:24,  1.67s/it] 40%|████      | 3954/9822 [2:02:07<2:43:17,  1.67s/it] 40%|████      | 3955/9822 [2:02:09<2:43:05,  1.67s/it] 40%|████      | 3956/9822 [2:02:10<2:41:46,  1.65s/it] 40%|████      | 3957/9822 [2:02:12<2:41:56,  1.66s/it] 40%|████      | 3958/9822 [2:02:14<2:41:58,  1.66s/it] 40%|████      | 3959/9822 [2:02:15<2:41:52,  1.66s/it] 40%|████      | 3960/9822 [2:02:17<2:41:47,  1.66s/it] 40%|████      | 3961/9822 [2:02:19<2:41:46,  1.66s/it] 40%|████      | 3962/9822 [2:02:20<2:42:17,  1.66s/it] 40%|████      | 3963/9822 [2:02:22<2:42:12,  1.66s/it] 40%|████      | 3964/9822 [2:02:24<2:42:21,  1.66s/it] 40%|████      | 3965/9822 [2:02:25<2:42:38,  1.67s/it] 40%|████      | 3966/9822 [2:02:27<2:42:31,  1.67s/it] 40%|████      | 3967/9822 [2:02:29<2:42:22,  1.66s/it] 40%|████      | 3968/9822 [2:02:30<2:42:11,  1.66s/it] 40%|████      | 3969/9822 [2:02:32<2:42:37,  1.67s/it] 40%|████      | 3970/9822 [2:02:34<2:42:49,  1.67s/it] 40%|████      | 3971/9822 [2:02:35<2:42:30,  1.67s/it] 40%|████      | 3972/9822 [2:02:37<2:42:08,  1.66s/it] 40%|████      | 3973/9822 [2:02:39<2:42:18,  1.66s/it] 40%|████      | 3974/9822 [2:02:40<2:42:05,  1.66s/it] 40%|████      | 3975/9822 [2:02:42<2:42:05,  1.66s/it] 40%|████      | 3976/9822 [2:02:44<2:41:59,  1.66s/it] 40%|████      | 3977/9822 [2:02:45<2:42:27,  1.67s/it] 41%|████      | 3978/9822 [2:02:47<2:42:33,  1.67s/it] 41%|████      | 3979/9822 [2:02:49<2:45:25,  1.70s/it] 41%|████      | 3980/9822 [2:02:50<2:44:42,  1.69s/it] 41%|████      | 3981/9822 [2:02:52<2:44:00,  1.68s/it] 41%|████      | 3982/9822 [2:02:54<2:43:21,  1.68s/it] 41%|████      | 3983/9822 [2:02:55<2:42:35,  1.67s/it] 41%|████      | 3984/9822 [2:02:57<2:42:17,  1.67s/it] 41%|████      | 3985/9822 [2:02:59<2:42:04,  1.67s/it] 41%|████      | 3986/9822 [2:03:00<2:42:02,  1.67s/it] 41%|████      | 3987/9822 [2:03:02<2:41:55,  1.67s/it] 41%|████      | 3988/9822 [2:03:04<2:41:44,  1.66s/it] 41%|████      | 3989/9822 [2:03:05<2:41:58,  1.67s/it] 41%|████      | 3990/9822 [2:03:07<2:41:50,  1.66s/it] 41%|████      | 3991/9822 [2:03:09<2:41:37,  1.66s/it] 41%|████      | 3992/9822 [2:03:10<2:41:27,  1.66s/it] 41%|████      | 3993/9822 [2:03:12<2:41:24,  1.66s/it] 41%|████      | 3994/9822 [2:03:14<2:41:35,  1.66s/it] 41%|████      | 3995/9822 [2:03:15<2:41:55,  1.67s/it] 41%|████      | 3996/9822 [2:03:17<2:42:15,  1.67s/it] 41%|████      | 3997/9822 [2:03:19<2:41:56,  1.67s/it] 41%|████      | 3998/9822 [2:03:20<2:41:59,  1.67s/it] 41%|████      | 3999/9822 [2:03:22<2:42:01,  1.67s/it] 41%|████      | 4000/9822 [2:03:24<2:41:34,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:44:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:44:11 - INFO - __main__ - ***** test Results*****
04/29/2024 13:44:11 - INFO - __main__ -   Training step = 4000
04/29/2024 13:44:11 - INFO - __main__ -  test_accuracy:0.8627379209370425 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:44:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:44:15 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:44:15 - INFO - __main__ -   Training step = 4000
04/29/2024 13:44:15 - INFO - __main__ -  eval_accuracy:0.8458440131819847 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:44:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:44:24 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:44:24 - INFO - __main__ -   Training step = 4000
04/29/2024 13:44:24 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 41%|████      | 4001/9822 [2:03:43<11:15:11,  6.96s/it] 41%|████      | 4002/9822 [2:03:45<8:41:26,  5.38s/it]  41%|████      | 4003/9822 [2:03:46<6:53:56,  4.27s/it] 41%|████      | 4004/9822 [2:03:48<5:38:19,  3.49s/it] 41%|████      | 4005/9822 [2:03:50<4:45:40,  2.95s/it] 41%|████      | 4006/9822 [2:03:51<4:08:36,  2.56s/it] 41%|████      | 4007/9822 [2:03:53<3:42:44,  2.30s/it] 41%|████      | 4008/9822 [2:03:55<3:24:34,  2.11s/it] 41%|████      | 4009/9822 [2:03:56<3:11:58,  1.98s/it] 41%|████      | 4010/9822 [2:03:58<3:03:19,  1.89s/it] 41%|████      | 4011/9822 [2:04:00<2:56:59,  1.83s/it] 41%|████      | 4012/9822 [2:04:01<2:52:45,  1.78s/it] 41%|████      | 4013/9822 [2:04:03<2:49:16,  1.75s/it] 41%|████      | 4014/9822 [2:04:05<2:47:27,  1.73s/it] 41%|████      | 4015/9822 [2:04:07<2:45:48,  1.71s/it] 41%|████      | 4016/9822 [2:04:08<2:44:24,  1.70s/it] 41%|████      | 4017/9822 [2:04:10<2:43:43,  1.69s/it] 41%|████      | 4018/9822 [2:04:12<2:43:08,  1.69s/it] 41%|████      | 4019/9822 [2:04:13<2:42:29,  1.68s/it] 41%|████      | 4020/9822 [2:04:15<2:42:17,  1.68s/it] 41%|████      | 4021/9822 [2:04:17<2:41:48,  1.67s/it] 41%|████      | 4022/9822 [2:04:18<2:41:41,  1.67s/it] 41%|████      | 4023/9822 [2:04:20<2:41:43,  1.67s/it] 41%|████      | 4024/9822 [2:04:22<2:41:46,  1.67s/it] 41%|████      | 4025/9822 [2:04:23<2:44:56,  1.71s/it] 41%|████      | 4026/9822 [2:04:25<2:43:47,  1.70s/it] 41%|████      | 4027/9822 [2:04:27<2:42:44,  1.68s/it] 41%|████      | 4028/9822 [2:04:28<2:42:31,  1.68s/it] 41%|████      | 4029/9822 [2:04:30<2:41:56,  1.68s/it] 41%|████      | 4030/9822 [2:04:32<2:41:32,  1.67s/it] 41%|████      | 4031/9822 [2:04:33<2:41:35,  1.67s/it] 41%|████      | 4032/9822 [2:04:35<2:41:13,  1.67s/it] 41%|████      | 4033/9822 [2:04:37<2:41:11,  1.67s/it] 41%|████      | 4034/9822 [2:04:38<2:41:32,  1.67s/it] 41%|████      | 4035/9822 [2:04:40<2:41:32,  1.67s/it] 41%|████      | 4036/9822 [2:04:42<2:41:35,  1.68s/it] 41%|████      | 4037/9822 [2:04:43<2:41:30,  1.68s/it] 41%|████      | 4038/9822 [2:04:45<2:41:17,  1.67s/it] 41%|████      | 4039/9822 [2:04:47<2:41:20,  1.67s/it] 41%|████      | 4040/9822 [2:04:48<2:41:09,  1.67s/it] 41%|████      | 4041/9822 [2:04:50<2:41:13,  1.67s/it] 41%|████      | 4042/9822 [2:04:52<2:39:39,  1.66s/it] 41%|████      | 4043/9822 [2:04:53<2:40:03,  1.66s/it] 41%|████      | 4044/9822 [2:04:55<2:40:39,  1.67s/it] 41%|████      | 4045/9822 [2:04:57<2:40:38,  1.67s/it] 41%|████      | 4046/9822 [2:04:58<2:41:03,  1.67s/it] 41%|████      | 4047/9822 [2:05:00<2:41:05,  1.67s/it] 41%|████      | 4048/9822 [2:05:02<2:41:26,  1.68s/it] 41%|████      | 4049/9822 [2:05:03<2:41:31,  1.68s/it] 41%|████      | 4050/9822 [2:05:05<2:41:39,  1.68s/it] 41%|████      | 4051/9822 [2:05:07<2:44:51,  1.71s/it] 41%|████▏     | 4052/9822 [2:05:09<2:43:29,  1.70s/it] 41%|████▏     | 4053/9822 [2:05:10<2:42:53,  1.69s/it] 41%|████▏     | 4054/9822 [2:05:12<2:42:26,  1.69s/it] 41%|████▏     | 4055/9822 [2:05:14<2:42:01,  1.69s/it] 41%|████▏     | 4056/9822 [2:05:15<2:41:42,  1.68s/it] 41%|████▏     | 4057/9822 [2:05:17<2:41:30,  1.68s/it] 41%|████▏     | 4058/9822 [2:05:19<2:41:28,  1.68s/it] 41%|████▏     | 4059/9822 [2:05:20<2:41:28,  1.68s/it] 41%|████▏     | 4060/9822 [2:05:22<2:41:18,  1.68s/it] 41%|████▏     | 4061/9822 [2:05:24<2:41:24,  1.68s/it] 41%|████▏     | 4062/9822 [2:05:25<2:41:30,  1.68s/it] 41%|████▏     | 4063/9822 [2:05:27<2:41:49,  1.69s/it] 41%|████▏     | 4064/9822 [2:05:29<2:41:24,  1.68s/it] 41%|████▏     | 4065/9822 [2:05:30<2:40:52,  1.68s/it] 41%|████▏     | 4066/9822 [2:05:32<2:40:33,  1.67s/it] 41%|████▏     | 4067/9822 [2:05:34<2:40:38,  1.67s/it] 41%|████▏     | 4068/9822 [2:05:35<2:40:29,  1.67s/it] 41%|████▏     | 4069/9822 [2:05:37<2:40:12,  1.67s/it] 41%|████▏     | 4070/9822 [2:05:39<2:39:56,  1.67s/it] 41%|████▏     | 4071/9822 [2:05:40<2:39:56,  1.67s/it] 41%|████▏     | 4072/9822 [2:05:42<2:40:01,  1.67s/it] 41%|████▏     | 4073/9822 [2:05:44<2:39:57,  1.67s/it] 41%|████▏     | 4074/9822 [2:05:45<2:39:58,  1.67s/it] 41%|████▏     | 4075/9822 [2:05:47<2:39:59,  1.67s/it] 41%|████▏     | 4076/9822 [2:05:49<2:39:48,  1.67s/it] 42%|████▏     | 4077/9822 [2:05:50<2:39:52,  1.67s/it] 42%|████▏     | 4078/9822 [2:05:52<2:39:35,  1.67s/it] 42%|████▏     | 4079/9822 [2:05:54<2:39:26,  1.67s/it] 42%|████▏     | 4080/9822 [2:05:55<2:39:09,  1.66s/it] 42%|████▏     | 4081/9822 [2:05:57<2:39:01,  1.66s/it] 42%|████▏     | 4082/9822 [2:05:59<2:39:00,  1.66s/it] 42%|████▏     | 4083/9822 [2:06:00<2:39:16,  1.67s/it] 42%|████▏     | 4084/9822 [2:06:02<2:42:11,  1.70s/it] 42%|████▏     | 4085/9822 [2:06:04<2:40:59,  1.68s/it] 42%|████▏     | 4086/9822 [2:06:06<2:40:33,  1.68s/it] 42%|████▏     | 4087/9822 [2:06:07<2:40:11,  1.68s/it] 42%|████▏     | 4088/9822 [2:06:09<2:39:59,  1.67s/it] 42%|████▏     | 4089/9822 [2:06:11<2:39:53,  1.67s/it] 42%|████▏     | 4090/9822 [2:06:12<2:39:55,  1.67s/it] 42%|████▏     | 4091/9822 [2:06:14<2:39:52,  1.67s/it] 42%|████▏     | 4092/9822 [2:06:16<2:40:03,  1.68s/it] 42%|████▏     | 4093/9822 [2:06:17<2:40:04,  1.68s/it] 42%|████▏     | 4094/9822 [2:06:19<2:39:56,  1.68s/it] 42%|████▏     | 4095/9822 [2:06:21<2:39:29,  1.67s/it] 42%|████▏     | 4096/9822 [2:06:22<2:39:11,  1.67s/it] 42%|████▏     | 4097/9822 [2:06:24<2:38:58,  1.67s/it] 42%|████▏     | 4098/9822 [2:06:26<2:38:54,  1.67s/it] 42%|████▏     | 4099/9822 [2:06:27<2:38:44,  1.66s/it] 42%|████▏     | 4100/9822 [2:06:29<2:38:58,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:47:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:47:16 - INFO - __main__ - ***** test Results*****
04/29/2024 13:47:16 - INFO - __main__ -   Training step = 4100
04/29/2024 13:47:16 - INFO - __main__ -  test_accuracy:0.8532210834553441 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:47:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:47:20 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:47:20 - INFO - __main__ -   Training step = 4100
04/29/2024 13:47:20 - INFO - __main__ -  eval_accuracy:0.8407176858293666 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:47:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:47:29 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:47:29 - INFO - __main__ -   Training step = 4100
04/29/2024 13:47:29 - INFO - __main__ -  eval_accuracy:0.909556938850238 
 42%|████▏     | 4101/9822 [2:06:48<11:02:21,  6.95s/it] 42%|████▏     | 4102/9822 [2:06:50<8:31:41,  5.37s/it]  42%|████▏     | 4103/9822 [2:06:52<6:45:51,  4.26s/it] 42%|████▏     | 4104/9822 [2:06:53<5:31:48,  3.48s/it] 42%|████▏     | 4105/9822 [2:06:55<4:40:10,  2.94s/it] 42%|████▏     | 4106/9822 [2:06:57<4:04:24,  2.57s/it] 42%|████▏     | 4107/9822 [2:06:58<3:39:01,  2.30s/it] 42%|████▏     | 4108/9822 [2:07:00<3:21:19,  2.11s/it] 42%|████▏     | 4109/9822 [2:07:02<3:09:01,  1.99s/it] 42%|████▏     | 4110/9822 [2:07:03<3:00:22,  1.89s/it] 42%|████▏     | 4111/9822 [2:07:05<2:57:11,  1.86s/it] 42%|████▏     | 4112/9822 [2:07:07<2:51:45,  1.80s/it] 42%|████▏     | 4113/9822 [2:07:08<2:47:39,  1.76s/it] 42%|████▏     | 4114/9822 [2:07:10<2:45:01,  1.73s/it] 42%|████▏     | 4115/9822 [2:07:12<2:43:10,  1.72s/it] 42%|████▏     | 4116/9822 [2:07:13<2:41:40,  1.70s/it] 42%|████▏     | 4117/9822 [2:07:15<2:40:45,  1.69s/it] 42%|████▏     | 4118/9822 [2:07:17<2:39:48,  1.68s/it] 42%|████▏     | 4119/9822 [2:07:18<2:39:21,  1.68s/it] 42%|████▏     | 4120/9822 [2:07:20<2:38:41,  1.67s/it] 42%|████▏     | 4121/9822 [2:07:22<2:38:36,  1.67s/it] 42%|████▏     | 4122/9822 [2:07:23<2:38:23,  1.67s/it] 42%|████▏     | 4123/9822 [2:07:25<2:38:30,  1.67s/it] 42%|████▏     | 4124/9822 [2:07:27<2:38:30,  1.67s/it] 42%|████▏     | 4125/9822 [2:07:28<2:38:08,  1.67s/it] 42%|████▏     | 4126/9822 [2:07:30<2:38:02,  1.66s/it] 42%|████▏     | 4127/9822 [2:07:32<2:38:12,  1.67s/it] 42%|████▏     | 4128/9822 [2:07:33<2:36:53,  1.65s/it] 42%|████▏     | 4129/9822 [2:07:35<2:37:26,  1.66s/it] 42%|████▏     | 4130/9822 [2:07:37<2:37:39,  1.66s/it] 42%|████▏     | 4131/9822 [2:07:38<2:37:37,  1.66s/it] 42%|████▏     | 4132/9822 [2:07:40<2:37:52,  1.66s/it] 42%|████▏     | 4133/9822 [2:07:42<2:38:02,  1.67s/it] 42%|████▏     | 4134/9822 [2:07:43<2:38:12,  1.67s/it] 42%|████▏     | 4135/9822 [2:07:45<2:38:06,  1.67s/it] 42%|████▏     | 4136/9822 [2:07:47<2:38:14,  1.67s/it] 42%|████▏     | 4137/9822 [2:07:48<2:38:07,  1.67s/it] 42%|████▏     | 4138/9822 [2:07:50<2:40:58,  1.70s/it] 42%|████▏     | 4139/9822 [2:07:52<2:39:59,  1.69s/it] 42%|████▏     | 4140/9822 [2:07:53<2:39:24,  1.68s/it] 42%|████▏     | 4141/9822 [2:07:55<2:38:57,  1.68s/it] 42%|████▏     | 4142/9822 [2:07:57<2:38:21,  1.67s/it] 42%|████▏     | 4143/9822 [2:07:58<2:38:01,  1.67s/it] 42%|████▏     | 4144/9822 [2:08:00<2:37:58,  1.67s/it] 42%|████▏     | 4145/9822 [2:08:02<2:38:08,  1.67s/it] 42%|████▏     | 4146/9822 [2:08:03<2:37:48,  1.67s/it] 42%|████▏     | 4147/9822 [2:08:05<2:37:31,  1.67s/it] 42%|████▏     | 4148/9822 [2:08:07<2:37:37,  1.67s/it] 42%|████▏     | 4149/9822 [2:08:08<2:37:54,  1.67s/it] 42%|████▏     | 4150/9822 [2:08:10<2:37:34,  1.67s/it] 42%|████▏     | 4151/9822 [2:08:12<2:37:25,  1.67s/it] 42%|████▏     | 4152/9822 [2:08:13<2:37:31,  1.67s/it] 42%|████▏     | 4153/9822 [2:08:15<2:37:38,  1.67s/it] 42%|████▏     | 4154/9822 [2:08:17<2:37:46,  1.67s/it] 42%|████▏     | 4155/9822 [2:08:18<2:37:54,  1.67s/it] 42%|████▏     | 4156/9822 [2:08:20<2:37:37,  1.67s/it] 42%|████▏     | 4157/9822 [2:08:22<2:37:22,  1.67s/it] 42%|████▏     | 4158/9822 [2:08:23<2:37:12,  1.67s/it] 42%|████▏     | 4159/9822 [2:08:25<2:37:29,  1.67s/it] 42%|████▏     | 4160/9822 [2:08:27<2:37:12,  1.67s/it] 42%|████▏     | 4161/9822 [2:08:28<2:36:57,  1.66s/it] 42%|████▏     | 4162/9822 [2:08:30<2:36:52,  1.66s/it] 42%|████▏     | 4163/9822 [2:08:32<2:36:54,  1.66s/it] 42%|████▏     | 4164/9822 [2:08:33<2:37:19,  1.67s/it] 42%|████▏     | 4165/9822 [2:08:35<2:40:22,  1.70s/it] 42%|████▏     | 4166/9822 [2:08:37<2:39:36,  1.69s/it] 42%|████▏     | 4167/9822 [2:08:39<2:38:53,  1.69s/it] 42%|████▏     | 4168/9822 [2:08:40<2:38:33,  1.68s/it] 42%|████▏     | 4169/9822 [2:08:42<2:38:27,  1.68s/it] 42%|████▏     | 4170/9822 [2:08:44<2:38:29,  1.68s/it] 42%|████▏     | 4171/9822 [2:08:45<2:38:06,  1.68s/it] 42%|████▏     | 4172/9822 [2:08:47<2:38:03,  1.68s/it] 42%|████▏     | 4173/9822 [2:08:49<2:38:02,  1.68s/it] 42%|████▏     | 4174/9822 [2:08:50<2:37:55,  1.68s/it] 43%|████▎     | 4175/9822 [2:08:52<2:37:48,  1.68s/it] 43%|████▎     | 4176/9822 [2:08:54<2:37:32,  1.67s/it] 43%|████▎     | 4177/9822 [2:08:55<2:37:53,  1.68s/it] 43%|████▎     | 4178/9822 [2:08:57<2:37:18,  1.67s/it] 43%|████▎     | 4179/9822 [2:08:59<2:37:25,  1.67s/it] 43%|████▎     | 4180/9822 [2:09:00<2:37:07,  1.67s/it] 43%|████▎     | 4181/9822 [2:09:02<2:37:00,  1.67s/it] 43%|████▎     | 4182/9822 [2:09:04<2:36:53,  1.67s/it] 43%|████▎     | 4183/9822 [2:09:05<2:36:44,  1.67s/it] 43%|████▎     | 4184/9822 [2:09:07<2:36:47,  1.67s/it] 43%|████▎     | 4185/9822 [2:09:09<2:36:43,  1.67s/it] 43%|████▎     | 4186/9822 [2:09:10<2:36:52,  1.67s/it] 43%|████▎     | 4187/9822 [2:09:12<2:36:50,  1.67s/it] 43%|████▎     | 4188/9822 [2:09:14<2:36:55,  1.67s/it] 43%|████▎     | 4189/9822 [2:09:15<2:37:15,  1.68s/it] 43%|████▎     | 4190/9822 [2:09:17<2:37:16,  1.68s/it] 43%|████▎     | 4191/9822 [2:09:19<2:37:12,  1.68s/it] 43%|████▎     | 4192/9822 [2:09:20<2:37:22,  1.68s/it] 43%|████▎     | 4193/9822 [2:09:22<2:37:30,  1.68s/it] 43%|████▎     | 4194/9822 [2:09:24<2:37:12,  1.68s/it] 43%|████▎     | 4195/9822 [2:09:25<2:36:56,  1.67s/it] 43%|████▎     | 4196/9822 [2:09:27<2:37:03,  1.68s/it] 43%|████▎     | 4197/9822 [2:09:29<2:37:03,  1.68s/it] 43%|████▎     | 4198/9822 [2:09:31<2:39:46,  1.70s/it] 43%|████▎     | 4199/9822 [2:09:32<2:38:40,  1.69s/it] 43%|████▎     | 4200/9822 [2:09:34<2:37:59,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:50:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:50:21 - INFO - __main__ - ***** test Results*****
04/29/2024 13:50:21 - INFO - __main__ -   Training step = 4200
04/29/2024 13:50:21 - INFO - __main__ -  test_accuracy:0.866398243045388 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:50:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:50:25 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:50:25 - INFO - __main__ -   Training step = 4200
04/29/2024 13:50:25 - INFO - __main__ -  eval_accuracy:0.8487733430977664 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:50:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:50:34 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:50:34 - INFO - __main__ -   Training step = 4200
04/29/2024 13:50:34 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 43%|████▎     | 4201/9822 [2:09:53<10:51:09,  6.95s/it] 43%|████▎     | 4202/9822 [2:09:55<8:22:39,  5.37s/it]  43%|████▎     | 4203/9822 [2:09:56<6:38:40,  4.26s/it] 43%|████▎     | 4204/9822 [2:09:58<5:26:08,  3.48s/it] 43%|████▎     | 4205/9822 [2:10:00<4:35:18,  2.94s/it] 43%|████▎     | 4206/9822 [2:10:02<3:59:47,  2.56s/it] 43%|████▎     | 4207/9822 [2:10:03<3:34:59,  2.30s/it] 43%|████▎     | 4208/9822 [2:10:05<3:17:38,  2.11s/it] 43%|████▎     | 4209/9822 [2:10:07<3:05:21,  1.98s/it] 43%|████▎     | 4210/9822 [2:10:08<2:56:51,  1.89s/it] 43%|████▎     | 4211/9822 [2:10:10<2:51:05,  1.83s/it] 43%|████▎     | 4212/9822 [2:10:12<2:47:08,  1.79s/it] 43%|████▎     | 4213/9822 [2:10:13<2:43:53,  1.75s/it] 43%|████▎     | 4214/9822 [2:10:15<2:40:31,  1.72s/it] 43%|████▎     | 4215/9822 [2:10:17<2:39:35,  1.71s/it] 43%|████▎     | 4216/9822 [2:10:18<2:38:39,  1.70s/it] 43%|████▎     | 4217/9822 [2:10:20<2:38:21,  1.70s/it] 43%|████▎     | 4218/9822 [2:10:22<2:37:53,  1.69s/it] 43%|████▎     | 4219/9822 [2:10:23<2:37:39,  1.69s/it] 43%|████▎     | 4220/9822 [2:10:25<2:37:19,  1.69s/it] 43%|████▎     | 4221/9822 [2:10:27<2:37:02,  1.68s/it] 43%|████▎     | 4222/9822 [2:10:28<2:36:50,  1.68s/it] 43%|████▎     | 4223/9822 [2:10:30<2:36:33,  1.68s/it] 43%|████▎     | 4224/9822 [2:10:32<2:36:29,  1.68s/it] 43%|████▎     | 4225/9822 [2:10:33<2:36:45,  1.68s/it] 43%|████▎     | 4226/9822 [2:10:35<2:36:35,  1.68s/it] 43%|████▎     | 4227/9822 [2:10:37<2:36:23,  1.68s/it] 43%|████▎     | 4228/9822 [2:10:38<2:36:32,  1.68s/it] 43%|████▎     | 4229/9822 [2:10:40<2:36:17,  1.68s/it] 43%|████▎     | 4230/9822 [2:10:42<2:35:56,  1.67s/it] 43%|████▎     | 4231/9822 [2:10:43<2:36:12,  1.68s/it] 43%|████▎     | 4232/9822 [2:10:45<2:35:51,  1.67s/it] 43%|████▎     | 4233/9822 [2:10:47<2:35:36,  1.67s/it] 43%|████▎     | 4234/9822 [2:10:49<2:38:33,  1.70s/it] 43%|████▎     | 4235/9822 [2:10:50<2:37:42,  1.69s/it] 43%|████▎     | 4236/9822 [2:10:52<2:37:25,  1.69s/it] 43%|████▎     | 4237/9822 [2:10:54<2:36:53,  1.69s/it] 43%|████▎     | 4238/9822 [2:10:55<2:36:20,  1.68s/it] 43%|████▎     | 4239/9822 [2:10:57<2:35:46,  1.67s/it] 43%|████▎     | 4240/9822 [2:10:59<2:35:53,  1.68s/it] 43%|████▎     | 4241/9822 [2:11:00<2:35:34,  1.67s/it] 43%|████▎     | 4242/9822 [2:11:02<2:35:42,  1.67s/it] 43%|████▎     | 4243/9822 [2:11:04<2:35:24,  1.67s/it] 43%|████▎     | 4244/9822 [2:11:05<2:35:29,  1.67s/it] 43%|████▎     | 4245/9822 [2:11:07<2:35:35,  1.67s/it] 43%|████▎     | 4246/9822 [2:11:09<2:35:22,  1.67s/it] 43%|████▎     | 4247/9822 [2:11:10<2:35:06,  1.67s/it] 43%|████▎     | 4248/9822 [2:11:12<2:35:02,  1.67s/it] 43%|████▎     | 4249/9822 [2:11:14<2:34:56,  1.67s/it] 43%|████▎     | 4250/9822 [2:11:15<2:34:43,  1.67s/it] 43%|████▎     | 4251/9822 [2:11:17<2:34:57,  1.67s/it] 43%|████▎     | 4252/9822 [2:11:19<2:34:47,  1.67s/it] 43%|████▎     | 4253/9822 [2:11:20<2:34:37,  1.67s/it] 43%|████▎     | 4254/9822 [2:11:22<2:34:47,  1.67s/it] 43%|████▎     | 4255/9822 [2:11:24<2:34:37,  1.67s/it] 43%|████▎     | 4256/9822 [2:11:25<2:34:27,  1.67s/it] 43%|████▎     | 4257/9822 [2:11:27<2:34:20,  1.66s/it] 43%|████▎     | 4258/9822 [2:11:29<2:34:33,  1.67s/it] 43%|████▎     | 4259/9822 [2:11:30<2:34:30,  1.67s/it] 43%|████▎     | 4260/9822 [2:11:32<2:34:56,  1.67s/it] 43%|████▎     | 4261/9822 [2:11:34<2:37:37,  1.70s/it] 43%|████▎     | 4262/9822 [2:11:35<2:36:46,  1.69s/it] 43%|████▎     | 4263/9822 [2:11:37<2:35:58,  1.68s/it] 43%|████▎     | 4264/9822 [2:11:39<2:35:19,  1.68s/it] 43%|████▎     | 4265/9822 [2:11:40<2:34:45,  1.67s/it] 43%|████▎     | 4266/9822 [2:11:42<2:34:53,  1.67s/it] 43%|████▎     | 4267/9822 [2:11:44<2:34:35,  1.67s/it] 43%|████▎     | 4268/9822 [2:11:45<2:34:25,  1.67s/it] 43%|████▎     | 4269/9822 [2:11:47<2:34:10,  1.67s/it] 43%|████▎     | 4270/9822 [2:11:49<2:34:02,  1.66s/it] 43%|████▎     | 4271/9822 [2:11:50<2:33:59,  1.66s/it] 43%|████▎     | 4272/9822 [2:11:52<2:34:00,  1.67s/it] 44%|████▎     | 4273/9822 [2:11:54<2:34:17,  1.67s/it] 44%|████▎     | 4274/9822 [2:11:55<2:34:24,  1.67s/it] 44%|████▎     | 4275/9822 [2:11:57<2:34:31,  1.67s/it] 44%|████▎     | 4276/9822 [2:11:59<2:34:28,  1.67s/it] 44%|████▎     | 4277/9822 [2:12:00<2:34:09,  1.67s/it] 44%|████▎     | 4278/9822 [2:12:02<2:33:59,  1.67s/it] 44%|████▎     | 4279/9822 [2:12:04<2:33:43,  1.66s/it] 44%|████▎     | 4280/9822 [2:12:05<2:33:56,  1.67s/it] 44%|████▎     | 4281/9822 [2:12:07<2:34:01,  1.67s/it] 44%|████▎     | 4282/9822 [2:12:09<2:34:16,  1.67s/it] 44%|████▎     | 4283/9822 [2:12:10<2:34:17,  1.67s/it] 44%|████▎     | 4284/9822 [2:12:12<2:34:10,  1.67s/it] 44%|████▎     | 4285/9822 [2:12:14<2:33:46,  1.67s/it] 44%|████▎     | 4286/9822 [2:12:15<2:34:07,  1.67s/it] 44%|████▎     | 4287/9822 [2:12:17<2:33:55,  1.67s/it] 44%|████▎     | 4288/9822 [2:12:19<2:36:44,  1.70s/it] 44%|████▎     | 4289/9822 [2:12:21<2:35:52,  1.69s/it] 44%|████▎     | 4290/9822 [2:12:22<2:35:23,  1.69s/it] 44%|████▎     | 4291/9822 [2:12:24<2:35:01,  1.68s/it] 44%|████▎     | 4292/9822 [2:12:26<2:34:29,  1.68s/it] 44%|████▎     | 4293/9822 [2:12:27<2:34:24,  1.68s/it] 44%|████▎     | 4294/9822 [2:12:29<2:34:35,  1.68s/it] 44%|████▎     | 4295/9822 [2:12:31<2:34:18,  1.68s/it] 44%|████▎     | 4296/9822 [2:12:32<2:34:07,  1.67s/it] 44%|████▎     | 4297/9822 [2:12:34<2:33:57,  1.67s/it] 44%|████▍     | 4298/9822 [2:12:36<2:34:02,  1.67s/it] 44%|████▍     | 4299/9822 [2:12:37<2:34:11,  1.68s/it] 44%|████▍     | 4300/9822 [2:12:39<2:32:50,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:53:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:53:26 - INFO - __main__ - ***** test Results*****
04/29/2024 13:53:26 - INFO - __main__ -   Training step = 4300
04/29/2024 13:53:26 - INFO - __main__ -  test_accuracy:0.8674963396778916 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:53:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:53:30 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:53:30 - INFO - __main__ -   Training step = 4300
04/29/2024 13:53:30 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:53:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:53:39 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:53:39 - INFO - __main__ -   Training step = 4300
04/29/2024 13:53:39 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 44%|████▍     | 4301/9822 [2:12:58<10:37:23,  6.93s/it] 44%|████▍     | 4302/9822 [2:13:00<8:12:07,  5.35s/it]  44%|████▍     | 4303/9822 [2:13:01<6:30:08,  4.24s/it] 44%|████▍     | 4304/9822 [2:13:03<5:18:44,  3.47s/it] 44%|████▍     | 4305/9822 [2:13:05<4:29:07,  2.93s/it] 44%|████▍     | 4306/9822 [2:13:06<3:54:00,  2.55s/it] 44%|████▍     | 4307/9822 [2:13:08<3:29:33,  2.28s/it] 44%|████▍     | 4308/9822 [2:13:10<3:12:26,  2.09s/it] 44%|████▍     | 4309/9822 [2:13:11<3:00:35,  1.97s/it] 44%|████▍     | 4310/9822 [2:13:13<2:52:20,  1.88s/it] 44%|████▍     | 4311/9822 [2:13:15<2:46:19,  1.81s/it] 44%|████▍     | 4312/9822 [2:13:16<2:41:58,  1.76s/it] 44%|████▍     | 4313/9822 [2:13:18<2:39:31,  1.74s/it] 44%|████▍     | 4314/9822 [2:13:20<2:37:30,  1.72s/it] 44%|████▍     | 4315/9822 [2:13:21<2:36:04,  1.70s/it] 44%|████▍     | 4316/9822 [2:13:23<2:35:10,  1.69s/it] 44%|████▍     | 4317/9822 [2:13:25<2:37:19,  1.71s/it] 44%|████▍     | 4318/9822 [2:13:26<2:35:52,  1.70s/it] 44%|████▍     | 4319/9822 [2:13:28<2:34:40,  1.69s/it] 44%|████▍     | 4320/9822 [2:13:30<2:34:09,  1.68s/it] 44%|████▍     | 4321/9822 [2:13:31<2:33:41,  1.68s/it] 44%|████▍     | 4322/9822 [2:13:33<2:33:26,  1.67s/it] 44%|████▍     | 4323/9822 [2:13:35<2:33:00,  1.67s/it] 44%|████▍     | 4324/9822 [2:13:36<2:32:46,  1.67s/it] 44%|████▍     | 4325/9822 [2:13:38<2:32:23,  1.66s/it] 44%|████▍     | 4326/9822 [2:13:40<2:32:24,  1.66s/it] 44%|████▍     | 4327/9822 [2:13:41<2:32:35,  1.67s/it] 44%|████▍     | 4328/9822 [2:13:43<2:32:31,  1.67s/it] 44%|████▍     | 4329/9822 [2:13:45<2:32:36,  1.67s/it] 44%|████▍     | 4330/9822 [2:13:46<2:32:17,  1.66s/it] 44%|████▍     | 4331/9822 [2:13:48<2:32:15,  1.66s/it] 44%|████▍     | 4332/9822 [2:13:50<2:32:14,  1.66s/it] 44%|████▍     | 4333/9822 [2:13:51<2:32:13,  1.66s/it] 44%|████▍     | 4334/9822 [2:13:53<2:31:56,  1.66s/it] 44%|████▍     | 4335/9822 [2:13:55<2:32:21,  1.67s/it] 44%|████▍     | 4336/9822 [2:13:56<2:32:27,  1.67s/it] 44%|████▍     | 4337/9822 [2:13:58<2:32:44,  1.67s/it] 44%|████▍     | 4338/9822 [2:14:00<2:32:22,  1.67s/it] 44%|████▍     | 4339/9822 [2:14:01<2:32:48,  1.67s/it] 44%|████▍     | 4340/9822 [2:14:03<2:33:56,  1.68s/it] 44%|████▍     | 4341/9822 [2:14:05<2:34:05,  1.69s/it] 44%|████▍     | 4342/9822 [2:14:07<2:34:05,  1.69s/it] 44%|████▍     | 4343/9822 [2:14:08<2:33:58,  1.69s/it] 44%|████▍     | 4344/9822 [2:14:10<2:33:41,  1.68s/it] 44%|████▍     | 4345/9822 [2:14:12<2:33:46,  1.68s/it] 44%|████▍     | 4346/9822 [2:14:13<2:33:49,  1.69s/it] 44%|████▍     | 4347/9822 [2:14:15<2:33:52,  1.69s/it] 44%|████▍     | 4348/9822 [2:14:17<2:33:47,  1.69s/it] 44%|████▍     | 4349/9822 [2:14:18<2:33:49,  1.69s/it] 44%|████▍     | 4350/9822 [2:14:20<2:36:31,  1.72s/it] 44%|████▍     | 4351/9822 [2:14:22<2:35:40,  1.71s/it] 44%|████▍     | 4352/9822 [2:14:24<2:35:11,  1.70s/it] 44%|████▍     | 4353/9822 [2:14:25<2:34:41,  1.70s/it] 44%|████▍     | 4354/9822 [2:14:27<2:34:25,  1.69s/it] 44%|████▍     | 4355/9822 [2:14:29<2:34:06,  1.69s/it] 44%|████▍     | 4356/9822 [2:14:30<2:33:51,  1.69s/it] 44%|████▍     | 4357/9822 [2:14:32<2:33:42,  1.69s/it] 44%|████▍     | 4358/9822 [2:14:34<2:33:40,  1.69s/it] 44%|████▍     | 4359/9822 [2:14:35<2:33:39,  1.69s/it] 44%|████▍     | 4360/9822 [2:14:37<2:33:39,  1.69s/it] 44%|████▍     | 4361/9822 [2:14:39<2:33:24,  1.69s/it] 44%|████▍     | 4362/9822 [2:14:40<2:33:21,  1.69s/it] 44%|████▍     | 4363/9822 [2:14:42<2:33:17,  1.68s/it] 44%|████▍     | 4364/9822 [2:14:44<2:33:14,  1.68s/it] 44%|████▍     | 4365/9822 [2:14:45<2:33:18,  1.69s/it] 44%|████▍     | 4366/9822 [2:14:47<2:33:02,  1.68s/it] 44%|████▍     | 4367/9822 [2:14:49<2:33:13,  1.69s/it] 44%|████▍     | 4368/9822 [2:14:50<2:33:16,  1.69s/it] 44%|████▍     | 4369/9822 [2:14:52<2:33:17,  1.69s/it] 44%|████▍     | 4370/9822 [2:14:54<2:33:10,  1.69s/it] 45%|████▍     | 4371/9822 [2:14:56<2:33:09,  1.69s/it] 45%|████▍     | 4372/9822 [2:14:57<2:35:47,  1.72s/it] 45%|████▍     | 4373/9822 [2:14:59<2:34:54,  1.71s/it] 45%|████▍     | 4374/9822 [2:15:01<2:34:23,  1.70s/it] 45%|████▍     | 4375/9822 [2:15:02<2:34:00,  1.70s/it] 45%|████▍     | 4376/9822 [2:15:04<2:33:44,  1.69s/it] 45%|████▍     | 4377/9822 [2:15:06<2:33:27,  1.69s/it] 45%|████▍     | 4378/9822 [2:15:07<2:33:19,  1.69s/it] 45%|████▍     | 4379/9822 [2:15:09<2:33:15,  1.69s/it] 45%|████▍     | 4380/9822 [2:15:11<2:33:05,  1.69s/it] 45%|████▍     | 4381/9822 [2:15:12<2:32:57,  1.69s/it] 45%|████▍     | 4382/9822 [2:15:14<2:33:04,  1.69s/it] 45%|████▍     | 4383/9822 [2:15:16<2:32:42,  1.68s/it] 45%|████▍     | 4384/9822 [2:15:18<2:32:40,  1.68s/it] 45%|████▍     | 4385/9822 [2:15:19<2:32:47,  1.69s/it] 45%|████▍     | 4386/9822 [2:15:21<2:31:26,  1.67s/it] 45%|████▍     | 4387/9822 [2:15:23<2:31:52,  1.68s/it] 45%|████▍     | 4388/9822 [2:15:24<2:32:03,  1.68s/it] 45%|████▍     | 4389/9822 [2:15:26<2:32:21,  1.68s/it] 45%|████▍     | 4390/9822 [2:15:28<2:32:23,  1.68s/it] 45%|████▍     | 4391/9822 [2:15:29<2:32:26,  1.68s/it] 45%|████▍     | 4392/9822 [2:15:31<2:32:22,  1.68s/it] 45%|████▍     | 4393/9822 [2:15:33<2:32:25,  1.68s/it] 45%|████▍     | 4394/9822 [2:15:34<2:32:27,  1.69s/it] 45%|████▍     | 4395/9822 [2:15:36<2:32:31,  1.69s/it] 45%|████▍     | 4396/9822 [2:15:38<2:32:27,  1.69s/it] 45%|████▍     | 4397/9822 [2:15:39<2:32:15,  1.68s/it] 45%|████▍     | 4398/9822 [2:15:41<2:32:15,  1.68s/it] 45%|████▍     | 4399/9822 [2:15:43<2:35:09,  1.72s/it] 45%|████▍     | 4400/9822 [2:15:45<2:34:19,  1.71s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:56:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:56:31 - INFO - __main__ - ***** test Results*****
04/29/2024 13:56:31 - INFO - __main__ -   Training step = 4400
04/29/2024 13:56:31 - INFO - __main__ -  test_accuracy:0.8656661786237189 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:56:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:56:36 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:56:36 - INFO - __main__ -   Training step = 4400
04/29/2024 13:56:36 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:56:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:56:44 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:56:44 - INFO - __main__ -   Training step = 4400
04/29/2024 13:56:44 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 45%|████▍     | 4401/9822 [2:16:04<10:29:17,  6.97s/it] 45%|████▍     | 4402/9822 [2:16:05<8:06:09,  5.38s/it]  45%|████▍     | 4403/9822 [2:16:07<6:25:41,  4.27s/it] 45%|████▍     | 4404/9822 [2:16:09<5:15:44,  3.50s/it] 45%|████▍     | 4405/9822 [2:16:11<4:26:41,  2.95s/it] 45%|████▍     | 4406/9822 [2:16:12<3:52:22,  2.57s/it] 45%|████▍     | 4407/9822 [2:16:14<3:28:19,  2.31s/it] 45%|████▍     | 4408/9822 [2:16:16<3:11:35,  2.12s/it] 45%|████▍     | 4409/9822 [2:16:17<2:59:30,  1.99s/it] 45%|████▍     | 4410/9822 [2:16:19<2:51:15,  1.90s/it] 45%|████▍     | 4411/9822 [2:16:21<2:45:38,  1.84s/it] 45%|████▍     | 4412/9822 [2:16:22<2:41:29,  1.79s/it] 45%|████▍     | 4413/9822 [2:16:24<2:38:39,  1.76s/it] 45%|████▍     | 4414/9822 [2:16:26<2:36:32,  1.74s/it] 45%|████▍     | 4415/9822 [2:16:27<2:35:03,  1.72s/it] 45%|████▍     | 4416/9822 [2:16:29<2:34:07,  1.71s/it] 45%|████▍     | 4417/9822 [2:16:31<2:33:28,  1.70s/it] 45%|████▍     | 4418/9822 [2:16:32<2:33:01,  1.70s/it] 45%|████▍     | 4419/9822 [2:16:34<2:32:35,  1.69s/it] 45%|████▌     | 4420/9822 [2:16:36<2:32:09,  1.69s/it] 45%|████▌     | 4421/9822 [2:16:38<2:32:11,  1.69s/it] 45%|████▌     | 4422/9822 [2:16:39<2:32:05,  1.69s/it] 45%|████▌     | 4423/9822 [2:16:41<2:31:59,  1.69s/it] 45%|████▌     | 4424/9822 [2:16:43<2:31:50,  1.69s/it] 45%|████▌     | 4425/9822 [2:16:44<2:31:43,  1.69s/it] 45%|████▌     | 4426/9822 [2:16:46<2:31:31,  1.68s/it] 45%|████▌     | 4427/9822 [2:16:48<2:34:29,  1.72s/it] 45%|████▌     | 4428/9822 [2:16:49<2:33:37,  1.71s/it] 45%|████▌     | 4429/9822 [2:16:51<2:33:02,  1.70s/it] 45%|████▌     | 4430/9822 [2:16:53<2:32:37,  1.70s/it] 45%|████▌     | 4431/9822 [2:16:54<2:32:12,  1.69s/it] 45%|████▌     | 4432/9822 [2:16:56<2:31:43,  1.69s/it] 45%|████▌     | 4433/9822 [2:16:58<2:31:01,  1.68s/it] 45%|████▌     | 4434/9822 [2:16:59<2:30:29,  1.68s/it] 45%|████▌     | 4435/9822 [2:17:01<2:30:16,  1.67s/it] 45%|████▌     | 4436/9822 [2:17:03<2:29:45,  1.67s/it] 45%|████▌     | 4437/9822 [2:17:04<2:29:24,  1.66s/it] 45%|████▌     | 4438/9822 [2:17:06<2:29:06,  1.66s/it] 45%|████▌     | 4439/9822 [2:17:08<2:28:55,  1.66s/it] 45%|████▌     | 4440/9822 [2:17:09<2:28:50,  1.66s/it] 45%|████▌     | 4441/9822 [2:17:11<2:28:41,  1.66s/it] 45%|████▌     | 4442/9822 [2:17:13<2:28:35,  1.66s/it] 45%|████▌     | 4443/9822 [2:17:14<2:28:35,  1.66s/it] 45%|████▌     | 4444/9822 [2:17:16<2:28:32,  1.66s/it] 45%|████▌     | 4445/9822 [2:17:18<2:28:28,  1.66s/it] 45%|████▌     | 4446/9822 [2:17:19<2:28:31,  1.66s/it] 45%|████▌     | 4447/9822 [2:17:21<2:28:22,  1.66s/it] 45%|████▌     | 4448/9822 [2:17:23<2:28:23,  1.66s/it] 45%|████▌     | 4449/9822 [2:17:24<2:28:16,  1.66s/it] 45%|████▌     | 4450/9822 [2:17:26<2:28:16,  1.66s/it] 45%|████▌     | 4451/9822 [2:17:28<2:28:23,  1.66s/it] 45%|████▌     | 4452/9822 [2:17:29<2:28:24,  1.66s/it] 45%|████▌     | 4453/9822 [2:17:31<2:28:28,  1.66s/it] 45%|████▌     | 4454/9822 [2:17:33<2:28:30,  1.66s/it] 45%|████▌     | 4455/9822 [2:17:34<2:28:21,  1.66s/it] 45%|████▌     | 4456/9822 [2:17:36<2:28:15,  1.66s/it] 45%|████▌     | 4457/9822 [2:17:38<2:28:20,  1.66s/it] 45%|████▌     | 4458/9822 [2:17:39<2:28:12,  1.66s/it] 45%|████▌     | 4459/9822 [2:17:41<2:28:27,  1.66s/it] 45%|████▌     | 4460/9822 [2:17:43<2:28:20,  1.66s/it] 45%|████▌     | 4461/9822 [2:17:44<2:28:11,  1.66s/it] 45%|████▌     | 4462/9822 [2:17:46<2:28:02,  1.66s/it] 45%|████▌     | 4463/9822 [2:17:48<2:28:10,  1.66s/it] 45%|████▌     | 4464/9822 [2:17:49<2:28:15,  1.66s/it] 45%|████▌     | 4465/9822 [2:17:51<2:28:18,  1.66s/it] 45%|████▌     | 4466/9822 [2:17:53<2:28:15,  1.66s/it] 45%|████▌     | 4467/9822 [2:17:54<2:28:42,  1.67s/it] 45%|████▌     | 4468/9822 [2:17:56<2:31:18,  1.70s/it] 45%|████▌     | 4469/9822 [2:17:58<2:30:34,  1.69s/it] 46%|████▌     | 4470/9822 [2:17:59<2:29:46,  1.68s/it] 46%|████▌     | 4471/9822 [2:18:01<2:29:12,  1.67s/it] 46%|████▌     | 4472/9822 [2:18:03<2:27:28,  1.65s/it] 46%|████▌     | 4473/9822 [2:18:04<2:27:41,  1.66s/it] 46%|████▌     | 4474/9822 [2:18:06<2:27:54,  1.66s/it] 46%|████▌     | 4475/9822 [2:18:08<2:27:50,  1.66s/it] 46%|████▌     | 4476/9822 [2:18:09<2:27:40,  1.66s/it] 46%|████▌     | 4477/9822 [2:18:11<2:27:40,  1.66s/it] 46%|████▌     | 4478/9822 [2:18:13<2:27:36,  1.66s/it] 46%|████▌     | 4479/9822 [2:18:14<2:27:43,  1.66s/it] 46%|████▌     | 4480/9822 [2:18:16<2:27:40,  1.66s/it] 46%|████▌     | 4481/9822 [2:18:18<2:27:36,  1.66s/it] 46%|████▌     | 4482/9822 [2:18:19<2:27:37,  1.66s/it] 46%|████▌     | 4483/9822 [2:18:21<2:27:36,  1.66s/it] 46%|████▌     | 4484/9822 [2:18:23<2:27:36,  1.66s/it] 46%|████▌     | 4485/9822 [2:18:24<2:27:31,  1.66s/it] 46%|████▌     | 4486/9822 [2:18:26<2:27:38,  1.66s/it] 46%|████▌     | 4487/9822 [2:18:27<2:27:34,  1.66s/it] 46%|████▌     | 4488/9822 [2:18:29<2:27:24,  1.66s/it] 46%|████▌     | 4489/9822 [2:18:31<2:27:28,  1.66s/it] 46%|████▌     | 4490/9822 [2:18:32<2:27:23,  1.66s/it] 46%|████▌     | 4491/9822 [2:18:34<2:27:19,  1.66s/it] 46%|████▌     | 4492/9822 [2:18:36<2:27:16,  1.66s/it] 46%|████▌     | 4493/9822 [2:18:37<2:27:32,  1.66s/it] 46%|████▌     | 4494/9822 [2:18:39<2:27:20,  1.66s/it] 46%|████▌     | 4495/9822 [2:18:41<2:30:11,  1.69s/it] 46%|████▌     | 4496/9822 [2:18:43<2:29:39,  1.69s/it] 46%|████▌     | 4497/9822 [2:18:44<2:28:58,  1.68s/it] 46%|████▌     | 4498/9822 [2:18:46<2:28:56,  1.68s/it] 46%|████▌     | 4499/9822 [2:18:48<2:28:26,  1.67s/it] 46%|████▌     | 4500/9822 [2:18:49<2:28:18,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 13:59:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:59:36 - INFO - __main__ - ***** test Results*****
04/29/2024 13:59:36 - INFO - __main__ -   Training step = 4500
04/29/2024 13:59:36 - INFO - __main__ -  test_accuracy:0.8660322108345534 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:59:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:59:40 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 13:59:40 - INFO - __main__ -   Training step = 4500
04/29/2024 13:59:40 - INFO - __main__ -  eval_accuracy:0.85060417429513 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 13:59:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 13:59:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 13:59:49 - INFO - __main__ -   Training step = 4500
04/29/2024 13:59:49 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 46%|████▌     | 4501/9822 [2:19:08<10:15:43,  6.94s/it] 46%|████▌     | 4502/9822 [2:19:10<7:55:05,  5.36s/it]  46%|████▌     | 4503/9822 [2:19:12<6:16:52,  4.25s/it] 46%|████▌     | 4504/9822 [2:19:13<5:08:03,  3.48s/it] 46%|████▌     | 4505/9822 [2:19:15<4:19:37,  2.93s/it] 46%|████▌     | 4506/9822 [2:19:17<3:45:39,  2.55s/it] 46%|████▌     | 4507/9822 [2:19:18<3:21:57,  2.28s/it] 46%|████▌     | 4508/9822 [2:19:20<3:05:22,  2.09s/it] 46%|████▌     | 4509/9822 [2:19:22<2:53:44,  1.96s/it] 46%|████▌     | 4510/9822 [2:19:23<2:45:59,  1.87s/it] 46%|████▌     | 4511/9822 [2:19:25<2:40:17,  1.81s/it] 46%|████▌     | 4512/9822 [2:19:27<2:36:21,  1.77s/it] 46%|████▌     | 4513/9822 [2:19:28<2:33:32,  1.74s/it] 46%|████▌     | 4514/9822 [2:19:30<2:31:25,  1.71s/it] 46%|████▌     | 4515/9822 [2:19:32<2:29:55,  1.69s/it] 46%|████▌     | 4516/9822 [2:19:33<2:29:00,  1.69s/it] 46%|████▌     | 4517/9822 [2:19:35<2:28:15,  1.68s/it] 46%|████▌     | 4518/9822 [2:19:37<2:27:47,  1.67s/it] 46%|████▌     | 4519/9822 [2:19:38<2:30:18,  1.70s/it] 46%|████▌     | 4520/9822 [2:19:40<2:29:10,  1.69s/it] 46%|████▌     | 4521/9822 [2:19:42<2:28:21,  1.68s/it] 46%|████▌     | 4522/9822 [2:19:43<2:28:25,  1.68s/it] 46%|████▌     | 4523/9822 [2:19:45<2:28:00,  1.68s/it] 46%|████▌     | 4524/9822 [2:19:47<2:27:41,  1.67s/it] 46%|████▌     | 4525/9822 [2:19:48<2:27:50,  1.67s/it] 46%|████▌     | 4526/9822 [2:19:50<2:27:21,  1.67s/it] 46%|████▌     | 4527/9822 [2:19:52<2:27:08,  1.67s/it] 46%|████▌     | 4528/9822 [2:19:53<2:27:02,  1.67s/it] 46%|████▌     | 4529/9822 [2:19:55<2:26:42,  1.66s/it] 46%|████▌     | 4530/9822 [2:19:57<2:26:37,  1.66s/it] 46%|████▌     | 4531/9822 [2:19:58<2:26:25,  1.66s/it] 46%|████▌     | 4532/9822 [2:20:00<2:26:20,  1.66s/it] 46%|████▌     | 4533/9822 [2:20:02<2:26:18,  1.66s/it] 46%|████▌     | 4534/9822 [2:20:03<2:26:30,  1.66s/it] 46%|████▌     | 4535/9822 [2:20:05<2:26:19,  1.66s/it] 46%|████▌     | 4536/9822 [2:20:07<2:26:11,  1.66s/it] 46%|████▌     | 4537/9822 [2:20:08<2:26:05,  1.66s/it] 46%|████▌     | 4538/9822 [2:20:10<2:25:57,  1.66s/it] 46%|████▌     | 4539/9822 [2:20:12<2:26:06,  1.66s/it] 46%|████▌     | 4540/9822 [2:20:13<2:26:09,  1.66s/it] 46%|████▌     | 4541/9822 [2:20:15<2:26:06,  1.66s/it] 46%|████▌     | 4542/9822 [2:20:17<2:26:12,  1.66s/it] 46%|████▋     | 4543/9822 [2:20:18<2:26:21,  1.66s/it] 46%|████▋     | 4544/9822 [2:20:20<2:26:12,  1.66s/it] 46%|████▋     | 4545/9822 [2:20:22<2:29:04,  1.69s/it] 46%|████▋     | 4546/9822 [2:20:23<2:28:23,  1.69s/it] 46%|████▋     | 4547/9822 [2:20:25<2:27:39,  1.68s/it] 46%|████▋     | 4548/9822 [2:20:27<2:27:02,  1.67s/it] 46%|████▋     | 4549/9822 [2:20:28<2:26:54,  1.67s/it] 46%|████▋     | 4550/9822 [2:20:30<2:26:30,  1.67s/it] 46%|████▋     | 4551/9822 [2:20:32<2:26:14,  1.66s/it] 46%|████▋     | 4552/9822 [2:20:33<2:26:20,  1.67s/it] 46%|████▋     | 4553/9822 [2:20:35<2:26:20,  1.67s/it] 46%|████▋     | 4554/9822 [2:20:37<2:26:04,  1.66s/it] 46%|████▋     | 4555/9822 [2:20:38<2:26:15,  1.67s/it] 46%|████▋     | 4556/9822 [2:20:40<2:26:10,  1.67s/it] 46%|████▋     | 4557/9822 [2:20:42<2:25:51,  1.66s/it] 46%|████▋     | 4558/9822 [2:20:43<2:24:43,  1.65s/it] 46%|████▋     | 4559/9822 [2:20:45<2:25:01,  1.65s/it] 46%|████▋     | 4560/9822 [2:20:47<2:25:22,  1.66s/it] 46%|████▋     | 4561/9822 [2:20:48<2:25:42,  1.66s/it] 46%|████▋     | 4562/9822 [2:20:50<2:25:38,  1.66s/it] 46%|████▋     | 4563/9822 [2:20:52<2:25:54,  1.66s/it] 46%|████▋     | 4564/9822 [2:20:53<2:26:10,  1.67s/it] 46%|████▋     | 4565/9822 [2:20:55<2:25:51,  1.66s/it] 46%|████▋     | 4566/9822 [2:20:57<2:25:49,  1.66s/it] 46%|████▋     | 4567/9822 [2:20:58<2:25:48,  1.66s/it] 47%|████▋     | 4568/9822 [2:21:00<2:25:59,  1.67s/it] 47%|████▋     | 4569/9822 [2:21:02<2:25:58,  1.67s/it] 47%|████▋     | 4570/9822 [2:21:03<2:26:02,  1.67s/it] 47%|████▋     | 4571/9822 [2:21:05<2:25:46,  1.67s/it] 47%|████▋     | 4572/9822 [2:21:07<2:25:50,  1.67s/it] 47%|████▋     | 4573/9822 [2:21:08<2:25:52,  1.67s/it] 47%|████▋     | 4574/9822 [2:21:10<2:26:29,  1.67s/it] 47%|████▋     | 4575/9822 [2:21:12<2:26:16,  1.67s/it] 47%|████▋     | 4576/9822 [2:21:13<2:25:56,  1.67s/it] 47%|████▋     | 4577/9822 [2:21:15<2:25:50,  1.67s/it] 47%|████▋     | 4578/9822 [2:21:17<2:28:21,  1.70s/it] 47%|████▋     | 4579/9822 [2:21:18<2:27:22,  1.69s/it] 47%|████▋     | 4580/9822 [2:21:20<2:26:42,  1.68s/it] 47%|████▋     | 4581/9822 [2:21:22<2:26:10,  1.67s/it] 47%|████▋     | 4582/9822 [2:21:23<2:25:58,  1.67s/it] 47%|████▋     | 4583/9822 [2:21:25<2:25:30,  1.67s/it] 47%|████▋     | 4584/9822 [2:21:27<2:25:13,  1.66s/it] 47%|████▋     | 4585/9822 [2:21:28<2:25:07,  1.66s/it] 47%|████▋     | 4586/9822 [2:21:30<2:25:39,  1.67s/it] 47%|████▋     | 4587/9822 [2:21:32<2:25:55,  1.67s/it] 47%|████▋     | 4588/9822 [2:21:33<2:25:39,  1.67s/it] 47%|████▋     | 4589/9822 [2:21:35<2:25:35,  1.67s/it] 47%|████▋     | 4590/9822 [2:21:37<2:25:14,  1.67s/it] 47%|████▋     | 4591/9822 [2:21:38<2:25:05,  1.66s/it] 47%|████▋     | 4592/9822 [2:21:40<2:24:49,  1.66s/it] 47%|████▋     | 4593/9822 [2:21:42<2:24:39,  1.66s/it] 47%|████▋     | 4594/9822 [2:21:43<2:24:33,  1.66s/it] 47%|████▋     | 4595/9822 [2:21:45<2:24:26,  1.66s/it] 47%|████▋     | 4596/9822 [2:21:47<2:24:21,  1.66s/it] 47%|████▋     | 4597/9822 [2:21:48<2:24:34,  1.66s/it] 47%|████▋     | 4598/9822 [2:21:50<2:24:25,  1.66s/it] 47%|████▋     | 4599/9822 [2:21:52<2:24:17,  1.66s/it] 47%|████▋     | 4600/9822 [2:21:53<2:26:56,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:02:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:02:40 - INFO - __main__ - ***** test Results*****
04/29/2024 14:02:40 - INFO - __main__ -   Training step = 4600
04/29/2024 14:02:40 - INFO - __main__ -  test_accuracy:0.8678623718887262 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:02:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:02:45 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:02:45 - INFO - __main__ -   Training step = 4600
04/29/2024 14:02:45 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:02:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:02:53 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:02:53 - INFO - __main__ -   Training step = 4600
04/29/2024 14:02:53 - INFO - __main__ -  eval_accuracy:0.9139509337239107 
 47%|████▋     | 4601/9822 [2:22:13<10:04:53,  6.95s/it] 47%|████▋     | 4602/9822 [2:22:14<7:46:40,  5.36s/it]  47%|████▋     | 4603/9822 [2:22:16<6:09:55,  4.25s/it] 47%|████▋     | 4604/9822 [2:22:18<5:02:21,  3.48s/it] 47%|████▋     | 4605/9822 [2:22:19<4:14:48,  2.93s/it] 47%|████▋     | 4606/9822 [2:22:21<3:41:36,  2.55s/it] 47%|████▋     | 4607/9822 [2:22:23<3:18:30,  2.28s/it] 47%|████▋     | 4608/9822 [2:22:24<3:02:05,  2.10s/it] 47%|████▋     | 4609/9822 [2:22:26<2:50:45,  1.97s/it] 47%|████▋     | 4610/9822 [2:22:28<2:42:57,  1.88s/it] 47%|████▋     | 4611/9822 [2:22:29<2:37:20,  1.81s/it] 47%|████▋     | 4612/9822 [2:22:31<2:33:21,  1.77s/it] 47%|████▋     | 4613/9822 [2:22:33<2:30:35,  1.73s/it] 47%|████▋     | 4614/9822 [2:22:34<2:28:38,  1.71s/it] 47%|████▋     | 4615/9822 [2:22:36<2:27:13,  1.70s/it] 47%|████▋     | 4616/9822 [2:22:38<2:26:28,  1.69s/it] 47%|████▋     | 4617/9822 [2:22:39<2:25:46,  1.68s/it] 47%|████▋     | 4618/9822 [2:22:41<2:25:04,  1.67s/it] 47%|████▋     | 4619/9822 [2:22:43<2:24:55,  1.67s/it] 47%|████▋     | 4620/9822 [2:22:44<2:24:39,  1.67s/it] 47%|████▋     | 4621/9822 [2:22:46<2:24:35,  1.67s/it] 47%|████▋     | 4622/9822 [2:22:48<2:24:37,  1.67s/it] 47%|████▋     | 4623/9822 [2:22:49<2:24:38,  1.67s/it] 47%|████▋     | 4624/9822 [2:22:51<2:26:56,  1.70s/it] 47%|████▋     | 4625/9822 [2:22:53<2:26:08,  1.69s/it] 47%|████▋     | 4626/9822 [2:22:54<2:25:12,  1.68s/it] 47%|████▋     | 4627/9822 [2:22:56<2:24:43,  1.67s/it] 47%|████▋     | 4628/9822 [2:22:58<2:24:41,  1.67s/it] 47%|████▋     | 4629/9822 [2:22:59<2:24:40,  1.67s/it] 47%|████▋     | 4630/9822 [2:23:01<2:24:31,  1.67s/it] 47%|████▋     | 4631/9822 [2:23:03<2:24:15,  1.67s/it] 47%|████▋     | 4632/9822 [2:23:04<2:24:00,  1.66s/it] 47%|████▋     | 4633/9822 [2:23:06<2:23:46,  1.66s/it] 47%|████▋     | 4634/9822 [2:23:08<2:23:48,  1.66s/it] 47%|████▋     | 4635/9822 [2:23:09<2:23:48,  1.66s/it] 47%|████▋     | 4636/9822 [2:23:11<2:23:39,  1.66s/it] 47%|████▋     | 4637/9822 [2:23:13<2:23:58,  1.67s/it] 47%|████▋     | 4638/9822 [2:23:14<2:24:01,  1.67s/it] 47%|████▋     | 4639/9822 [2:23:16<2:23:39,  1.66s/it] 47%|████▋     | 4640/9822 [2:23:18<2:23:54,  1.67s/it] 47%|████▋     | 4641/9822 [2:23:19<2:23:49,  1.67s/it] 47%|████▋     | 4642/9822 [2:23:21<2:23:34,  1.66s/it] 47%|████▋     | 4643/9822 [2:23:23<2:23:32,  1.66s/it] 47%|████▋     | 4644/9822 [2:23:24<2:22:06,  1.65s/it] 47%|████▋     | 4645/9822 [2:23:26<2:22:34,  1.65s/it] 47%|████▋     | 4646/9822 [2:23:28<2:22:41,  1.65s/it] 47%|████▋     | 4647/9822 [2:23:29<2:23:15,  1.66s/it] 47%|████▋     | 4648/9822 [2:23:31<2:23:15,  1.66s/it] 47%|████▋     | 4649/9822 [2:23:33<2:23:08,  1.66s/it] 47%|████▋     | 4650/9822 [2:23:34<2:25:41,  1.69s/it] 47%|████▋     | 4651/9822 [2:23:36<2:24:45,  1.68s/it] 47%|████▋     | 4652/9822 [2:23:38<2:24:20,  1.68s/it] 47%|████▋     | 4653/9822 [2:23:39<2:23:52,  1.67s/it] 47%|████▋     | 4654/9822 [2:23:41<2:23:42,  1.67s/it] 47%|████▋     | 4655/9822 [2:23:43<2:23:35,  1.67s/it] 47%|████▋     | 4656/9822 [2:23:44<2:23:12,  1.66s/it] 47%|████▋     | 4657/9822 [2:23:46<2:22:57,  1.66s/it] 47%|████▋     | 4658/9822 [2:23:48<2:23:01,  1.66s/it] 47%|████▋     | 4659/9822 [2:23:49<2:22:59,  1.66s/it] 47%|████▋     | 4660/9822 [2:23:51<2:23:03,  1.66s/it] 47%|████▋     | 4661/9822 [2:23:53<2:22:55,  1.66s/it] 47%|████▋     | 4662/9822 [2:23:54<2:22:44,  1.66s/it] 47%|████▋     | 4663/9822 [2:23:56<2:22:50,  1.66s/it] 47%|████▋     | 4664/9822 [2:23:58<2:22:58,  1.66s/it] 47%|████▋     | 4665/9822 [2:23:59<2:23:05,  1.66s/it] 48%|████▊     | 4666/9822 [2:24:01<2:23:04,  1.66s/it] 48%|████▊     | 4667/9822 [2:24:03<2:22:58,  1.66s/it] 48%|████▊     | 4668/9822 [2:24:04<2:22:56,  1.66s/it] 48%|████▊     | 4669/9822 [2:24:06<2:22:49,  1.66s/it] 48%|████▊     | 4670/9822 [2:24:08<2:22:35,  1.66s/it] 48%|████▊     | 4671/9822 [2:24:09<2:22:35,  1.66s/it] 48%|████▊     | 4672/9822 [2:24:11<2:22:32,  1.66s/it] 48%|████▊     | 4673/9822 [2:24:13<2:22:30,  1.66s/it] 48%|████▊     | 4674/9822 [2:24:14<2:22:20,  1.66s/it] 48%|████▊     | 4675/9822 [2:24:16<2:22:23,  1.66s/it] 48%|████▊     | 4676/9822 [2:24:18<2:22:17,  1.66s/it] 48%|████▊     | 4677/9822 [2:24:19<2:22:23,  1.66s/it] 48%|████▊     | 4678/9822 [2:24:21<2:22:41,  1.66s/it] 48%|████▊     | 4679/9822 [2:24:23<2:22:37,  1.66s/it] 48%|████▊     | 4680/9822 [2:24:24<2:22:25,  1.66s/it] 48%|████▊     | 4681/9822 [2:24:26<2:22:31,  1.66s/it] 48%|████▊     | 4682/9822 [2:24:28<2:22:18,  1.66s/it] 48%|████▊     | 4683/9822 [2:24:29<2:25:06,  1.69s/it] 48%|████▊     | 4684/9822 [2:24:31<2:24:20,  1.69s/it] 48%|████▊     | 4685/9822 [2:24:33<2:23:40,  1.68s/it] 48%|████▊     | 4686/9822 [2:24:34<2:23:04,  1.67s/it] 48%|████▊     | 4687/9822 [2:24:36<2:22:38,  1.67s/it] 48%|████▊     | 4688/9822 [2:24:38<2:22:32,  1.67s/it] 48%|████▊     | 4689/9822 [2:24:39<2:22:23,  1.66s/it] 48%|████▊     | 4690/9822 [2:24:41<2:22:31,  1.67s/it] 48%|████▊     | 4691/9822 [2:24:43<2:22:19,  1.66s/it] 48%|████▊     | 4692/9822 [2:24:44<2:22:23,  1.67s/it] 48%|████▊     | 4693/9822 [2:24:46<2:22:25,  1.67s/it] 48%|████▊     | 4694/9822 [2:24:48<2:22:23,  1.67s/it] 48%|████▊     | 4695/9822 [2:24:49<2:22:05,  1.66s/it] 48%|████▊     | 4696/9822 [2:24:51<2:22:22,  1.67s/it] 48%|████▊     | 4697/9822 [2:24:53<2:22:11,  1.66s/it] 48%|████▊     | 4698/9822 [2:24:54<2:22:07,  1.66s/it] 48%|████▊     | 4699/9822 [2:24:56<2:21:57,  1.66s/it] 48%|████▊     | 4700/9822 [2:24:58<2:21:49,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:05:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:05:44 - INFO - __main__ - ***** test Results*****
04/29/2024 14:05:44 - INFO - __main__ -   Training step = 4700
04/29/2024 14:05:44 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:05:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:05:49 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:05:49 - INFO - __main__ -   Training step = 4700
04/29/2024 14:05:49 - INFO - __main__ -  eval_accuracy:0.8473086781398755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:05:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:05:57 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:05:57 - INFO - __main__ -   Training step = 4700
04/29/2024 14:05:57 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 48%|████▊     | 4701/9822 [2:25:17<9:51:27,  6.93s/it] 48%|████▊     | 4702/9822 [2:25:18<7:36:18,  5.35s/it] 48%|████▊     | 4703/9822 [2:25:20<6:01:53,  4.24s/it] 48%|████▊     | 4704/9822 [2:25:22<4:55:48,  3.47s/it] 48%|████▊     | 4705/9822 [2:25:23<4:09:35,  2.93s/it] 48%|████▊     | 4706/9822 [2:25:25<3:37:08,  2.55s/it] 48%|████▊     | 4707/9822 [2:25:27<3:14:18,  2.28s/it] 48%|████▊     | 4708/9822 [2:25:28<2:58:28,  2.09s/it] 48%|████▊     | 4709/9822 [2:25:30<2:49:53,  1.99s/it] 48%|████▊     | 4710/9822 [2:25:32<2:41:26,  1.89s/it] 48%|████▊     | 4711/9822 [2:25:33<2:35:24,  1.82s/it] 48%|████▊     | 4712/9822 [2:25:35<2:31:03,  1.77s/it] 48%|████▊     | 4713/9822 [2:25:37<2:28:06,  1.74s/it] 48%|████▊     | 4714/9822 [2:25:38<2:26:10,  1.72s/it] 48%|████▊     | 4715/9822 [2:25:40<2:24:36,  1.70s/it] 48%|████▊     | 4716/9822 [2:25:42<2:23:27,  1.69s/it] 48%|████▊     | 4717/9822 [2:25:43<2:22:44,  1.68s/it] 48%|████▊     | 4718/9822 [2:25:45<2:22:10,  1.67s/it] 48%|████▊     | 4719/9822 [2:25:47<2:21:45,  1.67s/it] 48%|████▊     | 4720/9822 [2:25:48<2:21:44,  1.67s/it] 48%|████▊     | 4721/9822 [2:25:50<2:21:29,  1.66s/it] 48%|████▊     | 4722/9822 [2:25:52<2:21:41,  1.67s/it] 48%|████▊     | 4723/9822 [2:25:53<2:21:26,  1.66s/it] 48%|████▊     | 4724/9822 [2:25:55<2:21:07,  1.66s/it] 48%|████▊     | 4725/9822 [2:25:57<2:20:58,  1.66s/it] 48%|████▊     | 4726/9822 [2:25:58<2:20:56,  1.66s/it] 48%|████▊     | 4727/9822 [2:26:00<2:20:50,  1.66s/it] 48%|████▊     | 4728/9822 [2:26:02<2:21:01,  1.66s/it] 48%|████▊     | 4729/9822 [2:26:03<2:20:57,  1.66s/it] 48%|████▊     | 4730/9822 [2:26:05<2:19:49,  1.65s/it] 48%|████▊     | 4731/9822 [2:26:07<2:20:13,  1.65s/it] 48%|████▊     | 4732/9822 [2:26:08<2:20:14,  1.65s/it] 48%|████▊     | 4733/9822 [2:26:10<2:20:33,  1.66s/it] 48%|████▊     | 4734/9822 [2:26:12<2:20:58,  1.66s/it] 48%|████▊     | 4735/9822 [2:26:13<2:23:36,  1.69s/it] 48%|████▊     | 4736/9822 [2:26:15<2:22:41,  1.68s/it] 48%|████▊     | 4737/9822 [2:26:17<2:22:08,  1.68s/it] 48%|████▊     | 4738/9822 [2:26:18<2:21:42,  1.67s/it] 48%|████▊     | 4739/9822 [2:26:20<2:21:30,  1.67s/it] 48%|████▊     | 4740/9822 [2:26:22<2:21:14,  1.67s/it] 48%|████▊     | 4741/9822 [2:26:23<2:20:59,  1.66s/it] 48%|████▊     | 4742/9822 [2:26:25<2:20:50,  1.66s/it] 48%|████▊     | 4743/9822 [2:26:27<2:20:45,  1.66s/it] 48%|████▊     | 4744/9822 [2:26:28<2:20:30,  1.66s/it] 48%|████▊     | 4745/9822 [2:26:30<2:20:27,  1.66s/it] 48%|████▊     | 4746/9822 [2:26:32<2:20:28,  1.66s/it] 48%|████▊     | 4747/9822 [2:26:33<2:20:22,  1.66s/it] 48%|████▊     | 4748/9822 [2:26:35<2:20:31,  1.66s/it] 48%|████▊     | 4749/9822 [2:26:37<2:20:27,  1.66s/it] 48%|████▊     | 4750/9822 [2:26:38<2:20:26,  1.66s/it] 48%|████▊     | 4751/9822 [2:26:40<2:20:14,  1.66s/it] 48%|████▊     | 4752/9822 [2:26:42<2:20:16,  1.66s/it] 48%|████▊     | 4753/9822 [2:26:43<2:20:26,  1.66s/it] 48%|████▊     | 4754/9822 [2:26:45<2:20:23,  1.66s/it] 48%|████▊     | 4755/9822 [2:26:47<2:20:15,  1.66s/it] 48%|████▊     | 4756/9822 [2:26:48<2:20:18,  1.66s/it] 48%|████▊     | 4757/9822 [2:26:50<2:20:06,  1.66s/it] 48%|████▊     | 4758/9822 [2:26:52<2:20:07,  1.66s/it] 48%|████▊     | 4759/9822 [2:26:53<2:20:07,  1.66s/it] 48%|████▊     | 4760/9822 [2:26:55<2:19:57,  1.66s/it] 48%|████▊     | 4761/9822 [2:26:57<2:19:56,  1.66s/it] 48%|████▊     | 4762/9822 [2:26:58<2:20:12,  1.66s/it] 48%|████▊     | 4763/9822 [2:27:00<2:20:02,  1.66s/it] 49%|████▊     | 4764/9822 [2:27:02<2:20:03,  1.66s/it] 49%|████▊     | 4765/9822 [2:27:03<2:20:12,  1.66s/it] 49%|████▊     | 4766/9822 [2:27:05<2:20:11,  1.66s/it] 49%|████▊     | 4767/9822 [2:27:07<2:20:17,  1.67s/it] 49%|████▊     | 4768/9822 [2:27:08<2:20:19,  1.67s/it] 49%|████▊     | 4769/9822 [2:27:10<2:20:16,  1.67s/it] 49%|████▊     | 4770/9822 [2:27:12<2:20:09,  1.66s/it] 49%|████▊     | 4771/9822 [2:27:13<2:20:32,  1.67s/it] 49%|████▊     | 4772/9822 [2:27:15<2:20:17,  1.67s/it] 49%|████▊     | 4773/9822 [2:27:17<2:20:17,  1.67s/it] 49%|████▊     | 4774/9822 [2:27:18<2:20:13,  1.67s/it] 49%|████▊     | 4775/9822 [2:27:20<2:19:56,  1.66s/it] 49%|████▊     | 4776/9822 [2:27:22<2:22:21,  1.69s/it] 49%|████▊     | 4777/9822 [2:27:23<2:21:38,  1.68s/it] 49%|████▊     | 4778/9822 [2:27:25<2:20:51,  1.68s/it] 49%|████▊     | 4779/9822 [2:27:27<2:20:28,  1.67s/it] 49%|████▊     | 4780/9822 [2:27:28<2:20:22,  1.67s/it] 49%|████▊     | 4781/9822 [2:27:30<2:20:00,  1.67s/it] 49%|████▊     | 4782/9822 [2:27:32<2:19:51,  1.67s/it] 49%|████▊     | 4783/9822 [2:27:33<2:19:45,  1.66s/it] 49%|████▊     | 4784/9822 [2:27:35<2:19:47,  1.66s/it] 49%|████▊     | 4785/9822 [2:27:37<2:19:36,  1.66s/it] 49%|████▊     | 4786/9822 [2:27:38<2:19:28,  1.66s/it] 49%|████▊     | 4787/9822 [2:27:40<2:19:23,  1.66s/it] 49%|████▊     | 4788/9822 [2:27:42<2:19:20,  1.66s/it] 49%|████▉     | 4789/9822 [2:27:43<2:19:29,  1.66s/it] 49%|████▉     | 4790/9822 [2:27:45<2:19:44,  1.67s/it] 49%|████▉     | 4791/9822 [2:27:47<2:19:34,  1.66s/it] 49%|████▉     | 4792/9822 [2:27:48<2:19:43,  1.67s/it] 49%|████▉     | 4793/9822 [2:27:50<2:19:33,  1.66s/it] 49%|████▉     | 4794/9822 [2:27:52<2:19:19,  1.66s/it] 49%|████▉     | 4795/9822 [2:27:53<2:19:16,  1.66s/it] 49%|████▉     | 4796/9822 [2:27:55<2:19:09,  1.66s/it] 49%|████▉     | 4797/9822 [2:27:57<2:18:59,  1.66s/it] 49%|████▉     | 4798/9822 [2:27:58<2:19:08,  1.66s/it] 49%|████▉     | 4799/9822 [2:28:00<2:18:58,  1.66s/it] 49%|████▉     | 4800/9822 [2:28:02<2:19:14,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:08:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:08:48 - INFO - __main__ - ***** test Results*****
04/29/2024 14:08:48 - INFO - __main__ -   Training step = 4800
04/29/2024 14:08:48 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:08:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:08:53 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:08:53 - INFO - __main__ -   Training step = 4800
04/29/2024 14:08:53 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 14:08:53,251 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 14:08:53,251 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 14:08:53,293 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 14:08:54,441 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:09:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:09:02 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:09:02 - INFO - __main__ -   Training step = 4800
04/29/2024 14:09:02 - INFO - __main__ -  eval_accuracy:0.916147931160747 
 49%|████▉     | 4801/9822 [2:28:22<10:09:53,  7.29s/it] 49%|████▉     | 4802/9822 [2:28:24<7:50:59,  5.63s/it]  49%|████▉     | 4803/9822 [2:28:25<6:11:25,  4.44s/it] 49%|████▉     | 4804/9822 [2:28:27<5:01:33,  3.61s/it] 49%|████▉     | 4805/9822 [2:28:29<4:12:38,  3.02s/it] 49%|████▉     | 4806/9822 [2:28:30<3:38:46,  2.62s/it] 49%|████▉     | 4807/9822 [2:28:32<3:15:06,  2.33s/it] 49%|████▉     | 4808/9822 [2:28:34<2:58:32,  2.14s/it] 49%|████▉     | 4809/9822 [2:28:35<2:47:03,  2.00s/it] 49%|████▉     | 4810/9822 [2:28:37<2:38:34,  1.90s/it] 49%|████▉     | 4811/9822 [2:28:39<2:32:49,  1.83s/it] 49%|████▉     | 4812/9822 [2:28:40<2:28:37,  1.78s/it] 49%|████▉     | 4813/9822 [2:28:42<2:25:41,  1.75s/it] 49%|████▉     | 4814/9822 [2:28:44<2:23:31,  1.72s/it] 49%|████▉     | 4815/9822 [2:28:45<2:22:01,  1.70s/it] 49%|████▉     | 4816/9822 [2:28:47<2:19:44,  1.67s/it] 49%|████▉     | 4817/9822 [2:28:49<2:19:15,  1.67s/it] 49%|████▉     | 4818/9822 [2:28:50<2:18:55,  1.67s/it] 49%|████▉     | 4819/9822 [2:28:52<2:18:35,  1.66s/it] 49%|████▉     | 4820/9822 [2:28:54<2:18:35,  1.66s/it] 49%|████▉     | 4821/9822 [2:28:55<2:18:34,  1.66s/it] 49%|████▉     | 4822/9822 [2:28:57<2:18:43,  1.66s/it] 49%|████▉     | 4823/9822 [2:28:59<2:18:47,  1.67s/it] 49%|████▉     | 4824/9822 [2:29:00<2:18:33,  1.66s/it] 49%|████▉     | 4825/9822 [2:29:02<2:18:35,  1.66s/it] 49%|████▉     | 4826/9822 [2:29:04<2:18:29,  1.66s/it] 49%|████▉     | 4827/9822 [2:29:05<2:18:20,  1.66s/it] 49%|████▉     | 4828/9822 [2:29:07<2:18:28,  1.66s/it] 49%|████▉     | 4829/9822 [2:29:09<2:18:30,  1.66s/it] 49%|████▉     | 4830/9822 [2:29:10<2:18:22,  1.66s/it] 49%|████▉     | 4831/9822 [2:29:12<2:18:19,  1.66s/it] 49%|████▉     | 4832/9822 [2:29:14<2:18:18,  1.66s/it] 49%|████▉     | 4833/9822 [2:29:15<2:18:15,  1.66s/it] 49%|████▉     | 4834/9822 [2:29:17<2:20:59,  1.70s/it] 49%|████▉     | 4835/9822 [2:29:19<2:20:05,  1.69s/it] 49%|████▉     | 4836/9822 [2:29:20<2:19:20,  1.68s/it] 49%|████▉     | 4837/9822 [2:29:22<2:18:47,  1.67s/it] 49%|████▉     | 4838/9822 [2:29:24<2:18:41,  1.67s/it] 49%|████▉     | 4839/9822 [2:29:25<2:18:22,  1.67s/it] 49%|████▉     | 4840/9822 [2:29:27<2:18:11,  1.66s/it] 49%|████▉     | 4841/9822 [2:29:29<2:18:01,  1.66s/it] 49%|████▉     | 4842/9822 [2:29:30<2:17:50,  1.66s/it] 49%|████▉     | 4843/9822 [2:29:32<2:17:44,  1.66s/it] 49%|████▉     | 4844/9822 [2:29:34<2:17:45,  1.66s/it] 49%|████▉     | 4845/9822 [2:29:35<2:17:59,  1.66s/it] 49%|████▉     | 4846/9822 [2:29:37<2:18:01,  1.66s/it] 49%|████▉     | 4847/9822 [2:29:39<2:18:06,  1.67s/it] 49%|████▉     | 4848/9822 [2:29:40<2:17:50,  1.66s/it] 49%|████▉     | 4849/9822 [2:29:42<2:18:08,  1.67s/it] 49%|████▉     | 4850/9822 [2:29:44<2:17:58,  1.67s/it] 49%|████▉     | 4851/9822 [2:29:45<2:18:01,  1.67s/it] 49%|████▉     | 4852/9822 [2:29:47<2:17:57,  1.67s/it] 49%|████▉     | 4853/9822 [2:29:49<2:17:42,  1.66s/it] 49%|████▉     | 4854/9822 [2:29:50<2:17:30,  1.66s/it] 49%|████▉     | 4855/9822 [2:29:52<2:17:45,  1.66s/it] 49%|████▉     | 4856/9822 [2:29:54<2:18:08,  1.67s/it] 49%|████▉     | 4857/9822 [2:29:55<2:17:51,  1.67s/it] 49%|████▉     | 4858/9822 [2:29:57<2:17:57,  1.67s/it] 49%|████▉     | 4859/9822 [2:29:59<2:17:57,  1.67s/it] 49%|████▉     | 4860/9822 [2:30:00<2:17:45,  1.67s/it] 49%|████▉     | 4861/9822 [2:30:02<2:17:47,  1.67s/it] 50%|████▉     | 4862/9822 [2:30:04<2:17:46,  1.67s/it] 50%|████▉     | 4863/9822 [2:30:05<2:17:41,  1.67s/it] 50%|████▉     | 4864/9822 [2:30:07<2:17:35,  1.67s/it] 50%|████▉     | 4865/9822 [2:30:09<2:17:44,  1.67s/it] 50%|████▉     | 4866/9822 [2:30:10<2:17:33,  1.67s/it] 50%|████▉     | 4867/9822 [2:30:12<2:17:26,  1.66s/it] 50%|████▉     | 4868/9822 [2:30:14<2:17:27,  1.66s/it] 50%|████▉     | 4869/9822 [2:30:15<2:17:25,  1.66s/it] 50%|████▉     | 4870/9822 [2:30:17<2:17:26,  1.67s/it] 50%|████▉     | 4871/9822 [2:30:19<2:17:40,  1.67s/it] 50%|████▉     | 4872/9822 [2:30:20<2:17:27,  1.67s/it] 50%|████▉     | 4873/9822 [2:30:22<2:17:15,  1.66s/it] 50%|████▉     | 4874/9822 [2:30:24<2:19:37,  1.69s/it] 50%|████▉     | 4875/9822 [2:30:25<2:18:50,  1.68s/it] 50%|████▉     | 4876/9822 [2:30:27<2:18:20,  1.68s/it] 50%|████▉     | 4877/9822 [2:30:29<2:18:12,  1.68s/it] 50%|████▉     | 4878/9822 [2:30:30<2:17:54,  1.67s/it] 50%|████▉     | 4879/9822 [2:30:32<2:17:26,  1.67s/it] 50%|████▉     | 4880/9822 [2:30:34<2:17:31,  1.67s/it] 50%|████▉     | 4881/9822 [2:30:35<2:17:20,  1.67s/it] 50%|████▉     | 4882/9822 [2:30:37<2:17:14,  1.67s/it] 50%|████▉     | 4883/9822 [2:30:39<2:17:23,  1.67s/it] 50%|████▉     | 4884/9822 [2:30:40<2:17:06,  1.67s/it] 50%|████▉     | 4885/9822 [2:30:42<2:16:50,  1.66s/it] 50%|████▉     | 4886/9822 [2:30:44<2:16:50,  1.66s/it] 50%|████▉     | 4887/9822 [2:30:45<2:16:41,  1.66s/it] 50%|████▉     | 4888/9822 [2:30:47<2:16:44,  1.66s/it] 50%|████▉     | 4889/9822 [2:30:49<2:16:45,  1.66s/it] 50%|████▉     | 4890/9822 [2:30:50<2:16:41,  1.66s/it] 50%|████▉     | 4891/9822 [2:30:52<2:16:54,  1.67s/it] 50%|████▉     | 4892/9822 [2:30:54<2:16:44,  1.66s/it] 50%|████▉     | 4893/9822 [2:30:55<2:16:38,  1.66s/it] 50%|████▉     | 4894/9822 [2:30:57<2:17:16,  1.67s/it] 50%|████▉     | 4895/9822 [2:30:59<2:17:09,  1.67s/it] 50%|████▉     | 4896/9822 [2:31:00<2:16:58,  1.67s/it] 50%|████▉     | 4897/9822 [2:31:02<2:16:50,  1.67s/it] 50%|████▉     | 4898/9822 [2:31:04<2:16:38,  1.66s/it] 50%|████▉     | 4899/9822 [2:31:05<2:16:34,  1.66s/it] 50%|████▉     | 4900/9822 [2:31:07<2:16:47,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:11:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:11:54 - INFO - __main__ - ***** test Results*****
04/29/2024 14:11:54 - INFO - __main__ -   Training step = 4900
04/29/2024 14:11:54 - INFO - __main__ -  test_accuracy:0.8693265007320644 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:11:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:11:58 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:11:58 - INFO - __main__ -   Training step = 4900
04/29/2024 14:11:58 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:12:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:12:07 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:12:07 - INFO - __main__ -   Training step = 4900
04/29/2024 14:12:07 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 50%|████▉     | 4901/9822 [2:31:26<9:31:03,  6.96s/it] 50%|████▉     | 4902/9822 [2:31:28<7:19:20,  5.36s/it] 50%|████▉     | 4903/9822 [2:31:30<5:48:35,  4.25s/it] 50%|████▉     | 4904/9822 [2:31:31<4:44:59,  3.48s/it] 50%|████▉     | 4905/9822 [2:31:33<4:00:24,  2.93s/it] 50%|████▉     | 4906/9822 [2:31:35<3:29:03,  2.55s/it] 50%|████▉     | 4907/9822 [2:31:36<3:07:18,  2.29s/it] 50%|████▉     | 4908/9822 [2:31:38<2:51:52,  2.10s/it] 50%|████▉     | 4909/9822 [2:31:40<2:41:17,  1.97s/it] 50%|████▉     | 4910/9822 [2:31:41<2:33:38,  1.88s/it] 50%|█████     | 4911/9822 [2:31:43<2:28:37,  1.82s/it] 50%|█████     | 4912/9822 [2:31:45<2:24:47,  1.77s/it] 50%|█████     | 4913/9822 [2:31:46<2:22:17,  1.74s/it] 50%|█████     | 4914/9822 [2:31:48<2:20:24,  1.72s/it] 50%|█████     | 4915/9822 [2:31:50<2:19:17,  1.70s/it] 50%|█████     | 4916/9822 [2:31:51<2:18:22,  1.69s/it] 50%|█████     | 4917/9822 [2:31:53<2:17:42,  1.68s/it] 50%|█████     | 4918/9822 [2:31:55<2:17:01,  1.68s/it] 50%|█████     | 4919/9822 [2:31:56<2:16:49,  1.67s/it] 50%|█████     | 4920/9822 [2:31:58<2:16:41,  1.67s/it] 50%|█████     | 4921/9822 [2:32:00<2:16:22,  1.67s/it] 50%|█████     | 4922/9822 [2:32:01<2:16:14,  1.67s/it] 50%|█████     | 4923/9822 [2:32:03<2:16:17,  1.67s/it] 50%|█████     | 4924/9822 [2:32:05<2:16:04,  1.67s/it] 50%|█████     | 4925/9822 [2:32:06<2:18:40,  1.70s/it] 50%|█████     | 4926/9822 [2:32:08<2:17:38,  1.69s/it] 50%|█████     | 4927/9822 [2:32:10<2:17:03,  1.68s/it] 50%|█████     | 4928/9822 [2:32:11<2:16:39,  1.68s/it] 50%|█████     | 4929/9822 [2:32:13<2:16:16,  1.67s/it] 50%|█████     | 4930/9822 [2:32:15<2:15:57,  1.67s/it] 50%|█████     | 4931/9822 [2:32:16<2:15:54,  1.67s/it] 50%|█████     | 4932/9822 [2:32:18<2:15:40,  1.66s/it] 50%|█████     | 4933/9822 [2:32:20<2:15:42,  1.67s/it] 50%|█████     | 4934/9822 [2:32:21<2:15:38,  1.67s/it] 50%|█████     | 4935/9822 [2:32:23<2:15:31,  1.66s/it] 50%|█████     | 4936/9822 [2:32:25<2:15:23,  1.66s/it] 50%|█████     | 4937/9822 [2:32:26<2:15:36,  1.67s/it] 50%|█████     | 4938/9822 [2:32:28<2:15:41,  1.67s/it] 50%|█████     | 4939/9822 [2:32:30<2:15:37,  1.67s/it] 50%|█████     | 4940/9822 [2:32:31<2:15:37,  1.67s/it] 50%|█████     | 4941/9822 [2:32:33<2:15:25,  1.66s/it] 50%|█████     | 4942/9822 [2:32:35<2:15:18,  1.66s/it] 50%|█████     | 4943/9822 [2:32:36<2:15:19,  1.66s/it] 50%|█████     | 4944/9822 [2:32:38<2:15:15,  1.66s/it] 50%|█████     | 4945/9822 [2:32:40<2:15:23,  1.67s/it] 50%|█████     | 4946/9822 [2:32:41<2:15:13,  1.66s/it] 50%|█████     | 4947/9822 [2:32:43<2:15:25,  1.67s/it] 50%|█████     | 4948/9822 [2:32:45<2:15:08,  1.66s/it] 50%|█████     | 4949/9822 [2:32:46<2:15:16,  1.67s/it] 50%|█████     | 4950/9822 [2:32:48<2:15:24,  1.67s/it] 50%|█████     | 4951/9822 [2:32:50<2:18:00,  1.70s/it] 50%|█████     | 4952/9822 [2:32:51<2:17:07,  1.69s/it] 50%|█████     | 4953/9822 [2:32:53<2:16:32,  1.68s/it] 50%|█████     | 4954/9822 [2:32:55<2:16:17,  1.68s/it] 50%|█████     | 4955/9822 [2:32:56<2:16:11,  1.68s/it] 50%|█████     | 4956/9822 [2:32:58<2:16:09,  1.68s/it] 50%|█████     | 4957/9822 [2:33:00<2:15:36,  1.67s/it] 50%|█████     | 4958/9822 [2:33:01<2:15:43,  1.67s/it] 50%|█████     | 4959/9822 [2:33:03<2:15:23,  1.67s/it] 50%|█████     | 4960/9822 [2:33:05<2:15:05,  1.67s/it] 51%|█████     | 4961/9822 [2:33:06<2:14:59,  1.67s/it] 51%|█████     | 4962/9822 [2:33:08<2:15:10,  1.67s/it] 51%|█████     | 4963/9822 [2:33:10<2:15:00,  1.67s/it] 51%|█████     | 4964/9822 [2:33:11<2:15:07,  1.67s/it] 51%|█████     | 4965/9822 [2:33:13<2:14:53,  1.67s/it] 51%|█████     | 4966/9822 [2:33:15<2:14:37,  1.66s/it] 51%|█████     | 4967/9822 [2:33:16<2:14:37,  1.66s/it] 51%|█████     | 4968/9822 [2:33:18<2:14:35,  1.66s/it] 51%|█████     | 4969/9822 [2:33:20<2:14:30,  1.66s/it] 51%|█████     | 4970/9822 [2:33:21<2:14:20,  1.66s/it] 51%|█████     | 4971/9822 [2:33:23<2:14:16,  1.66s/it] 51%|█████     | 4972/9822 [2:33:25<2:14:20,  1.66s/it] 51%|█████     | 4973/9822 [2:33:26<2:14:20,  1.66s/it] 51%|█████     | 4974/9822 [2:33:28<2:14:33,  1.67s/it] 51%|█████     | 4975/9822 [2:33:30<2:14:37,  1.67s/it] 51%|█████     | 4976/9822 [2:33:31<2:14:43,  1.67s/it] 51%|█████     | 4977/9822 [2:33:33<2:14:52,  1.67s/it] 51%|█████     | 4978/9822 [2:33:35<2:15:11,  1.67s/it] 51%|█████     | 4979/9822 [2:33:36<2:15:39,  1.68s/it] 51%|█████     | 4980/9822 [2:33:38<2:15:48,  1.68s/it] 51%|█████     | 4981/9822 [2:33:40<2:15:24,  1.68s/it] 51%|█████     | 4982/9822 [2:33:41<2:15:08,  1.68s/it] 51%|█████     | 4983/9822 [2:33:43<2:14:52,  1.67s/it] 51%|█████     | 4984/9822 [2:33:45<2:17:11,  1.70s/it] 51%|█████     | 4985/9822 [2:33:47<2:16:08,  1.69s/it] 51%|█████     | 4986/9822 [2:33:48<2:15:21,  1.68s/it] 51%|█████     | 4987/9822 [2:33:50<2:14:58,  1.67s/it] 51%|█████     | 4988/9822 [2:33:51<2:13:37,  1.66s/it] 51%|█████     | 4989/9822 [2:33:53<2:13:44,  1.66s/it] 51%|█████     | 4990/9822 [2:33:55<2:13:44,  1.66s/it] 51%|█████     | 4991/9822 [2:33:56<2:13:43,  1.66s/it] 51%|█████     | 4992/9822 [2:33:58<2:13:38,  1.66s/it] 51%|█████     | 4993/9822 [2:34:00<2:13:39,  1.66s/it] 51%|█████     | 4994/9822 [2:34:01<2:13:54,  1.66s/it] 51%|█████     | 4995/9822 [2:34:03<2:13:48,  1.66s/it] 51%|█████     | 4996/9822 [2:34:05<2:13:38,  1.66s/it] 51%|█████     | 4997/9822 [2:34:06<2:13:41,  1.66s/it] 51%|█████     | 4998/9822 [2:34:08<2:13:41,  1.66s/it] 51%|█████     | 4999/9822 [2:34:10<2:13:42,  1.66s/it] 51%|█████     | 5000/9822 [2:34:11<2:14:00,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:14:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:14:58 - INFO - __main__ - ***** test Results*****
04/29/2024 14:14:58 - INFO - __main__ -   Training step = 5000
04/29/2024 14:14:58 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:15:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:15:03 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:15:03 - INFO - __main__ -   Training step = 5000
04/29/2024 14:15:03 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:15:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:15:11 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:15:11 - INFO - __main__ -   Training step = 5000
04/29/2024 14:15:11 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 51%|█████     | 5001/9822 [2:34:31<9:17:03,  6.93s/it] 51%|█████     | 5002/9822 [2:34:32<7:09:50,  5.35s/it] 51%|█████     | 5003/9822 [2:34:34<5:40:43,  4.24s/it] 51%|█████     | 5004/9822 [2:34:36<4:38:44,  3.47s/it] 51%|█████     | 5005/9822 [2:34:37<3:55:01,  2.93s/it] 51%|█████     | 5006/9822 [2:34:39<3:24:27,  2.55s/it] 51%|█████     | 5007/9822 [2:34:41<3:03:11,  2.28s/it] 51%|█████     | 5008/9822 [2:34:42<2:48:10,  2.10s/it] 51%|█████     | 5009/9822 [2:34:44<2:37:48,  1.97s/it] 51%|█████     | 5010/9822 [2:34:46<2:30:32,  1.88s/it] 51%|█████     | 5011/9822 [2:34:47<2:27:51,  1.84s/it] 51%|█████     | 5012/9822 [2:34:49<2:23:16,  1.79s/it] 51%|█████     | 5013/9822 [2:34:51<2:20:14,  1.75s/it] 51%|█████     | 5014/9822 [2:34:52<2:18:01,  1.72s/it] 51%|█████     | 5015/9822 [2:34:54<2:16:31,  1.70s/it] 51%|█████     | 5016/9822 [2:34:56<2:15:21,  1.69s/it] 51%|█████     | 5017/9822 [2:34:57<2:14:40,  1.68s/it] 51%|█████     | 5018/9822 [2:34:59<2:14:00,  1.67s/it] 51%|█████     | 5019/9822 [2:35:01<2:13:36,  1.67s/it] 51%|█████     | 5020/9822 [2:35:02<2:13:16,  1.67s/it] 51%|█████     | 5021/9822 [2:35:04<2:13:07,  1.66s/it] 51%|█████     | 5022/9822 [2:35:06<2:13:10,  1.66s/it] 51%|█████     | 5023/9822 [2:35:07<2:13:05,  1.66s/it] 51%|█████     | 5024/9822 [2:35:09<2:12:57,  1.66s/it] 51%|█████     | 5025/9822 [2:35:11<2:12:56,  1.66s/it] 51%|█████     | 5026/9822 [2:35:12<2:12:49,  1.66s/it] 51%|█████     | 5027/9822 [2:35:14<2:13:02,  1.66s/it] 51%|█████     | 5028/9822 [2:35:16<2:13:08,  1.67s/it] 51%|█████     | 5029/9822 [2:35:17<2:13:04,  1.67s/it] 51%|█████     | 5030/9822 [2:35:19<2:12:52,  1.66s/it] 51%|█████     | 5031/9822 [2:35:21<2:12:43,  1.66s/it] 51%|█████     | 5032/9822 [2:35:22<2:12:48,  1.66s/it] 51%|█████     | 5033/9822 [2:35:24<2:12:38,  1.66s/it] 51%|█████▏    | 5034/9822 [2:35:26<2:12:39,  1.66s/it] 51%|█████▏    | 5035/9822 [2:35:27<2:12:35,  1.66s/it] 51%|█████▏    | 5036/9822 [2:35:29<2:12:41,  1.66s/it] 51%|█████▏    | 5037/9822 [2:35:31<2:12:36,  1.66s/it] 51%|█████▏    | 5038/9822 [2:35:32<2:15:08,  1.70s/it] 51%|█████▏    | 5039/9822 [2:35:34<2:14:14,  1.68s/it] 51%|█████▏    | 5040/9822 [2:35:36<2:13:43,  1.68s/it] 51%|█████▏    | 5041/9822 [2:35:37<2:13:22,  1.67s/it] 51%|█████▏    | 5042/9822 [2:35:39<2:13:00,  1.67s/it] 51%|█████▏    | 5043/9822 [2:35:41<2:12:48,  1.67s/it] 51%|█████▏    | 5044/9822 [2:35:42<2:12:48,  1.67s/it] 51%|█████▏    | 5045/9822 [2:35:44<2:12:42,  1.67s/it] 51%|█████▏    | 5046/9822 [2:35:46<2:12:34,  1.67s/it] 51%|█████▏    | 5047/9822 [2:35:47<2:12:30,  1.67s/it] 51%|█████▏    | 5048/9822 [2:35:49<2:12:20,  1.66s/it] 51%|█████▏    | 5049/9822 [2:35:51<2:12:14,  1.66s/it] 51%|█████▏    | 5050/9822 [2:35:52<2:12:16,  1.66s/it] 51%|█████▏    | 5051/9822 [2:35:54<2:12:27,  1.67s/it] 51%|█████▏    | 5052/9822 [2:35:56<2:12:24,  1.67s/it] 51%|█████▏    | 5053/9822 [2:35:57<2:12:30,  1.67s/it] 51%|█████▏    | 5054/9822 [2:35:59<2:12:23,  1.67s/it] 51%|█████▏    | 5055/9822 [2:36:01<2:12:20,  1.67s/it] 51%|█████▏    | 5056/9822 [2:36:02<2:12:23,  1.67s/it] 51%|█████▏    | 5057/9822 [2:36:04<2:12:06,  1.66s/it] 51%|█████▏    | 5058/9822 [2:36:06<2:12:10,  1.66s/it] 52%|█████▏    | 5059/9822 [2:36:07<2:12:08,  1.66s/it] 52%|█████▏    | 5060/9822 [2:36:09<2:12:03,  1.66s/it] 52%|█████▏    | 5061/9822 [2:36:11<2:11:58,  1.66s/it] 52%|█████▏    | 5062/9822 [2:36:12<2:12:04,  1.66s/it] 52%|█████▏    | 5063/9822 [2:36:14<2:11:49,  1.66s/it] 52%|█████▏    | 5064/9822 [2:36:16<2:11:59,  1.66s/it] 52%|█████▏    | 5065/9822 [2:36:17<2:14:29,  1.70s/it] 52%|█████▏    | 5066/9822 [2:36:19<2:13:35,  1.69s/it] 52%|█████▏    | 5067/9822 [2:36:21<2:12:52,  1.68s/it] 52%|█████▏    | 5068/9822 [2:36:22<2:12:31,  1.67s/it] 52%|█████▏    | 5069/9822 [2:36:24<2:12:08,  1.67s/it] 52%|█████▏    | 5070/9822 [2:36:26<2:12:00,  1.67s/it] 52%|█████▏    | 5071/9822 [2:36:27<2:11:49,  1.66s/it] 52%|█████▏    | 5072/9822 [2:36:29<2:11:32,  1.66s/it] 52%|█████▏    | 5073/9822 [2:36:31<2:11:27,  1.66s/it] 52%|█████▏    | 5074/9822 [2:36:32<2:10:26,  1.65s/it] 52%|█████▏    | 5075/9822 [2:36:34<2:10:46,  1.65s/it] 52%|█████▏    | 5076/9822 [2:36:36<2:11:06,  1.66s/it] 52%|█████▏    | 5077/9822 [2:36:37<2:11:16,  1.66s/it] 52%|█████▏    | 5078/9822 [2:36:39<2:11:19,  1.66s/it] 52%|█████▏    | 5079/9822 [2:36:41<2:11:10,  1.66s/it] 52%|█████▏    | 5080/9822 [2:36:42<2:11:16,  1.66s/it] 52%|█████▏    | 5081/9822 [2:36:44<2:11:09,  1.66s/it] 52%|█████▏    | 5082/9822 [2:36:46<2:11:06,  1.66s/it] 52%|█████▏    | 5083/9822 [2:36:47<2:11:05,  1.66s/it] 52%|█████▏    | 5084/9822 [2:36:49<2:11:01,  1.66s/it] 52%|█████▏    | 5085/9822 [2:36:51<2:10:53,  1.66s/it] 52%|█████▏    | 5086/9822 [2:36:52<2:11:01,  1.66s/it] 52%|█████▏    | 5087/9822 [2:36:54<2:10:53,  1.66s/it] 52%|█████▏    | 5088/9822 [2:36:56<2:11:02,  1.66s/it] 52%|█████▏    | 5089/9822 [2:36:57<2:11:10,  1.66s/it] 52%|█████▏    | 5090/9822 [2:36:59<2:11:02,  1.66s/it] 52%|█████▏    | 5091/9822 [2:37:01<2:11:00,  1.66s/it] 52%|█████▏    | 5092/9822 [2:37:02<2:11:13,  1.66s/it] 52%|█████▏    | 5093/9822 [2:37:04<2:11:18,  1.67s/it] 52%|█████▏    | 5094/9822 [2:37:06<2:11:01,  1.66s/it] 52%|█████▏    | 5095/9822 [2:37:07<2:11:04,  1.66s/it] 52%|█████▏    | 5096/9822 [2:37:09<2:11:05,  1.66s/it] 52%|█████▏    | 5097/9822 [2:37:11<2:11:05,  1.66s/it] 52%|█████▏    | 5098/9822 [2:37:12<2:13:34,  1.70s/it] 52%|█████▏    | 5099/9822 [2:37:14<2:12:34,  1.68s/it] 52%|█████▏    | 5100/9822 [2:37:16<2:12:03,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:18:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:18:02 - INFO - __main__ - ***** test Results*****
04/29/2024 14:18:02 - INFO - __main__ -   Training step = 5100
04/29/2024 14:18:02 - INFO - __main__ -  test_accuracy:0.8671303074670571 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:18:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:18:07 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:18:07 - INFO - __main__ -   Training step = 5100
04/29/2024 14:18:07 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:18:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:18:15 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:18:15 - INFO - __main__ -   Training step = 5100
04/29/2024 14:18:15 - INFO - __main__ -  eval_accuracy:0.9154155986818016 
 52%|█████▏    | 5101/9822 [2:37:35<9:06:31,  6.95s/it] 52%|█████▏    | 5102/9822 [2:37:37<7:01:49,  5.36s/it] 52%|█████▏    | 5103/9822 [2:37:38<5:34:45,  4.26s/it] 52%|█████▏    | 5104/9822 [2:37:40<4:33:32,  3.48s/it] 52%|█████▏    | 5105/9822 [2:37:42<3:51:02,  2.94s/it] 52%|█████▏    | 5106/9822 [2:37:43<3:20:56,  2.56s/it] 52%|█████▏    | 5107/9822 [2:37:45<3:00:20,  2.29s/it] 52%|█████▏    | 5108/9822 [2:37:47<2:45:39,  2.11s/it] 52%|█████▏    | 5109/9822 [2:37:48<2:35:17,  1.98s/it] 52%|█████▏    | 5110/9822 [2:37:50<2:28:04,  1.89s/it] 52%|█████▏    | 5111/9822 [2:37:52<2:23:06,  1.82s/it] 52%|█████▏    | 5112/9822 [2:37:53<2:19:16,  1.77s/it] 52%|█████▏    | 5113/9822 [2:37:55<2:16:36,  1.74s/it] 52%|█████▏    | 5114/9822 [2:37:57<2:14:50,  1.72s/it] 52%|█████▏    | 5115/9822 [2:37:58<2:13:39,  1.70s/it] 52%|█████▏    | 5116/9822 [2:38:00<2:12:50,  1.69s/it] 52%|█████▏    | 5117/9822 [2:38:02<2:12:16,  1.69s/it] 52%|█████▏    | 5118/9822 [2:38:03<2:11:51,  1.68s/it] 52%|█████▏    | 5119/9822 [2:38:05<2:11:25,  1.68s/it] 52%|█████▏    | 5120/9822 [2:38:07<2:11:08,  1.67s/it] 52%|█████▏    | 5121/9822 [2:38:08<2:10:50,  1.67s/it] 52%|█████▏    | 5122/9822 [2:38:10<2:10:50,  1.67s/it] 52%|█████▏    | 5123/9822 [2:38:12<2:10:48,  1.67s/it] 52%|█████▏    | 5124/9822 [2:38:13<2:10:41,  1.67s/it] 52%|█████▏    | 5125/9822 [2:38:15<2:10:20,  1.66s/it] 52%|█████▏    | 5126/9822 [2:38:17<2:10:24,  1.67s/it] 52%|█████▏    | 5127/9822 [2:38:18<2:10:15,  1.66s/it] 52%|█████▏    | 5128/9822 [2:38:20<2:10:10,  1.66s/it] 52%|█████▏    | 5129/9822 [2:38:22<2:10:22,  1.67s/it] 52%|█████▏    | 5130/9822 [2:38:23<2:10:06,  1.66s/it] 52%|█████▏    | 5131/9822 [2:38:25<2:10:00,  1.66s/it] 52%|█████▏    | 5132/9822 [2:38:27<2:09:57,  1.66s/it] 52%|█████▏    | 5133/9822 [2:38:28<2:10:03,  1.66s/it] 52%|█████▏    | 5134/9822 [2:38:30<2:12:20,  1.69s/it] 52%|█████▏    | 5135/9822 [2:38:32<2:11:41,  1.69s/it] 52%|█████▏    | 5136/9822 [2:38:33<2:11:02,  1.68s/it] 52%|█████▏    | 5137/9822 [2:38:35<2:10:30,  1.67s/it] 52%|█████▏    | 5138/9822 [2:38:37<2:10:17,  1.67s/it] 52%|█████▏    | 5139/9822 [2:38:38<2:10:04,  1.67s/it] 52%|█████▏    | 5140/9822 [2:38:40<2:09:53,  1.66s/it] 52%|█████▏    | 5141/9822 [2:38:42<2:09:45,  1.66s/it] 52%|█████▏    | 5142/9822 [2:38:43<2:09:33,  1.66s/it] 52%|█████▏    | 5143/9822 [2:38:45<2:09:23,  1.66s/it] 52%|█████▏    | 5144/9822 [2:38:47<2:09:30,  1.66s/it] 52%|█████▏    | 5145/9822 [2:38:48<2:09:25,  1.66s/it] 52%|█████▏    | 5146/9822 [2:38:50<2:09:16,  1.66s/it] 52%|█████▏    | 5147/9822 [2:38:52<2:09:33,  1.66s/it] 52%|█████▏    | 5148/9822 [2:38:53<2:09:22,  1.66s/it] 52%|█████▏    | 5149/9822 [2:38:55<2:09:18,  1.66s/it] 52%|█████▏    | 5150/9822 [2:38:57<2:09:15,  1.66s/it] 52%|█████▏    | 5151/9822 [2:38:58<2:09:21,  1.66s/it] 52%|█████▏    | 5152/9822 [2:39:00<2:09:08,  1.66s/it] 52%|█████▏    | 5153/9822 [2:39:02<2:09:16,  1.66s/it] 52%|█████▏    | 5154/9822 [2:39:03<2:09:04,  1.66s/it] 52%|█████▏    | 5155/9822 [2:39:05<2:09:04,  1.66s/it] 52%|█████▏    | 5156/9822 [2:39:07<2:09:12,  1.66s/it] 53%|█████▎    | 5157/9822 [2:39:08<2:09:02,  1.66s/it] 53%|█████▎    | 5158/9822 [2:39:10<2:09:11,  1.66s/it] 53%|█████▎    | 5159/9822 [2:39:12<2:09:01,  1.66s/it] 53%|█████▎    | 5160/9822 [2:39:13<2:07:45,  1.64s/it] 53%|█████▎    | 5161/9822 [2:39:15<2:10:39,  1.68s/it] 53%|█████▎    | 5162/9822 [2:39:17<2:10:02,  1.67s/it] 53%|█████▎    | 5163/9822 [2:39:18<2:09:36,  1.67s/it] 53%|█████▎    | 5164/9822 [2:39:20<2:09:13,  1.66s/it] 53%|█████▎    | 5165/9822 [2:39:22<2:09:06,  1.66s/it] 53%|█████▎    | 5166/9822 [2:39:23<2:09:00,  1.66s/it] 53%|█████▎    | 5167/9822 [2:39:25<2:08:54,  1.66s/it] 53%|█████▎    | 5168/9822 [2:39:26<2:08:46,  1.66s/it] 53%|█████▎    | 5169/9822 [2:39:28<2:08:55,  1.66s/it] 53%|█████▎    | 5170/9822 [2:39:30<2:08:50,  1.66s/it] 53%|█████▎    | 5171/9822 [2:39:31<2:08:42,  1.66s/it] 53%|█████▎    | 5172/9822 [2:39:33<2:08:42,  1.66s/it] 53%|█████▎    | 5173/9822 [2:39:35<2:08:46,  1.66s/it] 53%|█████▎    | 5174/9822 [2:39:36<2:08:47,  1.66s/it] 53%|█████▎    | 5175/9822 [2:39:38<2:08:42,  1.66s/it] 53%|█████▎    | 5176/9822 [2:39:40<2:08:37,  1.66s/it] 53%|█████▎    | 5177/9822 [2:39:41<2:08:40,  1.66s/it] 53%|█████▎    | 5178/9822 [2:39:43<2:08:29,  1.66s/it] 53%|█████▎    | 5179/9822 [2:39:45<2:08:26,  1.66s/it] 53%|█████▎    | 5180/9822 [2:39:46<2:08:18,  1.66s/it] 53%|█████▎    | 5181/9822 [2:39:48<2:08:22,  1.66s/it] 53%|█████▎    | 5182/9822 [2:39:50<2:08:21,  1.66s/it] 53%|█████▎    | 5183/9822 [2:39:51<2:08:15,  1.66s/it] 53%|█████▎    | 5184/9822 [2:39:53<2:08:28,  1.66s/it] 53%|█████▎    | 5185/9822 [2:39:55<2:08:27,  1.66s/it] 53%|█████▎    | 5186/9822 [2:39:56<2:08:22,  1.66s/it] 53%|█████▎    | 5187/9822 [2:39:58<2:08:10,  1.66s/it] 53%|█████▎    | 5188/9822 [2:40:00<2:10:38,  1.69s/it] 53%|█████▎    | 5189/9822 [2:40:01<2:09:52,  1.68s/it] 53%|█████▎    | 5190/9822 [2:40:03<2:09:22,  1.68s/it] 53%|█████▎    | 5191/9822 [2:40:05<2:09:02,  1.67s/it] 53%|█████▎    | 5192/9822 [2:40:06<2:08:58,  1.67s/it] 53%|█████▎    | 5193/9822 [2:40:08<2:08:48,  1.67s/it] 53%|█████▎    | 5194/9822 [2:40:10<2:08:39,  1.67s/it] 53%|█████▎    | 5195/9822 [2:40:11<2:08:28,  1.67s/it] 53%|█████▎    | 5196/9822 [2:40:13<2:08:30,  1.67s/it] 53%|█████▎    | 5197/9822 [2:40:15<2:08:31,  1.67s/it] 53%|█████▎    | 5198/9822 [2:40:16<2:08:20,  1.67s/it] 53%|█████▎    | 5199/9822 [2:40:18<2:08:10,  1.66s/it] 53%|█████▎    | 5200/9822 [2:40:20<2:08:08,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:21:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:21:07 - INFO - __main__ - ***** test Results*****
04/29/2024 14:21:07 - INFO - __main__ -   Training step = 5200
04/29/2024 14:21:07 - INFO - __main__ -  test_accuracy:0.8707906295754027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:21:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:21:11 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:21:11 - INFO - __main__ -   Training step = 5200
04/29/2024 14:21:11 - INFO - __main__ -  eval_accuracy:0.8495056755767119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:21:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:21:20 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:21:20 - INFO - __main__ -   Training step = 5200
04/29/2024 14:21:20 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 53%|█████▎    | 5201/9822 [2:40:39<8:53:54,  6.93s/it] 53%|█████▎    | 5202/9822 [2:40:41<6:52:14,  5.35s/it] 53%|█████▎    | 5203/9822 [2:40:42<5:26:55,  4.25s/it] 53%|█████▎    | 5204/9822 [2:40:44<4:26:59,  3.47s/it] 53%|█████▎    | 5205/9822 [2:40:46<3:45:12,  2.93s/it] 53%|█████▎    | 5206/9822 [2:40:47<3:15:53,  2.55s/it] 53%|█████▎    | 5207/9822 [2:40:49<2:55:32,  2.28s/it] 53%|█████▎    | 5208/9822 [2:40:51<2:41:22,  2.10s/it] 53%|█████▎    | 5209/9822 [2:40:52<2:31:22,  1.97s/it] 53%|█████▎    | 5210/9822 [2:40:54<2:24:19,  1.88s/it] 53%|█████▎    | 5211/9822 [2:40:56<2:19:21,  1.81s/it] 53%|█████▎    | 5212/9822 [2:40:57<2:15:43,  1.77s/it] 53%|█████▎    | 5213/9822 [2:40:59<2:13:23,  1.74s/it] 53%|█████▎    | 5214/9822 [2:41:01<2:11:54,  1.72s/it] 53%|█████▎    | 5215/9822 [2:41:02<2:10:31,  1.70s/it] 53%|█████▎    | 5216/9822 [2:41:04<2:09:39,  1.69s/it] 53%|█████▎    | 5217/9822 [2:41:06<2:11:28,  1.71s/it] 53%|█████▎    | 5218/9822 [2:41:07<2:10:07,  1.70s/it] 53%|█████▎    | 5219/9822 [2:41:09<2:09:17,  1.69s/it] 53%|█████▎    | 5220/9822 [2:41:11<2:08:36,  1.68s/it] 53%|█████▎    | 5221/9822 [2:41:12<2:08:24,  1.67s/it] 53%|█████▎    | 5222/9822 [2:41:14<2:08:04,  1.67s/it] 53%|█████▎    | 5223/9822 [2:41:16<2:07:41,  1.67s/it] 53%|█████▎    | 5224/9822 [2:41:17<2:07:32,  1.66s/it] 53%|█████▎    | 5225/9822 [2:41:19<2:07:25,  1.66s/it] 53%|█████▎    | 5226/9822 [2:41:21<2:07:21,  1.66s/it] 53%|█████▎    | 5227/9822 [2:41:22<2:07:18,  1.66s/it] 53%|█████▎    | 5228/9822 [2:41:24<2:07:31,  1.67s/it] 53%|█████▎    | 5229/9822 [2:41:26<2:07:36,  1.67s/it] 53%|█████▎    | 5230/9822 [2:41:27<2:07:29,  1.67s/it] 53%|█████▎    | 5231/9822 [2:41:29<2:07:33,  1.67s/it] 53%|█████▎    | 5232/9822 [2:41:31<2:07:22,  1.66s/it] 53%|█████▎    | 5233/9822 [2:41:32<2:07:19,  1.66s/it] 53%|█████▎    | 5234/9822 [2:41:34<2:07:20,  1.67s/it] 53%|█████▎    | 5235/9822 [2:41:36<2:07:05,  1.66s/it] 53%|█████▎    | 5236/9822 [2:41:37<2:07:09,  1.66s/it] 53%|█████▎    | 5237/9822 [2:41:39<2:07:06,  1.66s/it] 53%|█████▎    | 5238/9822 [2:41:41<2:07:01,  1.66s/it] 53%|█████▎    | 5239/9822 [2:41:42<2:06:57,  1.66s/it] 53%|█████▎    | 5240/9822 [2:41:44<2:06:57,  1.66s/it] 53%|█████▎    | 5241/9822 [2:41:46<2:07:14,  1.67s/it] 53%|█████▎    | 5242/9822 [2:41:47<2:07:45,  1.67s/it] 53%|█████▎    | 5243/9822 [2:41:49<2:08:02,  1.68s/it] 53%|█████▎    | 5244/9822 [2:41:51<2:08:17,  1.68s/it] 53%|█████▎    | 5245/9822 [2:41:52<2:08:24,  1.68s/it] 53%|█████▎    | 5246/9822 [2:41:54<2:07:17,  1.67s/it] 53%|█████▎    | 5247/9822 [2:41:56<2:07:46,  1.68s/it] 53%|█████▎    | 5248/9822 [2:41:57<2:07:56,  1.68s/it] 53%|█████▎    | 5249/9822 [2:41:59<2:08:03,  1.68s/it] 53%|█████▎    | 5250/9822 [2:42:01<2:10:25,  1.71s/it] 53%|█████▎    | 5251/9822 [2:42:03<2:09:49,  1.70s/it] 53%|█████▎    | 5252/9822 [2:42:04<2:09:20,  1.70s/it] 53%|█████▎    | 5253/9822 [2:42:06<2:08:59,  1.69s/it] 53%|█████▎    | 5254/9822 [2:42:08<2:08:49,  1.69s/it] 54%|█████▎    | 5255/9822 [2:42:09<2:08:26,  1.69s/it] 54%|█████▎    | 5256/9822 [2:42:11<2:08:24,  1.69s/it] 54%|█████▎    | 5257/9822 [2:42:13<2:08:23,  1.69s/it] 54%|█████▎    | 5258/9822 [2:42:14<2:08:13,  1.69s/it] 54%|█████▎    | 5259/9822 [2:42:16<2:07:28,  1.68s/it] 54%|█████▎    | 5260/9822 [2:42:18<2:07:09,  1.67s/it] 54%|█████▎    | 5261/9822 [2:42:19<2:06:49,  1.67s/it] 54%|█████▎    | 5262/9822 [2:42:21<2:06:34,  1.67s/it] 54%|█████▎    | 5263/9822 [2:42:23<2:06:35,  1.67s/it] 54%|█████▎    | 5264/9822 [2:42:24<2:06:18,  1.66s/it] 54%|█████▎    | 5265/9822 [2:42:26<2:06:09,  1.66s/it] 54%|█████▎    | 5266/9822 [2:42:28<2:06:06,  1.66s/it] 54%|█████▎    | 5267/9822 [2:42:29<2:06:02,  1.66s/it] 54%|█████▎    | 5268/9822 [2:42:31<2:06:00,  1.66s/it] 54%|█████▎    | 5269/9822 [2:42:33<2:06:00,  1.66s/it] 54%|█████▎    | 5270/9822 [2:42:34<2:06:02,  1.66s/it] 54%|█████▎    | 5271/9822 [2:42:36<2:05:55,  1.66s/it] 54%|█████▎    | 5272/9822 [2:42:38<2:08:18,  1.69s/it] 54%|█████▎    | 5273/9822 [2:42:39<2:07:26,  1.68s/it] 54%|█████▎    | 5274/9822 [2:42:41<2:06:53,  1.67s/it] 54%|█████▎    | 5275/9822 [2:42:43<2:06:32,  1.67s/it] 54%|█████▎    | 5276/9822 [2:42:44<2:06:11,  1.67s/it] 54%|█████▎    | 5277/9822 [2:42:46<2:06:03,  1.66s/it] 54%|█████▎    | 5278/9822 [2:42:48<2:06:00,  1.66s/it] 54%|█████▎    | 5279/9822 [2:42:49<2:05:50,  1.66s/it] 54%|█████▍    | 5280/9822 [2:42:51<2:05:43,  1.66s/it] 54%|█████▍    | 5281/9822 [2:42:53<2:05:39,  1.66s/it] 54%|█████▍    | 5282/9822 [2:42:54<2:05:36,  1.66s/it] 54%|█████▍    | 5283/9822 [2:42:56<2:05:37,  1.66s/it] 54%|█████▍    | 5284/9822 [2:42:58<2:05:41,  1.66s/it] 54%|█████▍    | 5285/9822 [2:42:59<2:05:32,  1.66s/it] 54%|█████▍    | 5286/9822 [2:43:01<2:05:24,  1.66s/it] 54%|█████▍    | 5287/9822 [2:43:03<2:05:27,  1.66s/it] 54%|█████▍    | 5288/9822 [2:43:04<2:05:32,  1.66s/it] 54%|█████▍    | 5289/9822 [2:43:06<2:05:21,  1.66s/it] 54%|█████▍    | 5290/9822 [2:43:08<2:05:23,  1.66s/it] 54%|█████▍    | 5291/9822 [2:43:09<2:05:27,  1.66s/it] 54%|█████▍    | 5292/9822 [2:43:11<2:05:24,  1.66s/it] 54%|█████▍    | 5293/9822 [2:43:13<2:05:25,  1.66s/it] 54%|█████▍    | 5294/9822 [2:43:14<2:05:27,  1.66s/it] 54%|█████▍    | 5295/9822 [2:43:16<2:05:16,  1.66s/it] 54%|█████▍    | 5296/9822 [2:43:18<2:05:09,  1.66s/it] 54%|█████▍    | 5297/9822 [2:43:19<2:05:13,  1.66s/it] 54%|█████▍    | 5298/9822 [2:43:21<2:05:10,  1.66s/it] 54%|█████▍    | 5299/9822 [2:43:23<2:07:34,  1.69s/it] 54%|█████▍    | 5300/9822 [2:43:24<2:06:43,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:24:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:24:11 - INFO - __main__ - ***** test Results*****
04/29/2024 14:24:11 - INFO - __main__ -   Training step = 5300
04/29/2024 14:24:11 - INFO - __main__ -  test_accuracy:0.8693265007320644 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:24:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:24:16 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:24:16 - INFO - __main__ -   Training step = 5300
04/29/2024 14:24:16 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:24:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:24:24 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:24:24 - INFO - __main__ -   Training step = 5300
04/29/2024 14:24:24 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 54%|█████▍    | 5301/9822 [2:43:43<8:43:04,  6.94s/it] 54%|█████▍    | 5302/9822 [2:43:45<6:43:43,  5.36s/it] 54%|█████▍    | 5303/9822 [2:43:47<5:20:17,  4.25s/it] 54%|█████▍    | 5304/9822 [2:43:49<4:21:46,  3.48s/it] 54%|█████▍    | 5305/9822 [2:43:50<3:40:38,  2.93s/it] 54%|█████▍    | 5306/9822 [2:43:52<3:11:57,  2.55s/it] 54%|█████▍    | 5307/9822 [2:43:53<2:51:58,  2.29s/it] 54%|█████▍    | 5308/9822 [2:43:55<2:37:42,  2.10s/it] 54%|█████▍    | 5309/9822 [2:43:57<2:27:47,  1.96s/it] 54%|█████▍    | 5310/9822 [2:43:58<2:20:50,  1.87s/it] 54%|█████▍    | 5311/9822 [2:44:00<2:15:52,  1.81s/it] 54%|█████▍    | 5312/9822 [2:44:02<2:12:41,  1.77s/it] 54%|█████▍    | 5313/9822 [2:44:03<2:10:21,  1.73s/it] 54%|█████▍    | 5314/9822 [2:44:05<2:08:34,  1.71s/it] 54%|█████▍    | 5315/9822 [2:44:07<2:07:23,  1.70s/it] 54%|█████▍    | 5316/9822 [2:44:08<2:06:32,  1.69s/it] 54%|█████▍    | 5317/9822 [2:44:10<2:06:15,  1.68s/it] 54%|█████▍    | 5318/9822 [2:44:12<2:05:54,  1.68s/it] 54%|█████▍    | 5319/9822 [2:44:13<2:05:45,  1.68s/it] 54%|█████▍    | 5320/9822 [2:44:15<2:05:26,  1.67s/it] 54%|█████▍    | 5321/9822 [2:44:17<2:05:16,  1.67s/it] 54%|█████▍    | 5322/9822 [2:44:18<2:05:22,  1.67s/it] 54%|█████▍    | 5323/9822 [2:44:20<2:05:03,  1.67s/it] 54%|█████▍    | 5324/9822 [2:44:22<2:04:56,  1.67s/it] 54%|█████▍    | 5325/9822 [2:44:23<2:05:02,  1.67s/it] 54%|█████▍    | 5326/9822 [2:44:25<2:04:55,  1.67s/it] 54%|█████▍    | 5327/9822 [2:44:27<2:07:14,  1.70s/it] 54%|█████▍    | 5328/9822 [2:44:29<2:06:47,  1.69s/it] 54%|█████▍    | 5329/9822 [2:44:30<2:06:08,  1.68s/it] 54%|█████▍    | 5330/9822 [2:44:32<2:05:33,  1.68s/it] 54%|█████▍    | 5331/9822 [2:44:34<2:05:33,  1.68s/it] 54%|█████▍    | 5332/9822 [2:44:35<2:04:03,  1.66s/it] 54%|█████▍    | 5333/9822 [2:44:37<2:04:18,  1.66s/it] 54%|█████▍    | 5334/9822 [2:44:38<2:04:23,  1.66s/it] 54%|█████▍    | 5335/9822 [2:44:40<2:04:14,  1.66s/it] 54%|█████▍    | 5336/9822 [2:44:42<2:04:14,  1.66s/it] 54%|█████▍    | 5337/9822 [2:44:43<2:04:26,  1.66s/it] 54%|█████▍    | 5338/9822 [2:44:45<2:04:17,  1.66s/it] 54%|█████▍    | 5339/9822 [2:44:47<2:04:22,  1.66s/it] 54%|█████▍    | 5340/9822 [2:44:48<2:04:33,  1.67s/it] 54%|█████▍    | 5341/9822 [2:44:50<2:04:16,  1.66s/it] 54%|█████▍    | 5342/9822 [2:44:52<2:04:11,  1.66s/it] 54%|█████▍    | 5343/9822 [2:44:53<2:04:17,  1.67s/it] 54%|█████▍    | 5344/9822 [2:44:55<2:04:05,  1.66s/it] 54%|█████▍    | 5345/9822 [2:44:57<2:04:05,  1.66s/it] 54%|█████▍    | 5346/9822 [2:44:58<2:04:11,  1.66s/it] 54%|█████▍    | 5347/9822 [2:45:00<2:03:58,  1.66s/it] 54%|█████▍    | 5348/9822 [2:45:02<2:04:10,  1.67s/it] 54%|█████▍    | 5349/9822 [2:45:03<2:04:09,  1.67s/it] 54%|█████▍    | 5350/9822 [2:45:05<2:03:54,  1.66s/it] 54%|█████▍    | 5351/9822 [2:45:07<2:03:54,  1.66s/it] 54%|█████▍    | 5352/9822 [2:45:08<2:03:54,  1.66s/it] 55%|█████▍    | 5353/9822 [2:45:10<2:03:59,  1.66s/it] 55%|█████▍    | 5354/9822 [2:45:12<2:03:57,  1.66s/it] 55%|█████▍    | 5355/9822 [2:45:13<2:03:58,  1.67s/it] 55%|█████▍    | 5356/9822 [2:45:15<2:03:53,  1.66s/it] 55%|█████▍    | 5357/9822 [2:45:17<2:03:49,  1.66s/it] 55%|█████▍    | 5358/9822 [2:45:18<2:03:52,  1.66s/it] 55%|█████▍    | 5359/9822 [2:45:20<2:03:51,  1.67s/it] 55%|█████▍    | 5360/9822 [2:45:22<2:03:52,  1.67s/it] 55%|█████▍    | 5361/9822 [2:45:23<2:03:52,  1.67s/it] 55%|█████▍    | 5362/9822 [2:45:25<2:03:57,  1.67s/it] 55%|█████▍    | 5363/9822 [2:45:27<2:03:58,  1.67s/it] 55%|█████▍    | 5364/9822 [2:45:28<2:03:56,  1.67s/it] 55%|█████▍    | 5365/9822 [2:45:30<2:03:54,  1.67s/it] 55%|█████▍    | 5366/9822 [2:45:32<2:03:45,  1.67s/it] 55%|█████▍    | 5367/9822 [2:45:33<2:03:53,  1.67s/it] 55%|█████▍    | 5368/9822 [2:45:35<2:06:09,  1.70s/it] 55%|█████▍    | 5369/9822 [2:45:37<2:05:25,  1.69s/it] 55%|█████▍    | 5370/9822 [2:45:39<2:04:45,  1.68s/it] 55%|█████▍    | 5371/9822 [2:45:40<2:04:18,  1.68s/it] 55%|█████▍    | 5372/9822 [2:45:42<2:04:01,  1.67s/it] 55%|█████▍    | 5373/9822 [2:45:44<2:03:50,  1.67s/it] 55%|█████▍    | 5374/9822 [2:45:45<2:03:41,  1.67s/it] 55%|█████▍    | 5375/9822 [2:45:47<2:03:36,  1.67s/it] 55%|█████▍    | 5376/9822 [2:45:49<2:03:33,  1.67s/it] 55%|█████▍    | 5377/9822 [2:45:50<2:03:40,  1.67s/it] 55%|█████▍    | 5378/9822 [2:45:52<2:03:38,  1.67s/it] 55%|█████▍    | 5379/9822 [2:45:54<2:03:28,  1.67s/it] 55%|█████▍    | 5380/9822 [2:45:55<2:03:27,  1.67s/it] 55%|█████▍    | 5381/9822 [2:45:57<2:03:27,  1.67s/it] 55%|█████▍    | 5382/9822 [2:45:59<2:03:26,  1.67s/it] 55%|█████▍    | 5383/9822 [2:46:00<2:03:25,  1.67s/it] 55%|█████▍    | 5384/9822 [2:46:02<2:03:24,  1.67s/it] 55%|█████▍    | 5385/9822 [2:46:04<2:03:19,  1.67s/it] 55%|█████▍    | 5386/9822 [2:46:05<2:03:16,  1.67s/it] 55%|█████▍    | 5387/9822 [2:46:07<2:03:12,  1.67s/it] 55%|█████▍    | 5388/9822 [2:46:09<2:03:10,  1.67s/it] 55%|█████▍    | 5389/9822 [2:46:10<2:03:21,  1.67s/it] 55%|█████▍    | 5390/9822 [2:46:12<2:03:13,  1.67s/it] 55%|█████▍    | 5391/9822 [2:46:14<2:03:09,  1.67s/it] 55%|█████▍    | 5392/9822 [2:46:15<2:03:01,  1.67s/it] 55%|█████▍    | 5393/9822 [2:46:17<2:03:00,  1.67s/it] 55%|█████▍    | 5394/9822 [2:46:19<2:02:59,  1.67s/it] 55%|█████▍    | 5395/9822 [2:46:20<2:05:12,  1.70s/it] 55%|█████▍    | 5396/9822 [2:46:22<2:04:48,  1.69s/it] 55%|█████▍    | 5397/9822 [2:46:24<2:04:10,  1.68s/it] 55%|█████▍    | 5398/9822 [2:46:25<2:03:44,  1.68s/it] 55%|█████▍    | 5399/9822 [2:46:27<2:03:41,  1.68s/it] 55%|█████▍    | 5400/9822 [2:46:29<2:03:27,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:27:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:27:15 - INFO - __main__ - ***** test Results*****
04/29/2024 14:27:15 - INFO - __main__ -   Training step = 5400
04/29/2024 14:27:15 - INFO - __main__ -  test_accuracy:0.8685944363103953 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:27:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:27:20 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:27:20 - INFO - __main__ -   Training step = 5400
04/29/2024 14:27:20 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:27:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:27:28 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:27:28 - INFO - __main__ -   Training step = 5400
04/29/2024 14:27:28 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 55%|█████▍    | 5401/9822 [2:46:48<8:31:26,  6.94s/it] 55%|█████▍    | 5402/9822 [2:46:50<6:35:11,  5.36s/it] 55%|█████▌    | 5403/9822 [2:46:51<5:13:23,  4.26s/it] 55%|█████▌    | 5404/9822 [2:46:53<4:16:08,  3.48s/it] 55%|█████▌    | 5405/9822 [2:46:55<3:36:11,  2.94s/it] 55%|█████▌    | 5406/9822 [2:46:56<3:08:12,  2.56s/it] 55%|█████▌    | 5407/9822 [2:46:58<2:48:35,  2.29s/it] 55%|█████▌    | 5408/9822 [2:47:00<2:34:50,  2.10s/it] 55%|█████▌    | 5409/9822 [2:47:01<2:25:11,  1.97s/it] 55%|█████▌    | 5410/9822 [2:47:03<2:18:22,  1.88s/it] 55%|█████▌    | 5411/9822 [2:47:05<2:13:43,  1.82s/it] 55%|█████▌    | 5412/9822 [2:47:06<2:10:19,  1.77s/it] 55%|█████▌    | 5413/9822 [2:47:08<2:07:58,  1.74s/it] 55%|█████▌    | 5414/9822 [2:47:10<2:06:26,  1.72s/it] 55%|█████▌    | 5415/9822 [2:47:11<2:05:10,  1.70s/it] 55%|█████▌    | 5416/9822 [2:47:13<2:04:22,  1.69s/it] 55%|█████▌    | 5417/9822 [2:47:15<2:03:54,  1.69s/it] 55%|█████▌    | 5418/9822 [2:47:16<2:02:21,  1.67s/it] 55%|█████▌    | 5419/9822 [2:47:18<2:04:35,  1.70s/it] 55%|█████▌    | 5420/9822 [2:47:20<2:03:52,  1.69s/it] 55%|█████▌    | 5421/9822 [2:47:21<2:03:46,  1.69s/it] 55%|█████▌    | 5422/9822 [2:47:23<2:03:27,  1.68s/it] 55%|█████▌    | 5423/9822 [2:47:25<2:03:08,  1.68s/it] 55%|█████▌    | 5424/9822 [2:47:26<2:03:07,  1.68s/it] 55%|█████▌    | 5425/9822 [2:47:28<2:03:06,  1.68s/it] 55%|█████▌    | 5426/9822 [2:47:30<2:02:47,  1.68s/it] 55%|█████▌    | 5427/9822 [2:47:31<2:02:32,  1.67s/it] 55%|█████▌    | 5428/9822 [2:47:33<2:02:38,  1.67s/it] 55%|█████▌    | 5429/9822 [2:47:35<2:02:29,  1.67s/it] 55%|█████▌    | 5430/9822 [2:47:36<2:02:15,  1.67s/it] 55%|█████▌    | 5431/9822 [2:47:38<2:02:25,  1.67s/it] 55%|█████▌    | 5432/9822 [2:47:40<2:02:15,  1.67s/it] 55%|█████▌    | 5433/9822 [2:47:41<2:02:10,  1.67s/it] 55%|█████▌    | 5434/9822 [2:47:43<2:02:29,  1.68s/it] 55%|█████▌    | 5435/9822 [2:47:45<2:02:18,  1.67s/it] 55%|█████▌    | 5436/9822 [2:47:46<2:02:06,  1.67s/it] 55%|█████▌    | 5437/9822 [2:47:48<2:02:16,  1.67s/it] 55%|█████▌    | 5438/9822 [2:47:50<2:02:08,  1.67s/it] 55%|█████▌    | 5439/9822 [2:47:51<2:02:01,  1.67s/it] 55%|█████▌    | 5440/9822 [2:47:53<2:02:12,  1.67s/it] 55%|█████▌    | 5441/9822 [2:47:55<2:01:58,  1.67s/it] 55%|█████▌    | 5442/9822 [2:47:56<2:01:43,  1.67s/it] 55%|█████▌    | 5443/9822 [2:47:58<2:01:57,  1.67s/it] 55%|█████▌    | 5444/9822 [2:48:00<2:01:48,  1.67s/it] 55%|█████▌    | 5445/9822 [2:48:02<2:03:56,  1.70s/it] 55%|█████▌    | 5446/9822 [2:48:03<2:03:01,  1.69s/it] 55%|█████▌    | 5447/9822 [2:48:05<2:02:30,  1.68s/it] 55%|█████▌    | 5448/9822 [2:48:07<2:02:12,  1.68s/it] 55%|█████▌    | 5449/9822 [2:48:08<2:01:47,  1.67s/it] 55%|█████▌    | 5450/9822 [2:48:10<2:01:38,  1.67s/it] 55%|█████▌    | 5451/9822 [2:48:12<2:01:28,  1.67s/it] 56%|█████▌    | 5452/9822 [2:48:13<2:01:17,  1.67s/it] 56%|█████▌    | 5453/9822 [2:48:15<2:01:15,  1.67s/it] 56%|█████▌    | 5454/9822 [2:48:17<2:01:17,  1.67s/it] 56%|█████▌    | 5455/9822 [2:48:18<2:01:12,  1.67s/it] 56%|█████▌    | 5456/9822 [2:48:20<2:01:15,  1.67s/it] 56%|█████▌    | 5457/9822 [2:48:22<2:01:16,  1.67s/it] 56%|█████▌    | 5458/9822 [2:48:23<2:01:03,  1.66s/it] 56%|█████▌    | 5459/9822 [2:48:25<2:00:55,  1.66s/it] 56%|█████▌    | 5460/9822 [2:48:27<2:00:55,  1.66s/it] 56%|█████▌    | 5461/9822 [2:48:28<2:00:59,  1.66s/it] 56%|█████▌    | 5462/9822 [2:48:30<2:01:01,  1.67s/it] 56%|█████▌    | 5463/9822 [2:48:32<2:01:06,  1.67s/it] 56%|█████▌    | 5464/9822 [2:48:33<2:01:04,  1.67s/it] 56%|█████▌    | 5465/9822 [2:48:35<2:00:57,  1.67s/it] 56%|█████▌    | 5466/9822 [2:48:37<2:01:00,  1.67s/it] 56%|█████▌    | 5467/9822 [2:48:38<2:00:59,  1.67s/it] 56%|█████▌    | 5468/9822 [2:48:40<2:00:52,  1.67s/it] 56%|█████▌    | 5469/9822 [2:48:42<2:00:56,  1.67s/it] 56%|█████▌    | 5470/9822 [2:48:43<2:01:00,  1.67s/it] 56%|█████▌    | 5471/9822 [2:48:45<2:01:05,  1.67s/it] 56%|█████▌    | 5472/9822 [2:48:47<2:01:04,  1.67s/it] 56%|█████▌    | 5473/9822 [2:48:48<2:01:06,  1.67s/it] 56%|█████▌    | 5474/9822 [2:48:50<2:00:54,  1.67s/it] 56%|█████▌    | 5475/9822 [2:48:52<2:00:48,  1.67s/it] 56%|█████▌    | 5476/9822 [2:48:53<2:00:51,  1.67s/it] 56%|█████▌    | 5477/9822 [2:48:55<2:00:56,  1.67s/it] 56%|█████▌    | 5478/9822 [2:48:57<2:03:21,  1.70s/it] 56%|█████▌    | 5479/9822 [2:48:58<2:02:30,  1.69s/it] 56%|█████▌    | 5480/9822 [2:49:00<2:01:56,  1.69s/it] 56%|█████▌    | 5481/9822 [2:49:02<2:01:45,  1.68s/it] 56%|█████▌    | 5482/9822 [2:49:03<2:01:18,  1.68s/it] 56%|█████▌    | 5483/9822 [2:49:05<2:00:58,  1.67s/it] 56%|█████▌    | 5484/9822 [2:49:07<2:01:07,  1.68s/it] 56%|█████▌    | 5485/9822 [2:49:08<2:00:58,  1.67s/it] 56%|█████▌    | 5486/9822 [2:49:10<2:00:52,  1.67s/it] 56%|█████▌    | 5487/9822 [2:49:12<2:01:01,  1.68s/it] 56%|█████▌    | 5488/9822 [2:49:13<2:00:52,  1.67s/it] 56%|█████▌    | 5489/9822 [2:49:15<2:00:47,  1.67s/it] 56%|█████▌    | 5490/9822 [2:49:17<2:00:50,  1.67s/it] 56%|█████▌    | 5491/9822 [2:49:18<2:00:37,  1.67s/it] 56%|█████▌    | 5492/9822 [2:49:20<2:00:30,  1.67s/it] 56%|█████▌    | 5493/9822 [2:49:22<2:00:30,  1.67s/it] 56%|█████▌    | 5494/9822 [2:49:23<2:00:25,  1.67s/it] 56%|█████▌    | 5495/9822 [2:49:25<2:00:17,  1.67s/it] 56%|█████▌    | 5496/9822 [2:49:27<2:00:35,  1.67s/it] 56%|█████▌    | 5497/9822 [2:49:28<2:00:31,  1.67s/it] 56%|█████▌    | 5498/9822 [2:49:30<2:00:23,  1.67s/it] 56%|█████▌    | 5499/9822 [2:49:32<2:00:17,  1.67s/it] 56%|█████▌    | 5500/9822 [2:49:34<2:02:23,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:30:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:30:20 - INFO - __main__ - ***** test Results*****
04/29/2024 14:30:20 - INFO - __main__ -   Training step = 5500
04/29/2024 14:30:20 - INFO - __main__ -  test_accuracy:0.8722547584187409 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:30:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:30:25 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:30:25 - INFO - __main__ -   Training step = 5500
04/29/2024 14:30:25 - INFO - __main__ -  eval_accuracy:0.8462101794214574 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:30:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:30:33 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:30:33 - INFO - __main__ -   Training step = 5500
04/29/2024 14:30:33 - INFO - __main__ -  eval_accuracy:0.9157817649212743 
 56%|█████▌    | 5501/9822 [2:49:53<8:21:20,  6.96s/it] 56%|█████▌    | 5502/9822 [2:49:54<6:26:59,  5.37s/it] 56%|█████▌    | 5503/9822 [2:49:56<5:06:49,  4.26s/it] 56%|█████▌    | 5504/9822 [2:49:58<4:10:05,  3.48s/it] 56%|█████▌    | 5505/9822 [2:49:59<3:31:08,  2.93s/it] 56%|█████▌    | 5506/9822 [2:50:01<3:03:36,  2.55s/it] 56%|█████▌    | 5507/9822 [2:50:03<2:44:48,  2.29s/it] 56%|█████▌    | 5508/9822 [2:50:04<2:31:23,  2.11s/it] 56%|█████▌    | 5509/9822 [2:50:06<2:21:49,  1.97s/it] 56%|█████▌    | 5510/9822 [2:50:08<2:15:24,  1.88s/it] 56%|█████▌    | 5511/9822 [2:50:09<2:10:37,  1.82s/it] 56%|█████▌    | 5512/9822 [2:50:11<2:07:11,  1.77s/it] 56%|█████▌    | 5513/9822 [2:50:13<2:05:13,  1.74s/it] 56%|█████▌    | 5514/9822 [2:50:14<2:03:38,  1.72s/it] 56%|█████▌    | 5515/9822 [2:50:16<2:02:27,  1.71s/it] 56%|█████▌    | 5516/9822 [2:50:18<2:01:52,  1.70s/it] 56%|█████▌    | 5517/9822 [2:50:19<2:01:12,  1.69s/it] 56%|█████▌    | 5518/9822 [2:50:21<2:00:35,  1.68s/it] 56%|█████▌    | 5519/9822 [2:50:23<2:00:38,  1.68s/it] 56%|█████▌    | 5520/9822 [2:50:25<2:00:33,  1.68s/it] 56%|█████▌    | 5521/9822 [2:50:26<2:00:35,  1.68s/it] 56%|█████▌    | 5522/9822 [2:50:28<2:00:12,  1.68s/it] 56%|█████▌    | 5523/9822 [2:50:30<1:59:54,  1.67s/it] 56%|█████▌    | 5524/9822 [2:50:31<2:02:16,  1.71s/it] 56%|█████▋    | 5525/9822 [2:50:33<2:01:24,  1.70s/it] 56%|█████▋    | 5526/9822 [2:50:35<2:00:43,  1.69s/it] 56%|█████▋    | 5527/9822 [2:50:36<2:00:44,  1.69s/it] 56%|█████▋    | 5528/9822 [2:50:38<2:00:21,  1.68s/it] 56%|█████▋    | 5529/9822 [2:50:40<2:00:12,  1.68s/it] 56%|█████▋    | 5530/9822 [2:50:41<2:00:13,  1.68s/it] 56%|█████▋    | 5531/9822 [2:50:43<2:00:03,  1.68s/it] 56%|█████▋    | 5532/9822 [2:50:45<1:59:50,  1.68s/it] 56%|█████▋    | 5533/9822 [2:50:46<1:59:37,  1.67s/it] 56%|█████▋    | 5534/9822 [2:50:48<1:59:34,  1.67s/it] 56%|█████▋    | 5535/9822 [2:50:50<1:59:32,  1.67s/it] 56%|█████▋    | 5536/9822 [2:50:51<1:59:28,  1.67s/it] 56%|█████▋    | 5537/9822 [2:50:53<1:59:26,  1.67s/it] 56%|█████▋    | 5538/9822 [2:50:55<1:59:24,  1.67s/it] 56%|█████▋    | 5539/9822 [2:50:56<1:59:10,  1.67s/it] 56%|█████▋    | 5540/9822 [2:50:58<1:59:17,  1.67s/it] 56%|█████▋    | 5541/9822 [2:51:00<1:59:11,  1.67s/it] 56%|█████▋    | 5542/9822 [2:51:01<1:59:06,  1.67s/it] 56%|█████▋    | 5543/9822 [2:51:03<1:59:07,  1.67s/it] 56%|█████▋    | 5544/9822 [2:51:05<1:59:00,  1.67s/it] 56%|█████▋    | 5545/9822 [2:51:06<1:58:54,  1.67s/it] 56%|█████▋    | 5546/9822 [2:51:08<1:58:58,  1.67s/it] 56%|█████▋    | 5547/9822 [2:51:10<1:58:59,  1.67s/it] 56%|█████▋    | 5548/9822 [2:51:11<1:58:55,  1.67s/it] 56%|█████▋    | 5549/9822 [2:51:13<1:58:57,  1.67s/it] 57%|█████▋    | 5550/9822 [2:51:15<2:01:16,  1.70s/it] 57%|█████▋    | 5551/9822 [2:51:17<2:00:39,  1.70s/it] 57%|█████▋    | 5552/9822 [2:51:18<1:59:57,  1.69s/it] 57%|█████▋    | 5553/9822 [2:51:20<1:59:48,  1.68s/it] 57%|█████▋    | 5554/9822 [2:51:22<1:59:28,  1.68s/it] 57%|█████▋    | 5555/9822 [2:51:23<1:59:10,  1.68s/it] 57%|█████▋    | 5556/9822 [2:51:25<1:59:04,  1.67s/it] 57%|█████▋    | 5557/9822 [2:51:27<1:59:00,  1.67s/it] 57%|█████▋    | 5558/9822 [2:51:28<1:59:04,  1.68s/it] 57%|█████▋    | 5559/9822 [2:51:30<1:58:58,  1.67s/it] 57%|█████▋    | 5560/9822 [2:51:32<1:58:55,  1.67s/it] 57%|█████▋    | 5561/9822 [2:51:33<1:58:44,  1.67s/it] 57%|█████▋    | 5562/9822 [2:51:35<1:58:42,  1.67s/it] 57%|█████▋    | 5563/9822 [2:51:37<1:58:42,  1.67s/it] 57%|█████▋    | 5564/9822 [2:51:38<1:58:39,  1.67s/it] 57%|█████▋    | 5565/9822 [2:51:40<1:58:27,  1.67s/it] 57%|█████▋    | 5566/9822 [2:51:42<1:58:33,  1.67s/it] 57%|█████▋    | 5567/9822 [2:51:43<1:58:22,  1.67s/it] 57%|█████▋    | 5568/9822 [2:51:45<1:58:15,  1.67s/it] 57%|█████▋    | 5569/9822 [2:51:47<1:58:13,  1.67s/it] 57%|█████▋    | 5570/9822 [2:51:48<1:58:17,  1.67s/it] 57%|█████▋    | 5571/9822 [2:51:50<1:58:09,  1.67s/it] 57%|█████▋    | 5572/9822 [2:51:52<1:58:19,  1.67s/it] 57%|█████▋    | 5573/9822 [2:51:53<1:58:21,  1.67s/it] 57%|█████▋    | 5574/9822 [2:51:55<1:58:03,  1.67s/it] 57%|█████▋    | 5575/9822 [2:51:57<1:58:04,  1.67s/it] 57%|█████▋    | 5576/9822 [2:51:58<1:58:03,  1.67s/it] 57%|█████▋    | 5577/9822 [2:52:00<1:58:00,  1.67s/it] 57%|█████▋    | 5578/9822 [2:52:02<1:58:07,  1.67s/it] 57%|█████▋    | 5579/9822 [2:52:03<1:58:09,  1.67s/it] 57%|█████▋    | 5580/9822 [2:52:05<1:58:00,  1.67s/it] 57%|█████▋    | 5581/9822 [2:52:07<1:58:23,  1.67s/it] 57%|█████▋    | 5582/9822 [2:52:08<1:58:31,  1.68s/it] 57%|█████▋    | 5583/9822 [2:52:10<2:00:34,  1.71s/it] 57%|█████▋    | 5584/9822 [2:52:12<2:00:35,  1.71s/it] 57%|█████▋    | 5585/9822 [2:52:14<2:01:06,  1.71s/it] 57%|█████▋    | 5586/9822 [2:52:15<2:00:30,  1.71s/it] 57%|█████▋    | 5587/9822 [2:52:17<2:00:06,  1.70s/it] 57%|█████▋    | 5588/9822 [2:52:19<1:59:39,  1.70s/it] 57%|█████▋    | 5589/9822 [2:52:20<1:59:26,  1.69s/it] 57%|█████▋    | 5590/9822 [2:52:22<1:58:13,  1.68s/it] 57%|█████▋    | 5591/9822 [2:52:24<1:58:18,  1.68s/it] 57%|█████▋    | 5592/9822 [2:52:25<1:58:18,  1.68s/it] 57%|█████▋    | 5593/9822 [2:52:27<1:58:09,  1.68s/it] 57%|█████▋    | 5594/9822 [2:52:29<1:58:17,  1.68s/it] 57%|█████▋    | 5595/9822 [2:52:30<1:58:26,  1.68s/it] 57%|█████▋    | 5596/9822 [2:52:32<1:58:20,  1.68s/it] 57%|█████▋    | 5597/9822 [2:52:34<1:58:25,  1.68s/it] 57%|█████▋    | 5598/9822 [2:52:35<1:58:33,  1.68s/it] 57%|█████▋    | 5599/9822 [2:52:37<1:58:39,  1.69s/it] 57%|█████▋    | 5600/9822 [2:52:39<1:58:36,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:33:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:33:25 - INFO - __main__ - ***** test Results*****
04/29/2024 14:33:25 - INFO - __main__ -   Training step = 5600
04/29/2024 14:33:25 - INFO - __main__ -  test_accuracy:0.8700585651537335 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:33:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:33:30 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:33:30 - INFO - __main__ -   Training step = 5600
04/29/2024 14:33:30 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:33:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:33:39 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:33:39 - INFO - __main__ -   Training step = 5600
04/29/2024 14:33:39 - INFO - __main__ -  eval_accuracy:0.9091907726107653 
 57%|█████▋    | 5601/9822 [2:52:58<8:08:53,  6.95s/it] 57%|█████▋    | 5602/9822 [2:53:00<6:17:37,  5.37s/it] 57%|█████▋    | 5603/9822 [2:53:01<4:59:37,  4.26s/it] 57%|█████▋    | 5604/9822 [2:53:03<4:05:22,  3.49s/it] 57%|█████▋    | 5605/9822 [2:53:05<3:27:40,  2.95s/it] 57%|█████▋    | 5606/9822 [2:53:06<3:00:33,  2.57s/it] 57%|█████▋    | 5607/9822 [2:53:08<2:41:55,  2.30s/it] 57%|█████▋    | 5608/9822 [2:53:10<2:28:22,  2.11s/it] 57%|█████▋    | 5609/9822 [2:53:12<2:21:09,  2.01s/it] 57%|█████▋    | 5610/9822 [2:53:13<2:13:38,  1.90s/it] 57%|█████▋    | 5611/9822 [2:53:15<2:08:30,  1.83s/it] 57%|█████▋    | 5612/9822 [2:53:17<2:04:50,  1.78s/it] 57%|█████▋    | 5613/9822 [2:53:18<2:02:30,  1.75s/it] 57%|█████▋    | 5614/9822 [2:53:20<2:01:13,  1.73s/it] 57%|█████▋    | 5615/9822 [2:53:22<1:59:53,  1.71s/it] 57%|█████▋    | 5616/9822 [2:53:23<1:59:05,  1.70s/it] 57%|█████▋    | 5617/9822 [2:53:25<1:58:23,  1.69s/it] 57%|█████▋    | 5618/9822 [2:53:27<1:57:44,  1.68s/it] 57%|█████▋    | 5619/9822 [2:53:28<1:57:23,  1.68s/it] 57%|█████▋    | 5620/9822 [2:53:30<1:57:05,  1.67s/it] 57%|█████▋    | 5621/9822 [2:53:32<1:57:13,  1.67s/it] 57%|█████▋    | 5622/9822 [2:53:33<1:56:52,  1.67s/it] 57%|█████▋    | 5623/9822 [2:53:35<1:56:50,  1.67s/it] 57%|█████▋    | 5624/9822 [2:53:37<1:56:59,  1.67s/it] 57%|█████▋    | 5625/9822 [2:53:38<1:56:55,  1.67s/it] 57%|█████▋    | 5626/9822 [2:53:40<1:56:35,  1.67s/it] 57%|█████▋    | 5627/9822 [2:53:42<1:56:53,  1.67s/it] 57%|█████▋    | 5628/9822 [2:53:43<1:56:39,  1.67s/it] 57%|█████▋    | 5629/9822 [2:53:45<1:56:40,  1.67s/it] 57%|█████▋    | 5630/9822 [2:53:47<1:56:46,  1.67s/it] 57%|█████▋    | 5631/9822 [2:53:48<1:57:03,  1.68s/it] 57%|█████▋    | 5632/9822 [2:53:50<1:57:02,  1.68s/it] 57%|█████▋    | 5633/9822 [2:53:52<1:56:59,  1.68s/it] 57%|█████▋    | 5634/9822 [2:53:53<1:57:05,  1.68s/it] 57%|█████▋    | 5635/9822 [2:53:55<1:59:13,  1.71s/it] 57%|█████▋    | 5636/9822 [2:53:57<1:58:47,  1.70s/it] 57%|█████▋    | 5637/9822 [2:53:58<1:58:05,  1.69s/it] 57%|█████▋    | 5638/9822 [2:54:00<1:57:28,  1.68s/it] 57%|█████▋    | 5639/9822 [2:54:02<1:57:10,  1.68s/it] 57%|█████▋    | 5640/9822 [2:54:03<1:56:55,  1.68s/it] 57%|█████▋    | 5641/9822 [2:54:05<1:56:43,  1.68s/it] 57%|█████▋    | 5642/9822 [2:54:07<1:56:36,  1.67s/it] 57%|█████▋    | 5643/9822 [2:54:08<1:56:41,  1.68s/it] 57%|█████▋    | 5644/9822 [2:54:10<1:56:34,  1.67s/it] 57%|█████▋    | 5645/9822 [2:54:12<1:56:37,  1.68s/it] 57%|█████▋    | 5646/9822 [2:54:13<1:56:25,  1.67s/it] 57%|█████▋    | 5647/9822 [2:54:15<1:56:33,  1.68s/it] 58%|█████▊    | 5648/9822 [2:54:17<1:56:42,  1.68s/it] 58%|█████▊    | 5649/9822 [2:54:19<1:56:38,  1.68s/it] 58%|█████▊    | 5650/9822 [2:54:20<1:56:45,  1.68s/it] 58%|█████▊    | 5651/9822 [2:54:22<1:56:31,  1.68s/it] 58%|█████▊    | 5652/9822 [2:54:24<1:56:44,  1.68s/it] 58%|█████▊    | 5653/9822 [2:54:25<1:56:52,  1.68s/it] 58%|█████▊    | 5654/9822 [2:54:27<1:56:57,  1.68s/it] 58%|█████▊    | 5655/9822 [2:54:29<1:56:43,  1.68s/it] 58%|█████▊    | 5656/9822 [2:54:30<1:56:38,  1.68s/it] 58%|█████▊    | 5657/9822 [2:54:32<1:56:18,  1.68s/it] 58%|█████▊    | 5658/9822 [2:54:34<1:56:17,  1.68s/it] 58%|█████▊    | 5659/9822 [2:54:35<1:56:11,  1.67s/it] 58%|█████▊    | 5660/9822 [2:54:37<1:56:02,  1.67s/it] 58%|█████▊    | 5661/9822 [2:54:39<1:56:08,  1.67s/it] 58%|█████▊    | 5662/9822 [2:54:40<1:56:22,  1.68s/it] 58%|█████▊    | 5663/9822 [2:54:42<1:56:13,  1.68s/it] 58%|█████▊    | 5664/9822 [2:54:44<1:56:05,  1.68s/it] 58%|█████▊    | 5665/9822 [2:54:45<1:56:04,  1.68s/it] 58%|█████▊    | 5666/9822 [2:54:47<1:55:55,  1.67s/it] 58%|█████▊    | 5667/9822 [2:54:49<1:56:00,  1.68s/it] 58%|█████▊    | 5668/9822 [2:54:50<1:55:56,  1.67s/it] 58%|█████▊    | 5669/9822 [2:54:52<1:56:04,  1.68s/it] 58%|█████▊    | 5670/9822 [2:54:54<1:56:02,  1.68s/it] 58%|█████▊    | 5671/9822 [2:54:55<1:55:38,  1.67s/it] 58%|█████▊    | 5672/9822 [2:54:57<1:55:38,  1.67s/it] 58%|█████▊    | 5673/9822 [2:54:59<1:55:56,  1.68s/it] 58%|█████▊    | 5674/9822 [2:55:00<1:55:43,  1.67s/it] 58%|█████▊    | 5675/9822 [2:55:02<1:55:41,  1.67s/it] 58%|█████▊    | 5676/9822 [2:55:04<1:55:50,  1.68s/it] 58%|█████▊    | 5677/9822 [2:55:05<1:55:52,  1.68s/it] 58%|█████▊    | 5678/9822 [2:55:07<1:55:42,  1.68s/it] 58%|█████▊    | 5679/9822 [2:55:09<1:55:48,  1.68s/it] 58%|█████▊    | 5680/9822 [2:55:11<1:55:41,  1.68s/it] 58%|█████▊    | 5681/9822 [2:55:12<1:55:37,  1.68s/it] 58%|█████▊    | 5682/9822 [2:55:14<1:55:48,  1.68s/it] 58%|█████▊    | 5683/9822 [2:55:16<1:55:35,  1.68s/it] 58%|█████▊    | 5684/9822 [2:55:17<1:55:22,  1.67s/it] 58%|█████▊    | 5685/9822 [2:55:19<1:55:23,  1.67s/it] 58%|█████▊    | 5686/9822 [2:55:21<1:55:40,  1.68s/it] 58%|█████▊    | 5687/9822 [2:55:22<1:55:42,  1.68s/it] 58%|█████▊    | 5688/9822 [2:55:24<1:55:28,  1.68s/it] 58%|█████▊    | 5689/9822 [2:55:26<1:55:31,  1.68s/it] 58%|█████▊    | 5690/9822 [2:55:27<1:55:38,  1.68s/it] 58%|█████▊    | 5691/9822 [2:55:29<1:55:24,  1.68s/it] 58%|█████▊    | 5692/9822 [2:55:31<1:55:12,  1.67s/it] 58%|█████▊    | 5693/9822 [2:55:32<1:55:15,  1.67s/it] 58%|█████▊    | 5694/9822 [2:55:34<1:55:13,  1.67s/it] 58%|█████▊    | 5695/9822 [2:55:36<1:55:01,  1.67s/it] 58%|█████▊    | 5696/9822 [2:55:37<1:55:09,  1.67s/it] 58%|█████▊    | 5697/9822 [2:55:39<1:55:14,  1.68s/it] 58%|█████▊    | 5698/9822 [2:55:41<1:55:03,  1.67s/it] 58%|█████▊    | 5699/9822 [2:55:42<1:54:58,  1.67s/it] 58%|█████▊    | 5700/9822 [2:55:44<1:54:39,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:36:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:36:31 - INFO - __main__ - ***** test Results*****
04/29/2024 14:36:31 - INFO - __main__ -   Training step = 5700
04/29/2024 14:36:31 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:36:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:36:35 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:36:35 - INFO - __main__ -   Training step = 5700
04/29/2024 14:36:35 - INFO - __main__ -  eval_accuracy:0.8531673379714391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8535335042109118}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:36:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:36:44 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:36:44 - INFO - __main__ -   Training step = 5700
04/29/2024 14:36:44 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 58%|█████▊    | 5701/9822 [2:56:03<7:56:20,  6.94s/it] 58%|█████▊    | 5702/9822 [2:56:05<6:09:46,  5.39s/it] 58%|█████▊    | 5703/9822 [2:56:07<4:53:16,  4.27s/it] 58%|█████▊    | 5704/9822 [2:56:08<3:59:32,  3.49s/it] 58%|█████▊    | 5705/9822 [2:56:10<3:21:46,  2.94s/it] 58%|█████▊    | 5706/9822 [2:56:12<2:55:16,  2.56s/it] 58%|█████▊    | 5707/9822 [2:56:13<2:36:57,  2.29s/it] 58%|█████▊    | 5708/9822 [2:56:15<2:24:03,  2.10s/it] 58%|█████▊    | 5709/9822 [2:56:17<2:15:04,  1.97s/it] 58%|█████▊    | 5710/9822 [2:56:18<2:08:43,  1.88s/it] 58%|█████▊    | 5711/9822 [2:56:20<2:04:26,  1.82s/it] 58%|█████▊    | 5712/9822 [2:56:22<2:01:31,  1.77s/it] 58%|█████▊    | 5713/9822 [2:56:23<1:59:33,  1.75s/it] 58%|█████▊    | 5714/9822 [2:56:25<1:57:50,  1.72s/it] 58%|█████▊    | 5715/9822 [2:56:27<1:56:34,  1.70s/it] 58%|█████▊    | 5716/9822 [2:56:28<1:55:45,  1.69s/it] 58%|█████▊    | 5717/9822 [2:56:30<1:55:18,  1.69s/it] 58%|█████▊    | 5718/9822 [2:56:32<1:54:51,  1.68s/it] 58%|█████▊    | 5719/9822 [2:56:33<1:54:28,  1.67s/it] 58%|█████▊    | 5720/9822 [2:56:35<1:54:27,  1.67s/it] 58%|█████▊    | 5721/9822 [2:56:37<1:54:20,  1.67s/it] 58%|█████▊    | 5722/9822 [2:56:38<1:54:06,  1.67s/it] 58%|█████▊    | 5723/9822 [2:56:40<1:54:27,  1.68s/it] 58%|█████▊    | 5724/9822 [2:56:42<1:54:14,  1.67s/it] 58%|█████▊    | 5725/9822 [2:56:43<1:53:58,  1.67s/it] 58%|█████▊    | 5726/9822 [2:56:45<1:54:31,  1.68s/it] 58%|█████▊    | 5727/9822 [2:56:47<1:54:27,  1.68s/it] 58%|█████▊    | 5728/9822 [2:56:48<1:54:34,  1.68s/it] 58%|█████▊    | 5729/9822 [2:56:50<1:56:17,  1.70s/it] 58%|█████▊    | 5730/9822 [2:56:52<1:55:34,  1.69s/it] 58%|█████▊    | 5731/9822 [2:56:53<1:54:53,  1.69s/it] 58%|█████▊    | 5732/9822 [2:56:55<1:54:38,  1.68s/it] 58%|█████▊    | 5733/9822 [2:56:57<1:54:14,  1.68s/it] 58%|█████▊    | 5734/9822 [2:56:58<1:53:56,  1.67s/it] 58%|█████▊    | 5735/9822 [2:57:00<1:53:36,  1.67s/it] 58%|█████▊    | 5736/9822 [2:57:02<1:53:34,  1.67s/it] 58%|█████▊    | 5737/9822 [2:57:03<1:53:16,  1.66s/it] 58%|█████▊    | 5738/9822 [2:57:05<1:53:08,  1.66s/it] 58%|█████▊    | 5739/9822 [2:57:07<1:53:15,  1.66s/it] 58%|█████▊    | 5740/9822 [2:57:08<1:53:10,  1.66s/it] 58%|█████▊    | 5741/9822 [2:57:10<1:53:04,  1.66s/it] 58%|█████▊    | 5742/9822 [2:57:12<1:53:03,  1.66s/it] 58%|█████▊    | 5743/9822 [2:57:13<1:53:20,  1.67s/it] 58%|█████▊    | 5744/9822 [2:57:15<1:53:30,  1.67s/it] 58%|█████▊    | 5745/9822 [2:57:17<1:53:20,  1.67s/it] 59%|█████▊    | 5746/9822 [2:57:18<1:53:17,  1.67s/it] 59%|█████▊    | 5747/9822 [2:57:20<1:53:26,  1.67s/it] 59%|█████▊    | 5748/9822 [2:57:22<1:53:20,  1.67s/it] 59%|█████▊    | 5749/9822 [2:57:23<1:53:10,  1.67s/it] 59%|█████▊    | 5750/9822 [2:57:25<1:53:07,  1.67s/it] 59%|█████▊    | 5751/9822 [2:57:27<1:52:58,  1.67s/it] 59%|█████▊    | 5752/9822 [2:57:28<1:53:06,  1.67s/it] 59%|█████▊    | 5753/9822 [2:57:30<1:53:10,  1.67s/it] 59%|█████▊    | 5754/9822 [2:57:32<1:52:56,  1.67s/it] 59%|█████▊    | 5755/9822 [2:57:33<1:53:02,  1.67s/it] 59%|█████▊    | 5756/9822 [2:57:35<1:55:11,  1.70s/it] 59%|█████▊    | 5757/9822 [2:57:37<1:54:56,  1.70s/it] 59%|█████▊    | 5758/9822 [2:57:39<1:54:21,  1.69s/it] 59%|█████▊    | 5759/9822 [2:57:40<1:54:04,  1.68s/it] 59%|█████▊    | 5760/9822 [2:57:42<1:53:47,  1.68s/it] 59%|█████▊    | 5761/9822 [2:57:44<1:53:44,  1.68s/it] 59%|█████▊    | 5762/9822 [2:57:45<1:52:18,  1.66s/it] 59%|█████▊    | 5763/9822 [2:57:47<1:52:26,  1.66s/it] 59%|█████▊    | 5764/9822 [2:57:49<1:52:32,  1.66s/it] 59%|█████▊    | 5765/9822 [2:57:50<1:52:20,  1.66s/it] 59%|█████▊    | 5766/9822 [2:57:52<1:52:23,  1.66s/it] 59%|█████▊    | 5767/9822 [2:57:54<1:52:37,  1.67s/it] 59%|█████▊    | 5768/9822 [2:57:55<1:52:46,  1.67s/it] 59%|█████▊    | 5769/9822 [2:57:57<1:52:34,  1.67s/it] 59%|█████▊    | 5770/9822 [2:57:59<1:52:26,  1.66s/it] 59%|█████▉    | 5771/9822 [2:58:00<1:52:21,  1.66s/it] 59%|█████▉    | 5772/9822 [2:58:02<1:52:24,  1.67s/it] 59%|█████▉    | 5773/9822 [2:58:04<1:52:23,  1.67s/it] 59%|█████▉    | 5774/9822 [2:58:05<1:52:09,  1.66s/it] 59%|█████▉    | 5775/9822 [2:58:07<1:52:24,  1.67s/it] 59%|█████▉    | 5776/9822 [2:58:09<1:52:36,  1.67s/it] 59%|█████▉    | 5777/9822 [2:58:10<1:52:48,  1.67s/it] 59%|█████▉    | 5778/9822 [2:58:12<1:52:42,  1.67s/it] 59%|█████▉    | 5779/9822 [2:58:14<1:52:28,  1.67s/it] 59%|█████▉    | 5780/9822 [2:58:15<1:52:39,  1.67s/it] 59%|█████▉    | 5781/9822 [2:58:17<1:52:28,  1.67s/it] 59%|█████▉    | 5782/9822 [2:58:19<1:52:12,  1.67s/it] 59%|█████▉    | 5783/9822 [2:58:20<1:52:22,  1.67s/it] 59%|█████▉    | 5784/9822 [2:58:22<1:52:11,  1.67s/it] 59%|█████▉    | 5785/9822 [2:58:24<1:52:01,  1.66s/it] 59%|█████▉    | 5786/9822 [2:58:25<1:52:22,  1.67s/it] 59%|█████▉    | 5787/9822 [2:58:27<1:52:24,  1.67s/it] 59%|█████▉    | 5788/9822 [2:58:29<1:52:10,  1.67s/it] 59%|█████▉    | 5789/9822 [2:58:30<1:53:57,  1.70s/it] 59%|█████▉    | 5790/9822 [2:58:32<1:53:13,  1.68s/it] 59%|█████▉    | 5791/9822 [2:58:34<1:52:41,  1.68s/it] 59%|█████▉    | 5792/9822 [2:58:35<1:52:19,  1.67s/it] 59%|█████▉    | 5793/9822 [2:58:37<1:51:59,  1.67s/it] 59%|█████▉    | 5794/9822 [2:58:39<1:51:54,  1.67s/it] 59%|█████▉    | 5795/9822 [2:58:40<1:51:45,  1.67s/it] 59%|█████▉    | 5796/9822 [2:58:42<1:51:56,  1.67s/it] 59%|█████▉    | 5797/9822 [2:58:44<1:52:00,  1.67s/it] 59%|█████▉    | 5798/9822 [2:58:45<1:51:52,  1.67s/it] 59%|█████▉    | 5799/9822 [2:58:47<1:51:53,  1.67s/it] 59%|█████▉    | 5800/9822 [2:58:49<1:52:01,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:39:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:39:35 - INFO - __main__ - ***** test Results*****
04/29/2024 14:39:35 - INFO - __main__ -   Training step = 5800
04/29/2024 14:39:35 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:39:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:39:40 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:39:40 - INFO - __main__ -   Training step = 5800
04/29/2024 14:39:40 - INFO - __main__ -  eval_accuracy:0.8538996704503845 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 14:39:40,416 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 14:39:40,416 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 14:39:40,457 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 14:39:44,975 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:39:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:39:53 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:39:53 - INFO - __main__ -   Training step = 5800
04/29/2024 14:39:53 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 59%|█████▉    | 5801/9822 [2:59:12<9:15:38,  8.29s/it] 59%|█████▉    | 5802/9822 [2:59:14<7:02:13,  6.30s/it] 59%|█████▉    | 5803/9822 [2:59:16<5:28:58,  4.91s/it] 59%|█████▉    | 5804/9822 [2:59:17<4:23:29,  3.93s/it] 59%|█████▉    | 5805/9822 [2:59:19<3:37:43,  3.25s/it] 59%|█████▉    | 5806/9822 [2:59:21<3:05:51,  2.78s/it] 59%|█████▉    | 5807/9822 [2:59:22<2:43:47,  2.45s/it] 59%|█████▉    | 5808/9822 [2:59:24<2:28:11,  2.22s/it] 59%|█████▉    | 5809/9822 [2:59:26<2:17:10,  2.05s/it] 59%|█████▉    | 5810/9822 [2:59:27<2:10:01,  1.94s/it] 59%|█████▉    | 5811/9822 [2:59:29<2:04:45,  1.87s/it] 59%|█████▉    | 5812/9822 [2:59:31<2:01:00,  1.81s/it] 59%|█████▉    | 5813/9822 [2:59:33<2:00:33,  1.80s/it] 59%|█████▉    | 5814/9822 [2:59:34<1:58:03,  1.77s/it] 59%|█████▉    | 5815/9822 [2:59:36<1:56:07,  1.74s/it] 59%|█████▉    | 5816/9822 [2:59:38<1:54:33,  1.72s/it] 59%|█████▉    | 5817/9822 [2:59:39<1:53:32,  1.70s/it] 59%|█████▉    | 5818/9822 [2:59:41<1:52:41,  1.69s/it] 59%|█████▉    | 5819/9822 [2:59:43<1:52:10,  1.68s/it] 59%|█████▉    | 5820/9822 [2:59:44<1:51:42,  1.67s/it] 59%|█████▉    | 5821/9822 [2:59:46<1:51:42,  1.68s/it] 59%|█████▉    | 5822/9822 [2:59:48<1:51:25,  1.67s/it] 59%|█████▉    | 5823/9822 [2:59:49<1:51:20,  1.67s/it] 59%|█████▉    | 5824/9822 [2:59:51<1:51:19,  1.67s/it] 59%|█████▉    | 5825/9822 [2:59:53<1:51:11,  1.67s/it] 59%|█████▉    | 5826/9822 [2:59:54<1:51:05,  1.67s/it] 59%|█████▉    | 5827/9822 [2:59:56<1:51:11,  1.67s/it] 59%|█████▉    | 5828/9822 [2:59:58<1:51:02,  1.67s/it] 59%|█████▉    | 5829/9822 [2:59:59<1:50:56,  1.67s/it] 59%|█████▉    | 5830/9822 [3:00:01<1:50:48,  1.67s/it] 59%|█████▉    | 5831/9822 [3:00:03<1:50:37,  1.66s/it] 59%|█████▉    | 5832/9822 [3:00:04<1:50:26,  1.66s/it] 59%|█████▉    | 5833/9822 [3:00:06<1:50:32,  1.66s/it] 59%|█████▉    | 5834/9822 [3:00:08<1:50:26,  1.66s/it] 59%|█████▉    | 5835/9822 [3:00:09<1:50:31,  1.66s/it] 59%|█████▉    | 5836/9822 [3:00:11<1:50:36,  1.67s/it] 59%|█████▉    | 5837/9822 [3:00:13<1:50:46,  1.67s/it] 59%|█████▉    | 5838/9822 [3:00:14<1:50:39,  1.67s/it] 59%|█████▉    | 5839/9822 [3:00:16<1:50:36,  1.67s/it] 59%|█████▉    | 5840/9822 [3:00:18<1:50:28,  1.66s/it] 59%|█████▉    | 5841/9822 [3:00:19<1:50:24,  1.66s/it] 59%|█████▉    | 5842/9822 [3:00:21<1:50:40,  1.67s/it] 59%|█████▉    | 5843/9822 [3:00:23<1:50:31,  1.67s/it] 59%|█████▉    | 5844/9822 [3:00:24<1:50:28,  1.67s/it] 60%|█████▉    | 5845/9822 [3:00:26<1:50:39,  1.67s/it] 60%|█████▉    | 5846/9822 [3:00:28<1:50:24,  1.67s/it] 60%|█████▉    | 5847/9822 [3:00:29<1:50:17,  1.66s/it] 60%|█████▉    | 5848/9822 [3:00:31<1:49:14,  1.65s/it] 60%|█████▉    | 5849/9822 [3:00:33<1:49:56,  1.66s/it] 60%|█████▉    | 5850/9822 [3:00:34<1:50:09,  1.66s/it] 60%|█████▉    | 5851/9822 [3:00:36<1:50:16,  1.67s/it] 60%|█████▉    | 5852/9822 [3:00:38<1:50:15,  1.67s/it] 60%|█████▉    | 5853/9822 [3:00:39<1:50:11,  1.67s/it] 60%|█████▉    | 5854/9822 [3:00:41<1:52:17,  1.70s/it] 60%|█████▉    | 5855/9822 [3:00:43<1:51:30,  1.69s/it] 60%|█████▉    | 5856/9822 [3:00:44<1:50:58,  1.68s/it] 60%|█████▉    | 5857/9822 [3:00:46<1:50:52,  1.68s/it] 60%|█████▉    | 5858/9822 [3:00:48<1:50:47,  1.68s/it] 60%|█████▉    | 5859/9822 [3:00:49<1:50:29,  1.67s/it] 60%|█████▉    | 5860/9822 [3:00:51<1:50:20,  1.67s/it] 60%|█████▉    | 5861/9822 [3:00:53<1:50:36,  1.68s/it] 60%|█████▉    | 5862/9822 [3:00:54<1:50:45,  1.68s/it] 60%|█████▉    | 5863/9822 [3:00:56<1:50:49,  1.68s/it] 60%|█████▉    | 5864/9822 [3:00:58<1:50:58,  1.68s/it] 60%|█████▉    | 5865/9822 [3:00:59<1:50:52,  1.68s/it] 60%|█████▉    | 5866/9822 [3:01:01<1:50:46,  1.68s/it] 60%|█████▉    | 5867/9822 [3:01:03<1:50:36,  1.68s/it] 60%|█████▉    | 5868/9822 [3:01:04<1:50:23,  1.68s/it] 60%|█████▉    | 5869/9822 [3:01:06<1:50:08,  1.67s/it] 60%|█████▉    | 5870/9822 [3:01:08<1:50:16,  1.67s/it] 60%|█████▉    | 5871/9822 [3:01:09<1:50:10,  1.67s/it] 60%|█████▉    | 5872/9822 [3:01:11<1:49:58,  1.67s/it] 60%|█████▉    | 5873/9822 [3:01:13<1:50:03,  1.67s/it] 60%|█████▉    | 5874/9822 [3:01:14<1:49:56,  1.67s/it] 60%|█████▉    | 5875/9822 [3:01:16<1:49:52,  1.67s/it] 60%|█████▉    | 5876/9822 [3:01:18<1:49:57,  1.67s/it] 60%|█████▉    | 5877/9822 [3:01:19<1:49:47,  1.67s/it] 60%|█████▉    | 5878/9822 [3:01:21<1:49:47,  1.67s/it] 60%|█████▉    | 5879/9822 [3:01:23<1:49:48,  1.67s/it] 60%|█████▉    | 5880/9822 [3:01:24<1:49:45,  1.67s/it] 60%|█████▉    | 5881/9822 [3:01:26<1:51:54,  1.70s/it] 60%|█████▉    | 5882/9822 [3:01:28<1:51:14,  1.69s/it] 60%|█████▉    | 5883/9822 [3:01:30<1:50:51,  1.69s/it] 60%|█████▉    | 5884/9822 [3:01:31<1:50:35,  1.69s/it] 60%|█████▉    | 5885/9822 [3:01:33<1:50:14,  1.68s/it] 60%|█████▉    | 5886/9822 [3:01:35<1:49:58,  1.68s/it] 60%|█████▉    | 5887/9822 [3:01:36<1:49:40,  1.67s/it] 60%|█████▉    | 5888/9822 [3:01:38<1:49:23,  1.67s/it] 60%|█████▉    | 5889/9822 [3:01:40<1:49:24,  1.67s/it] 60%|█████▉    | 5890/9822 [3:01:41<1:49:06,  1.66s/it] 60%|█████▉    | 5891/9822 [3:01:43<1:49:00,  1.66s/it] 60%|█████▉    | 5892/9822 [3:01:45<1:48:52,  1.66s/it] 60%|█████▉    | 5893/9822 [3:01:46<1:48:54,  1.66s/it] 60%|██████    | 5894/9822 [3:01:48<1:48:50,  1.66s/it] 60%|██████    | 5895/9822 [3:01:50<1:48:40,  1.66s/it] 60%|██████    | 5896/9822 [3:01:51<1:48:47,  1.66s/it] 60%|██████    | 5897/9822 [3:01:53<1:48:59,  1.67s/it] 60%|██████    | 5898/9822 [3:01:55<1:48:56,  1.67s/it] 60%|██████    | 5899/9822 [3:01:56<1:49:06,  1.67s/it] 60%|██████    | 5900/9822 [3:01:58<1:48:57,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:42:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:42:45 - INFO - __main__ - ***** test Results*****
04/29/2024 14:42:45 - INFO - __main__ -   Training step = 5900
04/29/2024 14:42:45 - INFO - __main__ -  test_accuracy:0.8696925329428989 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:42:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:42:49 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:42:49 - INFO - __main__ -   Training step = 5900
04/29/2024 14:42:49 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:42:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:42:58 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:42:58 - INFO - __main__ -   Training step = 5900
04/29/2024 14:42:58 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 60%|██████    | 5901/9822 [3:02:17<7:33:01,  6.93s/it] 60%|██████    | 5902/9822 [3:02:19<5:49:47,  5.35s/it] 60%|██████    | 5903/9822 [3:02:20<4:37:20,  4.25s/it] 60%|██████    | 5904/9822 [3:02:22<3:46:31,  3.47s/it] 60%|██████    | 5905/9822 [3:02:24<3:11:19,  2.93s/it] 60%|██████    | 5906/9822 [3:02:25<2:46:22,  2.55s/it] 60%|██████    | 5907/9822 [3:02:27<2:28:48,  2.28s/it] 60%|██████    | 5908/9822 [3:02:29<2:18:41,  2.13s/it] 60%|██████    | 5909/9822 [3:02:31<2:09:43,  1.99s/it] 60%|██████    | 5910/9822 [3:02:32<2:03:33,  1.90s/it] 60%|██████    | 5911/9822 [3:02:34<1:58:53,  1.82s/it] 60%|██████    | 5912/9822 [3:02:36<1:55:47,  1.78s/it] 60%|██████    | 5913/9822 [3:02:37<1:53:24,  1.74s/it] 60%|██████    | 5914/9822 [3:02:39<1:51:56,  1.72s/it] 60%|██████    | 5915/9822 [3:02:41<1:51:10,  1.71s/it] 60%|██████    | 5916/9822 [3:02:42<1:50:26,  1.70s/it] 60%|██████    | 5917/9822 [3:02:44<1:50:03,  1.69s/it] 60%|██████    | 5918/9822 [3:02:46<1:49:35,  1.68s/it] 60%|██████    | 5919/9822 [3:02:47<1:49:17,  1.68s/it] 60%|██████    | 5920/9822 [3:02:49<1:48:53,  1.67s/it] 60%|██████    | 5921/9822 [3:02:51<1:48:41,  1.67s/it] 60%|██████    | 5922/9822 [3:02:52<1:48:42,  1.67s/it] 60%|██████    | 5923/9822 [3:02:54<1:48:45,  1.67s/it] 60%|██████    | 5924/9822 [3:02:56<1:48:51,  1.68s/it] 60%|██████    | 5925/9822 [3:02:57<1:48:42,  1.67s/it] 60%|██████    | 5926/9822 [3:02:59<1:48:38,  1.67s/it] 60%|██████    | 5927/9822 [3:03:01<1:48:36,  1.67s/it] 60%|██████    | 5928/9822 [3:03:02<1:48:28,  1.67s/it] 60%|██████    | 5929/9822 [3:03:04<1:48:31,  1.67s/it] 60%|██████    | 5930/9822 [3:03:06<1:48:29,  1.67s/it] 60%|██████    | 5931/9822 [3:03:07<1:48:22,  1.67s/it] 60%|██████    | 5932/9822 [3:03:09<1:48:09,  1.67s/it] 60%|██████    | 5933/9822 [3:03:11<1:48:00,  1.67s/it] 60%|██████    | 5934/9822 [3:03:12<1:49:15,  1.69s/it] 60%|██████    | 5935/9822 [3:03:14<1:48:59,  1.68s/it] 60%|██████    | 5936/9822 [3:03:16<1:48:33,  1.68s/it] 60%|██████    | 5937/9822 [3:03:17<1:48:14,  1.67s/it] 60%|██████    | 5938/9822 [3:03:19<1:48:03,  1.67s/it] 60%|██████    | 5939/9822 [3:03:21<1:48:14,  1.67s/it] 60%|██████    | 5940/9822 [3:03:22<1:48:25,  1.68s/it] 60%|██████    | 5941/9822 [3:03:24<1:48:17,  1.67s/it] 60%|██████    | 5942/9822 [3:03:26<1:48:04,  1.67s/it] 61%|██████    | 5943/9822 [3:03:27<1:47:54,  1.67s/it] 61%|██████    | 5944/9822 [3:03:29<1:47:44,  1.67s/it] 61%|██████    | 5945/9822 [3:03:31<1:47:26,  1.66s/it] 61%|██████    | 5946/9822 [3:03:32<1:47:42,  1.67s/it] 61%|██████    | 5947/9822 [3:03:34<1:47:34,  1.67s/it] 61%|██████    | 5948/9822 [3:03:36<1:47:21,  1.66s/it] 61%|██████    | 5949/9822 [3:03:37<1:47:35,  1.67s/it] 61%|██████    | 5950/9822 [3:03:39<1:47:29,  1.67s/it] 61%|██████    | 5951/9822 [3:03:41<1:47:33,  1.67s/it] 61%|██████    | 5952/9822 [3:03:42<1:47:17,  1.66s/it] 61%|██████    | 5953/9822 [3:03:44<1:47:05,  1.66s/it] 61%|██████    | 5954/9822 [3:03:46<1:47:04,  1.66s/it] 61%|██████    | 5955/9822 [3:03:47<1:47:05,  1.66s/it] 61%|██████    | 5956/9822 [3:03:49<1:47:00,  1.66s/it] 61%|██████    | 5957/9822 [3:03:51<1:47:01,  1.66s/it] 61%|██████    | 5958/9822 [3:03:52<1:47:09,  1.66s/it] 61%|██████    | 5959/9822 [3:03:54<1:47:01,  1.66s/it] 61%|██████    | 5960/9822 [3:03:56<1:47:03,  1.66s/it] 61%|██████    | 5961/9822 [3:03:57<1:46:52,  1.66s/it] 61%|██████    | 5962/9822 [3:03:59<1:47:00,  1.66s/it] 61%|██████    | 5963/9822 [3:04:01<1:46:54,  1.66s/it] 61%|██████    | 5964/9822 [3:04:02<1:46:43,  1.66s/it] 61%|██████    | 5965/9822 [3:04:04<1:46:43,  1.66s/it] 61%|██████    | 5966/9822 [3:04:06<1:46:58,  1.66s/it] 61%|██████    | 5967/9822 [3:04:07<1:49:00,  1.70s/it] 61%|██████    | 5968/9822 [3:04:09<1:48:13,  1.68s/it] 61%|██████    | 5969/9822 [3:04:11<1:48:29,  1.69s/it] 61%|██████    | 5970/9822 [3:04:12<1:47:58,  1.68s/it] 61%|██████    | 5971/9822 [3:04:14<1:47:27,  1.67s/it] 61%|██████    | 5972/9822 [3:04:16<1:47:03,  1.67s/it] 61%|██████    | 5973/9822 [3:04:17<1:46:52,  1.67s/it] 61%|██████    | 5974/9822 [3:04:19<1:46:42,  1.66s/it] 61%|██████    | 5975/9822 [3:04:21<1:46:29,  1.66s/it] 61%|██████    | 5976/9822 [3:04:22<1:46:24,  1.66s/it] 61%|██████    | 5977/9822 [3:04:24<1:46:16,  1.66s/it] 61%|██████    | 5978/9822 [3:04:26<1:46:39,  1.66s/it] 61%|██████    | 5979/9822 [3:04:27<1:46:38,  1.67s/it] 61%|██████    | 5980/9822 [3:04:29<1:46:24,  1.66s/it] 61%|██████    | 5981/9822 [3:04:31<1:46:18,  1.66s/it] 61%|██████    | 5982/9822 [3:04:32<1:46:14,  1.66s/it] 61%|██████    | 5983/9822 [3:04:34<1:46:11,  1.66s/it] 61%|██████    | 5984/9822 [3:04:36<1:47:20,  1.68s/it] 61%|██████    | 5985/9822 [3:04:37<1:47:04,  1.67s/it] 61%|██████    | 5986/9822 [3:04:39<1:46:47,  1.67s/it] 61%|██████    | 5987/9822 [3:04:41<1:46:52,  1.67s/it] 61%|██████    | 5988/9822 [3:04:42<1:46:41,  1.67s/it] 61%|██████    | 5989/9822 [3:04:44<1:48:20,  1.70s/it] 61%|██████    | 5990/9822 [3:04:46<1:47:48,  1.69s/it] 61%|██████    | 5991/9822 [3:04:47<1:47:15,  1.68s/it] 61%|██████    | 5992/9822 [3:04:49<1:46:47,  1.67s/it] 61%|██████    | 5993/9822 [3:04:51<1:46:23,  1.67s/it] 61%|██████    | 5994/9822 [3:04:52<1:46:25,  1.67s/it] 61%|██████    | 5995/9822 [3:04:54<1:46:23,  1.67s/it] 61%|██████    | 5996/9822 [3:04:56<1:46:20,  1.67s/it] 61%|██████    | 5997/9822 [3:04:57<1:46:19,  1.67s/it] 61%|██████    | 5998/9822 [3:04:59<1:46:08,  1.67s/it] 61%|██████    | 5999/9822 [3:05:01<1:46:04,  1.66s/it] 61%|██████    | 6000/9822 [3:05:02<1:46:01,  1.66s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:45:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:45:49 - INFO - __main__ - ***** test Results*****
04/29/2024 14:45:49 - INFO - __main__ -   Training step = 6000
04/29/2024 14:45:49 - INFO - __main__ -  test_accuracy:0.8707906295754027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:45:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:45:54 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:45:54 - INFO - __main__ -   Training step = 6000
04/29/2024 14:45:54 - INFO - __main__ -  eval_accuracy:0.8502380080556573 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:46:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:46:02 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:46:02 - INFO - __main__ -   Training step = 6000
04/29/2024 14:46:02 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 61%|██████    | 6001/9822 [3:05:22<7:21:07,  6.93s/it] 61%|██████    | 6002/9822 [3:05:23<5:40:30,  5.35s/it] 61%|██████    | 6003/9822 [3:05:25<4:30:12,  4.25s/it] 61%|██████    | 6004/9822 [3:05:27<3:41:00,  3.47s/it] 61%|██████    | 6005/9822 [3:05:28<3:06:23,  2.93s/it] 61%|██████    | 6006/9822 [3:05:30<2:42:09,  2.55s/it] 61%|██████    | 6007/9822 [3:05:32<2:25:26,  2.29s/it] 61%|██████    | 6008/9822 [3:05:33<2:13:30,  2.10s/it] 61%|██████    | 6009/9822 [3:05:35<2:05:16,  1.97s/it] 61%|██████    | 6010/9822 [3:05:37<1:59:28,  1.88s/it] 61%|██████    | 6011/9822 [3:05:38<1:55:13,  1.81s/it] 61%|██████    | 6012/9822 [3:05:40<1:52:10,  1.77s/it] 61%|██████    | 6013/9822 [3:05:42<1:52:18,  1.77s/it] 61%|██████    | 6014/9822 [3:05:43<1:50:24,  1.74s/it] 61%|██████    | 6015/9822 [3:05:45<1:48:56,  1.72s/it] 61%|██████▏   | 6016/9822 [3:05:47<1:47:59,  1.70s/it] 61%|██████▏   | 6017/9822 [3:05:48<1:47:31,  1.70s/it] 61%|██████▏   | 6018/9822 [3:05:50<1:47:03,  1.69s/it] 61%|██████▏   | 6019/9822 [3:05:52<1:46:46,  1.68s/it] 61%|██████▏   | 6020/9822 [3:05:53<1:45:28,  1.66s/it] 61%|██████▏   | 6021/9822 [3:05:55<1:45:17,  1.66s/it] 61%|██████▏   | 6022/9822 [3:05:57<1:45:14,  1.66s/it] 61%|██████▏   | 6023/9822 [3:05:58<1:45:21,  1.66s/it] 61%|██████▏   | 6024/9822 [3:06:00<1:45:08,  1.66s/it] 61%|██████▏   | 6025/9822 [3:06:02<1:45:14,  1.66s/it] 61%|██████▏   | 6026/9822 [3:06:03<1:45:12,  1.66s/it] 61%|██████▏   | 6027/9822 [3:06:05<1:45:17,  1.66s/it] 61%|██████▏   | 6028/9822 [3:06:07<1:45:33,  1.67s/it] 61%|██████▏   | 6029/9822 [3:06:08<1:45:30,  1.67s/it] 61%|██████▏   | 6030/9822 [3:06:10<1:45:36,  1.67s/it] 61%|██████▏   | 6031/9822 [3:06:12<1:45:27,  1.67s/it] 61%|██████▏   | 6032/9822 [3:06:13<1:45:31,  1.67s/it] 61%|██████▏   | 6033/9822 [3:06:15<1:45:27,  1.67s/it] 61%|██████▏   | 6034/9822 [3:06:17<1:45:31,  1.67s/it] 61%|██████▏   | 6035/9822 [3:06:18<1:45:16,  1.67s/it] 61%|██████▏   | 6036/9822 [3:06:20<1:45:10,  1.67s/it] 61%|██████▏   | 6037/9822 [3:06:22<1:44:56,  1.66s/it] 61%|██████▏   | 6038/9822 [3:06:23<1:44:54,  1.66s/it] 61%|██████▏   | 6039/9822 [3:06:25<1:46:48,  1.69s/it] 61%|██████▏   | 6040/9822 [3:06:27<1:46:22,  1.69s/it] 62%|██████▏   | 6041/9822 [3:06:29<1:46:20,  1.69s/it] 62%|██████▏   | 6042/9822 [3:06:30<1:45:55,  1.68s/it] 62%|██████▏   | 6043/9822 [3:06:32<1:45:30,  1.68s/it] 62%|██████▏   | 6044/9822 [3:06:34<1:45:15,  1.67s/it] 62%|██████▏   | 6045/9822 [3:06:35<1:45:05,  1.67s/it] 62%|██████▏   | 6046/9822 [3:06:37<1:44:53,  1.67s/it] 62%|██████▏   | 6047/9822 [3:06:39<1:45:02,  1.67s/it] 62%|██████▏   | 6048/9822 [3:06:40<1:44:47,  1.67s/it] 62%|██████▏   | 6049/9822 [3:06:42<1:44:34,  1.66s/it] 62%|██████▏   | 6050/9822 [3:06:43<1:44:33,  1.66s/it] 62%|██████▏   | 6051/9822 [3:06:45<1:44:22,  1.66s/it] 62%|██████▏   | 6052/9822 [3:06:47<1:44:19,  1.66s/it] 62%|██████▏   | 6053/9822 [3:06:48<1:44:11,  1.66s/it] 62%|██████▏   | 6054/9822 [3:06:50<1:44:08,  1.66s/it] 62%|██████▏   | 6055/9822 [3:06:52<1:44:18,  1.66s/it] 62%|██████▏   | 6056/9822 [3:06:53<1:44:22,  1.66s/it] 62%|██████▏   | 6057/9822 [3:06:55<1:44:23,  1.66s/it] 62%|██████▏   | 6058/9822 [3:06:57<1:44:22,  1.66s/it] 62%|██████▏   | 6059/9822 [3:06:58<1:44:19,  1.66s/it] 62%|██████▏   | 6060/9822 [3:07:00<1:44:07,  1.66s/it] 62%|██████▏   | 6061/9822 [3:07:02<1:44:03,  1.66s/it] 62%|██████▏   | 6062/9822 [3:07:03<1:44:03,  1.66s/it] 62%|██████▏   | 6063/9822 [3:07:05<1:44:01,  1.66s/it] 62%|██████▏   | 6064/9822 [3:07:07<1:43:59,  1.66s/it] 62%|██████▏   | 6065/9822 [3:07:08<1:44:08,  1.66s/it] 62%|██████▏   | 6066/9822 [3:07:10<1:44:03,  1.66s/it] 62%|██████▏   | 6067/9822 [3:07:12<1:44:16,  1.67s/it] 62%|██████▏   | 6068/9822 [3:07:13<1:44:18,  1.67s/it] 62%|██████▏   | 6069/9822 [3:07:15<1:44:16,  1.67s/it] 62%|██████▏   | 6070/9822 [3:07:17<1:44:16,  1.67s/it] 62%|██████▏   | 6071/9822 [3:07:18<1:44:05,  1.67s/it] 62%|██████▏   | 6072/9822 [3:07:20<1:46:01,  1.70s/it] 62%|██████▏   | 6073/9822 [3:07:22<1:45:43,  1.69s/it] 62%|██████▏   | 6074/9822 [3:07:24<1:45:25,  1.69s/it] 62%|██████▏   | 6075/9822 [3:07:25<1:45:06,  1.68s/it] 62%|██████▏   | 6076/9822 [3:07:27<1:44:46,  1.68s/it] 62%|██████▏   | 6077/9822 [3:07:29<1:44:32,  1.67s/it] 62%|██████▏   | 6078/9822 [3:07:30<1:44:37,  1.68s/it] 62%|██████▏   | 6079/9822 [3:07:32<1:44:34,  1.68s/it] 62%|██████▏   | 6080/9822 [3:07:34<1:44:17,  1.67s/it] 62%|██████▏   | 6081/9822 [3:07:35<1:44:10,  1.67s/it] 62%|██████▏   | 6082/9822 [3:07:37<1:43:54,  1.67s/it] 62%|██████▏   | 6083/9822 [3:07:39<1:43:48,  1.67s/it] 62%|██████▏   | 6084/9822 [3:07:40<1:43:50,  1.67s/it] 62%|██████▏   | 6085/9822 [3:07:42<1:43:40,  1.66s/it] 62%|██████▏   | 6086/9822 [3:07:44<1:43:39,  1.66s/it] 62%|██████▏   | 6087/9822 [3:07:45<1:43:40,  1.67s/it] 62%|██████▏   | 6088/9822 [3:07:47<1:43:37,  1.67s/it] 62%|██████▏   | 6089/9822 [3:07:49<1:43:41,  1.67s/it] 62%|██████▏   | 6090/9822 [3:07:50<1:43:52,  1.67s/it] 62%|██████▏   | 6091/9822 [3:07:52<1:43:48,  1.67s/it] 62%|██████▏   | 6092/9822 [3:07:54<1:44:06,  1.67s/it] 62%|██████▏   | 6093/9822 [3:07:55<1:44:16,  1.68s/it] 62%|██████▏   | 6094/9822 [3:07:57<1:46:22,  1.71s/it] 62%|██████▏   | 6095/9822 [3:07:59<1:45:49,  1.70s/it] 62%|██████▏   | 6096/9822 [3:08:00<1:45:25,  1.70s/it] 62%|██████▏   | 6097/9822 [3:08:02<1:45:08,  1.69s/it] 62%|██████▏   | 6098/9822 [3:08:04<1:44:32,  1.68s/it] 62%|██████▏   | 6099/9822 [3:08:05<1:44:18,  1.68s/it] 62%|██████▏   | 6100/9822 [3:08:07<1:43:57,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:48:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:48:54 - INFO - __main__ - ***** test Results*****
04/29/2024 14:48:54 - INFO - __main__ -   Training step = 6100
04/29/2024 14:48:54 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:48:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:48:58 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:48:58 - INFO - __main__ -   Training step = 6100
04/29/2024 14:48:58 - INFO - __main__ -  eval_accuracy:0.8531673379714391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:49:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:49:07 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:49:07 - INFO - __main__ -   Training step = 6100
04/29/2024 14:49:07 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 62%|██████▏   | 6101/9822 [3:08:26<7:10:18,  6.94s/it] 62%|██████▏   | 6102/9822 [3:08:28<5:32:01,  5.36s/it] 62%|██████▏   | 6103/9822 [3:08:30<4:23:19,  4.25s/it] 62%|██████▏   | 6104/9822 [3:08:31<3:35:08,  3.47s/it] 62%|██████▏   | 6105/9822 [3:08:33<3:01:21,  2.93s/it] 62%|██████▏   | 6106/9822 [3:08:35<2:37:03,  2.54s/it] 62%|██████▏   | 6107/9822 [3:08:36<2:20:47,  2.27s/it] 62%|██████▏   | 6108/9822 [3:08:38<2:09:22,  2.09s/it] 62%|██████▏   | 6109/9822 [3:08:40<2:01:22,  1.96s/it] 62%|██████▏   | 6110/9822 [3:08:41<1:55:57,  1.87s/it] 62%|██████▏   | 6111/9822 [3:08:43<1:51:57,  1.81s/it] 62%|██████▏   | 6112/9822 [3:08:45<1:49:09,  1.77s/it] 62%|██████▏   | 6113/9822 [3:08:46<1:47:18,  1.74s/it] 62%|██████▏   | 6114/9822 [3:08:48<1:45:57,  1.71s/it] 62%|██████▏   | 6115/9822 [3:08:50<1:44:54,  1.70s/it] 62%|██████▏   | 6116/9822 [3:08:51<1:44:16,  1.69s/it] 62%|██████▏   | 6117/9822 [3:08:53<1:45:36,  1.71s/it] 62%|██████▏   | 6118/9822 [3:08:55<1:44:49,  1.70s/it] 62%|██████▏   | 6119/9822 [3:08:56<1:44:01,  1.69s/it] 62%|██████▏   | 6120/9822 [3:08:58<1:43:46,  1.68s/it] 62%|██████▏   | 6121/9822 [3:09:00<1:43:19,  1.68s/it] 62%|██████▏   | 6122/9822 [3:09:01<1:43:06,  1.67s/it] 62%|██████▏   | 6123/9822 [3:09:03<1:42:58,  1.67s/it] 62%|██████▏   | 6124/9822 [3:09:05<1:42:39,  1.67s/it] 62%|██████▏   | 6125/9822 [3:09:06<1:42:30,  1.66s/it] 62%|██████▏   | 6126/9822 [3:09:08<1:42:27,  1.66s/it] 62%|██████▏   | 6127/9822 [3:09:10<1:42:13,  1.66s/it] 62%|██████▏   | 6128/9822 [3:09:11<1:42:12,  1.66s/it] 62%|██████▏   | 6129/9822 [3:09:13<1:42:11,  1.66s/it] 62%|██████▏   | 6130/9822 [3:09:15<1:42:02,  1.66s/it] 62%|██████▏   | 6131/9822 [3:09:16<1:42:15,  1.66s/it] 62%|██████▏   | 6132/9822 [3:09:18<1:42:14,  1.66s/it] 62%|██████▏   | 6133/9822 [3:09:20<1:42:15,  1.66s/it] 62%|██████▏   | 6134/9822 [3:09:21<1:42:11,  1.66s/it] 62%|██████▏   | 6135/9822 [3:09:23<1:42:09,  1.66s/it] 62%|██████▏   | 6136/9822 [3:09:25<1:42:11,  1.66s/it] 62%|██████▏   | 6137/9822 [3:09:26<1:42:12,  1.66s/it] 62%|██████▏   | 6138/9822 [3:09:28<1:42:33,  1.67s/it] 63%|██████▎   | 6139/9822 [3:09:30<1:42:33,  1.67s/it] 63%|██████▎   | 6140/9822 [3:09:31<1:42:25,  1.67s/it] 63%|██████▎   | 6141/9822 [3:09:33<1:42:12,  1.67s/it] 63%|██████▎   | 6142/9822 [3:09:35<1:41:57,  1.66s/it] 63%|██████▎   | 6143/9822 [3:09:36<1:42:01,  1.66s/it] 63%|██████▎   | 6144/9822 [3:09:38<1:43:44,  1.69s/it] 63%|██████▎   | 6145/9822 [3:09:40<1:43:08,  1.68s/it] 63%|██████▎   | 6146/9822 [3:09:41<1:42:54,  1.68s/it] 63%|██████▎   | 6147/9822 [3:09:43<1:42:34,  1.67s/it] 63%|██████▎   | 6148/9822 [3:09:45<1:42:16,  1.67s/it] 63%|██████▎   | 6149/9822 [3:09:46<1:42:00,  1.67s/it] 63%|██████▎   | 6150/9822 [3:09:48<1:41:46,  1.66s/it] 63%|██████▎   | 6151/9822 [3:09:50<1:41:34,  1.66s/it] 63%|██████▎   | 6152/9822 [3:09:51<1:41:36,  1.66s/it] 63%|██████▎   | 6153/9822 [3:09:53<1:41:33,  1.66s/it] 63%|██████▎   | 6154/9822 [3:09:55<1:41:25,  1.66s/it] 63%|██████▎   | 6155/9822 [3:09:56<1:41:28,  1.66s/it] 63%|██████▎   | 6156/9822 [3:09:58<1:41:28,  1.66s/it] 63%|██████▎   | 6157/9822 [3:10:00<1:41:29,  1.66s/it] 63%|██████▎   | 6158/9822 [3:10:01<1:41:36,  1.66s/it] 63%|██████▎   | 6159/9822 [3:10:03<1:42:02,  1.67s/it] 63%|██████▎   | 6160/9822 [3:10:05<1:42:12,  1.67s/it] 63%|██████▎   | 6161/9822 [3:10:06<1:41:55,  1.67s/it] 63%|██████▎   | 6162/9822 [3:10:08<1:41:42,  1.67s/it] 63%|██████▎   | 6163/9822 [3:10:10<1:41:23,  1.66s/it] 63%|██████▎   | 6164/9822 [3:10:11<1:41:28,  1.66s/it] 63%|██████▎   | 6165/9822 [3:10:13<1:41:30,  1.67s/it] 63%|██████▎   | 6166/9822 [3:10:15<1:41:33,  1.67s/it] 63%|██████▎   | 6167/9822 [3:10:16<1:41:28,  1.67s/it] 63%|██████▎   | 6168/9822 [3:10:18<1:41:20,  1.66s/it] 63%|██████▎   | 6169/9822 [3:10:20<1:41:28,  1.67s/it] 63%|██████▎   | 6170/9822 [3:10:21<1:41:21,  1.67s/it] 63%|██████▎   | 6171/9822 [3:10:23<1:41:17,  1.66s/it] 63%|██████▎   | 6172/9822 [3:10:25<1:41:31,  1.67s/it] 63%|██████▎   | 6173/9822 [3:10:26<1:41:25,  1.67s/it] 63%|██████▎   | 6174/9822 [3:10:28<1:41:21,  1.67s/it] 63%|██████▎   | 6175/9822 [3:10:30<1:41:29,  1.67s/it] 63%|██████▎   | 6176/9822 [3:10:31<1:41:29,  1.67s/it] 63%|██████▎   | 6177/9822 [3:10:33<1:43:20,  1.70s/it] 63%|██████▎   | 6178/9822 [3:10:35<1:42:43,  1.69s/it] 63%|██████▎   | 6179/9822 [3:10:36<1:42:18,  1.68s/it] 63%|██████▎   | 6180/9822 [3:10:38<1:42:09,  1.68s/it] 63%|██████▎   | 6181/9822 [3:10:40<1:41:45,  1.68s/it] 63%|██████▎   | 6182/9822 [3:10:41<1:41:44,  1.68s/it] 63%|██████▎   | 6183/9822 [3:10:43<1:41:32,  1.67s/it] 63%|██████▎   | 6184/9822 [3:10:45<1:41:20,  1.67s/it] 63%|██████▎   | 6185/9822 [3:10:46<1:41:15,  1.67s/it] 63%|██████▎   | 6186/9822 [3:10:48<1:41:24,  1.67s/it] 63%|██████▎   | 6187/9822 [3:10:50<1:41:11,  1.67s/it] 63%|██████▎   | 6188/9822 [3:10:51<1:41:07,  1.67s/it] 63%|██████▎   | 6189/9822 [3:10:53<1:40:59,  1.67s/it] 63%|██████▎   | 6190/9822 [3:10:55<1:40:54,  1.67s/it] 63%|██████▎   | 6191/9822 [3:10:56<1:40:52,  1.67s/it] 63%|██████▎   | 6192/9822 [3:10:58<1:39:50,  1.65s/it] 63%|██████▎   | 6193/9822 [3:11:00<1:39:56,  1.65s/it] 63%|██████▎   | 6194/9822 [3:11:01<1:40:28,  1.66s/it] 63%|██████▎   | 6195/9822 [3:11:03<1:40:36,  1.66s/it] 63%|██████▎   | 6196/9822 [3:11:05<1:40:38,  1.67s/it] 63%|██████▎   | 6197/9822 [3:11:06<1:40:42,  1.67s/it] 63%|██████▎   | 6198/9822 [3:11:08<1:40:48,  1.67s/it] 63%|██████▎   | 6199/9822 [3:11:10<1:42:58,  1.71s/it] 63%|██████▎   | 6200/9822 [3:11:12<1:42:29,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:51:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:51:58 - INFO - __main__ - ***** test Results*****
04/29/2024 14:51:58 - INFO - __main__ -   Training step = 6200
04/29/2024 14:51:58 - INFO - __main__ -  test_accuracy:0.8715226939970717 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:52:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:52:03 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:52:03 - INFO - __main__ -   Training step = 6200
04/29/2024 14:52:03 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:52:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:52:11 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:52:11 - INFO - __main__ -   Training step = 6200
04/29/2024 14:52:11 - INFO - __main__ -  eval_accuracy:0.9088246063712926 
 63%|██████▎   | 6201/9822 [3:11:31<6:59:55,  6.96s/it] 63%|██████▎   | 6202/9822 [3:11:32<5:24:10,  5.37s/it] 63%|██████▎   | 6203/9822 [3:11:34<4:17:27,  4.27s/it] 63%|██████▎   | 6204/9822 [3:11:36<3:30:32,  3.49s/it] 63%|██████▎   | 6205/9822 [3:11:37<2:57:37,  2.95s/it] 63%|██████▎   | 6206/9822 [3:11:39<2:34:31,  2.56s/it] 63%|██████▎   | 6207/9822 [3:11:41<2:18:15,  2.29s/it] 63%|██████▎   | 6208/9822 [3:11:42<2:06:53,  2.11s/it] 63%|██████▎   | 6209/9822 [3:11:44<1:59:11,  1.98s/it] 63%|██████▎   | 6210/9822 [3:11:46<1:53:36,  1.89s/it] 63%|██████▎   | 6211/9822 [3:11:48<1:49:49,  1.82s/it] 63%|██████▎   | 6212/9822 [3:11:49<1:47:03,  1.78s/it] 63%|██████▎   | 6213/9822 [3:11:51<1:45:00,  1.75s/it] 63%|██████▎   | 6214/9822 [3:11:53<1:43:39,  1.72s/it] 63%|██████▎   | 6215/9822 [3:11:54<1:42:48,  1.71s/it] 63%|██████▎   | 6216/9822 [3:11:56<1:41:52,  1.70s/it] 63%|██████▎   | 6217/9822 [3:11:58<1:41:21,  1.69s/it] 63%|██████▎   | 6218/9822 [3:11:59<1:41:05,  1.68s/it] 63%|██████▎   | 6219/9822 [3:12:01<1:40:51,  1.68s/it] 63%|██████▎   | 6220/9822 [3:12:03<1:40:35,  1.68s/it] 63%|██████▎   | 6221/9822 [3:12:04<1:40:31,  1.67s/it] 63%|██████▎   | 6222/9822 [3:12:06<1:40:17,  1.67s/it] 63%|██████▎   | 6223/9822 [3:12:08<1:42:04,  1.70s/it] 63%|██████▎   | 6224/9822 [3:12:09<1:41:12,  1.69s/it] 63%|██████▎   | 6225/9822 [3:12:11<1:40:42,  1.68s/it] 63%|██████▎   | 6226/9822 [3:12:13<1:40:27,  1.68s/it] 63%|██████▎   | 6227/9822 [3:12:14<1:40:08,  1.67s/it] 63%|██████▎   | 6228/9822 [3:12:16<1:40:05,  1.67s/it] 63%|██████▎   | 6229/9822 [3:12:18<1:40:15,  1.67s/it] 63%|██████▎   | 6230/9822 [3:12:19<1:39:57,  1.67s/it] 63%|██████▎   | 6231/9822 [3:12:21<1:39:48,  1.67s/it] 63%|██████▎   | 6232/9822 [3:12:23<1:39:36,  1.66s/it] 63%|██████▎   | 6233/9822 [3:12:24<1:39:29,  1.66s/it] 63%|██████▎   | 6234/9822 [3:12:26<1:39:27,  1.66s/it] 63%|██████▎   | 6235/9822 [3:12:28<1:39:28,  1.66s/it] 63%|██████▎   | 6236/9822 [3:12:29<1:39:17,  1.66s/it] 64%|██████▎   | 6237/9822 [3:12:31<1:39:28,  1.66s/it] 64%|██████▎   | 6238/9822 [3:12:33<1:39:41,  1.67s/it] 64%|██████▎   | 6239/9822 [3:12:34<1:39:38,  1.67s/it] 64%|██████▎   | 6240/9822 [3:12:36<1:39:37,  1.67s/it] 64%|██████▎   | 6241/9822 [3:12:38<1:39:51,  1.67s/it] 64%|██████▎   | 6242/9822 [3:12:39<1:39:53,  1.67s/it] 64%|██████▎   | 6243/9822 [3:12:41<1:39:48,  1.67s/it] 64%|██████▎   | 6244/9822 [3:12:43<1:40:03,  1.68s/it] 64%|██████▎   | 6245/9822 [3:12:44<1:39:54,  1.68s/it] 64%|██████▎   | 6246/9822 [3:12:46<1:39:54,  1.68s/it] 64%|██████▎   | 6247/9822 [3:12:48<1:39:56,  1.68s/it] 64%|██████▎   | 6248/9822 [3:12:49<1:39:50,  1.68s/it] 64%|██████▎   | 6249/9822 [3:12:51<1:41:47,  1.71s/it] 64%|██████▎   | 6250/9822 [3:12:53<1:41:01,  1.70s/it] 64%|██████▎   | 6251/9822 [3:12:54<1:40:27,  1.69s/it] 64%|██████▎   | 6252/9822 [3:12:56<1:40:05,  1.68s/it] 64%|██████▎   | 6253/9822 [3:12:58<1:39:45,  1.68s/it] 64%|██████▎   | 6254/9822 [3:12:59<1:39:30,  1.67s/it] 64%|██████▎   | 6255/9822 [3:13:01<1:39:17,  1.67s/it] 64%|██████▎   | 6256/9822 [3:13:03<1:39:09,  1.67s/it] 64%|██████▎   | 6257/9822 [3:13:05<1:39:18,  1.67s/it] 64%|██████▎   | 6258/9822 [3:13:06<1:39:26,  1.67s/it] 64%|██████▎   | 6259/9822 [3:13:08<1:39:18,  1.67s/it] 64%|██████▎   | 6260/9822 [3:13:10<1:39:07,  1.67s/it] 64%|██████▎   | 6261/9822 [3:13:11<1:39:08,  1.67s/it] 64%|██████▍   | 6262/9822 [3:13:13<1:39:04,  1.67s/it] 64%|██████▍   | 6263/9822 [3:13:15<1:39:11,  1.67s/it] 64%|██████▍   | 6264/9822 [3:13:16<1:39:12,  1.67s/it] 64%|██████▍   | 6265/9822 [3:13:18<1:39:25,  1.68s/it] 64%|██████▍   | 6266/9822 [3:13:20<1:39:26,  1.68s/it] 64%|██████▍   | 6267/9822 [3:13:21<1:39:28,  1.68s/it] 64%|██████▍   | 6268/9822 [3:13:23<1:39:35,  1.68s/it] 64%|██████▍   | 6269/9822 [3:13:25<1:39:36,  1.68s/it] 64%|██████▍   | 6270/9822 [3:13:26<1:39:25,  1.68s/it] 64%|██████▍   | 6271/9822 [3:13:28<1:39:27,  1.68s/it] 64%|██████▍   | 6272/9822 [3:13:30<1:39:29,  1.68s/it] 64%|██████▍   | 6273/9822 [3:13:31<1:39:12,  1.68s/it] 64%|██████▍   | 6274/9822 [3:13:33<1:39:05,  1.68s/it] 64%|██████▍   | 6275/9822 [3:13:35<1:38:57,  1.67s/it] 64%|██████▍   | 6276/9822 [3:13:36<1:38:48,  1.67s/it] 64%|██████▍   | 6277/9822 [3:13:38<1:38:33,  1.67s/it] 64%|██████▍   | 6278/9822 [3:13:40<1:37:42,  1.65s/it] 64%|██████▍   | 6279/9822 [3:13:41<1:37:52,  1.66s/it] 64%|██████▍   | 6280/9822 [3:13:43<1:38:10,  1.66s/it] 64%|██████▍   | 6281/9822 [3:13:45<1:38:19,  1.67s/it] 64%|██████▍   | 6282/9822 [3:13:46<1:40:15,  1.70s/it] 64%|██████▍   | 6283/9822 [3:13:48<1:39:38,  1.69s/it] 64%|██████▍   | 6284/9822 [3:13:50<1:39:10,  1.68s/it] 64%|██████▍   | 6285/9822 [3:13:51<1:38:46,  1.68s/it] 64%|██████▍   | 6286/9822 [3:13:53<1:38:54,  1.68s/it] 64%|██████▍   | 6287/9822 [3:13:55<1:38:57,  1.68s/it] 64%|██████▍   | 6288/9822 [3:13:56<1:38:48,  1.68s/it] 64%|██████▍   | 6289/9822 [3:13:58<1:38:36,  1.67s/it] 64%|██████▍   | 6290/9822 [3:14:00<1:38:32,  1.67s/it] 64%|██████▍   | 6291/9822 [3:14:01<1:38:27,  1.67s/it] 64%|██████▍   | 6292/9822 [3:14:03<1:38:10,  1.67s/it] 64%|██████▍   | 6293/9822 [3:14:05<1:38:10,  1.67s/it] 64%|██████▍   | 6294/9822 [3:14:06<1:38:08,  1.67s/it] 64%|██████▍   | 6295/9822 [3:14:08<1:37:56,  1.67s/it] 64%|██████▍   | 6296/9822 [3:14:10<1:37:53,  1.67s/it] 64%|██████▍   | 6297/9822 [3:14:11<1:38:10,  1.67s/it] 64%|██████▍   | 6298/9822 [3:14:13<1:38:08,  1.67s/it] 64%|██████▍   | 6299/9822 [3:14:15<1:37:59,  1.67s/it] 64%|██████▍   | 6300/9822 [3:14:16<1:38:03,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:55:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:55:03 - INFO - __main__ - ***** test Results*****
04/29/2024 14:55:03 - INFO - __main__ -   Training step = 6300
04/29/2024 14:55:03 - INFO - __main__ -  test_accuracy:0.8707906295754027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:55:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:55:08 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:55:08 - INFO - __main__ -   Training step = 6300
04/29/2024 14:55:08 - INFO - __main__ -  eval_accuracy:0.8484071768582937 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:55:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:55:16 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:55:16 - INFO - __main__ -   Training step = 6300
04/29/2024 14:55:16 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 64%|██████▍   | 6301/9822 [3:14:36<6:46:57,  6.93s/it] 64%|██████▍   | 6302/9822 [3:14:37<5:14:05,  5.35s/it] 64%|██████▍   | 6303/9822 [3:14:39<4:09:04,  4.25s/it] 64%|██████▍   | 6304/9822 [3:14:41<3:23:37,  3.47s/it] 64%|██████▍   | 6305/9822 [3:14:42<2:51:51,  2.93s/it] 64%|██████▍   | 6306/9822 [3:14:44<2:31:24,  2.58s/it] 64%|██████▍   | 6307/9822 [3:14:46<2:15:22,  2.31s/it] 64%|██████▍   | 6308/9822 [3:14:47<2:04:08,  2.12s/it] 64%|██████▍   | 6309/9822 [3:14:49<1:56:15,  1.99s/it] 64%|██████▍   | 6310/9822 [3:14:51<1:50:37,  1.89s/it] 64%|██████▍   | 6311/9822 [3:14:52<1:46:42,  1.82s/it] 64%|██████▍   | 6312/9822 [3:14:54<1:44:05,  1.78s/it] 64%|██████▍   | 6313/9822 [3:14:56<1:42:03,  1.74s/it] 64%|██████▍   | 6314/9822 [3:14:57<1:40:45,  1.72s/it] 64%|██████▍   | 6315/9822 [3:14:59<1:39:45,  1.71s/it] 64%|██████▍   | 6316/9822 [3:15:01<1:38:52,  1.69s/it] 64%|██████▍   | 6317/9822 [3:15:02<1:38:17,  1.68s/it] 64%|██████▍   | 6318/9822 [3:15:04<1:37:50,  1.68s/it] 64%|██████▍   | 6319/9822 [3:15:06<1:37:33,  1.67s/it] 64%|██████▍   | 6320/9822 [3:15:07<1:37:37,  1.67s/it] 64%|██████▍   | 6321/9822 [3:15:09<1:37:30,  1.67s/it] 64%|██████▍   | 6322/9822 [3:15:11<1:37:26,  1.67s/it] 64%|██████▍   | 6323/9822 [3:15:12<1:37:21,  1.67s/it] 64%|██████▍   | 6324/9822 [3:15:14<1:37:11,  1.67s/it] 64%|██████▍   | 6325/9822 [3:15:16<1:36:58,  1.66s/it] 64%|██████▍   | 6326/9822 [3:15:17<1:37:02,  1.67s/it] 64%|██████▍   | 6327/9822 [3:15:19<1:36:57,  1.66s/it] 64%|██████▍   | 6328/9822 [3:15:21<1:37:05,  1.67s/it] 64%|██████▍   | 6329/9822 [3:15:22<1:36:58,  1.67s/it] 64%|██████▍   | 6330/9822 [3:15:24<1:36:46,  1.66s/it] 64%|██████▍   | 6331/9822 [3:15:26<1:36:48,  1.66s/it] 64%|██████▍   | 6332/9822 [3:15:27<1:36:57,  1.67s/it] 64%|██████▍   | 6333/9822 [3:15:29<1:38:40,  1.70s/it] 64%|██████▍   | 6334/9822 [3:15:31<1:38:03,  1.69s/it] 64%|██████▍   | 6335/9822 [3:15:33<1:37:38,  1.68s/it] 65%|██████▍   | 6336/9822 [3:15:34<1:37:19,  1.68s/it] 65%|██████▍   | 6337/9822 [3:15:36<1:37:09,  1.67s/it] 65%|██████▍   | 6338/9822 [3:15:38<1:36:57,  1.67s/it] 65%|██████▍   | 6339/9822 [3:15:39<1:36:47,  1.67s/it] 65%|██████▍   | 6340/9822 [3:15:41<1:36:56,  1.67s/it] 65%|██████▍   | 6341/9822 [3:15:43<1:36:45,  1.67s/it] 65%|██████▍   | 6342/9822 [3:15:44<1:36:40,  1.67s/it] 65%|██████▍   | 6343/9822 [3:15:46<1:36:30,  1.66s/it] 65%|██████▍   | 6344/9822 [3:15:48<1:36:34,  1.67s/it] 65%|██████▍   | 6345/9822 [3:15:49<1:36:43,  1.67s/it] 65%|██████▍   | 6346/9822 [3:15:51<1:36:41,  1.67s/it] 65%|██████▍   | 6347/9822 [3:15:53<1:36:40,  1.67s/it] 65%|██████▍   | 6348/9822 [3:15:54<1:36:32,  1.67s/it] 65%|██████▍   | 6349/9822 [3:15:56<1:36:35,  1.67s/it] 65%|██████▍   | 6350/9822 [3:15:58<1:36:28,  1.67s/it] 65%|██████▍   | 6351/9822 [3:15:59<1:36:17,  1.66s/it] 65%|██████▍   | 6352/9822 [3:16:01<1:36:23,  1.67s/it] 65%|██████▍   | 6353/9822 [3:16:03<1:36:17,  1.67s/it] 65%|██████▍   | 6354/9822 [3:16:04<1:36:18,  1.67s/it] 65%|██████▍   | 6355/9822 [3:16:06<1:36:37,  1.67s/it] 65%|██████▍   | 6356/9822 [3:16:08<1:36:51,  1.68s/it] 65%|██████▍   | 6357/9822 [3:16:09<1:36:57,  1.68s/it] 65%|██████▍   | 6358/9822 [3:16:11<1:37:02,  1.68s/it] 65%|██████▍   | 6359/9822 [3:16:13<1:36:56,  1.68s/it] 65%|██████▍   | 6360/9822 [3:16:14<1:38:30,  1.71s/it] 65%|██████▍   | 6361/9822 [3:16:16<1:37:41,  1.69s/it] 65%|██████▍   | 6362/9822 [3:16:18<1:37:13,  1.69s/it] 65%|██████▍   | 6363/9822 [3:16:19<1:36:51,  1.68s/it] 65%|██████▍   | 6364/9822 [3:16:21<1:35:53,  1.66s/it] 65%|██████▍   | 6365/9822 [3:16:23<1:36:08,  1.67s/it] 65%|██████▍   | 6366/9822 [3:16:24<1:36:11,  1.67s/it] 65%|██████▍   | 6367/9822 [3:16:26<1:37:00,  1.68s/it] 65%|██████▍   | 6368/9822 [3:16:28<1:36:41,  1.68s/it] 65%|██████▍   | 6369/9822 [3:16:29<1:36:22,  1.67s/it] 65%|██████▍   | 6370/9822 [3:16:31<1:36:06,  1.67s/it] 65%|██████▍   | 6371/9822 [3:16:33<1:36:06,  1.67s/it] 65%|██████▍   | 6372/9822 [3:16:34<1:36:00,  1.67s/it] 65%|██████▍   | 6373/9822 [3:16:36<1:35:54,  1.67s/it] 65%|██████▍   | 6374/9822 [3:16:38<1:35:47,  1.67s/it] 65%|██████▍   | 6375/9822 [3:16:39<1:35:42,  1.67s/it] 65%|██████▍   | 6376/9822 [3:16:41<1:35:46,  1.67s/it] 65%|██████▍   | 6377/9822 [3:16:43<1:35:45,  1.67s/it] 65%|██████▍   | 6378/9822 [3:16:44<1:35:39,  1.67s/it] 65%|██████▍   | 6379/9822 [3:16:46<1:35:33,  1.67s/it] 65%|██████▍   | 6380/9822 [3:16:48<1:35:25,  1.66s/it] 65%|██████▍   | 6381/9822 [3:16:49<1:35:25,  1.66s/it] 65%|██████▍   | 6382/9822 [3:16:51<1:35:23,  1.66s/it] 65%|██████▍   | 6383/9822 [3:16:53<1:35:21,  1.66s/it] 65%|██████▍   | 6384/9822 [3:16:54<1:35:17,  1.66s/it] 65%|██████▌   | 6385/9822 [3:16:56<1:35:16,  1.66s/it] 65%|██████▌   | 6386/9822 [3:16:58<1:35:13,  1.66s/it] 65%|██████▌   | 6387/9822 [3:16:59<1:35:17,  1.66s/it] 65%|██████▌   | 6388/9822 [3:17:01<1:35:24,  1.67s/it] 65%|██████▌   | 6389/9822 [3:17:03<1:35:19,  1.67s/it] 65%|██████▌   | 6390/9822 [3:17:04<1:35:19,  1.67s/it] 65%|██████▌   | 6391/9822 [3:17:06<1:35:21,  1.67s/it] 65%|██████▌   | 6392/9822 [3:17:08<1:35:22,  1.67s/it] 65%|██████▌   | 6393/9822 [3:17:10<1:37:02,  1.70s/it] 65%|██████▌   | 6394/9822 [3:17:11<1:36:40,  1.69s/it] 65%|██████▌   | 6395/9822 [3:17:13<1:36:17,  1.69s/it] 65%|██████▌   | 6396/9822 [3:17:15<1:35:53,  1.68s/it] 65%|██████▌   | 6397/9822 [3:17:16<1:35:30,  1.67s/it] 65%|██████▌   | 6398/9822 [3:17:18<1:35:23,  1.67s/it] 65%|██████▌   | 6399/9822 [3:17:20<1:35:21,  1.67s/it] 65%|██████▌   | 6400/9822 [3:17:21<1:35:30,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 14:58:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:58:08 - INFO - __main__ - ***** test Results*****
04/29/2024 14:58:08 - INFO - __main__ -   Training step = 6400
04/29/2024 14:58:08 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:58:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:58:12 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 14:58:12 - INFO - __main__ -   Training step = 6400
04/29/2024 14:58:12 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 14:58:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 14:58:21 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 14:58:21 - INFO - __main__ -   Training step = 6400
04/29/2024 14:58:21 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 65%|██████▌   | 6401/9822 [3:17:40<6:35:49,  6.94s/it] 65%|██████▌   | 6402/9822 [3:17:42<5:05:38,  5.36s/it] 65%|██████▌   | 6403/9822 [3:17:44<4:02:26,  4.25s/it] 65%|██████▌   | 6404/9822 [3:17:45<3:18:04,  3.48s/it] 65%|██████▌   | 6405/9822 [3:17:47<2:46:54,  2.93s/it] 65%|██████▌   | 6406/9822 [3:17:49<2:25:19,  2.55s/it] 65%|██████▌   | 6407/9822 [3:17:50<2:10:04,  2.29s/it] 65%|██████▌   | 6408/9822 [3:17:52<1:59:24,  2.10s/it] 65%|██████▌   | 6409/9822 [3:17:54<1:51:59,  1.97s/it] 65%|██████▌   | 6410/9822 [3:17:55<1:46:40,  1.88s/it] 65%|██████▌   | 6411/9822 [3:17:57<1:43:09,  1.81s/it] 65%|██████▌   | 6412/9822 [3:17:59<1:40:33,  1.77s/it] 65%|██████▌   | 6413/9822 [3:18:00<1:38:43,  1.74s/it] 65%|██████▌   | 6414/9822 [3:18:02<1:37:17,  1.71s/it] 65%|██████▌   | 6415/9822 [3:18:04<1:36:23,  1.70s/it] 65%|██████▌   | 6416/9822 [3:18:05<1:35:48,  1.69s/it] 65%|██████▌   | 6417/9822 [3:18:07<1:35:18,  1.68s/it] 65%|██████▌   | 6418/9822 [3:18:09<1:35:15,  1.68s/it] 65%|██████▌   | 6419/9822 [3:18:10<1:35:06,  1.68s/it] 65%|██████▌   | 6420/9822 [3:18:12<1:34:45,  1.67s/it] 65%|██████▌   | 6421/9822 [3:18:14<1:34:39,  1.67s/it] 65%|██████▌   | 6422/9822 [3:18:15<1:34:44,  1.67s/it] 65%|██████▌   | 6423/9822 [3:18:17<1:34:40,  1.67s/it] 65%|██████▌   | 6424/9822 [3:18:19<1:34:27,  1.67s/it] 65%|██████▌   | 6425/9822 [3:18:20<1:34:25,  1.67s/it] 65%|██████▌   | 6426/9822 [3:18:22<1:34:20,  1.67s/it] 65%|██████▌   | 6427/9822 [3:18:24<1:34:21,  1.67s/it] 65%|██████▌   | 6428/9822 [3:18:25<1:34:29,  1.67s/it] 65%|██████▌   | 6429/9822 [3:18:27<1:36:04,  1.70s/it] 65%|██████▌   | 6430/9822 [3:18:29<1:35:26,  1.69s/it] 65%|██████▌   | 6431/9822 [3:18:31<1:35:17,  1.69s/it] 65%|██████▌   | 6432/9822 [3:18:32<1:34:58,  1.68s/it] 65%|██████▌   | 6433/9822 [3:18:34<1:34:41,  1.68s/it] 66%|██████▌   | 6434/9822 [3:18:36<1:34:37,  1.68s/it] 66%|██████▌   | 6435/9822 [3:18:37<1:34:27,  1.67s/it] 66%|██████▌   | 6436/9822 [3:18:39<1:34:30,  1.67s/it] 66%|██████▌   | 6437/9822 [3:18:41<1:34:28,  1.67s/it] 66%|██████▌   | 6438/9822 [3:18:42<1:34:19,  1.67s/it] 66%|██████▌   | 6439/9822 [3:18:44<1:34:28,  1.68s/it] 66%|██████▌   | 6440/9822 [3:18:46<1:34:24,  1.67s/it] 66%|██████▌   | 6441/9822 [3:18:47<1:34:19,  1.67s/it] 66%|██████▌   | 6442/9822 [3:18:49<1:34:04,  1.67s/it] 66%|██████▌   | 6443/9822 [3:18:51<1:34:20,  1.68s/it] 66%|██████▌   | 6444/9822 [3:18:52<1:34:20,  1.68s/it] 66%|██████▌   | 6445/9822 [3:18:54<1:34:17,  1.68s/it] 66%|██████▌   | 6446/9822 [3:18:56<1:34:12,  1.67s/it] 66%|██████▌   | 6447/9822 [3:18:57<1:34:04,  1.67s/it] 66%|██████▌   | 6448/9822 [3:18:59<1:33:50,  1.67s/it] 66%|██████▌   | 6449/9822 [3:19:01<1:33:45,  1.67s/it] 66%|██████▌   | 6450/9822 [3:19:02<1:33:03,  1.66s/it] 66%|██████▌   | 6451/9822 [3:19:04<1:33:14,  1.66s/it] 66%|██████▌   | 6452/9822 [3:19:06<1:33:39,  1.67s/it] 66%|██████▌   | 6453/9822 [3:19:07<1:33:42,  1.67s/it] 66%|██████▌   | 6454/9822 [3:19:09<1:33:43,  1.67s/it] 66%|██████▌   | 6455/9822 [3:19:11<1:33:36,  1.67s/it] 66%|██████▌   | 6456/9822 [3:19:12<1:35:34,  1.70s/it] 66%|██████▌   | 6457/9822 [3:19:14<1:35:06,  1.70s/it] 66%|██████▌   | 6458/9822 [3:19:16<1:34:31,  1.69s/it] 66%|██████▌   | 6459/9822 [3:19:17<1:34:18,  1.68s/it] 66%|██████▌   | 6460/9822 [3:19:19<1:34:23,  1.68s/it] 66%|██████▌   | 6461/9822 [3:19:21<1:34:10,  1.68s/it] 66%|██████▌   | 6462/9822 [3:19:22<1:34:15,  1.68s/it] 66%|██████▌   | 6463/9822 [3:19:24<1:34:03,  1.68s/it] 66%|██████▌   | 6464/9822 [3:19:26<1:33:52,  1.68s/it] 66%|██████▌   | 6465/9822 [3:19:27<1:33:40,  1.67s/it] 66%|██████▌   | 6466/9822 [3:19:29<1:33:36,  1.67s/it] 66%|██████▌   | 6467/9822 [3:19:31<1:33:34,  1.67s/it] 66%|██████▌   | 6468/9822 [3:19:33<1:33:48,  1.68s/it] 66%|██████▌   | 6469/9822 [3:19:34<1:33:51,  1.68s/it] 66%|██████▌   | 6470/9822 [3:19:36<1:33:36,  1.68s/it] 66%|██████▌   | 6471/9822 [3:19:38<1:33:41,  1.68s/it] 66%|██████▌   | 6472/9822 [3:19:39<1:33:29,  1.67s/it] 66%|██████▌   | 6473/9822 [3:19:41<1:33:32,  1.68s/it] 66%|██████▌   | 6474/9822 [3:19:43<1:33:16,  1.67s/it] 66%|██████▌   | 6475/9822 [3:19:44<1:33:11,  1.67s/it] 66%|██████▌   | 6476/9822 [3:19:46<1:33:14,  1.67s/it] 66%|██████▌   | 6477/9822 [3:19:48<1:33:12,  1.67s/it] 66%|██████▌   | 6478/9822 [3:19:49<1:33:12,  1.67s/it] 66%|██████▌   | 6479/9822 [3:19:51<1:33:09,  1.67s/it] 66%|██████▌   | 6480/9822 [3:19:53<1:33:12,  1.67s/it] 66%|██████▌   | 6481/9822 [3:19:54<1:33:13,  1.67s/it] 66%|██████▌   | 6482/9822 [3:19:56<1:33:13,  1.67s/it] 66%|██████▌   | 6483/9822 [3:19:58<1:34:51,  1.70s/it] 66%|██████▌   | 6484/9822 [3:19:59<1:34:18,  1.70s/it] 66%|██████▌   | 6485/9822 [3:20:01<1:33:50,  1.69s/it] 66%|██████▌   | 6486/9822 [3:20:03<1:33:15,  1.68s/it] 66%|██████▌   | 6487/9822 [3:20:04<1:33:06,  1.68s/it] 66%|██████▌   | 6488/9822 [3:20:06<1:32:59,  1.67s/it] 66%|██████▌   | 6489/9822 [3:20:08<1:32:52,  1.67s/it] 66%|██████▌   | 6490/9822 [3:20:09<1:32:46,  1.67s/it] 66%|██████▌   | 6491/9822 [3:20:11<1:32:35,  1.67s/it] 66%|██████▌   | 6492/9822 [3:20:13<1:32:29,  1.67s/it] 66%|██████▌   | 6493/9822 [3:20:14<1:32:33,  1.67s/it] 66%|██████▌   | 6494/9822 [3:20:16<1:32:27,  1.67s/it] 66%|██████▌   | 6495/9822 [3:20:18<1:32:32,  1.67s/it] 66%|██████▌   | 6496/9822 [3:20:19<1:32:41,  1.67s/it] 66%|██████▌   | 6497/9822 [3:20:21<1:32:26,  1.67s/it] 66%|██████▌   | 6498/9822 [3:20:23<1:32:13,  1.66s/it] 66%|██████▌   | 6499/9822 [3:20:24<1:32:10,  1.66s/it] 66%|██████▌   | 6500/9822 [3:20:26<1:32:14,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:01:13 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:01:13 - INFO - __main__ - ***** test Results*****
04/29/2024 15:01:13 - INFO - __main__ -   Training step = 6500
04/29/2024 15:01:13 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:01:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:01:17 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:01:17 - INFO - __main__ -   Training step = 6500
04/29/2024 15:01:17 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8696925329428989}
04/29/2024 15:01:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:01:26 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:01:26 - INFO - __main__ -   Training step = 6500
04/29/2024 15:01:26 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 66%|██████▌   | 6501/9822 [3:20:45<6:24:04,  6.94s/it] 66%|██████▌   | 6502/9822 [3:20:47<4:56:26,  5.36s/it] 66%|██████▌   | 6503/9822 [3:20:49<3:55:16,  4.25s/it] 66%|██████▌   | 6504/9822 [3:20:50<3:12:29,  3.48s/it] 66%|██████▌   | 6505/9822 [3:20:52<2:42:37,  2.94s/it] 66%|██████▌   | 6506/9822 [3:20:54<2:21:32,  2.56s/it] 66%|██████▌   | 6507/9822 [3:20:55<2:06:58,  2.30s/it] 66%|██████▋   | 6508/9822 [3:20:57<1:56:32,  2.11s/it] 66%|██████▋   | 6509/9822 [3:20:59<1:49:19,  1.98s/it] 66%|██████▋   | 6510/9822 [3:21:00<1:44:12,  1.89s/it] 66%|██████▋   | 6511/9822 [3:21:02<1:40:47,  1.83s/it] 66%|██████▋   | 6512/9822 [3:21:04<1:38:12,  1.78s/it] 66%|██████▋   | 6513/9822 [3:21:05<1:36:23,  1.75s/it] 66%|██████▋   | 6514/9822 [3:21:07<1:36:45,  1.76s/it] 66%|██████▋   | 6515/9822 [3:21:09<1:35:11,  1.73s/it] 66%|██████▋   | 6516/9822 [3:21:11<1:34:12,  1.71s/it] 66%|██████▋   | 6517/9822 [3:21:12<1:33:32,  1.70s/it] 66%|██████▋   | 6518/9822 [3:21:14<1:32:56,  1.69s/it] 66%|██████▋   | 6519/9822 [3:21:16<1:32:45,  1.68s/it] 66%|██████▋   | 6520/9822 [3:21:17<1:32:32,  1.68s/it] 66%|██████▋   | 6521/9822 [3:21:19<1:32:32,  1.68s/it] 66%|██████▋   | 6522/9822 [3:21:21<1:32:27,  1.68s/it] 66%|██████▋   | 6523/9822 [3:21:22<1:32:21,  1.68s/it] 66%|██████▋   | 6524/9822 [3:21:24<1:32:18,  1.68s/it] 66%|██████▋   | 6525/9822 [3:21:26<1:31:57,  1.67s/it] 66%|██████▋   | 6526/9822 [3:21:27<1:31:43,  1.67s/it] 66%|██████▋   | 6527/9822 [3:21:29<1:31:35,  1.67s/it] 66%|██████▋   | 6528/9822 [3:21:31<1:31:50,  1.67s/it] 66%|██████▋   | 6529/9822 [3:21:32<1:31:36,  1.67s/it] 66%|██████▋   | 6530/9822 [3:21:34<1:31:38,  1.67s/it] 66%|██████▋   | 6531/9822 [3:21:36<1:31:28,  1.67s/it] 67%|██████▋   | 6532/9822 [3:21:37<1:31:39,  1.67s/it] 67%|██████▋   | 6533/9822 [3:21:39<1:31:39,  1.67s/it] 67%|██████▋   | 6534/9822 [3:21:41<1:31:43,  1.67s/it] 67%|██████▋   | 6535/9822 [3:21:42<1:31:52,  1.68s/it] 67%|██████▋   | 6536/9822 [3:21:44<1:30:51,  1.66s/it] 67%|██████▋   | 6537/9822 [3:21:46<1:31:07,  1.66s/it] 67%|██████▋   | 6538/9822 [3:21:47<1:31:17,  1.67s/it] 67%|██████▋   | 6539/9822 [3:21:49<1:31:18,  1.67s/it] 67%|██████▋   | 6540/9822 [3:21:51<1:31:18,  1.67s/it] 67%|██████▋   | 6541/9822 [3:21:52<1:31:16,  1.67s/it] 67%|██████▋   | 6542/9822 [3:21:54<1:31:09,  1.67s/it] 67%|██████▋   | 6543/9822 [3:21:56<1:31:03,  1.67s/it] 67%|██████▋   | 6544/9822 [3:21:57<1:30:54,  1.66s/it] 67%|██████▋   | 6545/9822 [3:21:59<1:30:52,  1.66s/it] 67%|██████▋   | 6546/9822 [3:22:01<1:30:54,  1.67s/it] 67%|██████▋   | 6547/9822 [3:22:02<1:30:51,  1.66s/it] 67%|██████▋   | 6548/9822 [3:22:03<1:22:29,  1.51s/it] 67%|██████▋   | 6549/9822 [3:22:05<1:25:06,  1.56s/it] 67%|██████▋   | 6550/9822 [3:22:07<1:26:50,  1.59s/it] 67%|██████▋   | 6551/9822 [3:22:08<1:27:59,  1.61s/it] 67%|██████▋   | 6552/9822 [3:22:10<1:28:49,  1.63s/it] 67%|██████▋   | 6553/9822 [3:22:12<1:29:20,  1.64s/it] 67%|██████▋   | 6554/9822 [3:22:13<1:29:41,  1.65s/it] 67%|██████▋   | 6555/9822 [3:22:15<1:31:49,  1.69s/it] 67%|██████▋   | 6556/9822 [3:22:17<1:31:38,  1.68s/it] 67%|██████▋   | 6557/9822 [3:22:19<1:31:13,  1.68s/it] 67%|██████▋   | 6558/9822 [3:22:20<1:31:14,  1.68s/it] 67%|██████▋   | 6559/9822 [3:22:22<1:31:07,  1.68s/it] 67%|██████▋   | 6560/9822 [3:22:24<1:31:02,  1.67s/it] 67%|██████▋   | 6561/9822 [3:22:25<1:31:05,  1.68s/it] 67%|██████▋   | 6562/9822 [3:22:27<1:30:56,  1.67s/it] 67%|██████▋   | 6563/9822 [3:22:29<1:30:43,  1.67s/it] 67%|██████▋   | 6564/9822 [3:22:30<1:30:36,  1.67s/it] 67%|██████▋   | 6565/9822 [3:22:32<1:30:42,  1.67s/it] 67%|██████▋   | 6566/9822 [3:22:34<1:30:47,  1.67s/it] 67%|██████▋   | 6567/9822 [3:22:35<1:30:35,  1.67s/it] 67%|██████▋   | 6568/9822 [3:22:37<1:30:30,  1.67s/it] 67%|██████▋   | 6569/9822 [3:22:39<1:30:27,  1.67s/it] 67%|██████▋   | 6570/9822 [3:22:40<1:30:17,  1.67s/it] 67%|██████▋   | 6571/9822 [3:22:42<1:30:17,  1.67s/it] 67%|██████▋   | 6572/9822 [3:22:44<1:30:18,  1.67s/it] 67%|██████▋   | 6573/9822 [3:22:45<1:30:13,  1.67s/it] 67%|██████▋   | 6574/9822 [3:22:47<1:30:23,  1.67s/it] 67%|██████▋   | 6575/9822 [3:22:49<1:30:20,  1.67s/it] 67%|██████▋   | 6576/9822 [3:22:50<1:30:03,  1.66s/it] 67%|██████▋   | 6577/9822 [3:22:52<1:30:03,  1.67s/it] 67%|██████▋   | 6578/9822 [3:22:54<1:30:02,  1.67s/it] 67%|██████▋   | 6579/9822 [3:22:55<1:30:05,  1.67s/it] 67%|██████▋   | 6580/9822 [3:22:57<1:30:08,  1.67s/it] 67%|██████▋   | 6581/9822 [3:22:59<1:30:07,  1.67s/it] 67%|██████▋   | 6582/9822 [3:23:00<1:31:43,  1.70s/it] 67%|██████▋   | 6583/9822 [3:23:02<1:31:09,  1.69s/it] 67%|██████▋   | 6584/9822 [3:23:04<1:30:51,  1.68s/it] 67%|██████▋   | 6585/9822 [3:23:05<1:30:28,  1.68s/it] 67%|██████▋   | 6586/9822 [3:23:07<1:30:15,  1.67s/it] 67%|██████▋   | 6587/9822 [3:23:09<1:30:07,  1.67s/it] 67%|██████▋   | 6588/9822 [3:23:10<1:29:59,  1.67s/it] 67%|██████▋   | 6589/9822 [3:23:12<1:29:47,  1.67s/it] 67%|██████▋   | 6590/9822 [3:23:14<1:29:51,  1.67s/it] 67%|██████▋   | 6591/9822 [3:23:15<1:29:52,  1.67s/it] 67%|██████▋   | 6592/9822 [3:23:17<1:29:41,  1.67s/it] 67%|██████▋   | 6593/9822 [3:23:19<1:29:47,  1.67s/it] 67%|██████▋   | 6594/9822 [3:23:20<1:29:54,  1.67s/it] 67%|██████▋   | 6595/9822 [3:23:22<1:29:51,  1.67s/it] 67%|██████▋   | 6596/9822 [3:23:24<1:29:39,  1.67s/it] 67%|██████▋   | 6597/9822 [3:23:25<1:29:37,  1.67s/it] 67%|██████▋   | 6598/9822 [3:23:27<1:29:29,  1.67s/it] 67%|██████▋   | 6599/9822 [3:23:29<1:29:21,  1.66s/it] 67%|██████▋   | 6600/9822 [3:23:30<1:29:32,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:04:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:04:17 - INFO - __main__ - ***** test Results*****
04/29/2024 15:04:17 - INFO - __main__ -   Training step = 6600
04/29/2024 15:04:17 - INFO - __main__ -  test_accuracy:0.8726207906295754 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:04:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:04:22 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:04:22 - INFO - __main__ -   Training step = 6600
04/29/2024 15:04:22 - INFO - __main__ -  eval_accuracy:0.85463200292933 
[INFO|tokenization_utils_base.py:2094] 2024-04-29 15:04:22,068 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-04-29 15:04:22,068 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-04-29 15:04:22,110 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-04-29 15:04:23,240 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:04:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:04:31 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:04:31 - INFO - __main__ -   Training step = 6600
04/29/2024 15:04:31 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 67%|██████▋   | 6601/9822 [3:23:51<6:31:01,  7.28s/it] 67%|██████▋   | 6602/9822 [3:23:52<5:00:22,  5.60s/it] 67%|██████▋   | 6603/9822 [3:23:54<3:56:56,  4.42s/it] 67%|██████▋   | 6604/9822 [3:23:56<3:12:42,  3.59s/it] 67%|██████▋   | 6605/9822 [3:23:57<2:41:44,  3.02s/it] 67%|██████▋   | 6606/9822 [3:23:59<2:20:00,  2.61s/it] 67%|██████▋   | 6607/9822 [3:24:01<2:04:48,  2.33s/it] 67%|██████▋   | 6608/9822 [3:24:02<1:54:03,  2.13s/it] 67%|██████▋   | 6609/9822 [3:24:04<1:46:28,  1.99s/it] 67%|██████▋   | 6610/9822 [3:24:06<1:43:00,  1.92s/it] 67%|██████▋   | 6611/9822 [3:24:07<1:38:47,  1.85s/it] 67%|██████▋   | 6612/9822 [3:24:09<1:35:47,  1.79s/it] 67%|██████▋   | 6613/9822 [3:24:11<1:33:54,  1.76s/it] 67%|██████▋   | 6614/9822 [3:24:12<1:32:24,  1.73s/it] 67%|██████▋   | 6615/9822 [3:24:14<1:31:15,  1.71s/it] 67%|██████▋   | 6616/9822 [3:24:16<1:30:33,  1.69s/it] 67%|██████▋   | 6617/9822 [3:24:17<1:30:07,  1.69s/it] 67%|██████▋   | 6618/9822 [3:24:19<1:29:32,  1.68s/it] 67%|██████▋   | 6619/9822 [3:24:21<1:29:12,  1.67s/it] 67%|██████▋   | 6620/9822 [3:24:22<1:29:02,  1.67s/it] 67%|██████▋   | 6621/9822 [3:24:24<1:28:57,  1.67s/it] 67%|██████▋   | 6622/9822 [3:24:26<1:28:02,  1.65s/it] 67%|██████▋   | 6623/9822 [3:24:27<1:28:13,  1.65s/it] 67%|██████▋   | 6624/9822 [3:24:29<1:28:13,  1.66s/it] 67%|██████▋   | 6625/9822 [3:24:31<1:28:20,  1.66s/it] 67%|██████▋   | 6626/9822 [3:24:32<1:28:27,  1.66s/it] 67%|██████▋   | 6627/9822 [3:24:34<1:28:33,  1.66s/it] 67%|██████▋   | 6628/9822 [3:24:36<1:28:32,  1.66s/it] 67%|██████▋   | 6629/9822 [3:24:37<1:28:23,  1.66s/it] 68%|██████▊   | 6630/9822 [3:24:39<1:28:26,  1.66s/it] 68%|██████▊   | 6631/9822 [3:24:41<1:28:27,  1.66s/it] 68%|██████▊   | 6632/9822 [3:24:42<1:28:39,  1.67s/it] 68%|██████▊   | 6633/9822 [3:24:44<1:28:41,  1.67s/it] 68%|██████▊   | 6634/9822 [3:24:46<1:28:43,  1.67s/it] 68%|██████▊   | 6635/9822 [3:24:47<1:28:29,  1.67s/it] 68%|██████▊   | 6636/9822 [3:24:49<1:28:20,  1.66s/it] 68%|██████▊   | 6637/9822 [3:24:51<1:28:24,  1.67s/it] 68%|██████▊   | 6638/9822 [3:24:52<1:28:21,  1.67s/it] 68%|██████▊   | 6639/9822 [3:24:54<1:28:24,  1.67s/it] 68%|██████▊   | 6640/9822 [3:24:56<1:28:32,  1.67s/it] 68%|██████▊   | 6641/9822 [3:24:57<1:28:24,  1.67s/it] 68%|██████▊   | 6642/9822 [3:24:59<1:28:16,  1.67s/it] 68%|██████▊   | 6643/9822 [3:25:01<1:28:25,  1.67s/it] 68%|██████▊   | 6644/9822 [3:25:02<1:28:27,  1.67s/it] 68%|██████▊   | 6645/9822 [3:25:04<1:28:17,  1.67s/it] 68%|██████▊   | 6646/9822 [3:25:06<1:28:18,  1.67s/it] 68%|██████▊   | 6647/9822 [3:25:07<1:28:19,  1.67s/it] 68%|██████▊   | 6648/9822 [3:25:09<1:28:07,  1.67s/it] 68%|██████▊   | 6649/9822 [3:25:11<1:28:06,  1.67s/it] 68%|██████▊   | 6650/9822 [3:25:13<1:29:49,  1.70s/it] 68%|██████▊   | 6651/9822 [3:25:14<1:29:21,  1.69s/it] 68%|██████▊   | 6652/9822 [3:25:16<1:29:02,  1.69s/it] 68%|██████▊   | 6653/9822 [3:25:18<1:28:43,  1.68s/it] 68%|██████▊   | 6654/9822 [3:25:19<1:28:31,  1.68s/it] 68%|██████▊   | 6655/9822 [3:25:21<1:28:15,  1.67s/it] 68%|██████▊   | 6656/9822 [3:25:23<1:28:04,  1.67s/it] 68%|██████▊   | 6657/9822 [3:25:24<1:27:58,  1.67s/it] 68%|██████▊   | 6658/9822 [3:25:26<1:27:59,  1.67s/it] 68%|██████▊   | 6659/9822 [3:25:28<1:27:57,  1.67s/it] 68%|██████▊   | 6660/9822 [3:25:29<1:27:56,  1.67s/it] 68%|██████▊   | 6661/9822 [3:25:31<1:28:04,  1.67s/it] 68%|██████▊   | 6662/9822 [3:25:33<1:27:56,  1.67s/it] 68%|██████▊   | 6663/9822 [3:25:34<1:27:48,  1.67s/it] 68%|██████▊   | 6664/9822 [3:25:36<1:27:50,  1.67s/it] 68%|██████▊   | 6665/9822 [3:25:38<1:27:48,  1.67s/it] 68%|██████▊   | 6666/9822 [3:25:39<1:27:46,  1.67s/it] 68%|██████▊   | 6667/9822 [3:25:41<1:27:39,  1.67s/it] 68%|██████▊   | 6668/9822 [3:25:43<1:27:38,  1.67s/it] 68%|██████▊   | 6669/9822 [3:25:44<1:27:36,  1.67s/it] 68%|██████▊   | 6670/9822 [3:25:46<1:27:43,  1.67s/it] 68%|██████▊   | 6671/9822 [3:25:48<1:27:41,  1.67s/it] 68%|██████▊   | 6672/9822 [3:25:49<1:27:37,  1.67s/it] 68%|██████▊   | 6673/9822 [3:25:51<1:27:33,  1.67s/it] 68%|██████▊   | 6674/9822 [3:25:53<1:27:31,  1.67s/it] 68%|██████▊   | 6675/9822 [3:25:54<1:27:31,  1.67s/it] 68%|██████▊   | 6676/9822 [3:25:56<1:27:28,  1.67s/it] 68%|██████▊   | 6677/9822 [3:25:58<1:29:11,  1.70s/it] 68%|██████▊   | 6678/9822 [3:25:59<1:28:35,  1.69s/it] 68%|██████▊   | 6679/9822 [3:26:01<1:28:18,  1.69s/it] 68%|██████▊   | 6680/9822 [3:26:03<1:28:05,  1.68s/it] 68%|██████▊   | 6681/9822 [3:26:04<1:27:42,  1.68s/it] 68%|██████▊   | 6682/9822 [3:26:06<1:27:33,  1.67s/it] 68%|██████▊   | 6683/9822 [3:26:08<1:27:32,  1.67s/it] 68%|██████▊   | 6684/9822 [3:26:09<1:27:31,  1.67s/it] 68%|██████▊   | 6685/9822 [3:26:11<1:27:34,  1.68s/it] 68%|██████▊   | 6686/9822 [3:26:13<1:27:35,  1.68s/it] 68%|██████▊   | 6687/9822 [3:26:14<1:27:27,  1.67s/it] 68%|██████▊   | 6688/9822 [3:26:16<1:27:12,  1.67s/it] 68%|██████▊   | 6689/9822 [3:26:18<1:27:08,  1.67s/it] 68%|██████▊   | 6690/9822 [3:26:19<1:26:58,  1.67s/it] 68%|██████▊   | 6691/9822 [3:26:21<1:26:54,  1.67s/it] 68%|██████▊   | 6692/9822 [3:26:23<1:26:53,  1.67s/it] 68%|██████▊   | 6693/9822 [3:26:24<1:26:46,  1.66s/it] 68%|██████▊   | 6694/9822 [3:26:26<1:26:46,  1.66s/it] 68%|██████▊   | 6695/9822 [3:26:28<1:26:46,  1.67s/it] 68%|██████▊   | 6696/9822 [3:26:29<1:26:48,  1.67s/it] 68%|██████▊   | 6697/9822 [3:26:31<1:26:46,  1.67s/it] 68%|██████▊   | 6698/9822 [3:26:33<1:26:46,  1.67s/it] 68%|██████▊   | 6699/9822 [3:26:34<1:26:44,  1.67s/it] 68%|██████▊   | 6700/9822 [3:26:36<1:26:39,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:07:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:07:23 - INFO - __main__ - ***** test Results*****
04/29/2024 15:07:23 - INFO - __main__ -   Training step = 6700
04/29/2024 15:07:23 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:07:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:07:27 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:07:27 - INFO - __main__ -   Training step = 6700
04/29/2024 15:07:27 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:07:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:07:36 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:07:36 - INFO - __main__ -   Training step = 6700
04/29/2024 15:07:36 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 68%|██████▊   | 6701/9822 [3:26:55<6:00:57,  6.94s/it] 68%|██████▊   | 6702/9822 [3:26:57<4:38:29,  5.36s/it] 68%|██████▊   | 6703/9822 [3:26:59<3:40:50,  4.25s/it] 68%|██████▊   | 6704/9822 [3:27:00<3:02:16,  3.51s/it] 68%|██████▊   | 6705/9822 [3:27:02<2:33:34,  2.96s/it] 68%|██████▊   | 6706/9822 [3:27:04<2:13:23,  2.57s/it] 68%|██████▊   | 6707/9822 [3:27:05<1:59:14,  2.30s/it] 68%|██████▊   | 6708/9822 [3:27:07<1:48:39,  2.09s/it] 68%|██████▊   | 6709/9822 [3:27:09<1:41:54,  1.96s/it] 68%|██████▊   | 6710/9822 [3:27:10<1:37:19,  1.88s/it] 68%|██████▊   | 6711/9822 [3:27:12<1:34:01,  1.81s/it] 68%|██████▊   | 6712/9822 [3:27:14<1:31:39,  1.77s/it] 68%|██████▊   | 6713/9822 [3:27:15<1:29:52,  1.73s/it] 68%|██████▊   | 6714/9822 [3:27:17<1:28:50,  1.72s/it] 68%|██████▊   | 6715/9822 [3:27:19<1:27:56,  1.70s/it] 68%|██████▊   | 6716/9822 [3:27:20<1:27:20,  1.69s/it] 68%|██████▊   | 6717/9822 [3:27:22<1:26:59,  1.68s/it] 68%|██████▊   | 6718/9822 [3:27:24<1:26:41,  1.68s/it] 68%|██████▊   | 6719/9822 [3:27:25<1:26:20,  1.67s/it] 68%|██████▊   | 6720/9822 [3:27:27<1:26:09,  1.67s/it] 68%|██████▊   | 6721/9822 [3:27:29<1:26:08,  1.67s/it] 68%|██████▊   | 6722/9822 [3:27:30<1:26:04,  1.67s/it] 68%|██████▊   | 6723/9822 [3:27:32<1:26:00,  1.67s/it] 68%|██████▊   | 6724/9822 [3:27:34<1:26:01,  1.67s/it] 68%|██████▊   | 6725/9822 [3:27:35<1:25:58,  1.67s/it] 68%|██████▊   | 6726/9822 [3:27:37<1:26:07,  1.67s/it] 68%|██████▊   | 6727/9822 [3:27:39<1:26:10,  1.67s/it] 68%|██████▊   | 6728/9822 [3:27:40<1:26:16,  1.67s/it] 69%|██████▊   | 6729/9822 [3:27:42<1:26:01,  1.67s/it] 69%|██████▊   | 6730/9822 [3:27:44<1:27:27,  1.70s/it] 69%|██████▊   | 6731/9822 [3:27:45<1:26:49,  1.69s/it] 69%|██████▊   | 6732/9822 [3:27:47<1:26:35,  1.68s/it] 69%|██████▊   | 6733/9822 [3:27:49<1:26:22,  1.68s/it] 69%|██████▊   | 6734/9822 [3:27:50<1:26:22,  1.68s/it] 69%|██████▊   | 6735/9822 [3:27:52<1:26:23,  1.68s/it] 69%|██████▊   | 6736/9822 [3:27:54<1:26:18,  1.68s/it] 69%|██████▊   | 6737/9822 [3:27:55<1:26:02,  1.67s/it] 69%|██████▊   | 6738/9822 [3:27:57<1:26:00,  1.67s/it] 69%|██████▊   | 6739/9822 [3:27:59<1:25:43,  1.67s/it] 69%|██████▊   | 6740/9822 [3:28:00<1:25:33,  1.67s/it] 69%|██████▊   | 6741/9822 [3:28:02<1:25:34,  1.67s/it] 69%|██████▊   | 6742/9822 [3:28:04<1:25:40,  1.67s/it] 69%|██████▊   | 6743/9822 [3:28:05<1:25:33,  1.67s/it] 69%|██████▊   | 6744/9822 [3:28:07<1:25:30,  1.67s/it] 69%|██████▊   | 6745/9822 [3:28:09<1:25:30,  1.67s/it] 69%|██████▊   | 6746/9822 [3:28:10<1:25:33,  1.67s/it] 69%|██████▊   | 6747/9822 [3:28:12<1:25:42,  1.67s/it] 69%|██████▊   | 6748/9822 [3:28:14<1:25:29,  1.67s/it] 69%|██████▊   | 6749/9822 [3:28:15<1:25:24,  1.67s/it] 69%|██████▊   | 6750/9822 [3:28:17<1:25:35,  1.67s/it] 69%|██████▊   | 6751/9822 [3:28:19<1:25:34,  1.67s/it] 69%|██████▊   | 6752/9822 [3:28:20<1:25:32,  1.67s/it] 69%|██████▉   | 6753/9822 [3:28:22<1:25:35,  1.67s/it] 69%|██████▉   | 6754/9822 [3:28:24<1:25:20,  1.67s/it] 69%|██████▉   | 6755/9822 [3:28:25<1:25:22,  1.67s/it] 69%|██████▉   | 6756/9822 [3:28:27<1:25:29,  1.67s/it] 69%|██████▉   | 6757/9822 [3:28:29<1:25:26,  1.67s/it] 69%|██████▉   | 6758/9822 [3:28:30<1:25:28,  1.67s/it] 69%|██████▉   | 6759/9822 [3:28:32<1:25:10,  1.67s/it] 69%|██████▉   | 6760/9822 [3:28:34<1:25:09,  1.67s/it] 69%|██████▉   | 6761/9822 [3:28:35<1:25:13,  1.67s/it] 69%|██████▉   | 6762/9822 [3:28:37<1:25:06,  1.67s/it] 69%|██████▉   | 6763/9822 [3:28:39<1:26:47,  1.70s/it] 69%|██████▉   | 6764/9822 [3:28:41<1:26:17,  1.69s/it] 69%|██████▉   | 6765/9822 [3:28:42<1:25:46,  1.68s/it] 69%|██████▉   | 6766/9822 [3:28:44<1:25:20,  1.68s/it] 69%|██████▉   | 6767/9822 [3:28:46<1:25:10,  1.67s/it] 69%|██████▉   | 6768/9822 [3:28:47<1:24:57,  1.67s/it] 69%|██████▉   | 6769/9822 [3:28:49<1:24:57,  1.67s/it] 69%|██████▉   | 6770/9822 [3:28:51<1:25:11,  1.67s/it] 69%|██████▉   | 6771/9822 [3:28:52<1:25:01,  1.67s/it] 69%|██████▉   | 6772/9822 [3:28:54<1:24:47,  1.67s/it] 69%|██████▉   | 6773/9822 [3:28:56<1:25:00,  1.67s/it] 69%|██████▉   | 6774/9822 [3:28:57<1:25:30,  1.68s/it] 69%|██████▉   | 6775/9822 [3:28:59<1:25:26,  1.68s/it] 69%|██████▉   | 6776/9822 [3:29:01<1:25:22,  1.68s/it] 69%|██████▉   | 6777/9822 [3:29:02<1:24:53,  1.67s/it] 69%|██████▉   | 6778/9822 [3:29:04<1:24:52,  1.67s/it] 69%|██████▉   | 6779/9822 [3:29:06<1:24:48,  1.67s/it] 69%|██████▉   | 6780/9822 [3:29:07<1:24:42,  1.67s/it] 69%|██████▉   | 6781/9822 [3:29:09<1:24:53,  1.67s/it] 69%|██████▉   | 6782/9822 [3:29:11<1:24:52,  1.68s/it] 69%|██████▉   | 6783/9822 [3:29:12<1:24:33,  1.67s/it] 69%|██████▉   | 6784/9822 [3:29:14<1:24:19,  1.67s/it] 69%|██████▉   | 6785/9822 [3:29:16<1:25:46,  1.69s/it] 69%|██████▉   | 6786/9822 [3:29:17<1:25:11,  1.68s/it] 69%|██████▉   | 6787/9822 [3:29:19<1:24:54,  1.68s/it] 69%|██████▉   | 6788/9822 [3:29:21<1:24:40,  1.67s/it] 69%|██████▉   | 6789/9822 [3:29:22<1:24:32,  1.67s/it] 69%|██████▉   | 6790/9822 [3:29:24<1:24:25,  1.67s/it] 69%|██████▉   | 6791/9822 [3:29:26<1:24:33,  1.67s/it] 69%|██████▉   | 6792/9822 [3:29:27<1:24:38,  1.68s/it] 69%|██████▉   | 6793/9822 [3:29:29<1:24:37,  1.68s/it] 69%|██████▉   | 6794/9822 [3:29:31<1:23:38,  1.66s/it] 69%|██████▉   | 6795/9822 [3:29:32<1:23:57,  1.66s/it] 69%|██████▉   | 6796/9822 [3:29:34<1:24:29,  1.68s/it] 69%|██████▉   | 6797/9822 [3:29:36<1:24:18,  1.67s/it] 69%|██████▉   | 6798/9822 [3:29:37<1:24:21,  1.67s/it] 69%|██████▉   | 6799/9822 [3:29:39<1:24:23,  1.68s/it] 69%|██████▉   | 6800/9822 [3:29:41<1:24:30,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:10:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:10:28 - INFO - __main__ - ***** test Results*****
04/29/2024 15:10:28 - INFO - __main__ -   Training step = 6800
04/29/2024 15:10:28 - INFO - __main__ -  test_accuracy:0.8700585651537335 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:10:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:10:32 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:10:32 - INFO - __main__ -   Training step = 6800
04/29/2024 15:10:32 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:10:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:10:41 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:10:41 - INFO - __main__ -   Training step = 6800
04/29/2024 15:10:41 - INFO - __main__ -  eval_accuracy:0.9099231050897107 
 69%|██████▉   | 6801/9822 [3:30:00<5:49:50,  6.95s/it] 69%|██████▉   | 6802/9822 [3:30:02<4:30:12,  5.37s/it] 69%|██████▉   | 6803/9822 [3:30:03<3:34:11,  4.26s/it] 69%|██████▉   | 6804/9822 [3:30:05<2:54:56,  3.48s/it] 69%|██████▉   | 6805/9822 [3:30:07<2:27:22,  2.93s/it] 69%|██████▉   | 6806/9822 [3:30:08<2:08:03,  2.55s/it] 69%|██████▉   | 6807/9822 [3:30:10<1:54:34,  2.28s/it] 69%|██████▉   | 6808/9822 [3:30:12<1:45:21,  2.10s/it] 69%|██████▉   | 6809/9822 [3:30:13<1:38:50,  1.97s/it] 69%|██████▉   | 6810/9822 [3:30:15<1:34:12,  1.88s/it] 69%|██████▉   | 6811/9822 [3:30:17<1:32:26,  1.84s/it] 69%|██████▉   | 6812/9822 [3:30:18<1:29:40,  1.79s/it] 69%|██████▉   | 6813/9822 [3:30:20<1:27:47,  1.75s/it] 69%|██████▉   | 6814/9822 [3:30:22<1:26:22,  1.72s/it] 69%|██████▉   | 6815/9822 [3:30:23<1:25:35,  1.71s/it] 69%|██████▉   | 6816/9822 [3:30:25<1:24:53,  1.69s/it] 69%|██████▉   | 6817/9822 [3:30:27<1:24:19,  1.68s/it] 69%|██████▉   | 6818/9822 [3:30:28<1:24:09,  1.68s/it] 69%|██████▉   | 6819/9822 [3:30:30<1:23:52,  1.68s/it] 69%|██████▉   | 6820/9822 [3:30:32<1:23:38,  1.67s/it] 69%|██████▉   | 6821/9822 [3:30:33<1:23:44,  1.67s/it] 69%|██████▉   | 6822/9822 [3:30:35<1:23:25,  1.67s/it] 69%|██████▉   | 6823/9822 [3:30:37<1:23:23,  1.67s/it] 69%|██████▉   | 6824/9822 [3:30:38<1:23:07,  1.66s/it] 69%|██████▉   | 6825/9822 [3:30:40<1:23:01,  1.66s/it] 69%|██████▉   | 6826/9822 [3:30:42<1:23:03,  1.66s/it] 70%|██████▉   | 6827/9822 [3:30:43<1:22:55,  1.66s/it] 70%|██████▉   | 6828/9822 [3:30:45<1:22:54,  1.66s/it] 70%|██████▉   | 6829/9822 [3:30:47<1:22:51,  1.66s/it] 70%|██████▉   | 6830/9822 [3:30:48<1:22:56,  1.66s/it] 70%|██████▉   | 6831/9822 [3:30:50<1:22:58,  1.66s/it] 70%|██████▉   | 6832/9822 [3:30:52<1:22:48,  1.66s/it] 70%|██████▉   | 6833/9822 [3:30:53<1:22:52,  1.66s/it] 70%|██████▉   | 6834/9822 [3:30:55<1:22:55,  1.67s/it] 70%|██████▉   | 6835/9822 [3:30:57<1:22:54,  1.67s/it] 70%|██████▉   | 6836/9822 [3:30:58<1:22:58,  1.67s/it] 70%|██████▉   | 6837/9822 [3:31:00<1:22:56,  1.67s/it] 70%|██████▉   | 6838/9822 [3:31:02<1:24:35,  1.70s/it] 70%|██████▉   | 6839/9822 [3:31:04<1:24:03,  1.69s/it] 70%|██████▉   | 6840/9822 [3:31:05<1:23:28,  1.68s/it] 70%|██████▉   | 6841/9822 [3:31:07<1:23:19,  1.68s/it] 70%|██████▉   | 6842/9822 [3:31:09<1:23:05,  1.67s/it] 70%|██████▉   | 6843/9822 [3:31:10<1:22:56,  1.67s/it] 70%|██████▉   | 6844/9822 [3:31:12<1:22:49,  1.67s/it] 70%|██████▉   | 6845/9822 [3:31:14<1:22:46,  1.67s/it] 70%|██████▉   | 6846/9822 [3:31:15<1:22:49,  1.67s/it] 70%|██████▉   | 6847/9822 [3:31:17<1:22:47,  1.67s/it] 70%|██████▉   | 6848/9822 [3:31:19<1:22:41,  1.67s/it] 70%|██████▉   | 6849/9822 [3:31:20<1:22:39,  1.67s/it] 70%|██████▉   | 6850/9822 [3:31:22<1:22:30,  1.67s/it] 70%|██████▉   | 6851/9822 [3:31:24<1:22:20,  1.66s/it] 70%|██████▉   | 6852/9822 [3:31:25<1:22:17,  1.66s/it] 70%|██████▉   | 6853/9822 [3:31:27<1:22:15,  1.66s/it] 70%|██████▉   | 6854/9822 [3:31:29<1:22:24,  1.67s/it] 70%|██████▉   | 6855/9822 [3:31:30<1:22:23,  1.67s/it] 70%|██████▉   | 6856/9822 [3:31:32<1:22:15,  1.66s/it] 70%|██████▉   | 6857/9822 [3:31:33<1:22:06,  1.66s/it] 70%|██████▉   | 6858/9822 [3:31:35<1:22:01,  1.66s/it] 70%|██████▉   | 6859/9822 [3:31:37<1:22:02,  1.66s/it] 70%|██████▉   | 6860/9822 [3:31:38<1:21:55,  1.66s/it] 70%|██████▉   | 6861/9822 [3:31:40<1:21:55,  1.66s/it] 70%|██████▉   | 6862/9822 [3:31:42<1:21:59,  1.66s/it] 70%|██████▉   | 6863/9822 [3:31:43<1:21:53,  1.66s/it] 70%|██████▉   | 6864/9822 [3:31:45<1:21:58,  1.66s/it] 70%|██████▉   | 6865/9822 [3:31:47<1:21:57,  1.66s/it] 70%|██████▉   | 6866/9822 [3:31:48<1:21:50,  1.66s/it] 70%|██████▉   | 6867/9822 [3:31:50<1:21:53,  1.66s/it] 70%|██████▉   | 6868/9822 [3:31:52<1:21:48,  1.66s/it] 70%|██████▉   | 6869/9822 [3:31:53<1:21:53,  1.66s/it] 70%|██████▉   | 6870/9822 [3:31:55<1:22:01,  1.67s/it] 70%|██████▉   | 6871/9822 [3:31:57<1:23:32,  1.70s/it] 70%|██████▉   | 6872/9822 [3:31:59<1:22:57,  1.69s/it] 70%|██████▉   | 6873/9822 [3:32:00<1:22:34,  1.68s/it] 70%|██████▉   | 6874/9822 [3:32:02<1:22:19,  1.68s/it] 70%|██████▉   | 6875/9822 [3:32:04<1:22:02,  1.67s/it] 70%|███████   | 6876/9822 [3:32:05<1:21:55,  1.67s/it] 70%|███████   | 6877/9822 [3:32:07<1:21:54,  1.67s/it] 70%|███████   | 6878/9822 [3:32:09<1:21:47,  1.67s/it] 70%|███████   | 6879/9822 [3:32:10<1:21:41,  1.67s/it] 70%|███████   | 6880/9822 [3:32:12<1:20:59,  1.65s/it] 70%|███████   | 6881/9822 [3:32:13<1:21:16,  1.66s/it] 70%|███████   | 6882/9822 [3:32:15<1:21:20,  1.66s/it] 70%|███████   | 6883/9822 [3:32:17<1:21:24,  1.66s/it] 70%|███████   | 6884/9822 [3:32:18<1:21:31,  1.66s/it] 70%|███████   | 6885/9822 [3:32:20<1:21:24,  1.66s/it] 70%|███████   | 6886/9822 [3:32:22<1:21:31,  1.67s/it] 70%|███████   | 6887/9822 [3:32:23<1:21:35,  1.67s/it] 70%|███████   | 6888/9822 [3:32:25<1:21:36,  1.67s/it] 70%|███████   | 6889/9822 [3:32:27<1:21:41,  1.67s/it] 70%|███████   | 6890/9822 [3:32:29<1:21:45,  1.67s/it] 70%|███████   | 6891/9822 [3:32:30<1:21:38,  1.67s/it] 70%|███████   | 6892/9822 [3:32:32<1:21:30,  1.67s/it] 70%|███████   | 6893/9822 [3:32:34<1:22:58,  1.70s/it] 70%|███████   | 6894/9822 [3:32:35<1:22:20,  1.69s/it] 70%|███████   | 6895/9822 [3:32:37<1:21:51,  1.68s/it] 70%|███████   | 6896/9822 [3:32:39<1:21:35,  1.67s/it] 70%|███████   | 6897/9822 [3:32:40<1:21:25,  1.67s/it] 70%|███████   | 6898/9822 [3:32:42<1:21:16,  1.67s/it] 70%|███████   | 6899/9822 [3:32:44<1:21:08,  1.67s/it] 70%|███████   | 6900/9822 [3:32:45<1:21:11,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1870, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:13:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:13:32 - INFO - __main__ - ***** test Results*****
04/29/2024 15:13:32 - INFO - __main__ -   Training step = 6900
04/29/2024 15:13:32 - INFO - __main__ -  test_accuracy:0.8693265007320644 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:13:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:13:36 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:13:36 - INFO - __main__ -   Training step = 6900
04/29/2024 15:13:36 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:13:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:13:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:13:45 - INFO - __main__ -   Training step = 6900
04/29/2024 15:13:45 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 70%|███████   | 6901/9822 [3:33:04<5:37:33,  6.93s/it] 70%|███████   | 6902/9822 [3:33:06<4:20:31,  5.35s/it] 70%|███████   | 6903/9822 [3:33:08<3:26:28,  4.24s/it] 70%|███████   | 6904/9822 [3:33:09<2:48:48,  3.47s/it] 70%|███████   | 6905/9822 [3:33:11<2:22:23,  2.93s/it] 70%|███████   | 6906/9822 [3:33:13<2:03:50,  2.55s/it] 70%|███████   | 6907/9822 [3:33:14<1:50:50,  2.28s/it] 70%|███████   | 6908/9822 [3:33:16<1:41:55,  2.10s/it] 70%|███████   | 6909/9822 [3:33:18<1:35:28,  1.97s/it] 70%|███████   | 6910/9822 [3:33:19<1:31:05,  1.88s/it] 70%|███████   | 6911/9822 [3:33:21<1:27:49,  1.81s/it] 70%|███████   | 6912/9822 [3:33:23<1:25:50,  1.77s/it] 70%|███████   | 6913/9822 [3:33:24<1:24:16,  1.74s/it] 70%|███████   | 6914/9822 [3:33:26<1:23:06,  1.71s/it] 70%|███████   | 6915/9822 [3:33:28<1:22:48,  1.71s/it] 70%|███████   | 6916/9822 [3:33:29<1:22:13,  1.70s/it] 70%|███████   | 6917/9822 [3:33:31<1:21:54,  1.69s/it] 70%|███████   | 6918/9822 [3:33:33<1:21:45,  1.69s/it] 70%|███████   | 6919/9822 [3:33:35<1:22:43,  1.71s/it] 70%|███████   | 6920/9822 [3:33:36<1:22:04,  1.70s/it] 70%|███████   | 6921/9822 [3:33:38<1:21:36,  1.69s/it] 70%|███████   | 6922/9822 [3:33:40<1:21:08,  1.68s/it] 70%|███████   | 6923/9822 [3:33:41<1:21:04,  1.68s/it] 70%|███████   | 6924/9822 [3:33:43<1:20:54,  1.68s/it] 71%|███████   | 6925/9822 [3:33:45<1:20:34,  1.67s/it] 71%|███████   | 6926/9822 [3:33:46<1:20:24,  1.67s/it] 71%|███████   | 6927/9822 [3:33:48<1:20:15,  1.66s/it] 71%|███████   | 6928/9822 [3:33:50<1:20:10,  1.66s/it] 71%|███████   | 6929/9822 [3:33:51<1:20:05,  1.66s/it] 71%|███████   | 6930/9822 [3:33:53<1:20:06,  1.66s/it] 71%|███████   | 6931/9822 [3:33:55<1:20:03,  1.66s/it] 71%|███████   | 6932/9822 [3:33:56<1:20:01,  1.66s/it] 71%|███████   | 6933/9822 [3:33:58<1:20:01,  1.66s/it] 71%|███████   | 6934/9822 [3:34:00<1:20:11,  1.67s/it] 71%|███████   | 6935/9822 [3:34:01<1:20:00,  1.66s/it] 71%|███████   | 6936/9822 [3:34:03<1:19:57,  1.66s/it] 71%|███████   | 6937/9822 [3:34:05<1:19:52,  1.66s/it] 71%|███████   | 6938/9822 [3:34:06<1:19:54,  1.66s/it] 71%|███████   | 6939/9822 [3:34:08<1:19:47,  1.66s/it] 71%|███████   | 6940/9822 [3:34:09<1:19:43,  1.66s/it] 71%|███████   | 6941/9822 [3:34:11<1:19:43,  1.66s/it] 71%|███████   | 6942/9822 [3:34:13<1:19:45,  1.66s/it] 71%|███████   | 6943/9822 [3:34:14<1:19:49,  1.66s/it] 71%|███████   | 6944/9822 [3:34:16<1:19:56,  1.67s/it] 71%|███████   | 6945/9822 [3:34:18<1:19:57,  1.67s/it] 71%|███████   | 6946/9822 [3:34:20<1:21:28,  1.70s/it] 71%|███████   | 6947/9822 [3:34:21<1:20:55,  1.69s/it] 71%|███████   | 6948/9822 [3:34:23<1:20:26,  1.68s/it] 71%|███████   | 6949/9822 [3:34:25<1:20:04,  1.67s/it] 71%|███████   | 6950/9822 [3:34:26<1:19:52,  1.67s/it] 71%|███████   | 6951/9822 [3:34:28<1:19:37,  1.66s/it] 71%|███████   | 6952/9822 [3:34:30<1:19:32,  1.66s/it] 71%|███████   | 6953/9822 [3:34:31<1:19:27,  1.66s/it] 71%|███████   | 6954/9822 [3:34:33<1:19:31,  1.66s/it] 71%|███████   | 6955/9822 [3:34:35<1:19:30,  1.66s/it] 71%|███████   | 6956/9822 [3:34:36<1:19:40,  1.67s/it] 71%|███████   | 6957/9822 [3:34:38<1:19:36,  1.67s/it] 71%|███████   | 6958/9822 [3:34:40<1:19:25,  1.66s/it] 71%|███████   | 6959/9822 [3:34:41<1:19:18,  1.66s/it] 71%|███████   | 6960/9822 [3:34:43<1:19:17,  1.66s/it] 71%|███████   | 6961/9822 [3:34:45<1:19:17,  1.66s/it] 71%|███████   | 6962/9822 [3:34:46<1:19:15,  1.66s/it] 71%|███████   | 6963/9822 [3:34:48<1:19:06,  1.66s/it] 71%|███████   | 6964/9822 [3:34:50<1:19:08,  1.66s/it] 71%|███████   | 6965/9822 [3:34:51<1:19:10,  1.66s/it] 71%|███████   | 6966/9822 [3:34:53<1:18:39,  1.65s/it] 71%|███████   | 6967/9822 [3:34:54<1:18:51,  1.66s/it] 71%|███████   | 6968/9822 [3:34:56<1:18:54,  1.66s/it] 71%|███████   | 6969/9822 [3:34:58<1:18:58,  1.66s/it] 71%|███████   | 6970/9822 [3:34:59<1:19:02,  1.66s/it] 71%|███████   | 6971/9822 [3:35:01<1:19:00,  1.66s/it] 71%|███████   | 6972/9822 [3:35:03<1:19:00,  1.66s/it] 71%|███████   | 6973/9822 [3:35:04<1:18:55,  1.66s/it] 71%|███████   | 6974/9822 [3:35:06<1:18:59,  1.66s/it] 71%|███████   | 6975/9822 [3:35:08<1:19:01,  1.67s/it] 71%|███████   | 6976/9822 [3:35:09<1:19:05,  1.67s/it] 71%|███████   | 6977/9822 [3:35:11<1:19:07,  1.67s/it] 71%|███████   | 6978/9822 [3:35:13<1:19:05,  1.67s/it] 71%|███████   | 6979/9822 [3:35:15<1:20:34,  1.70s/it] 71%|███████   | 6980/9822 [3:35:16<1:20:06,  1.69s/it] 71%|███████   | 6981/9822 [3:35:18<1:19:47,  1.69s/it] 71%|███████   | 6982/9822 [3:35:20<1:19:32,  1.68s/it] 71%|███████   | 6983/9822 [3:35:21<1:19:28,  1.68s/it] 71%|███████   | 6984/9822 [3:35:23<1:19:08,  1.67s/it] 71%|███████   | 6985/9822 [3:35:25<1:19:07,  1.67s/it] 71%|███████   | 6986/9822 [3:35:26<1:19:08,  1.67s/it] 71%|███████   | 6987/9822 [3:35:28<1:18:54,  1.67s/it] 71%|███████   | 6988/9822 [3:35:30<1:18:51,  1.67s/it] 71%|███████   | 6989/9822 [3:35:31<1:18:53,  1.67s/it] 71%|███████   | 6990/9822 [3:35:33<1:18:55,  1.67s/it] 71%|███████   | 6991/9822 [3:35:35<1:18:46,  1.67s/it] 71%|███████   | 6992/9822 [3:35:36<1:18:48,  1.67s/it] 71%|███████   | 6993/9822 [3:35:38<1:18:51,  1.67s/it] 71%|███████   | 6994/9822 [3:35:40<1:18:42,  1.67s/it] 71%|███████   | 6995/9822 [3:35:41<1:18:40,  1.67s/it] 71%|███████   | 6996/9822 [3:35:43<1:18:47,  1.67s/it] 71%|███████   | 6997/9822 [3:35:45<1:18:37,  1.67s/it] 71%|███████   | 6998/9822 [3:35:46<1:18:37,  1.67s/it] 71%|███████▏  | 6999/9822 [3:35:48<1:18:25,  1.67s/it] 71%|███████▏  | 7000/9822 [3:35:50<1:18:19,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:16:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:16:36 - INFO - __main__ - ***** test Results*****
04/29/2024 15:16:36 - INFO - __main__ -   Training step = 7000
04/29/2024 15:16:36 - INFO - __main__ -  test_accuracy:0.8715226939970717 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:16:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:16:41 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:16:41 - INFO - __main__ -   Training step = 7000
04/29/2024 15:16:41 - INFO - __main__ -  eval_accuracy:0.85060417429513 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:16:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:16:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:16:49 - INFO - __main__ -   Training step = 7000
04/29/2024 15:16:49 - INFO - __main__ -  eval_accuracy:0.909556938850238 
 71%|███████▏  | 7001/9822 [3:36:09<5:27:26,  6.96s/it] 71%|███████▏  | 7002/9822 [3:36:11<4:12:35,  5.37s/it] 71%|███████▏  | 7003/9822 [3:36:12<3:20:06,  4.26s/it] 71%|███████▏  | 7004/9822 [3:36:14<2:43:44,  3.49s/it] 71%|███████▏  | 7005/9822 [3:36:16<2:18:00,  2.94s/it] 71%|███████▏  | 7006/9822 [3:36:17<2:00:20,  2.56s/it] 71%|███████▏  | 7007/9822 [3:36:19<1:47:38,  2.29s/it] 71%|███████▏  | 7008/9822 [3:36:21<1:38:46,  2.11s/it] 71%|███████▏  | 7009/9822 [3:36:22<1:32:29,  1.97s/it] 71%|███████▏  | 7010/9822 [3:36:24<1:28:03,  1.88s/it] 71%|███████▏  | 7011/9822 [3:36:26<1:25:08,  1.82s/it] 71%|███████▏  | 7012/9822 [3:36:27<1:23:08,  1.78s/it] 71%|███████▏  | 7013/9822 [3:36:29<1:21:43,  1.75s/it] 71%|███████▏  | 7014/9822 [3:36:31<1:20:35,  1.72s/it] 71%|███████▏  | 7015/9822 [3:36:32<1:19:44,  1.70s/it] 71%|███████▏  | 7016/9822 [3:36:34<1:19:25,  1.70s/it] 71%|███████▏  | 7017/9822 [3:36:36<1:19:10,  1.69s/it] 71%|███████▏  | 7018/9822 [3:36:37<1:18:44,  1.68s/it] 71%|███████▏  | 7019/9822 [3:36:39<1:18:39,  1.68s/it] 71%|███████▏  | 7020/9822 [3:36:41<1:18:38,  1.68s/it] 71%|███████▏  | 7021/9822 [3:36:42<1:18:22,  1.68s/it] 71%|███████▏  | 7022/9822 [3:36:44<1:18:23,  1.68s/it] 72%|███████▏  | 7023/9822 [3:36:46<1:18:10,  1.68s/it] 72%|███████▏  | 7024/9822 [3:36:47<1:18:00,  1.67s/it] 72%|███████▏  | 7025/9822 [3:36:49<1:19:37,  1.71s/it] 72%|███████▏  | 7026/9822 [3:36:51<1:19:22,  1.70s/it] 72%|███████▏  | 7027/9822 [3:36:53<1:18:59,  1.70s/it] 72%|███████▏  | 7028/9822 [3:36:54<1:18:40,  1.69s/it] 72%|███████▏  | 7029/9822 [3:36:56<1:18:26,  1.69s/it] 72%|███████▏  | 7030/9822 [3:36:58<1:18:03,  1.68s/it] 72%|███████▏  | 7031/9822 [3:36:59<1:17:58,  1.68s/it] 72%|███████▏  | 7032/9822 [3:37:01<1:17:53,  1.67s/it] 72%|███████▏  | 7033/9822 [3:37:03<1:17:48,  1.67s/it] 72%|███████▏  | 7034/9822 [3:37:04<1:17:39,  1.67s/it] 72%|███████▏  | 7035/9822 [3:37:06<1:17:31,  1.67s/it] 72%|███████▏  | 7036/9822 [3:37:08<1:17:32,  1.67s/it] 72%|███████▏  | 7037/9822 [3:37:09<1:17:44,  1.67s/it] 72%|███████▏  | 7038/9822 [3:37:11<1:17:53,  1.68s/it] 72%|███████▏  | 7039/9822 [3:37:13<1:18:00,  1.68s/it] 72%|███████▏  | 7040/9822 [3:37:14<1:17:54,  1.68s/it] 72%|███████▏  | 7041/9822 [3:37:16<1:17:48,  1.68s/it] 72%|███████▏  | 7042/9822 [3:37:18<1:17:37,  1.68s/it] 72%|███████▏  | 7043/9822 [3:37:19<1:17:49,  1.68s/it] 72%|███████▏  | 7044/9822 [3:37:21<1:17:34,  1.68s/it] 72%|███████▏  | 7045/9822 [3:37:23<1:17:43,  1.68s/it] 72%|███████▏  | 7046/9822 [3:37:24<1:17:45,  1.68s/it] 72%|███████▏  | 7047/9822 [3:37:26<1:17:47,  1.68s/it] 72%|███████▏  | 7048/9822 [3:37:28<1:17:32,  1.68s/it] 72%|███████▏  | 7049/9822 [3:37:29<1:17:28,  1.68s/it] 72%|███████▏  | 7050/9822 [3:37:31<1:17:16,  1.67s/it] 72%|███████▏  | 7051/9822 [3:37:33<1:18:52,  1.71s/it] 72%|███████▏  | 7052/9822 [3:37:35<1:17:46,  1.68s/it] 72%|███████▏  | 7053/9822 [3:37:36<1:17:32,  1.68s/it] 72%|███████▏  | 7054/9822 [3:37:38<1:17:33,  1.68s/it] 72%|███████▏  | 7055/9822 [3:37:40<1:17:18,  1.68s/it] 72%|███████▏  | 7056/9822 [3:37:41<1:17:19,  1.68s/it] 72%|███████▏  | 7057/9822 [3:37:43<1:17:17,  1.68s/it] 72%|███████▏  | 7058/9822 [3:37:45<1:17:08,  1.67s/it] 72%|███████▏  | 7059/9822 [3:37:46<1:17:19,  1.68s/it] 72%|███████▏  | 7060/9822 [3:37:48<1:17:19,  1.68s/it] 72%|███████▏  | 7061/9822 [3:37:50<1:17:20,  1.68s/it] 72%|███████▏  | 7062/9822 [3:37:51<1:17:22,  1.68s/it] 72%|███████▏  | 7063/9822 [3:37:53<1:17:13,  1.68s/it] 72%|███████▏  | 7064/9822 [3:37:55<1:17:05,  1.68s/it] 72%|███████▏  | 7065/9822 [3:37:56<1:17:05,  1.68s/it] 72%|███████▏  | 7066/9822 [3:37:58<1:17:11,  1.68s/it] 72%|███████▏  | 7067/9822 [3:38:00<1:17:13,  1.68s/it] 72%|███████▏  | 7068/9822 [3:38:01<1:17:04,  1.68s/it] 72%|███████▏  | 7069/9822 [3:38:03<1:16:59,  1.68s/it] 72%|███████▏  | 7070/9822 [3:38:05<1:17:05,  1.68s/it] 72%|███████▏  | 7071/9822 [3:38:06<1:17:07,  1.68s/it] 72%|███████▏  | 7072/9822 [3:38:08<1:17:09,  1.68s/it] 72%|███████▏  | 7073/9822 [3:38:10<1:17:09,  1.68s/it] 72%|███████▏  | 7074/9822 [3:38:11<1:17:11,  1.69s/it] 72%|███████▏  | 7075/9822 [3:38:13<1:17:06,  1.68s/it] 72%|███████▏  | 7076/9822 [3:38:15<1:17:02,  1.68s/it] 72%|███████▏  | 7077/9822 [3:38:17<1:17:00,  1.68s/it] 72%|███████▏  | 7078/9822 [3:38:18<1:16:50,  1.68s/it] 72%|███████▏  | 7079/9822 [3:38:20<1:16:48,  1.68s/it] 72%|███████▏  | 7080/9822 [3:38:22<1:16:46,  1.68s/it] 72%|███████▏  | 7081/9822 [3:38:23<1:16:47,  1.68s/it] 72%|███████▏  | 7082/9822 [3:38:25<1:16:45,  1.68s/it] 72%|███████▏  | 7083/9822 [3:38:27<1:16:40,  1.68s/it] 72%|███████▏  | 7084/9822 [3:38:28<1:18:03,  1.71s/it] 72%|███████▏  | 7085/9822 [3:38:30<1:17:37,  1.70s/it] 72%|███████▏  | 7086/9822 [3:38:32<1:17:20,  1.70s/it] 72%|███████▏  | 7087/9822 [3:38:33<1:17:09,  1.69s/it] 72%|███████▏  | 7088/9822 [3:38:35<1:17:00,  1.69s/it] 72%|███████▏  | 7089/9822 [3:38:37<1:16:50,  1.69s/it] 72%|███████▏  | 7090/9822 [3:38:38<1:16:43,  1.68s/it] 72%|███████▏  | 7091/9822 [3:38:40<1:16:36,  1.68s/it] 72%|███████▏  | 7092/9822 [3:38:42<1:16:33,  1.68s/it] 72%|███████▏  | 7093/9822 [3:38:44<1:16:32,  1.68s/it] 72%|███████▏  | 7094/9822 [3:38:45<1:16:31,  1.68s/it] 72%|███████▏  | 7095/9822 [3:38:47<1:16:32,  1.68s/it] 72%|███████▏  | 7096/9822 [3:38:49<1:16:32,  1.68s/it] 72%|███████▏  | 7097/9822 [3:38:50<1:16:24,  1.68s/it] 72%|███████▏  | 7098/9822 [3:38:52<1:16:25,  1.68s/it] 72%|███████▏  | 7099/9822 [3:38:54<1:16:16,  1.68s/it] 72%|███████▏  | 7100/9822 [3:38:55<1:16:09,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:19:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:19:42 - INFO - __main__ - ***** test Results*****
04/29/2024 15:19:42 - INFO - __main__ -   Training step = 7100
04/29/2024 15:19:42 - INFO - __main__ -  test_accuracy:0.8700585651537335 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:19:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:19:46 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:19:46 - INFO - __main__ -   Training step = 7100
04/29/2024 15:19:46 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:19:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:19:55 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:19:55 - INFO - __main__ -   Training step = 7100
04/29/2024 15:19:55 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 72%|███████▏  | 7101/9822 [3:39:15<5:14:53,  6.94s/it] 72%|███████▏  | 7102/9822 [3:39:16<4:03:11,  5.36s/it] 72%|███████▏  | 7103/9822 [3:39:18<3:12:54,  4.26s/it] 72%|███████▏  | 7104/9822 [3:39:20<2:37:33,  3.48s/it] 72%|███████▏  | 7105/9822 [3:39:21<2:12:56,  2.94s/it] 72%|███████▏  | 7106/9822 [3:39:23<1:55:47,  2.56s/it] 72%|███████▏  | 7107/9822 [3:39:25<1:43:39,  2.29s/it] 72%|███████▏  | 7108/9822 [3:39:26<1:35:18,  2.11s/it] 72%|███████▏  | 7109/9822 [3:39:28<1:29:27,  1.98s/it] 72%|███████▏  | 7110/9822 [3:39:30<1:25:22,  1.89s/it] 72%|███████▏  | 7111/9822 [3:39:31<1:23:43,  1.85s/it] 72%|███████▏  | 7112/9822 [3:39:33<1:21:00,  1.79s/it] 72%|███████▏  | 7113/9822 [3:39:35<1:19:04,  1.75s/it] 72%|███████▏  | 7114/9822 [3:39:36<1:17:48,  1.72s/it] 72%|███████▏  | 7115/9822 [3:39:38<1:16:48,  1.70s/it] 72%|███████▏  | 7116/9822 [3:39:40<1:16:11,  1.69s/it] 72%|███████▏  | 7117/9822 [3:39:41<1:15:46,  1.68s/it] 72%|███████▏  | 7118/9822 [3:39:43<1:15:21,  1.67s/it] 72%|███████▏  | 7119/9822 [3:39:45<1:15:03,  1.67s/it] 72%|███████▏  | 7120/9822 [3:39:46<1:15:08,  1.67s/it] 73%|███████▎  | 7121/9822 [3:39:48<1:15:16,  1.67s/it] 73%|███████▎  | 7122/9822 [3:39:50<1:15:26,  1.68s/it] 73%|███████▎  | 7123/9822 [3:39:51<1:15:12,  1.67s/it] 73%|███████▎  | 7124/9822 [3:39:53<1:15:20,  1.68s/it] 73%|███████▎  | 7125/9822 [3:39:55<1:15:22,  1.68s/it] 73%|███████▎  | 7126/9822 [3:39:56<1:15:20,  1.68s/it] 73%|███████▎  | 7127/9822 [3:39:58<1:15:12,  1.67s/it] 73%|███████▎  | 7128/9822 [3:40:00<1:15:11,  1.67s/it] 73%|███████▎  | 7129/9822 [3:40:01<1:15:14,  1.68s/it] 73%|███████▎  | 7130/9822 [3:40:03<1:15:14,  1.68s/it] 73%|███████▎  | 7131/9822 [3:40:05<1:15:13,  1.68s/it] 73%|███████▎  | 7132/9822 [3:40:06<1:15:13,  1.68s/it] 73%|███████▎  | 7133/9822 [3:40:08<1:15:08,  1.68s/it] 73%|███████▎  | 7134/9822 [3:40:10<1:15:04,  1.68s/it] 73%|███████▎  | 7135/9822 [3:40:11<1:14:57,  1.67s/it] 73%|███████▎  | 7136/9822 [3:40:13<1:15:03,  1.68s/it] 73%|███████▎  | 7137/9822 [3:40:15<1:15:00,  1.68s/it] 73%|███████▎  | 7138/9822 [3:40:16<1:15:34,  1.69s/it] 73%|███████▎  | 7139/9822 [3:40:18<1:15:16,  1.68s/it] 73%|███████▎  | 7140/9822 [3:40:20<1:15:02,  1.68s/it] 73%|███████▎  | 7141/9822 [3:40:21<1:14:48,  1.67s/it] 73%|███████▎  | 7142/9822 [3:40:23<1:14:54,  1.68s/it] 73%|███████▎  | 7143/9822 [3:40:25<1:14:36,  1.67s/it] 73%|███████▎  | 7144/9822 [3:40:26<1:14:34,  1.67s/it] 73%|███████▎  | 7145/9822 [3:40:28<1:14:43,  1.67s/it] 73%|███████▎  | 7146/9822 [3:40:30<1:14:48,  1.68s/it] 73%|███████▎  | 7147/9822 [3:40:32<1:14:46,  1.68s/it] 73%|███████▎  | 7148/9822 [3:40:33<1:14:47,  1.68s/it] 73%|███████▎  | 7149/9822 [3:40:35<1:14:50,  1.68s/it] 73%|███████▎  | 7150/9822 [3:40:37<1:14:50,  1.68s/it] 73%|███████▎  | 7151/9822 [3:40:38<1:14:42,  1.68s/it] 73%|███████▎  | 7152/9822 [3:40:40<1:14:35,  1.68s/it] 73%|███████▎  | 7153/9822 [3:40:42<1:14:36,  1.68s/it] 73%|███████▎  | 7154/9822 [3:40:43<1:14:25,  1.67s/it] 73%|███████▎  | 7155/9822 [3:40:45<1:14:22,  1.67s/it] 73%|███████▎  | 7156/9822 [3:40:47<1:14:16,  1.67s/it] 73%|███████▎  | 7157/9822 [3:40:48<1:14:13,  1.67s/it] 73%|███████▎  | 7158/9822 [3:40:50<1:14:05,  1.67s/it] 73%|███████▎  | 7159/9822 [3:40:52<1:13:58,  1.67s/it] 73%|███████▎  | 7160/9822 [3:40:53<1:13:54,  1.67s/it] 73%|███████▎  | 7161/9822 [3:40:55<1:13:49,  1.66s/it] 73%|███████▎  | 7162/9822 [3:40:57<1:13:52,  1.67s/it] 73%|███████▎  | 7163/9822 [3:40:58<1:13:57,  1.67s/it] 73%|███████▎  | 7164/9822 [3:41:00<1:13:47,  1.67s/it] 73%|███████▎  | 7165/9822 [3:41:02<1:15:08,  1.70s/it] 73%|███████▎  | 7166/9822 [3:41:03<1:14:36,  1.69s/it] 73%|███████▎  | 7167/9822 [3:41:05<1:14:12,  1.68s/it] 73%|███████▎  | 7168/9822 [3:41:07<1:13:56,  1.67s/it] 73%|███████▎  | 7169/9822 [3:41:08<1:13:46,  1.67s/it] 73%|███████▎  | 7170/9822 [3:41:10<1:14:04,  1.68s/it] 73%|███████▎  | 7171/9822 [3:41:12<1:13:59,  1.67s/it] 73%|███████▎  | 7172/9822 [3:41:13<1:13:52,  1.67s/it] 73%|███████▎  | 7173/9822 [3:41:15<1:13:42,  1.67s/it] 73%|███████▎  | 7174/9822 [3:41:17<1:13:35,  1.67s/it] 73%|███████▎  | 7175/9822 [3:41:18<1:13:31,  1.67s/it] 73%|███████▎  | 7176/9822 [3:41:20<1:13:36,  1.67s/it] 73%|███████▎  | 7177/9822 [3:41:22<1:13:40,  1.67s/it] 73%|███████▎  | 7178/9822 [3:41:23<1:13:42,  1.67s/it] 73%|███████▎  | 7179/9822 [3:41:25<1:13:35,  1.67s/it] 73%|███████▎  | 7180/9822 [3:41:27<1:13:29,  1.67s/it] 73%|███████▎  | 7181/9822 [3:41:28<1:13:17,  1.67s/it] 73%|███████▎  | 7182/9822 [3:41:30<1:13:27,  1.67s/it] 73%|███████▎  | 7183/9822 [3:41:32<1:13:29,  1.67s/it] 73%|███████▎  | 7184/9822 [3:41:33<1:13:21,  1.67s/it] 73%|███████▎  | 7185/9822 [3:41:35<1:13:28,  1.67s/it] 73%|███████▎  | 7186/9822 [3:41:37<1:13:32,  1.67s/it] 73%|███████▎  | 7187/9822 [3:41:38<1:13:38,  1.68s/it] 73%|███████▎  | 7188/9822 [3:41:40<1:13:36,  1.68s/it] 73%|███████▎  | 7189/9822 [3:41:42<1:13:28,  1.67s/it] 73%|███████▎  | 7190/9822 [3:41:43<1:13:32,  1.68s/it] 73%|███████▎  | 7191/9822 [3:41:45<1:13:36,  1.68s/it] 73%|███████▎  | 7192/9822 [3:41:47<1:13:24,  1.67s/it] 73%|███████▎  | 7193/9822 [3:41:48<1:13:17,  1.67s/it] 73%|███████▎  | 7194/9822 [3:41:50<1:13:17,  1.67s/it] 73%|███████▎  | 7195/9822 [3:41:52<1:13:14,  1.67s/it] 73%|███████▎  | 7196/9822 [3:41:53<1:13:02,  1.67s/it] 73%|███████▎  | 7197/9822 [3:41:55<1:13:08,  1.67s/it] 73%|███████▎  | 7198/9822 [3:41:57<1:14:36,  1.71s/it] 73%|███████▎  | 7199/9822 [3:41:59<1:14:10,  1.70s/it] 73%|███████▎  | 7200/9822 [3:42:00<1:13:59,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:22:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:22:47 - INFO - __main__ - ***** test Results*****
04/29/2024 15:22:47 - INFO - __main__ -   Training step = 7200
04/29/2024 15:22:47 - INFO - __main__ -  test_accuracy:0.8704245973645681 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:22:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:22:52 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:22:52 - INFO - __main__ -   Training step = 7200
04/29/2024 15:22:52 - INFO - __main__ -  eval_accuracy:0.8509703405346027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:23:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:23:00 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:23:00 - INFO - __main__ -   Training step = 7200
04/29/2024 15:23:00 - INFO - __main__ -  eval_accuracy:0.9102892713291835 
 73%|███████▎  | 7201/9822 [3:42:20<5:03:32,  6.95s/it] 73%|███████▎  | 7202/9822 [3:42:21<3:54:11,  5.36s/it] 73%|███████▎  | 7203/9822 [3:42:23<3:05:49,  4.26s/it] 73%|███████▎  | 7204/9822 [3:42:25<2:31:52,  3.48s/it] 73%|███████▎  | 7205/9822 [3:42:26<2:08:05,  2.94s/it] 73%|███████▎  | 7206/9822 [3:42:28<1:51:17,  2.55s/it] 73%|███████▎  | 7207/9822 [3:42:30<1:39:35,  2.29s/it] 73%|███████▎  | 7208/9822 [3:42:31<1:31:31,  2.10s/it] 73%|███████▎  | 7209/9822 [3:42:33<1:25:41,  1.97s/it] 73%|███████▎  | 7210/9822 [3:42:35<1:21:53,  1.88s/it] 73%|███████▎  | 7211/9822 [3:42:36<1:19:13,  1.82s/it] 73%|███████▎  | 7212/9822 [3:42:38<1:17:18,  1.78s/it] 73%|███████▎  | 7213/9822 [3:42:40<1:16:09,  1.75s/it] 73%|███████▎  | 7214/9822 [3:42:41<1:15:18,  1.73s/it] 73%|███████▎  | 7215/9822 [3:42:43<1:14:36,  1.72s/it] 73%|███████▎  | 7216/9822 [3:42:45<1:14:10,  1.71s/it] 73%|███████▎  | 7217/9822 [3:42:46<1:13:53,  1.70s/it] 73%|███████▎  | 7218/9822 [3:42:48<1:13:29,  1.69s/it] 73%|███████▎  | 7219/9822 [3:42:50<1:13:13,  1.69s/it] 74%|███████▎  | 7220/9822 [3:42:51<1:13:09,  1.69s/it] 74%|███████▎  | 7221/9822 [3:42:53<1:12:55,  1.68s/it] 74%|███████▎  | 7222/9822 [3:42:55<1:12:46,  1.68s/it] 74%|███████▎  | 7223/9822 [3:42:56<1:12:33,  1.68s/it] 74%|███████▎  | 7224/9822 [3:42:58<1:11:39,  1.66s/it] 74%|███████▎  | 7225/9822 [3:43:00<1:11:50,  1.66s/it] 74%|███████▎  | 7226/9822 [3:43:01<1:12:00,  1.66s/it] 74%|███████▎  | 7227/9822 [3:43:03<1:11:58,  1.66s/it] 74%|███████▎  | 7228/9822 [3:43:05<1:11:58,  1.66s/it] 74%|███████▎  | 7229/9822 [3:43:06<1:12:09,  1.67s/it] 74%|███████▎  | 7230/9822 [3:43:08<1:11:56,  1.67s/it] 74%|███████▎  | 7231/9822 [3:43:10<1:11:53,  1.66s/it] 74%|███████▎  | 7232/9822 [3:43:11<1:11:59,  1.67s/it] 74%|███████▎  | 7233/9822 [3:43:13<1:11:52,  1.67s/it] 74%|███████▎  | 7234/9822 [3:43:15<1:13:08,  1.70s/it] 74%|███████▎  | 7235/9822 [3:43:16<1:12:38,  1.68s/it] 74%|███████▎  | 7236/9822 [3:43:18<1:12:17,  1.68s/it] 74%|███████▎  | 7237/9822 [3:43:20<1:12:03,  1.67s/it] 74%|███████▎  | 7238/9822 [3:43:21<1:12:02,  1.67s/it] 74%|███████▎  | 7239/9822 [3:43:23<1:12:01,  1.67s/it] 74%|███████▎  | 7240/9822 [3:43:25<1:11:55,  1.67s/it] 74%|███████▎  | 7241/9822 [3:43:26<1:11:57,  1.67s/it] 74%|███████▎  | 7242/9822 [3:43:28<1:11:56,  1.67s/it] 74%|███████▎  | 7243/9822 [3:43:30<1:12:00,  1.68s/it] 74%|███████▍  | 7244/9822 [3:43:31<1:12:05,  1.68s/it] 74%|███████▍  | 7245/9822 [3:43:33<1:12:05,  1.68s/it] 74%|███████▍  | 7246/9822 [3:43:35<1:11:58,  1.68s/it] 74%|███████▍  | 7247/9822 [3:43:36<1:11:52,  1.67s/it] 74%|███████▍  | 7248/9822 [3:43:38<1:11:36,  1.67s/it] 74%|███████▍  | 7249/9822 [3:43:40<1:11:41,  1.67s/it] 74%|███████▍  | 7250/9822 [3:43:41<1:11:42,  1.67s/it] 74%|███████▍  | 7251/9822 [3:43:43<1:11:34,  1.67s/it] 74%|███████▍  | 7252/9822 [3:43:45<1:11:42,  1.67s/it] 74%|███████▍  | 7253/9822 [3:43:47<1:11:45,  1.68s/it] 74%|███████▍  | 7254/9822 [3:43:48<1:11:44,  1.68s/it] 74%|███████▍  | 7255/9822 [3:43:50<1:11:44,  1.68s/it] 74%|███████▍  | 7256/9822 [3:43:52<1:11:39,  1.68s/it] 74%|███████▍  | 7257/9822 [3:43:53<1:11:27,  1.67s/it] 74%|███████▍  | 7258/9822 [3:43:55<1:11:31,  1.67s/it] 74%|███████▍  | 7259/9822 [3:43:57<1:11:30,  1.67s/it] 74%|███████▍  | 7260/9822 [3:43:58<1:11:28,  1.67s/it] 74%|███████▍  | 7261/9822 [3:44:00<1:12:38,  1.70s/it] 74%|███████▍  | 7262/9822 [3:44:02<1:12:10,  1.69s/it] 74%|███████▍  | 7263/9822 [3:44:03<1:11:57,  1.69s/it] 74%|███████▍  | 7264/9822 [3:44:05<1:11:41,  1.68s/it] 74%|███████▍  | 7265/9822 [3:44:07<1:11:31,  1.68s/it] 74%|███████▍  | 7266/9822 [3:44:08<1:11:28,  1.68s/it] 74%|███████▍  | 7267/9822 [3:44:10<1:11:25,  1.68s/it] 74%|███████▍  | 7268/9822 [3:44:12<1:11:23,  1.68s/it] 74%|███████▍  | 7269/9822 [3:44:13<1:11:24,  1.68s/it] 74%|███████▍  | 7270/9822 [3:44:15<1:11:25,  1.68s/it] 74%|███████▍  | 7271/9822 [3:44:17<1:11:13,  1.68s/it] 74%|███████▍  | 7272/9822 [3:44:18<1:11:16,  1.68s/it] 74%|███████▍  | 7273/9822 [3:44:20<1:11:17,  1.68s/it] 74%|███████▍  | 7274/9822 [3:44:22<1:11:10,  1.68s/it] 74%|███████▍  | 7275/9822 [3:44:23<1:11:01,  1.67s/it] 74%|███████▍  | 7276/9822 [3:44:25<1:10:47,  1.67s/it] 74%|███████▍  | 7277/9822 [3:44:27<1:10:46,  1.67s/it] 74%|███████▍  | 7278/9822 [3:44:28<1:10:45,  1.67s/it] 74%|███████▍  | 7279/9822 [3:44:30<1:10:45,  1.67s/it] 74%|███████▍  | 7280/9822 [3:44:32<1:10:51,  1.67s/it] 74%|███████▍  | 7281/9822 [3:44:33<1:10:43,  1.67s/it] 74%|███████▍  | 7282/9822 [3:44:35<1:10:31,  1.67s/it] 74%|███████▍  | 7283/9822 [3:44:37<1:10:33,  1.67s/it] 74%|███████▍  | 7284/9822 [3:44:38<1:10:34,  1.67s/it] 74%|███████▍  | 7285/9822 [3:44:40<1:10:32,  1.67s/it] 74%|███████▍  | 7286/9822 [3:44:42<1:10:36,  1.67s/it] 74%|███████▍  | 7287/9822 [3:44:43<1:10:38,  1.67s/it] 74%|███████▍  | 7288/9822 [3:44:45<1:11:53,  1.70s/it] 74%|███████▍  | 7289/9822 [3:44:47<1:11:30,  1.69s/it] 74%|███████▍  | 7290/9822 [3:44:49<1:11:04,  1.68s/it] 74%|███████▍  | 7291/9822 [3:44:50<1:10:56,  1.68s/it] 74%|███████▍  | 7292/9822 [3:44:52<1:10:42,  1.68s/it] 74%|███████▍  | 7293/9822 [3:44:54<1:10:34,  1.67s/it] 74%|███████▍  | 7294/9822 [3:44:55<1:10:28,  1.67s/it] 74%|███████▍  | 7295/9822 [3:44:57<1:10:19,  1.67s/it] 74%|███████▍  | 7296/9822 [3:44:59<1:10:25,  1.67s/it] 74%|███████▍  | 7297/9822 [3:45:00<1:10:23,  1.67s/it] 74%|███████▍  | 7298/9822 [3:45:02<1:10:15,  1.67s/it] 74%|███████▍  | 7299/9822 [3:45:04<1:10:20,  1.67s/it] 74%|███████▍  | 7300/9822 [3:45:05<1:10:12,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:25:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:25:52 - INFO - __main__ - ***** test Results*****
04/29/2024 15:25:52 - INFO - __main__ -   Training step = 7300
04/29/2024 15:25:52 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:25:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:25:56 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:25:56 - INFO - __main__ -   Training step = 7300
04/29/2024 15:25:56 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:26:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:26:05 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:26:05 - INFO - __main__ -   Training step = 7300
04/29/2024 15:26:05 - INFO - __main__ -  eval_accuracy:0.9139509337239107 
 74%|███████▍  | 7301/9822 [3:45:24<4:51:24,  6.94s/it] 74%|███████▍  | 7302/9822 [3:45:26<3:44:46,  5.35s/it] 74%|███████▍  | 7303/9822 [3:45:28<2:58:10,  4.24s/it] 74%|███████▍  | 7304/9822 [3:45:29<2:25:37,  3.47s/it] 74%|███████▍  | 7305/9822 [3:45:31<2:02:49,  2.93s/it] 74%|███████▍  | 7306/9822 [3:45:33<1:46:54,  2.55s/it] 74%|███████▍  | 7307/9822 [3:45:34<1:35:45,  2.28s/it] 74%|███████▍  | 7308/9822 [3:45:36<1:27:59,  2.10s/it] 74%|███████▍  | 7309/9822 [3:45:38<1:22:26,  1.97s/it] 74%|███████▍  | 7310/9822 [3:45:39<1:18:06,  1.87s/it] 74%|███████▍  | 7311/9822 [3:45:41<1:15:41,  1.81s/it] 74%|███████▍  | 7312/9822 [3:45:43<1:13:49,  1.76s/it] 74%|███████▍  | 7313/9822 [3:45:44<1:12:44,  1.74s/it] 74%|███████▍  | 7314/9822 [3:45:46<1:11:49,  1.72s/it] 74%|███████▍  | 7315/9822 [3:45:48<1:11:08,  1.70s/it] 74%|███████▍  | 7316/9822 [3:45:49<1:10:53,  1.70s/it] 74%|███████▍  | 7317/9822 [3:45:51<1:11:51,  1.72s/it] 75%|███████▍  | 7318/9822 [3:45:53<1:11:10,  1.71s/it] 75%|███████▍  | 7319/9822 [3:45:55<1:10:32,  1.69s/it] 75%|███████▍  | 7320/9822 [3:45:56<1:10:23,  1.69s/it] 75%|███████▍  | 7321/9822 [3:45:58<1:10:18,  1.69s/it] 75%|███████▍  | 7322/9822 [3:46:00<1:10:04,  1.68s/it] 75%|███████▍  | 7323/9822 [3:46:01<1:09:54,  1.68s/it] 75%|███████▍  | 7324/9822 [3:46:03<1:10:13,  1.69s/it] 75%|███████▍  | 7325/9822 [3:46:05<1:10:01,  1.68s/it] 75%|███████▍  | 7326/9822 [3:46:06<1:09:54,  1.68s/it] 75%|███████▍  | 7327/9822 [3:46:08<1:09:50,  1.68s/it] 75%|███████▍  | 7328/9822 [3:46:10<1:09:48,  1.68s/it] 75%|███████▍  | 7329/9822 [3:46:11<1:09:44,  1.68s/it] 75%|███████▍  | 7330/9822 [3:46:13<1:09:31,  1.67s/it] 75%|███████▍  | 7331/9822 [3:46:15<1:09:29,  1.67s/it] 75%|███████▍  | 7332/9822 [3:46:16<1:09:30,  1.67s/it] 75%|███████▍  | 7333/9822 [3:46:18<1:09:36,  1.68s/it] 75%|███████▍  | 7334/9822 [3:46:20<1:09:21,  1.67s/it] 75%|███████▍  | 7335/9822 [3:46:21<1:09:13,  1.67s/it] 75%|███████▍  | 7336/9822 [3:46:23<1:09:11,  1.67s/it] 75%|███████▍  | 7337/9822 [3:46:25<1:09:09,  1.67s/it] 75%|███████▍  | 7338/9822 [3:46:26<1:09:03,  1.67s/it] 75%|███████▍  | 7339/9822 [3:46:28<1:09:07,  1.67s/it] 75%|███████▍  | 7340/9822 [3:46:30<1:08:56,  1.67s/it] 75%|███████▍  | 7341/9822 [3:46:31<1:08:59,  1.67s/it] 75%|███████▍  | 7342/9822 [3:46:33<1:09:03,  1.67s/it] 75%|███████▍  | 7343/9822 [3:46:35<1:08:54,  1.67s/it] 75%|███████▍  | 7344/9822 [3:46:36<1:09:04,  1.67s/it] 75%|███████▍  | 7345/9822 [3:46:38<1:09:03,  1.67s/it] 75%|███████▍  | 7346/9822 [3:46:40<1:09:00,  1.67s/it] 75%|███████▍  | 7347/9822 [3:46:41<1:09:05,  1.67s/it] 75%|███████▍  | 7348/9822 [3:46:43<1:09:02,  1.67s/it] 75%|███████▍  | 7349/9822 [3:46:45<1:08:51,  1.67s/it] 75%|███████▍  | 7350/9822 [3:46:47<1:10:11,  1.70s/it] 75%|███████▍  | 7351/9822 [3:46:48<1:09:49,  1.70s/it] 75%|███████▍  | 7352/9822 [3:46:50<1:09:33,  1.69s/it] 75%|███████▍  | 7353/9822 [3:46:52<1:09:21,  1.69s/it] 75%|███████▍  | 7354/9822 [3:46:53<1:09:03,  1.68s/it] 75%|███████▍  | 7355/9822 [3:46:55<1:09:06,  1.68s/it] 75%|███████▍  | 7356/9822 [3:46:57<1:09:06,  1.68s/it] 75%|███████▍  | 7357/9822 [3:46:58<1:08:49,  1.68s/it] 75%|███████▍  | 7358/9822 [3:47:00<1:08:39,  1.67s/it] 75%|███████▍  | 7359/9822 [3:47:02<1:08:46,  1.68s/it] 75%|███████▍  | 7360/9822 [3:47:03<1:08:41,  1.67s/it] 75%|███████▍  | 7361/9822 [3:47:05<1:08:42,  1.68s/it] 75%|███████▍  | 7362/9822 [3:47:07<1:08:46,  1.68s/it] 75%|███████▍  | 7363/9822 [3:47:08<1:08:46,  1.68s/it] 75%|███████▍  | 7364/9822 [3:47:10<1:08:49,  1.68s/it] 75%|███████▍  | 7365/9822 [3:47:12<1:08:45,  1.68s/it] 75%|███████▍  | 7366/9822 [3:47:13<1:08:41,  1.68s/it] 75%|███████▌  | 7367/9822 [3:47:15<1:08:34,  1.68s/it] 75%|███████▌  | 7368/9822 [3:47:17<1:08:21,  1.67s/it] 75%|███████▌  | 7369/9822 [3:47:18<1:08:29,  1.68s/it] 75%|███████▌  | 7370/9822 [3:47:20<1:08:25,  1.67s/it] 75%|███████▌  | 7371/9822 [3:47:22<1:08:28,  1.68s/it] 75%|███████▌  | 7372/9822 [3:47:24<1:09:42,  1.71s/it] 75%|███████▌  | 7373/9822 [3:47:25<1:09:09,  1.69s/it] 75%|███████▌  | 7374/9822 [3:47:27<1:08:53,  1.69s/it] 75%|███████▌  | 7375/9822 [3:47:29<1:08:39,  1.68s/it] 75%|███████▌  | 7376/9822 [3:47:30<1:08:27,  1.68s/it] 75%|███████▌  | 7377/9822 [3:47:32<1:08:17,  1.68s/it] 75%|███████▌  | 7378/9822 [3:47:34<1:08:17,  1.68s/it] 75%|███████▌  | 7379/9822 [3:47:35<1:08:07,  1.67s/it] 75%|███████▌  | 7380/9822 [3:47:37<1:08:04,  1.67s/it] 75%|███████▌  | 7381/9822 [3:47:39<1:08:08,  1.68s/it] 75%|███████▌  | 7382/9822 [3:47:40<1:08:06,  1.67s/it] 75%|███████▌  | 7383/9822 [3:47:42<1:08:06,  1.68s/it] 75%|███████▌  | 7384/9822 [3:47:44<1:08:06,  1.68s/it] 75%|███████▌  | 7385/9822 [3:47:45<1:07:54,  1.67s/it] 75%|███████▌  | 7386/9822 [3:47:47<1:07:49,  1.67s/it] 75%|███████▌  | 7387/9822 [3:47:49<1:07:44,  1.67s/it] 75%|███████▌  | 7388/9822 [3:47:50<1:07:38,  1.67s/it] 75%|███████▌  | 7389/9822 [3:47:52<1:07:43,  1.67s/it] 75%|███████▌  | 7390/9822 [3:47:54<1:07:43,  1.67s/it] 75%|███████▌  | 7391/9822 [3:47:55<1:07:31,  1.67s/it] 75%|███████▌  | 7392/9822 [3:47:57<1:07:36,  1.67s/it] 75%|███████▌  | 7393/9822 [3:47:59<1:07:36,  1.67s/it] 75%|███████▌  | 7394/9822 [3:48:00<1:07:32,  1.67s/it] 75%|███████▌  | 7395/9822 [3:48:02<1:07:25,  1.67s/it] 75%|███████▌  | 7396/9822 [3:48:04<1:06:49,  1.65s/it] 75%|███████▌  | 7397/9822 [3:48:05<1:07:03,  1.66s/it] 75%|███████▌  | 7398/9822 [3:48:07<1:07:14,  1.66s/it] 75%|███████▌  | 7399/9822 [3:48:09<1:08:52,  1.71s/it] 75%|███████▌  | 7400/9822 [3:48:10<1:08:16,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:28:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:28:57 - INFO - __main__ - ***** test Results*****
04/29/2024 15:28:57 - INFO - __main__ -   Training step = 7400
04/29/2024 15:28:57 - INFO - __main__ -  test_accuracy:0.8682284040995608 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:29:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:29:02 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:29:02 - INFO - __main__ -   Training step = 7400
04/29/2024 15:29:02 - INFO - __main__ -  eval_accuracy:0.85060417429513 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:29:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:29:10 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:29:10 - INFO - __main__ -   Training step = 7400
04/29/2024 15:29:10 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 75%|███████▌  | 7401/9822 [3:48:30<4:40:26,  6.95s/it] 75%|███████▌  | 7402/9822 [3:48:31<3:36:18,  5.36s/it] 75%|███████▌  | 7403/9822 [3:48:33<2:51:29,  4.25s/it] 75%|███████▌  | 7404/9822 [3:48:35<2:20:05,  3.48s/it] 75%|███████▌  | 7405/9822 [3:48:36<1:58:07,  2.93s/it] 75%|███████▌  | 7406/9822 [3:48:38<1:42:46,  2.55s/it] 75%|███████▌  | 7407/9822 [3:48:40<1:31:56,  2.28s/it] 75%|███████▌  | 7408/9822 [3:48:41<1:24:29,  2.10s/it] 75%|███████▌  | 7409/9822 [3:48:43<1:19:08,  1.97s/it] 75%|███████▌  | 7410/9822 [3:48:45<1:15:31,  1.88s/it] 75%|███████▌  | 7411/9822 [3:48:46<1:12:54,  1.81s/it] 75%|███████▌  | 7412/9822 [3:48:48<1:11:07,  1.77s/it] 75%|███████▌  | 7413/9822 [3:48:50<1:09:50,  1.74s/it] 75%|███████▌  | 7414/9822 [3:48:51<1:09:08,  1.72s/it] 75%|███████▌  | 7415/9822 [3:48:53<1:08:28,  1.71s/it] 76%|███████▌  | 7416/9822 [3:48:55<1:07:56,  1.69s/it] 76%|███████▌  | 7417/9822 [3:48:56<1:07:46,  1.69s/it] 76%|███████▌  | 7418/9822 [3:48:58<1:07:37,  1.69s/it] 76%|███████▌  | 7419/9822 [3:49:00<1:07:28,  1.68s/it] 76%|███████▌  | 7420/9822 [3:49:01<1:07:14,  1.68s/it] 76%|███████▌  | 7421/9822 [3:49:03<1:07:04,  1.68s/it] 76%|███████▌  | 7422/9822 [3:49:05<1:06:53,  1.67s/it] 76%|███████▌  | 7423/9822 [3:49:06<1:06:55,  1.67s/it] 76%|███████▌  | 7424/9822 [3:49:08<1:06:52,  1.67s/it] 76%|███████▌  | 7425/9822 [3:49:10<1:06:44,  1.67s/it] 76%|███████▌  | 7426/9822 [3:49:11<1:06:46,  1.67s/it] 76%|███████▌  | 7427/9822 [3:49:13<1:07:56,  1.70s/it] 76%|███████▌  | 7428/9822 [3:49:15<1:07:30,  1.69s/it] 76%|███████▌  | 7429/9822 [3:49:16<1:07:21,  1.69s/it] 76%|███████▌  | 7430/9822 [3:49:18<1:07:15,  1.69s/it] 76%|███████▌  | 7431/9822 [3:49:20<1:07:19,  1.69s/it] 76%|███████▌  | 7432/9822 [3:49:21<1:07:16,  1.69s/it] 76%|███████▌  | 7433/9822 [3:49:23<1:07:11,  1.69s/it] 76%|███████▌  | 7434/9822 [3:49:25<1:07:07,  1.69s/it] 76%|███████▌  | 7435/9822 [3:49:27<1:06:58,  1.68s/it] 76%|███████▌  | 7436/9822 [3:49:28<1:06:46,  1.68s/it] 76%|███████▌  | 7437/9822 [3:49:30<1:06:43,  1.68s/it] 76%|███████▌  | 7438/9822 [3:49:32<1:06:40,  1.68s/it] 76%|███████▌  | 7439/9822 [3:49:33<1:06:36,  1.68s/it] 76%|███████▌  | 7440/9822 [3:49:35<1:06:37,  1.68s/it] 76%|███████▌  | 7441/9822 [3:49:37<1:06:37,  1.68s/it] 76%|███████▌  | 7442/9822 [3:49:38<1:06:39,  1.68s/it] 76%|███████▌  | 7443/9822 [3:49:40<1:06:36,  1.68s/it] 76%|███████▌  | 7444/9822 [3:49:42<1:06:49,  1.69s/it] 76%|███████▌  | 7445/9822 [3:49:43<1:06:44,  1.68s/it] 76%|███████▌  | 7446/9822 [3:49:45<1:06:32,  1.68s/it] 76%|███████▌  | 7447/9822 [3:49:47<1:06:31,  1.68s/it] 76%|███████▌  | 7448/9822 [3:49:48<1:06:26,  1.68s/it] 76%|███████▌  | 7449/9822 [3:49:50<1:06:21,  1.68s/it] 76%|███████▌  | 7450/9822 [3:49:52<1:06:19,  1.68s/it] 76%|███████▌  | 7451/9822 [3:49:53<1:06:05,  1.67s/it] 76%|███████▌  | 7452/9822 [3:49:55<1:06:01,  1.67s/it] 76%|███████▌  | 7453/9822 [3:49:57<1:06:03,  1.67s/it] 76%|███████▌  | 7454/9822 [3:49:58<1:06:06,  1.67s/it] 76%|███████▌  | 7455/9822 [3:50:00<1:06:01,  1.67s/it] 76%|███████▌  | 7456/9822 [3:50:02<1:06:12,  1.68s/it] 76%|███████▌  | 7457/9822 [3:50:03<1:06:02,  1.68s/it] 76%|███████▌  | 7458/9822 [3:50:05<1:05:53,  1.67s/it] 76%|███████▌  | 7459/9822 [3:50:07<1:05:44,  1.67s/it] 76%|███████▌  | 7460/9822 [3:50:08<1:05:41,  1.67s/it] 76%|███████▌  | 7461/9822 [3:50:10<1:05:32,  1.67s/it] 76%|███████▌  | 7462/9822 [3:50:12<1:05:23,  1.66s/it] 76%|███████▌  | 7463/9822 [3:50:13<1:05:21,  1.66s/it] 76%|███████▌  | 7464/9822 [3:50:15<1:05:12,  1.66s/it] 76%|███████▌  | 7465/9822 [3:50:17<1:05:23,  1.66s/it] 76%|███████▌  | 7466/9822 [3:50:18<1:05:31,  1.67s/it] 76%|███████▌  | 7467/9822 [3:50:20<1:05:30,  1.67s/it] 76%|███████▌  | 7468/9822 [3:50:22<1:06:40,  1.70s/it] 76%|███████▌  | 7469/9822 [3:50:24<1:06:27,  1.69s/it] 76%|███████▌  | 7470/9822 [3:50:25<1:06:06,  1.69s/it] 76%|███████▌  | 7471/9822 [3:50:27<1:05:55,  1.68s/it] 76%|███████▌  | 7472/9822 [3:50:29<1:05:51,  1.68s/it] 76%|███████▌  | 7473/9822 [3:50:30<1:05:45,  1.68s/it] 76%|███████▌  | 7474/9822 [3:50:32<1:05:30,  1.67s/it] 76%|███████▌  | 7475/9822 [3:50:34<1:05:16,  1.67s/it] 76%|███████▌  | 7476/9822 [3:50:35<1:05:10,  1.67s/it] 76%|███████▌  | 7477/9822 [3:50:37<1:05:10,  1.67s/it] 76%|███████▌  | 7478/9822 [3:50:39<1:05:09,  1.67s/it] 76%|███████▌  | 7479/9822 [3:50:40<1:05:04,  1.67s/it] 76%|███████▌  | 7480/9822 [3:50:42<1:05:07,  1.67s/it] 76%|███████▌  | 7481/9822 [3:50:44<1:05:29,  1.68s/it] 76%|███████▌  | 7482/9822 [3:50:45<1:04:51,  1.66s/it] 76%|███████▌  | 7483/9822 [3:50:47<1:04:50,  1.66s/it] 76%|███████▌  | 7484/9822 [3:50:49<1:04:52,  1.67s/it] 76%|███████▌  | 7485/9822 [3:50:50<1:05:04,  1.67s/it] 76%|███████▌  | 7486/9822 [3:50:52<1:05:07,  1.67s/it] 76%|███████▌  | 7487/9822 [3:50:54<1:04:54,  1.67s/it] 76%|███████▌  | 7488/9822 [3:50:55<1:04:49,  1.67s/it] 76%|███████▌  | 7489/9822 [3:50:57<1:04:57,  1.67s/it] 76%|███████▋  | 7490/9822 [3:50:59<1:04:56,  1.67s/it] 76%|███████▋  | 7491/9822 [3:51:00<1:04:54,  1.67s/it] 76%|███████▋  | 7492/9822 [3:51:02<1:04:57,  1.67s/it] 76%|███████▋  | 7493/9822 [3:51:04<1:04:55,  1.67s/it] 76%|███████▋  | 7494/9822 [3:51:05<1:04:53,  1.67s/it] 76%|███████▋  | 7495/9822 [3:51:07<1:06:01,  1.70s/it] 76%|███████▋  | 7496/9822 [3:51:09<1:05:32,  1.69s/it] 76%|███████▋  | 7497/9822 [3:51:10<1:05:26,  1.69s/it] 76%|███████▋  | 7498/9822 [3:51:12<1:05:15,  1.68s/it] 76%|███████▋  | 7499/9822 [3:51:14<1:05:05,  1.68s/it] 76%|███████▋  | 7500/9822 [3:51:15<1:04:57,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:32:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:32:02 - INFO - __main__ - ***** test Results*****
04/29/2024 15:32:02 - INFO - __main__ -   Training step = 7500
04/29/2024 15:32:02 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:32:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:32:07 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:32:07 - INFO - __main__ -   Training step = 7500
04/29/2024 15:32:07 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:32:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:32:15 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:32:15 - INFO - __main__ -   Training step = 7500
04/29/2024 15:32:15 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 76%|███████▋  | 7501/9822 [3:51:35<4:28:23,  6.94s/it] 76%|███████▋  | 7502/9822 [3:51:36<3:27:03,  5.35s/it] 76%|███████▋  | 7503/9822 [3:51:38<2:44:03,  4.24s/it] 76%|███████▋  | 7504/9822 [3:51:40<2:14:02,  3.47s/it] 76%|███████▋  | 7505/9822 [3:51:41<1:53:01,  2.93s/it] 76%|███████▋  | 7506/9822 [3:51:43<1:38:37,  2.55s/it] 76%|███████▋  | 7507/9822 [3:51:45<1:28:16,  2.29s/it] 76%|███████▋  | 7508/9822 [3:51:46<1:21:11,  2.11s/it] 76%|███████▋  | 7509/9822 [3:51:48<1:16:14,  1.98s/it] 76%|███████▋  | 7510/9822 [3:51:50<1:12:49,  1.89s/it] 76%|███████▋  | 7511/9822 [3:51:51<1:10:21,  1.83s/it] 76%|███████▋  | 7512/9822 [3:51:53<1:08:44,  1.79s/it] 76%|███████▋  | 7513/9822 [3:51:55<1:07:31,  1.75s/it] 77%|███████▋  | 7514/9822 [3:51:56<1:06:28,  1.73s/it] 77%|███████▋  | 7515/9822 [3:51:58<1:05:43,  1.71s/it] 77%|███████▋  | 7516/9822 [3:52:00<1:05:22,  1.70s/it] 77%|███████▋  | 7517/9822 [3:52:01<1:05:11,  1.70s/it] 77%|███████▋  | 7518/9822 [3:52:03<1:04:53,  1.69s/it] 77%|███████▋  | 7519/9822 [3:52:05<1:06:00,  1.72s/it] 77%|███████▋  | 7520/9822 [3:52:07<1:05:29,  1.71s/it] 77%|███████▋  | 7521/9822 [3:52:08<1:05:12,  1.70s/it] 77%|███████▋  | 7522/9822 [3:52:10<1:04:56,  1.69s/it] 77%|███████▋  | 7523/9822 [3:52:12<1:04:38,  1.69s/it] 77%|███████▋  | 7524/9822 [3:52:13<1:04:32,  1.69s/it] 77%|███████▋  | 7525/9822 [3:52:15<1:04:29,  1.68s/it] 77%|███████▋  | 7526/9822 [3:52:17<1:04:27,  1.68s/it] 77%|███████▋  | 7527/9822 [3:52:18<1:04:19,  1.68s/it] 77%|███████▋  | 7528/9822 [3:52:20<1:04:09,  1.68s/it] 77%|███████▋  | 7529/9822 [3:52:22<1:04:00,  1.67s/it] 77%|███████▋  | 7530/9822 [3:52:23<1:03:54,  1.67s/it] 77%|███████▋  | 7531/9822 [3:52:25<1:03:46,  1.67s/it] 77%|███████▋  | 7532/9822 [3:52:27<1:03:46,  1.67s/it] 77%|███████▋  | 7533/9822 [3:52:28<1:03:51,  1.67s/it] 77%|███████▋  | 7534/9822 [3:52:30<1:03:55,  1.68s/it] 77%|███████▋  | 7535/9822 [3:52:32<1:03:59,  1.68s/it] 77%|███████▋  | 7536/9822 [3:52:33<1:04:15,  1.69s/it] 77%|███████▋  | 7537/9822 [3:52:35<1:04:08,  1.68s/it] 77%|███████▋  | 7538/9822 [3:52:37<1:04:09,  1.69s/it] 77%|███████▋  | 7539/9822 [3:52:38<1:03:58,  1.68s/it] 77%|███████▋  | 7540/9822 [3:52:40<1:03:48,  1.68s/it] 77%|███████▋  | 7541/9822 [3:52:42<1:03:43,  1.68s/it] 77%|███████▋  | 7542/9822 [3:52:43<1:03:48,  1.68s/it] 77%|███████▋  | 7543/9822 [3:52:45<1:03:36,  1.67s/it] 77%|███████▋  | 7544/9822 [3:52:47<1:03:41,  1.68s/it] 77%|███████▋  | 7545/9822 [3:52:49<1:05:00,  1.71s/it] 77%|███████▋  | 7546/9822 [3:52:50<1:04:34,  1.70s/it] 77%|███████▋  | 7547/9822 [3:52:52<1:04:24,  1.70s/it] 77%|███████▋  | 7548/9822 [3:52:54<1:04:03,  1.69s/it] 77%|███████▋  | 7549/9822 [3:52:55<1:03:51,  1.69s/it] 77%|███████▋  | 7550/9822 [3:52:57<1:03:48,  1.68s/it] 77%|███████▋  | 7551/9822 [3:52:59<1:03:43,  1.68s/it] 77%|███████▋  | 7552/9822 [3:53:00<1:03:37,  1.68s/it] 77%|███████▋  | 7553/9822 [3:53:02<1:03:40,  1.68s/it] 77%|███████▋  | 7554/9822 [3:53:04<1:03:39,  1.68s/it] 77%|███████▋  | 7555/9822 [3:53:05<1:03:37,  1.68s/it] 77%|███████▋  | 7556/9822 [3:53:07<1:03:29,  1.68s/it] 77%|███████▋  | 7557/9822 [3:53:09<1:03:22,  1.68s/it] 77%|███████▋  | 7558/9822 [3:53:10<1:03:22,  1.68s/it] 77%|███████▋  | 7559/9822 [3:53:12<1:03:22,  1.68s/it] 77%|███████▋  | 7560/9822 [3:53:14<1:03:20,  1.68s/it] 77%|███████▋  | 7561/9822 [3:53:15<1:03:17,  1.68s/it] 77%|███████▋  | 7562/9822 [3:53:17<1:03:11,  1.68s/it] 77%|███████▋  | 7563/9822 [3:53:19<1:03:05,  1.68s/it] 77%|███████▋  | 7564/9822 [3:53:21<1:02:55,  1.67s/it] 77%|███████▋  | 7565/9822 [3:53:22<1:02:58,  1.67s/it] 77%|███████▋  | 7566/9822 [3:53:24<1:03:00,  1.68s/it] 77%|███████▋  | 7567/9822 [3:53:26<1:03:40,  1.69s/it] 77%|███████▋  | 7568/9822 [3:53:27<1:02:56,  1.68s/it] 77%|███████▋  | 7569/9822 [3:53:29<1:03:00,  1.68s/it] 77%|███████▋  | 7570/9822 [3:53:31<1:02:58,  1.68s/it] 77%|███████▋  | 7571/9822 [3:53:32<1:02:58,  1.68s/it] 77%|███████▋  | 7572/9822 [3:53:34<1:02:57,  1.68s/it] 77%|███████▋  | 7573/9822 [3:53:36<1:03:01,  1.68s/it] 77%|███████▋  | 7574/9822 [3:53:37<1:03:01,  1.68s/it] 77%|███████▋  | 7575/9822 [3:53:39<1:03:05,  1.68s/it] 77%|███████▋  | 7576/9822 [3:53:41<1:03:03,  1.68s/it] 77%|███████▋  | 7577/9822 [3:53:42<1:03:01,  1.68s/it] 77%|███████▋  | 7578/9822 [3:53:44<1:04:08,  1.72s/it] 77%|███████▋  | 7579/9822 [3:53:46<1:03:42,  1.70s/it] 77%|███████▋  | 7580/9822 [3:53:48<1:03:20,  1.70s/it] 77%|███████▋  | 7581/9822 [3:53:49<1:03:03,  1.69s/it] 77%|███████▋  | 7582/9822 [3:53:51<1:02:50,  1.68s/it] 77%|███████▋  | 7583/9822 [3:53:53<1:02:49,  1.68s/it] 77%|███████▋  | 7584/9822 [3:53:54<1:02:48,  1.68s/it] 77%|███████▋  | 7585/9822 [3:53:56<1:02:44,  1.68s/it] 77%|███████▋  | 7586/9822 [3:53:58<1:02:43,  1.68s/it] 77%|███████▋  | 7587/9822 [3:53:59<1:02:41,  1.68s/it] 77%|███████▋  | 7588/9822 [3:54:01<1:02:28,  1.68s/it] 77%|███████▋  | 7589/9822 [3:54:03<1:02:35,  1.68s/it] 77%|███████▋  | 7590/9822 [3:54:04<1:02:29,  1.68s/it] 77%|███████▋  | 7591/9822 [3:54:06<1:02:42,  1.69s/it] 77%|███████▋  | 7592/9822 [3:54:08<1:02:39,  1.69s/it] 77%|███████▋  | 7593/9822 [3:54:09<1:02:37,  1.69s/it] 77%|███████▋  | 7594/9822 [3:54:11<1:02:35,  1.69s/it] 77%|███████▋  | 7595/9822 [3:54:13<1:02:33,  1.69s/it] 77%|███████▋  | 7596/9822 [3:54:14<1:02:34,  1.69s/it] 77%|███████▋  | 7597/9822 [3:54:16<1:02:47,  1.69s/it] 77%|███████▋  | 7598/9822 [3:54:18<1:02:34,  1.69s/it] 77%|███████▋  | 7599/9822 [3:54:20<1:02:31,  1.69s/it] 77%|███████▋  | 7600/9822 [3:54:21<1:03:49,  1.72s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:35:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:35:08 - INFO - __main__ - ***** test Results*****
04/29/2024 15:35:08 - INFO - __main__ -   Training step = 7600
04/29/2024 15:35:08 - INFO - __main__ -  test_accuracy:0.8704245973645681 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:35:13 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:35:13 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:35:13 - INFO - __main__ -   Training step = 7600
04/29/2024 15:35:13 - INFO - __main__ -  eval_accuracy:0.85060417429513 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:35:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:35:21 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:35:21 - INFO - __main__ -   Training step = 7600
04/29/2024 15:35:21 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 77%|███████▋  | 7601/9822 [3:54:41<4:19:45,  7.02s/it] 77%|███████▋  | 7602/9822 [3:54:42<3:20:17,  5.41s/it] 77%|███████▋  | 7603/9822 [3:54:44<2:38:42,  4.29s/it] 77%|███████▋  | 7604/9822 [3:54:46<2:10:05,  3.52s/it] 77%|███████▋  | 7605/9822 [3:54:47<1:49:38,  2.97s/it] 77%|███████▋  | 7606/9822 [3:54:49<1:35:12,  2.58s/it] 77%|███████▋  | 7607/9822 [3:54:51<1:25:00,  2.30s/it] 77%|███████▋  | 7608/9822 [3:54:52<1:18:06,  2.12s/it] 77%|███████▋  | 7609/9822 [3:54:54<1:13:08,  1.98s/it] 77%|███████▋  | 7610/9822 [3:54:56<1:09:46,  1.89s/it] 77%|███████▋  | 7611/9822 [3:54:57<1:07:19,  1.83s/it] 77%|███████▋  | 7612/9822 [3:54:59<1:05:34,  1.78s/it] 78%|███████▊  | 7613/9822 [3:55:01<1:04:16,  1.75s/it] 78%|███████▊  | 7614/9822 [3:55:02<1:03:23,  1.72s/it] 78%|███████▊  | 7615/9822 [3:55:04<1:02:59,  1.71s/it] 78%|███████▊  | 7616/9822 [3:55:06<1:02:37,  1.70s/it] 78%|███████▊  | 7617/9822 [3:55:08<1:02:24,  1.70s/it] 78%|███████▊  | 7618/9822 [3:55:09<1:02:07,  1.69s/it] 78%|███████▊  | 7619/9822 [3:55:11<1:02:08,  1.69s/it] 78%|███████▊  | 7620/9822 [3:55:13<1:01:52,  1.69s/it] 78%|███████▊  | 7621/9822 [3:55:14<1:01:44,  1.68s/it] 78%|███████▊  | 7622/9822 [3:55:16<1:01:44,  1.68s/it] 78%|███████▊  | 7623/9822 [3:55:18<1:01:44,  1.68s/it] 78%|███████▊  | 7624/9822 [3:55:19<1:03:25,  1.73s/it] 78%|███████▊  | 7625/9822 [3:55:21<1:02:56,  1.72s/it] 78%|███████▊  | 7626/9822 [3:55:23<1:02:32,  1.71s/it] 78%|███████▊  | 7627/9822 [3:55:25<1:02:13,  1.70s/it] 78%|███████▊  | 7628/9822 [3:55:26<1:01:59,  1.70s/it] 78%|███████▊  | 7629/9822 [3:55:28<1:01:55,  1.69s/it] 78%|███████▊  | 7630/9822 [3:55:30<1:01:39,  1.69s/it] 78%|███████▊  | 7631/9822 [3:55:31<1:01:39,  1.69s/it] 78%|███████▊  | 7632/9822 [3:55:33<1:01:33,  1.69s/it] 78%|███████▊  | 7633/9822 [3:55:35<1:01:30,  1.69s/it] 78%|███████▊  | 7634/9822 [3:55:36<1:01:26,  1.68s/it] 78%|███████▊  | 7635/9822 [3:55:38<1:01:16,  1.68s/it] 78%|███████▊  | 7636/9822 [3:55:40<1:01:11,  1.68s/it] 78%|███████▊  | 7637/9822 [3:55:41<1:01:12,  1.68s/it] 78%|███████▊  | 7638/9822 [3:55:43<1:01:20,  1.69s/it] 78%|███████▊  | 7639/9822 [3:55:45<1:01:17,  1.68s/it] 78%|███████▊  | 7640/9822 [3:55:46<1:01:05,  1.68s/it] 78%|███████▊  | 7641/9822 [3:55:48<1:01:15,  1.69s/it] 78%|███████▊  | 7642/9822 [3:55:50<1:01:04,  1.68s/it] 78%|███████▊  | 7643/9822 [3:55:51<1:01:10,  1.68s/it] 78%|███████▊  | 7644/9822 [3:55:53<1:01:16,  1.69s/it] 78%|███████▊  | 7645/9822 [3:55:55<1:01:11,  1.69s/it] 78%|███████▊  | 7646/9822 [3:55:57<1:01:01,  1.68s/it] 78%|███████▊  | 7647/9822 [3:55:58<1:00:54,  1.68s/it] 78%|███████▊  | 7648/9822 [3:56:00<1:00:49,  1.68s/it] 78%|███████▊  | 7649/9822 [3:56:02<1:00:49,  1.68s/it] 78%|███████▊  | 7650/9822 [3:56:03<1:02:02,  1.71s/it] 78%|███████▊  | 7651/9822 [3:56:05<1:01:52,  1.71s/it] 78%|███████▊  | 7652/9822 [3:56:07<1:01:35,  1.70s/it] 78%|███████▊  | 7653/9822 [3:56:08<1:01:38,  1.70s/it] 78%|███████▊  | 7654/9822 [3:56:10<1:00:48,  1.68s/it] 78%|███████▊  | 7655/9822 [3:56:12<1:00:45,  1.68s/it] 78%|███████▊  | 7656/9822 [3:56:13<1:00:47,  1.68s/it] 78%|███████▊  | 7657/9822 [3:56:15<1:00:43,  1.68s/it] 78%|███████▊  | 7658/9822 [3:56:17<1:01:08,  1.70s/it] 78%|███████▊  | 7659/9822 [3:56:19<1:00:59,  1.69s/it] 78%|███████▊  | 7660/9822 [3:56:20<1:00:51,  1.69s/it] 78%|███████▊  | 7661/9822 [3:56:22<1:00:49,  1.69s/it] 78%|███████▊  | 7662/9822 [3:56:24<1:00:50,  1.69s/it] 78%|███████▊  | 7663/9822 [3:56:25<1:01:10,  1.70s/it] 78%|███████▊  | 7664/9822 [3:56:27<1:00:52,  1.69s/it] 78%|███████▊  | 7665/9822 [3:56:29<1:00:57,  1.70s/it] 78%|███████▊  | 7666/9822 [3:56:30<1:01:02,  1.70s/it] 78%|███████▊  | 7667/9822 [3:56:32<1:00:45,  1.69s/it] 78%|███████▊  | 7668/9822 [3:56:34<1:00:31,  1.69s/it] 78%|███████▊  | 7669/9822 [3:56:35<1:00:26,  1.68s/it] 78%|███████▊  | 7670/9822 [3:56:37<1:00:32,  1.69s/it] 78%|███████▊  | 7671/9822 [3:56:39<1:00:28,  1.69s/it] 78%|███████▊  | 7672/9822 [3:56:40<1:00:24,  1.69s/it] 78%|███████▊  | 7673/9822 [3:56:42<1:00:21,  1.69s/it] 78%|███████▊  | 7674/9822 [3:56:44<1:00:29,  1.69s/it] 78%|███████▊  | 7675/9822 [3:56:46<1:00:37,  1.69s/it] 78%|███████▊  | 7676/9822 [3:56:47<1:00:36,  1.69s/it] 78%|███████▊  | 7677/9822 [3:56:49<1:00:38,  1.70s/it] 78%|███████▊  | 7678/9822 [3:56:51<1:00:26,  1.69s/it] 78%|███████▊  | 7679/9822 [3:56:52<1:00:31,  1.69s/it] 78%|███████▊  | 7680/9822 [3:56:54<1:00:23,  1.69s/it] 78%|███████▊  | 7681/9822 [3:56:56<1:00:19,  1.69s/it] 78%|███████▊  | 7682/9822 [3:56:57<1:00:16,  1.69s/it] 78%|███████▊  | 7683/9822 [3:56:59<1:01:25,  1.72s/it] 78%|███████▊  | 7684/9822 [3:57:01<1:01:01,  1.71s/it] 78%|███████▊  | 7685/9822 [3:57:03<1:01:05,  1.72s/it] 78%|███████▊  | 7686/9822 [3:57:04<1:00:48,  1.71s/it] 78%|███████▊  | 7687/9822 [3:57:06<1:00:46,  1.71s/it] 78%|███████▊  | 7688/9822 [3:57:08<1:00:37,  1.70s/it] 78%|███████▊  | 7689/9822 [3:57:09<1:00:25,  1.70s/it] 78%|███████▊  | 7690/9822 [3:57:11<1:00:18,  1.70s/it] 78%|███████▊  | 7691/9822 [3:57:13<1:00:00,  1.69s/it] 78%|███████▊  | 7692/9822 [3:57:14<59:56,  1.69s/it]   78%|███████▊  | 7693/9822 [3:57:16<59:55,  1.69s/it] 78%|███████▊  | 7694/9822 [3:57:18<59:47,  1.69s/it] 78%|███████▊  | 7695/9822 [3:57:19<59:35,  1.68s/it] 78%|███████▊  | 7696/9822 [3:57:21<59:35,  1.68s/it] 78%|███████▊  | 7697/9822 [3:57:23<59:40,  1.69s/it] 78%|███████▊  | 7698/9822 [3:57:25<1:00:02,  1.70s/it] 78%|███████▊  | 7699/9822 [3:57:26<59:56,  1.69s/it]   78%|███████▊  | 7700/9822 [3:57:28<59:59,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.3714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:38:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:38:15 - INFO - __main__ - ***** test Results*****
04/29/2024 15:38:15 - INFO - __main__ -   Training step = 7700
04/29/2024 15:38:15 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:38:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:38:19 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:38:19 - INFO - __main__ -   Training step = 7700
04/29/2024 15:38:19 - INFO - __main__ -  eval_accuracy:0.8509703405346027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:38:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:38:28 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:38:28 - INFO - __main__ -   Training step = 7700
04/29/2024 15:38:28 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 78%|███████▊  | 7701/9822 [3:57:47<4:07:57,  7.01s/it] 78%|███████▊  | 7702/9822 [3:57:49<3:11:15,  5.41s/it] 78%|███████▊  | 7703/9822 [3:57:51<2:31:41,  4.30s/it] 78%|███████▊  | 7704/9822 [3:57:52<2:04:02,  3.51s/it] 78%|███████▊  | 7705/9822 [3:57:54<1:44:44,  2.97s/it] 78%|███████▊  | 7706/9822 [3:57:56<1:31:04,  2.58s/it] 78%|███████▊  | 7707/9822 [3:57:58<1:21:57,  2.32s/it] 78%|███████▊  | 7708/9822 [3:57:59<1:15:07,  2.13s/it] 78%|███████▊  | 7709/9822 [3:58:01<1:11:55,  2.04s/it] 78%|███████▊  | 7710/9822 [3:58:03<1:08:04,  1.93s/it] 79%|███████▊  | 7711/9822 [3:58:04<1:05:16,  1.86s/it] 79%|███████▊  | 7712/9822 [3:58:06<1:03:41,  1.81s/it] 79%|███████▊  | 7713/9822 [3:58:08<1:02:29,  1.78s/it] 79%|███████▊  | 7714/9822 [3:58:10<1:01:22,  1.75s/it] 79%|███████▊  | 7715/9822 [3:58:11<1:00:37,  1.73s/it] 79%|███████▊  | 7716/9822 [3:58:13<1:00:13,  1.72s/it] 79%|███████▊  | 7717/9822 [3:58:15<59:47,  1.70s/it]   79%|███████▊  | 7718/9822 [3:58:16<59:34,  1.70s/it] 79%|███████▊  | 7719/9822 [3:58:18<59:37,  1.70s/it] 79%|███████▊  | 7720/9822 [3:58:20<59:25,  1.70s/it] 79%|███████▊  | 7721/9822 [3:58:21<59:23,  1.70s/it] 79%|███████▊  | 7722/9822 [3:58:23<59:09,  1.69s/it] 79%|███████▊  | 7723/9822 [3:58:25<59:01,  1.69s/it] 79%|███████▊  | 7724/9822 [3:58:26<58:55,  1.69s/it] 79%|███████▊  | 7725/9822 [3:58:28<58:55,  1.69s/it] 79%|███████▊  | 7726/9822 [3:58:30<59:01,  1.69s/it] 79%|███████▊  | 7727/9822 [3:58:31<59:04,  1.69s/it] 79%|███████▊  | 7728/9822 [3:58:33<59:29,  1.70s/it] 79%|███████▊  | 7729/9822 [3:58:35<59:12,  1.70s/it] 79%|███████▊  | 7730/9822 [3:58:37<59:00,  1.69s/it] 79%|███████▊  | 7731/9822 [3:58:38<59:09,  1.70s/it] 79%|███████▊  | 7732/9822 [3:58:40<59:10,  1.70s/it] 79%|███████▊  | 7733/9822 [3:58:42<59:13,  1.70s/it] 79%|███████▊  | 7734/9822 [3:58:43<58:59,  1.69s/it] 79%|███████▉  | 7735/9822 [3:58:45<1:00:15,  1.73s/it] 79%|███████▉  | 7736/9822 [3:58:47<59:47,  1.72s/it]   79%|███████▉  | 7737/9822 [3:58:49<59:23,  1.71s/it] 79%|███████▉  | 7738/9822 [3:58:50<59:06,  1.70s/it] 79%|███████▉  | 7739/9822 [3:58:52<59:01,  1.70s/it] 79%|███████▉  | 7740/9822 [3:58:54<58:24,  1.68s/it] 79%|███████▉  | 7741/9822 [3:58:55<58:26,  1.69s/it] 79%|███████▉  | 7742/9822 [3:58:57<58:37,  1.69s/it] 79%|███████▉  | 7743/9822 [3:58:59<58:31,  1.69s/it] 79%|███████▉  | 7744/9822 [3:59:00<58:38,  1.69s/it] 79%|███████▉  | 7745/9822 [3:59:02<58:36,  1.69s/it] 79%|███████▉  | 7746/9822 [3:59:04<58:29,  1.69s/it] 79%|███████▉  | 7747/9822 [3:59:05<58:50,  1.70s/it] 79%|███████▉  | 7748/9822 [3:59:07<58:42,  1.70s/it] 79%|███████▉  | 7749/9822 [3:59:09<58:53,  1.70s/it] 79%|███████▉  | 7750/9822 [3:59:11<58:43,  1.70s/it] 79%|███████▉  | 7751/9822 [3:59:12<58:52,  1.71s/it] 79%|███████▉  | 7752/9822 [3:59:14<58:41,  1.70s/it] 79%|███████▉  | 7753/9822 [3:59:16<58:26,  1.69s/it] 79%|███████▉  | 7754/9822 [3:59:17<58:31,  1.70s/it] 79%|███████▉  | 7755/9822 [3:59:19<58:35,  1.70s/it] 79%|███████▉  | 7756/9822 [3:59:21<58:16,  1.69s/it] 79%|███████▉  | 7757/9822 [3:59:22<58:15,  1.69s/it] 79%|███████▉  | 7758/9822 [3:59:24<58:16,  1.69s/it] 79%|███████▉  | 7759/9822 [3:59:26<58:25,  1.70s/it] 79%|███████▉  | 7760/9822 [3:59:28<58:15,  1.70s/it] 79%|███████▉  | 7761/9822 [3:59:29<58:17,  1.70s/it] 79%|███████▉  | 7762/9822 [3:59:31<58:11,  1.69s/it] 79%|███████▉  | 7763/9822 [3:59:33<58:21,  1.70s/it] 79%|███████▉  | 7764/9822 [3:59:34<58:05,  1.69s/it] 79%|███████▉  | 7765/9822 [3:59:36<58:02,  1.69s/it] 79%|███████▉  | 7766/9822 [3:59:38<58:07,  1.70s/it] 79%|███████▉  | 7767/9822 [3:59:39<58:03,  1.70s/it] 79%|███████▉  | 7768/9822 [3:59:41<58:29,  1.71s/it] 79%|███████▉  | 7769/9822 [3:59:43<58:11,  1.70s/it] 79%|███████▉  | 7770/9822 [3:59:45<58:13,  1.70s/it] 79%|███████▉  | 7771/9822 [3:59:46<58:09,  1.70s/it] 79%|███████▉  | 7772/9822 [3:59:48<58:08,  1.70s/it] 79%|███████▉  | 7773/9822 [3:59:50<58:03,  1.70s/it] 79%|███████▉  | 7774/9822 [3:59:51<57:54,  1.70s/it] 79%|███████▉  | 7775/9822 [3:59:53<57:49,  1.70s/it] 79%|███████▉  | 7776/9822 [3:59:55<59:02,  1.73s/it] 79%|███████▉  | 7777/9822 [3:59:57<58:51,  1.73s/it] 79%|███████▉  | 7778/9822 [3:59:58<58:30,  1.72s/it] 79%|███████▉  | 7779/9822 [4:00:00<58:16,  1.71s/it] 79%|███████▉  | 7780/9822 [4:00:02<58:19,  1.71s/it] 79%|███████▉  | 7781/9822 [4:00:03<58:14,  1.71s/it] 79%|███████▉  | 7782/9822 [4:00:05<58:09,  1.71s/it] 79%|███████▉  | 7783/9822 [4:00:07<57:53,  1.70s/it] 79%|███████▉  | 7784/9822 [4:00:08<57:56,  1.71s/it] 79%|███████▉  | 7785/9822 [4:00:10<57:43,  1.70s/it] 79%|███████▉  | 7786/9822 [4:00:12<57:39,  1.70s/it] 79%|███████▉  | 7787/9822 [4:00:14<57:46,  1.70s/it] 79%|███████▉  | 7788/9822 [4:00:15<57:39,  1.70s/it] 79%|███████▉  | 7789/9822 [4:00:17<57:51,  1.71s/it] 79%|███████▉  | 7790/9822 [4:00:19<57:36,  1.70s/it] 79%|███████▉  | 7791/9822 [4:00:20<57:44,  1.71s/it] 79%|███████▉  | 7792/9822 [4:00:22<57:35,  1.70s/it] 79%|███████▉  | 7793/9822 [4:00:24<57:33,  1.70s/it] 79%|███████▉  | 7794/9822 [4:00:25<57:19,  1.70s/it] 79%|███████▉  | 7795/9822 [4:00:27<57:10,  1.69s/it] 79%|███████▉  | 7796/9822 [4:00:29<57:35,  1.71s/it] 79%|███████▉  | 7797/9822 [4:00:31<57:14,  1.70s/it] 79%|███████▉  | 7798/9822 [4:00:32<57:25,  1.70s/it] 79%|███████▉  | 7799/9822 [4:00:34<57:20,  1.70s/it] 79%|███████▉  | 7800/9822 [4:00:36<57:09,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1847, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:41:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:41:22 - INFO - __main__ - ***** test Results*****
04/29/2024 15:41:22 - INFO - __main__ -   Training step = 7800
04/29/2024 15:41:22 - INFO - __main__ -  test_accuracy:0.8707906295754027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:41:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:41:27 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:41:27 - INFO - __main__ -   Training step = 7800
04/29/2024 15:41:27 - INFO - __main__ -  eval_accuracy:0.8484071768582937 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:41:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:41:36 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:41:36 - INFO - __main__ -   Training step = 7800
04/29/2024 15:41:36 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 79%|███████▉  | 7801/9822 [4:00:55<3:58:23,  7.08s/it] 79%|███████▉  | 7802/9822 [4:00:57<3:05:06,  5.50s/it] 79%|███████▉  | 7803/9822 [4:00:59<2:26:52,  4.36s/it] 79%|███████▉  | 7804/9822 [4:01:00<1:59:52,  3.56s/it] 79%|███████▉  | 7805/9822 [4:01:02<1:41:08,  3.01s/it] 79%|███████▉  | 7806/9822 [4:01:04<1:27:48,  2.61s/it] 79%|███████▉  | 7807/9822 [4:01:06<1:18:34,  2.34s/it] 79%|███████▉  | 7808/9822 [4:01:07<1:12:22,  2.16s/it] 80%|███████▉  | 7809/9822 [4:01:09<1:08:00,  2.03s/it] 80%|███████▉  | 7810/9822 [4:01:11<1:04:36,  1.93s/it] 80%|███████▉  | 7811/9822 [4:01:12<1:02:16,  1.86s/it] 80%|███████▉  | 7812/9822 [4:01:14<1:00:35,  1.81s/it] 80%|███████▉  | 7813/9822 [4:01:16<59:21,  1.77s/it]   80%|███████▉  | 7814/9822 [4:01:18<58:32,  1.75s/it] 80%|███████▉  | 7815/9822 [4:01:19<58:21,  1.74s/it] 80%|███████▉  | 7816/9822 [4:01:21<58:02,  1.74s/it] 80%|███████▉  | 7817/9822 [4:01:23<57:47,  1.73s/it] 80%|███████▉  | 7818/9822 [4:01:24<57:29,  1.72s/it] 80%|███████▉  | 7819/9822 [4:01:26<57:17,  1.72s/it] 80%|███████▉  | 7820/9822 [4:01:28<57:28,  1.72s/it] 80%|███████▉  | 7821/9822 [4:01:30<57:11,  1.72s/it] 80%|███████▉  | 7822/9822 [4:01:31<56:59,  1.71s/it] 80%|███████▉  | 7823/9822 [4:01:33<56:51,  1.71s/it] 80%|███████▉  | 7824/9822 [4:01:35<57:03,  1.71s/it] 80%|███████▉  | 7825/9822 [4:01:36<56:51,  1.71s/it] 80%|███████▉  | 7826/9822 [4:01:38<56:25,  1.70s/it] 80%|███████▉  | 7827/9822 [4:01:40<56:16,  1.69s/it] 80%|███████▉  | 7828/9822 [4:01:41<56:09,  1.69s/it] 80%|███████▉  | 7829/9822 [4:01:43<57:20,  1.73s/it] 80%|███████▉  | 7830/9822 [4:01:45<56:55,  1.71s/it] 80%|███████▉  | 7831/9822 [4:01:47<56:39,  1.71s/it] 80%|███████▉  | 7832/9822 [4:01:48<56:29,  1.70s/it] 80%|███████▉  | 7833/9822 [4:01:50<56:37,  1.71s/it] 80%|███████▉  | 7834/9822 [4:01:52<56:36,  1.71s/it] 80%|███████▉  | 7835/9822 [4:01:53<56:25,  1.70s/it] 80%|███████▉  | 7836/9822 [4:01:55<56:39,  1.71s/it] 80%|███████▉  | 7837/9822 [4:01:57<56:33,  1.71s/it] 80%|███████▉  | 7838/9822 [4:01:59<56:24,  1.71s/it] 80%|███████▉  | 7839/9822 [4:02:00<56:23,  1.71s/it] 80%|███████▉  | 7840/9822 [4:02:02<56:13,  1.70s/it] 80%|███████▉  | 7841/9822 [4:02:04<56:36,  1.71s/it] 80%|███████▉  | 7842/9822 [4:02:05<56:19,  1.71s/it] 80%|███████▉  | 7843/9822 [4:02:07<56:27,  1.71s/it] 80%|███████▉  | 7844/9822 [4:02:09<56:15,  1.71s/it] 80%|███████▉  | 7845/9822 [4:02:10<56:11,  1.71s/it] 80%|███████▉  | 7846/9822 [4:02:12<56:03,  1.70s/it] 80%|███████▉  | 7847/9822 [4:02:14<55:59,  1.70s/it] 80%|███████▉  | 7848/9822 [4:02:16<56:01,  1.70s/it] 80%|███████▉  | 7849/9822 [4:02:17<56:01,  1.70s/it] 80%|███████▉  | 7850/9822 [4:02:19<56:10,  1.71s/it] 80%|███████▉  | 7851/9822 [4:02:21<56:02,  1.71s/it] 80%|███████▉  | 7852/9822 [4:02:22<55:45,  1.70s/it] 80%|███████▉  | 7853/9822 [4:02:24<55:56,  1.70s/it] 80%|███████▉  | 7854/9822 [4:02:26<55:52,  1.70s/it] 80%|███████▉  | 7855/9822 [4:02:28<55:57,  1.71s/it] 80%|███████▉  | 7856/9822 [4:02:29<56:47,  1.73s/it] 80%|███████▉  | 7857/9822 [4:02:31<56:37,  1.73s/it] 80%|████████  | 7858/9822 [4:02:33<56:22,  1.72s/it] 80%|████████  | 7859/9822 [4:02:34<56:06,  1.72s/it] 80%|████████  | 7860/9822 [4:02:36<55:58,  1.71s/it] 80%|████████  | 7861/9822 [4:02:38<55:48,  1.71s/it] 80%|████████  | 7862/9822 [4:02:40<56:03,  1.72s/it] 80%|████████  | 7863/9822 [4:02:41<55:52,  1.71s/it] 80%|████████  | 7864/9822 [4:02:43<55:53,  1.71s/it] 80%|████████  | 7865/9822 [4:02:45<55:46,  1.71s/it] 80%|████████  | 7866/9822 [4:02:46<55:39,  1.71s/it] 80%|████████  | 7867/9822 [4:02:48<55:39,  1.71s/it] 80%|████████  | 7868/9822 [4:02:50<55:30,  1.70s/it] 80%|████████  | 7869/9822 [4:02:52<55:26,  1.70s/it] 80%|████████  | 7870/9822 [4:02:53<55:22,  1.70s/it] 80%|████████  | 7871/9822 [4:02:55<55:26,  1.71s/it] 80%|████████  | 7872/9822 [4:02:57<55:29,  1.71s/it] 80%|████████  | 7873/9822 [4:02:58<55:27,  1.71s/it] 80%|████████  | 7874/9822 [4:03:00<55:27,  1.71s/it] 80%|████████  | 7875/9822 [4:03:02<55:19,  1.71s/it] 80%|████████  | 7876/9822 [4:03:03<55:23,  1.71s/it] 80%|████████  | 7877/9822 [4:03:05<55:16,  1.70s/it] 80%|████████  | 7878/9822 [4:03:07<55:19,  1.71s/it] 80%|████████  | 7879/9822 [4:03:09<55:12,  1.70s/it] 80%|████████  | 7880/9822 [4:03:10<55:05,  1.70s/it] 80%|████████  | 7881/9822 [4:03:12<55:00,  1.70s/it] 80%|████████  | 7882/9822 [4:03:14<54:56,  1.70s/it] 80%|████████  | 7883/9822 [4:03:15<54:55,  1.70s/it] 80%|████████  | 7884/9822 [4:03:17<54:49,  1.70s/it] 80%|████████  | 7885/9822 [4:03:19<54:57,  1.70s/it] 80%|████████  | 7886/9822 [4:03:20<55:04,  1.71s/it] 80%|████████  | 7887/9822 [4:03:22<55:00,  1.71s/it] 80%|████████  | 7888/9822 [4:03:24<55:01,  1.71s/it] 80%|████████  | 7889/9822 [4:03:26<55:57,  1.74s/it] 80%|████████  | 7890/9822 [4:03:27<56:07,  1.74s/it] 80%|████████  | 7891/9822 [4:03:29<55:40,  1.73s/it] 80%|████████  | 7892/9822 [4:03:31<55:30,  1.73s/it] 80%|████████  | 7893/9822 [4:03:33<55:07,  1.71s/it] 80%|████████  | 7894/9822 [4:03:34<54:56,  1.71s/it] 80%|████████  | 7895/9822 [4:03:36<55:05,  1.72s/it] 80%|████████  | 7896/9822 [4:03:38<54:48,  1.71s/it] 80%|████████  | 7897/9822 [4:03:39<54:41,  1.70s/it] 80%|████████  | 7898/9822 [4:03:41<54:37,  1.70s/it] 80%|████████  | 7899/9822 [4:03:43<54:41,  1.71s/it] 80%|████████  | 7900/9822 [4:03:44<54:30,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:44:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:44:31 - INFO - __main__ - ***** test Results*****
04/29/2024 15:44:31 - INFO - __main__ -   Training step = 7900
04/29/2024 15:44:31 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:44:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:44:36 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:44:36 - INFO - __main__ -   Training step = 7900
04/29/2024 15:44:36 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:44:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:44:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:44:45 - INFO - __main__ -   Training step = 7900
04/29/2024 15:44:45 - INFO - __main__ -  eval_accuracy:0.9154155986818016 
 80%|████████  | 7901/9822 [4:04:04<3:46:29,  7.07s/it] 80%|████████  | 7902/9822 [4:04:06<2:54:47,  5.46s/it] 80%|████████  | 7903/9822 [4:04:07<2:18:31,  4.33s/it] 80%|████████  | 7904/9822 [4:04:09<1:53:12,  3.54s/it] 80%|████████  | 7905/9822 [4:04:11<1:35:23,  2.99s/it] 80%|████████  | 7906/9822 [4:04:13<1:23:07,  2.60s/it] 81%|████████  | 7907/9822 [4:04:14<1:14:16,  2.33s/it] 81%|████████  | 7908/9822 [4:04:16<1:08:09,  2.14s/it] 81%|████████  | 7909/9822 [4:04:18<1:04:06,  2.01s/it] 81%|████████  | 7910/9822 [4:04:19<1:01:33,  1.93s/it] 81%|████████  | 7911/9822 [4:04:21<59:20,  1.86s/it]   81%|████████  | 7912/9822 [4:04:23<57:36,  1.81s/it] 81%|████████  | 7913/9822 [4:04:25<56:40,  1.78s/it] 81%|████████  | 7914/9822 [4:04:26<55:54,  1.76s/it] 81%|████████  | 7915/9822 [4:04:28<55:27,  1.74s/it] 81%|████████  | 7916/9822 [4:04:30<55:03,  1.73s/it] 81%|████████  | 7917/9822 [4:04:31<54:53,  1.73s/it] 81%|████████  | 7918/9822 [4:04:33<54:51,  1.73s/it] 81%|████████  | 7919/9822 [4:04:35<55:29,  1.75s/it] 81%|████████  | 7920/9822 [4:04:37<54:58,  1.73s/it] 81%|████████  | 7921/9822 [4:04:38<54:38,  1.72s/it] 81%|████████  | 7922/9822 [4:04:40<54:17,  1.71s/it] 81%|████████  | 7923/9822 [4:04:42<54:09,  1.71s/it] 81%|████████  | 7924/9822 [4:04:43<54:12,  1.71s/it] 81%|████████  | 7925/9822 [4:04:45<54:02,  1.71s/it] 81%|████████  | 7926/9822 [4:04:47<54:06,  1.71s/it] 81%|████████  | 7927/9822 [4:04:49<54:00,  1.71s/it] 81%|████████  | 7928/9822 [4:04:50<53:53,  1.71s/it] 81%|████████  | 7929/9822 [4:04:52<53:42,  1.70s/it] 81%|████████  | 7930/9822 [4:04:54<53:39,  1.70s/it] 81%|████████  | 7931/9822 [4:04:55<53:53,  1.71s/it] 81%|████████  | 7932/9822 [4:04:57<53:45,  1.71s/it] 81%|████████  | 7933/9822 [4:04:59<53:58,  1.71s/it] 81%|████████  | 7934/9822 [4:05:00<53:36,  1.70s/it] 81%|████████  | 7935/9822 [4:05:02<53:25,  1.70s/it] 81%|████████  | 7936/9822 [4:05:04<53:32,  1.70s/it] 81%|████████  | 7937/9822 [4:05:06<53:18,  1.70s/it] 81%|████████  | 7938/9822 [4:05:07<53:31,  1.70s/it] 81%|████████  | 7939/9822 [4:05:09<53:29,  1.70s/it] 81%|████████  | 7940/9822 [4:05:11<53:20,  1.70s/it] 81%|████████  | 7941/9822 [4:05:12<53:23,  1.70s/it] 81%|████████  | 7942/9822 [4:05:14<53:11,  1.70s/it] 81%|████████  | 7943/9822 [4:05:16<53:03,  1.69s/it] 81%|████████  | 7944/9822 [4:05:17<52:59,  1.69s/it] 81%|████████  | 7945/9822 [4:05:19<53:04,  1.70s/it] 81%|████████  | 7946/9822 [4:05:21<53:59,  1.73s/it] 81%|████████  | 7947/9822 [4:05:23<53:37,  1.72s/it] 81%|████████  | 7948/9822 [4:05:24<53:25,  1.71s/it] 81%|████████  | 7949/9822 [4:05:26<53:13,  1.71s/it] 81%|████████  | 7950/9822 [4:05:28<53:03,  1.70s/it] 81%|████████  | 7951/9822 [4:05:29<53:06,  1.70s/it] 81%|████████  | 7952/9822 [4:05:31<53:01,  1.70s/it] 81%|████████  | 7953/9822 [4:05:33<53:02,  1.70s/it] 81%|████████  | 7954/9822 [4:05:35<52:57,  1.70s/it] 81%|████████  | 7955/9822 [4:05:36<52:54,  1.70s/it] 81%|████████  | 7956/9822 [4:05:38<52:55,  1.70s/it] 81%|████████  | 7957/9822 [4:05:40<52:57,  1.70s/it] 81%|████████  | 7958/9822 [4:05:41<52:45,  1.70s/it] 81%|████████  | 7959/9822 [4:05:43<52:52,  1.70s/it] 81%|████████  | 7960/9822 [4:05:45<52:42,  1.70s/it] 81%|████████  | 7961/9822 [4:05:46<52:31,  1.69s/it] 81%|████████  | 7962/9822 [4:05:48<52:30,  1.69s/it] 81%|████████  | 7963/9822 [4:05:50<52:25,  1.69s/it] 81%|████████  | 7964/9822 [4:05:51<52:22,  1.69s/it] 81%|████████  | 7965/9822 [4:05:53<52:22,  1.69s/it] 81%|████████  | 7966/9822 [4:05:55<52:22,  1.69s/it] 81%|████████  | 7967/9822 [4:05:57<52:22,  1.69s/it] 81%|████████  | 7968/9822 [4:05:58<52:14,  1.69s/it] 81%|████████  | 7969/9822 [4:06:00<52:33,  1.70s/it] 81%|████████  | 7970/9822 [4:06:02<52:27,  1.70s/it] 81%|████████  | 7971/9822 [4:06:03<52:29,  1.70s/it] 81%|████████  | 7972/9822 [4:06:05<52:27,  1.70s/it] 81%|████████  | 7973/9822 [4:06:07<53:20,  1.73s/it] 81%|████████  | 7974/9822 [4:06:09<53:05,  1.72s/it] 81%|████████  | 7975/9822 [4:06:10<53:02,  1.72s/it] 81%|████████  | 7976/9822 [4:06:12<53:02,  1.72s/it] 81%|████████  | 7977/9822 [4:06:14<52:56,  1.72s/it] 81%|████████  | 7978/9822 [4:06:15<52:56,  1.72s/it] 81%|████████  | 7979/9822 [4:06:17<52:56,  1.72s/it] 81%|████████  | 7980/9822 [4:06:19<52:44,  1.72s/it] 81%|████████▏ | 7981/9822 [4:06:21<52:34,  1.71s/it] 81%|████████▏ | 7982/9822 [4:06:22<52:22,  1.71s/it] 81%|████████▏ | 7983/9822 [4:06:24<52:20,  1.71s/it] 81%|████████▏ | 7984/9822 [4:06:26<52:12,  1.70s/it] 81%|████████▏ | 7985/9822 [4:06:27<52:10,  1.70s/it] 81%|████████▏ | 7986/9822 [4:06:29<52:03,  1.70s/it] 81%|████████▏ | 7987/9822 [4:06:31<52:03,  1.70s/it] 81%|████████▏ | 7988/9822 [4:06:33<52:08,  1.71s/it] 81%|████████▏ | 7989/9822 [4:06:34<52:07,  1.71s/it] 81%|████████▏ | 7990/9822 [4:06:36<52:17,  1.71s/it] 81%|████████▏ | 7991/9822 [4:06:38<52:13,  1.71s/it] 81%|████████▏ | 7992/9822 [4:06:39<52:00,  1.71s/it] 81%|████████▏ | 7993/9822 [4:06:41<52:01,  1.71s/it] 81%|████████▏ | 7994/9822 [4:06:43<51:55,  1.70s/it] 81%|████████▏ | 7995/9822 [4:06:44<51:44,  1.70s/it] 81%|████████▏ | 7996/9822 [4:06:46<51:58,  1.71s/it] 81%|████████▏ | 7997/9822 [4:06:48<51:54,  1.71s/it] 81%|████████▏ | 7998/9822 [4:06:50<51:20,  1.69s/it] 81%|████████▏ | 7999/9822 [4:06:51<51:25,  1.69s/it] 81%|████████▏ | 8000/9822 [4:06:53<51:27,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:47:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:47:40 - INFO - __main__ - ***** test Results*****
04/29/2024 15:47:40 - INFO - __main__ -   Training step = 8000
04/29/2024 15:47:40 - INFO - __main__ -  test_accuracy:0.8740849194729137 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:47:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:47:44 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:47:44 - INFO - __main__ -   Training step = 8000
04/29/2024 15:47:44 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:47:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:47:53 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:47:53 - INFO - __main__ -   Training step = 8000
04/29/2024 15:47:53 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 81%|████████▏ | 8001/9822 [4:07:13<3:34:34,  7.07s/it] 81%|████████▏ | 8002/9822 [4:07:14<2:45:28,  5.46s/it] 81%|████████▏ | 8003/9822 [4:07:16<2:11:15,  4.33s/it] 81%|████████▏ | 8004/9822 [4:07:18<1:47:30,  3.55s/it] 82%|████████▏ | 8005/9822 [4:07:19<1:31:39,  3.03s/it] 82%|████████▏ | 8006/9822 [4:07:21<1:19:53,  2.64s/it] 82%|████████▏ | 8007/9822 [4:07:23<1:11:21,  2.36s/it] 82%|████████▏ | 8008/9822 [4:07:25<1:05:16,  2.16s/it] 82%|████████▏ | 8009/9822 [4:07:26<1:00:57,  2.02s/it] 82%|████████▏ | 8010/9822 [4:07:28<58:01,  1.92s/it]   82%|████████▏ | 8011/9822 [4:07:30<55:53,  1.85s/it] 82%|████████▏ | 8012/9822 [4:07:31<54:29,  1.81s/it] 82%|████████▏ | 8013/9822 [4:07:33<53:46,  1.78s/it] 82%|████████▏ | 8014/9822 [4:07:35<52:57,  1.76s/it] 82%|████████▏ | 8015/9822 [4:07:36<52:26,  1.74s/it] 82%|████████▏ | 8016/9822 [4:07:38<52:13,  1.74s/it] 82%|████████▏ | 8017/9822 [4:07:40<51:46,  1.72s/it] 82%|████████▏ | 8018/9822 [4:07:42<51:35,  1.72s/it] 82%|████████▏ | 8019/9822 [4:07:43<51:18,  1.71s/it] 82%|████████▏ | 8020/9822 [4:07:45<51:21,  1.71s/it] 82%|████████▏ | 8021/9822 [4:07:47<51:11,  1.71s/it] 82%|████████▏ | 8022/9822 [4:07:48<51:03,  1.70s/it] 82%|████████▏ | 8023/9822 [4:07:50<50:58,  1.70s/it] 82%|████████▏ | 8024/9822 [4:07:52<50:53,  1.70s/it] 82%|████████▏ | 8025/9822 [4:07:53<50:46,  1.70s/it] 82%|████████▏ | 8026/9822 [4:07:55<50:46,  1.70s/it] 82%|████████▏ | 8027/9822 [4:07:57<50:52,  1.70s/it] 82%|████████▏ | 8028/9822 [4:07:59<50:41,  1.70s/it] 82%|████████▏ | 8029/9822 [4:08:00<50:37,  1.69s/it] 82%|████████▏ | 8030/9822 [4:08:02<50:35,  1.69s/it] 82%|████████▏ | 8031/9822 [4:08:04<50:43,  1.70s/it] 82%|████████▏ | 8032/9822 [4:08:05<50:48,  1.70s/it] 82%|████████▏ | 8033/9822 [4:08:07<50:47,  1.70s/it] 82%|████████▏ | 8034/9822 [4:08:09<50:40,  1.70s/it] 82%|████████▏ | 8035/9822 [4:08:10<50:32,  1.70s/it] 82%|████████▏ | 8036/9822 [4:08:12<50:42,  1.70s/it] 82%|████████▏ | 8037/9822 [4:08:14<50:38,  1.70s/it] 82%|████████▏ | 8038/9822 [4:08:16<51:35,  1.74s/it] 82%|████████▏ | 8039/9822 [4:08:17<51:10,  1.72s/it] 82%|████████▏ | 8040/9822 [4:08:19<50:52,  1.71s/it] 82%|████████▏ | 8041/9822 [4:08:21<50:54,  1.71s/it] 82%|████████▏ | 8042/9822 [4:08:23<50:43,  1.71s/it] 82%|████████▏ | 8043/9822 [4:08:24<50:24,  1.70s/it] 82%|████████▏ | 8044/9822 [4:08:26<50:15,  1.70s/it] 82%|████████▏ | 8045/9822 [4:08:28<50:18,  1.70s/it] 82%|████████▏ | 8046/9822 [4:08:29<50:10,  1.69s/it] 82%|████████▏ | 8047/9822 [4:08:31<50:14,  1.70s/it] 82%|████████▏ | 8048/9822 [4:08:33<50:13,  1.70s/it] 82%|████████▏ | 8049/9822 [4:08:34<50:22,  1.70s/it] 82%|████████▏ | 8050/9822 [4:08:36<50:22,  1.71s/it] 82%|████████▏ | 8051/9822 [4:08:38<50:19,  1.71s/it] 82%|████████▏ | 8052/9822 [4:08:40<50:21,  1.71s/it] 82%|████████▏ | 8053/9822 [4:08:41<50:10,  1.70s/it] 82%|████████▏ | 8054/9822 [4:08:43<50:02,  1.70s/it] 82%|████████▏ | 8055/9822 [4:08:45<49:54,  1.69s/it] 82%|████████▏ | 8056/9822 [4:08:46<49:45,  1.69s/it] 82%|████████▏ | 8057/9822 [4:08:48<49:41,  1.69s/it] 82%|████████▏ | 8058/9822 [4:08:50<49:40,  1.69s/it] 82%|████████▏ | 8059/9822 [4:08:51<49:54,  1.70s/it] 82%|████████▏ | 8060/9822 [4:08:53<50:37,  1.72s/it] 82%|████████▏ | 8061/9822 [4:08:55<50:26,  1.72s/it] 82%|████████▏ | 8062/9822 [4:08:57<50:09,  1.71s/it] 82%|████████▏ | 8063/9822 [4:08:58<50:10,  1.71s/it] 82%|████████▏ | 8064/9822 [4:09:00<49:55,  1.70s/it] 82%|████████▏ | 8065/9822 [4:09:02<49:42,  1.70s/it] 82%|████████▏ | 8066/9822 [4:09:03<49:39,  1.70s/it] 82%|████████▏ | 8067/9822 [4:09:05<49:32,  1.69s/it] 82%|████████▏ | 8068/9822 [4:09:07<49:34,  1.70s/it] 82%|████████▏ | 8069/9822 [4:09:08<49:39,  1.70s/it] 82%|████████▏ | 8070/9822 [4:09:10<49:33,  1.70s/it] 82%|████████▏ | 8071/9822 [4:09:12<49:23,  1.69s/it] 82%|████████▏ | 8072/9822 [4:09:13<49:15,  1.69s/it] 82%|████████▏ | 8073/9822 [4:09:15<49:08,  1.69s/it] 82%|████████▏ | 8074/9822 [4:09:17<49:00,  1.68s/it] 82%|████████▏ | 8075/9822 [4:09:19<49:06,  1.69s/it] 82%|████████▏ | 8076/9822 [4:09:20<49:01,  1.68s/it] 82%|████████▏ | 8077/9822 [4:09:22<49:12,  1.69s/it] 82%|████████▏ | 8078/9822 [4:09:24<49:16,  1.70s/it] 82%|████████▏ | 8079/9822 [4:09:25<49:13,  1.69s/it] 82%|████████▏ | 8080/9822 [4:09:27<49:14,  1.70s/it] 82%|████████▏ | 8081/9822 [4:09:29<49:23,  1.70s/it] 82%|████████▏ | 8082/9822 [4:09:30<49:12,  1.70s/it] 82%|████████▏ | 8083/9822 [4:09:32<49:16,  1.70s/it] 82%|████████▏ | 8084/9822 [4:09:34<48:47,  1.68s/it] 82%|████████▏ | 8085/9822 [4:09:35<48:54,  1.69s/it] 82%|████████▏ | 8086/9822 [4:09:37<49:05,  1.70s/it] 82%|████████▏ | 8087/9822 [4:09:39<49:59,  1.73s/it] 82%|████████▏ | 8088/9822 [4:09:41<49:34,  1.72s/it] 82%|████████▏ | 8089/9822 [4:09:42<49:35,  1.72s/it] 82%|████████▏ | 8090/9822 [4:09:44<49:42,  1.72s/it] 82%|████████▏ | 8091/9822 [4:09:46<49:24,  1.71s/it] 82%|████████▏ | 8092/9822 [4:09:47<49:19,  1.71s/it] 82%|████████▏ | 8093/9822 [4:09:49<49:30,  1.72s/it] 82%|████████▏ | 8094/9822 [4:09:51<49:29,  1.72s/it] 82%|████████▏ | 8095/9822 [4:09:53<49:17,  1.71s/it] 82%|████████▏ | 8096/9822 [4:09:54<49:10,  1.71s/it] 82%|████████▏ | 8097/9822 [4:09:56<49:13,  1.71s/it] 82%|████████▏ | 8098/9822 [4:09:58<49:09,  1.71s/it] 82%|████████▏ | 8099/9822 [4:09:59<48:59,  1.71s/it] 82%|████████▏ | 8100/9822 [4:10:01<48:53,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:50:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:50:48 - INFO - __main__ - ***** test Results*****
04/29/2024 15:50:48 - INFO - __main__ -   Training step = 8100
04/29/2024 15:50:48 - INFO - __main__ -  test_accuracy:0.8707906295754027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:50:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:50:53 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:50:53 - INFO - __main__ -   Training step = 8100
04/29/2024 15:50:53 - INFO - __main__ -  eval_accuracy:0.8502380080556573 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:51:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:51:01 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:51:01 - INFO - __main__ -   Training step = 8100
04/29/2024 15:51:01 - INFO - __main__ -  eval_accuracy:0.9146832662028561 
 82%|████████▏ | 8101/9822 [4:10:21<3:21:47,  7.03s/it] 82%|████████▏ | 8102/9822 [4:10:22<2:35:45,  5.43s/it] 82%|████████▏ | 8103/9822 [4:10:24<2:03:42,  4.32s/it] 83%|████████▎ | 8104/9822 [4:10:26<1:41:10,  3.53s/it] 83%|████████▎ | 8105/9822 [4:10:27<1:25:20,  2.98s/it] 83%|████████▎ | 8106/9822 [4:10:29<1:14:19,  2.60s/it] 83%|████████▎ | 8107/9822 [4:10:31<1:06:26,  2.32s/it] 83%|████████▎ | 8108/9822 [4:10:33<1:01:15,  2.14s/it] 83%|████████▎ | 8109/9822 [4:10:34<57:18,  2.01s/it]   83%|████████▎ | 8110/9822 [4:10:36<54:40,  1.92s/it] 83%|████████▎ | 8111/9822 [4:10:38<52:45,  1.85s/it] 83%|████████▎ | 8112/9822 [4:10:39<51:26,  1.81s/it] 83%|████████▎ | 8113/9822 [4:10:41<50:34,  1.78s/it] 83%|████████▎ | 8114/9822 [4:10:43<49:53,  1.75s/it] 83%|████████▎ | 8115/9822 [4:10:45<50:23,  1.77s/it] 83%|████████▎ | 8116/9822 [4:10:46<49:39,  1.75s/it] 83%|████████▎ | 8117/9822 [4:10:48<49:12,  1.73s/it] 83%|████████▎ | 8118/9822 [4:10:50<48:55,  1.72s/it] 83%|████████▎ | 8119/9822 [4:10:51<48:56,  1.72s/it] 83%|████████▎ | 8120/9822 [4:10:53<48:59,  1.73s/it] 83%|████████▎ | 8121/9822 [4:10:55<48:33,  1.71s/it] 83%|████████▎ | 8122/9822 [4:10:56<48:20,  1.71s/it] 83%|████████▎ | 8123/9822 [4:10:58<48:12,  1.70s/it] 83%|████████▎ | 8124/9822 [4:11:00<48:02,  1.70s/it] 83%|████████▎ | 8125/9822 [4:11:02<48:00,  1.70s/it] 83%|████████▎ | 8126/9822 [4:11:03<47:56,  1.70s/it] 83%|████████▎ | 8127/9822 [4:11:05<48:02,  1.70s/it] 83%|████████▎ | 8128/9822 [4:11:07<47:57,  1.70s/it] 83%|████████▎ | 8129/9822 [4:11:08<48:05,  1.70s/it] 83%|████████▎ | 8130/9822 [4:11:10<48:03,  1.70s/it] 83%|████████▎ | 8131/9822 [4:11:12<47:58,  1.70s/it] 83%|████████▎ | 8132/9822 [4:11:14<48:02,  1.71s/it] 83%|████████▎ | 8133/9822 [4:11:15<47:50,  1.70s/it] 83%|████████▎ | 8134/9822 [4:11:17<47:52,  1.70s/it] 83%|████████▎ | 8135/9822 [4:11:19<47:42,  1.70s/it] 83%|████████▎ | 8136/9822 [4:11:20<47:42,  1.70s/it] 83%|████████▎ | 8137/9822 [4:11:22<47:41,  1.70s/it] 83%|████████▎ | 8138/9822 [4:11:24<47:41,  1.70s/it] 83%|████████▎ | 8139/9822 [4:11:25<47:42,  1.70s/it] 83%|████████▎ | 8140/9822 [4:11:27<47:40,  1.70s/it] 83%|████████▎ | 8141/9822 [4:11:29<47:41,  1.70s/it] 83%|████████▎ | 8142/9822 [4:11:30<47:24,  1.69s/it] 83%|████████▎ | 8143/9822 [4:11:32<47:20,  1.69s/it] 83%|████████▎ | 8144/9822 [4:11:34<47:12,  1.69s/it] 83%|████████▎ | 8145/9822 [4:11:36<47:02,  1.68s/it] 83%|████████▎ | 8146/9822 [4:11:37<46:58,  1.68s/it] 83%|████████▎ | 8147/9822 [4:11:39<47:02,  1.68s/it] 83%|████████▎ | 8148/9822 [4:11:41<47:21,  1.70s/it] 83%|████████▎ | 8149/9822 [4:11:42<47:16,  1.70s/it] 83%|████████▎ | 8150/9822 [4:11:44<47:16,  1.70s/it] 83%|████████▎ | 8151/9822 [4:11:46<47:17,  1.70s/it] 83%|████████▎ | 8152/9822 [4:11:47<47:16,  1.70s/it] 83%|████████▎ | 8153/9822 [4:11:49<47:18,  1.70s/it] 83%|████████▎ | 8154/9822 [4:11:51<47:14,  1.70s/it] 83%|████████▎ | 8155/9822 [4:11:53<47:15,  1.70s/it] 83%|████████▎ | 8156/9822 [4:11:54<48:04,  1.73s/it] 83%|████████▎ | 8157/9822 [4:11:56<48:01,  1.73s/it] 83%|████████▎ | 8158/9822 [4:11:58<47:41,  1.72s/it] 83%|████████▎ | 8159/9822 [4:11:59<47:21,  1.71s/it] 83%|████████▎ | 8160/9822 [4:12:01<47:21,  1.71s/it] 83%|████████▎ | 8161/9822 [4:12:03<47:08,  1.70s/it] 83%|████████▎ | 8162/9822 [4:12:04<46:59,  1.70s/it] 83%|████████▎ | 8163/9822 [4:12:06<46:58,  1.70s/it] 83%|████████▎ | 8164/9822 [4:12:08<47:02,  1.70s/it] 83%|████████▎ | 8165/9822 [4:12:10<46:48,  1.69s/it] 83%|████████▎ | 8166/9822 [4:12:11<46:46,  1.69s/it] 83%|████████▎ | 8167/9822 [4:12:13<46:46,  1.70s/it] 83%|████████▎ | 8168/9822 [4:12:15<46:36,  1.69s/it] 83%|████████▎ | 8169/9822 [4:12:16<46:33,  1.69s/it] 83%|████████▎ | 8170/9822 [4:12:18<46:11,  1.68s/it] 83%|████████▎ | 8171/9822 [4:12:20<46:16,  1.68s/it] 83%|████████▎ | 8172/9822 [4:12:21<46:23,  1.69s/it] 83%|████████▎ | 8173/9822 [4:12:23<46:28,  1.69s/it] 83%|████████▎ | 8174/9822 [4:12:25<46:23,  1.69s/it] 83%|████████▎ | 8175/9822 [4:12:26<46:25,  1.69s/it] 83%|████████▎ | 8176/9822 [4:12:28<46:25,  1.69s/it] 83%|████████▎ | 8177/9822 [4:12:30<46:18,  1.69s/it] 83%|████████▎ | 8178/9822 [4:12:32<46:18,  1.69s/it] 83%|████████▎ | 8179/9822 [4:12:33<46:19,  1.69s/it] 83%|████████▎ | 8180/9822 [4:12:35<46:18,  1.69s/it] 83%|████████▎ | 8181/9822 [4:12:37<46:20,  1.69s/it] 83%|████████▎ | 8182/9822 [4:12:38<46:17,  1.69s/it] 83%|████████▎ | 8183/9822 [4:12:40<47:10,  1.73s/it] 83%|████████▎ | 8184/9822 [4:12:42<46:49,  1.72s/it] 83%|████████▎ | 8185/9822 [4:12:43<46:32,  1.71s/it] 83%|████████▎ | 8186/9822 [4:12:45<46:26,  1.70s/it] 83%|████████▎ | 8187/9822 [4:12:47<46:19,  1.70s/it] 83%|████████▎ | 8188/9822 [4:12:49<46:12,  1.70s/it] 83%|████████▎ | 8189/9822 [4:12:50<46:07,  1.69s/it] 83%|████████▎ | 8190/9822 [4:12:52<46:07,  1.70s/it] 83%|████████▎ | 8191/9822 [4:12:54<46:03,  1.69s/it] 83%|████████▎ | 8192/9822 [4:12:55<46:01,  1.69s/it] 83%|████████▎ | 8193/9822 [4:12:57<46:02,  1.70s/it] 83%|████████▎ | 8194/9822 [4:12:59<46:00,  1.70s/it] 83%|████████▎ | 8195/9822 [4:13:00<45:55,  1.69s/it] 83%|████████▎ | 8196/9822 [4:13:02<45:51,  1.69s/it] 83%|████████▎ | 8197/9822 [4:13:04<45:50,  1.69s/it] 83%|████████▎ | 8198/9822 [4:13:05<45:49,  1.69s/it] 83%|████████▎ | 8199/9822 [4:13:07<45:49,  1.69s/it] 83%|████████▎ | 8200/9822 [4:13:09<45:41,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:53:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:53:56 - INFO - __main__ - ***** test Results*****
04/29/2024 15:53:56 - INFO - __main__ -   Training step = 8200
04/29/2024 15:53:56 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:54:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:54:00 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:54:00 - INFO - __main__ -   Training step = 8200
04/29/2024 15:54:00 - INFO - __main__ -  eval_accuracy:0.8502380080556573 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:54:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:54:09 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:54:09 - INFO - __main__ -   Training step = 8200
04/29/2024 15:54:09 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 83%|████████▎ | 8201/9822 [4:13:28<3:09:29,  7.01s/it] 84%|████████▎ | 8202/9822 [4:13:30<2:26:15,  5.42s/it] 84%|████████▎ | 8203/9822 [4:13:32<1:56:03,  4.30s/it] 84%|████████▎ | 8204/9822 [4:13:33<1:34:56,  3.52s/it] 84%|████████▎ | 8205/9822 [4:13:35<1:20:09,  2.97s/it] 84%|████████▎ | 8206/9822 [4:13:37<1:09:47,  2.59s/it] 84%|████████▎ | 8207/9822 [4:13:38<1:02:27,  2.32s/it] 84%|████████▎ | 8208/9822 [4:13:40<57:22,  2.13s/it]   84%|████████▎ | 8209/9822 [4:13:42<54:39,  2.03s/it] 84%|████████▎ | 8210/9822 [4:13:44<51:56,  1.93s/it] 84%|████████▎ | 8211/9822 [4:13:45<50:00,  1.86s/it] 84%|████████▎ | 8212/9822 [4:13:47<48:35,  1.81s/it] 84%|████████▎ | 8213/9822 [4:13:49<47:42,  1.78s/it] 84%|████████▎ | 8214/9822 [4:13:50<46:58,  1.75s/it] 84%|████████▎ | 8215/9822 [4:13:52<46:29,  1.74s/it] 84%|████████▎ | 8216/9822 [4:13:54<46:10,  1.73s/it] 84%|████████▎ | 8217/9822 [4:13:56<45:57,  1.72s/it] 84%|████████▎ | 8218/9822 [4:13:57<45:44,  1.71s/it] 84%|████████▎ | 8219/9822 [4:13:59<45:32,  1.70s/it] 84%|████████▎ | 8220/9822 [4:14:01<45:24,  1.70s/it] 84%|████████▎ | 8221/9822 [4:14:02<45:41,  1.71s/it] 84%|████████▎ | 8222/9822 [4:14:04<45:33,  1.71s/it] 84%|████████▎ | 8223/9822 [4:14:06<45:24,  1.70s/it] 84%|████████▎ | 8224/9822 [4:14:07<45:19,  1.70s/it] 84%|████████▎ | 8225/9822 [4:14:09<45:18,  1.70s/it] 84%|████████▍ | 8226/9822 [4:14:11<45:13,  1.70s/it] 84%|████████▍ | 8227/9822 [4:14:13<45:17,  1.70s/it] 84%|████████▍ | 8228/9822 [4:14:14<45:12,  1.70s/it] 84%|████████▍ | 8229/9822 [4:14:16<44:57,  1.69s/it] 84%|████████▍ | 8230/9822 [4:14:18<44:57,  1.69s/it] 84%|████████▍ | 8231/9822 [4:14:19<44:55,  1.69s/it] 84%|████████▍ | 8232/9822 [4:14:21<44:51,  1.69s/it] 84%|████████▍ | 8233/9822 [4:14:23<44:52,  1.69s/it] 84%|████████▍ | 8234/9822 [4:14:24<44:51,  1.69s/it] 84%|████████▍ | 8235/9822 [4:14:26<44:51,  1.70s/it] 84%|████████▍ | 8236/9822 [4:14:28<45:41,  1.73s/it] 84%|████████▍ | 8237/9822 [4:14:30<45:24,  1.72s/it] 84%|████████▍ | 8238/9822 [4:14:31<45:11,  1.71s/it] 84%|████████▍ | 8239/9822 [4:14:33<45:04,  1.71s/it] 84%|████████▍ | 8240/9822 [4:14:35<44:57,  1.70s/it] 84%|████████▍ | 8241/9822 [4:14:36<44:45,  1.70s/it] 84%|████████▍ | 8242/9822 [4:14:38<44:42,  1.70s/it] 84%|████████▍ | 8243/9822 [4:14:40<44:35,  1.69s/it] 84%|████████▍ | 8244/9822 [4:14:41<44:28,  1.69s/it] 84%|████████▍ | 8245/9822 [4:14:43<44:27,  1.69s/it] 84%|████████▍ | 8246/9822 [4:14:45<44:28,  1.69s/it] 84%|████████▍ | 8247/9822 [4:14:47<44:27,  1.69s/it] 84%|████████▍ | 8248/9822 [4:14:48<44:27,  1.69s/it] 84%|████████▍ | 8249/9822 [4:14:50<44:20,  1.69s/it] 84%|████████▍ | 8250/9822 [4:14:52<44:19,  1.69s/it] 84%|████████▍ | 8251/9822 [4:14:53<44:20,  1.69s/it] 84%|████████▍ | 8252/9822 [4:14:55<44:18,  1.69s/it] 84%|████████▍ | 8253/9822 [4:14:57<44:19,  1.69s/it] 84%|████████▍ | 8254/9822 [4:14:58<44:15,  1.69s/it] 84%|████████▍ | 8255/9822 [4:15:00<44:10,  1.69s/it] 84%|████████▍ | 8256/9822 [4:15:02<43:45,  1.68s/it] 84%|████████▍ | 8257/9822 [4:15:03<43:46,  1.68s/it] 84%|████████▍ | 8258/9822 [4:15:05<43:46,  1.68s/it] 84%|████████▍ | 8259/9822 [4:15:07<43:52,  1.68s/it] 84%|████████▍ | 8260/9822 [4:15:08<43:51,  1.68s/it] 84%|████████▍ | 8261/9822 [4:15:10<43:54,  1.69s/it] 84%|████████▍ | 8262/9822 [4:15:12<43:51,  1.69s/it] 84%|████████▍ | 8263/9822 [4:15:14<43:53,  1.69s/it] 84%|████████▍ | 8264/9822 [4:15:15<43:53,  1.69s/it] 84%|████████▍ | 8265/9822 [4:15:17<43:55,  1.69s/it] 84%|████████▍ | 8266/9822 [4:15:19<43:57,  1.70s/it] 84%|████████▍ | 8267/9822 [4:15:20<43:58,  1.70s/it] 84%|████████▍ | 8268/9822 [4:15:22<44:05,  1.70s/it] 84%|████████▍ | 8269/9822 [4:15:24<44:41,  1.73s/it] 84%|████████▍ | 8270/9822 [4:15:26<44:22,  1.72s/it] 84%|████████▍ | 8271/9822 [4:15:27<44:12,  1.71s/it] 84%|████████▍ | 8272/9822 [4:15:29<44:24,  1.72s/it] 84%|████████▍ | 8273/9822 [4:15:31<44:10,  1.71s/it] 84%|████████▍ | 8274/9822 [4:15:32<43:56,  1.70s/it] 84%|████████▍ | 8275/9822 [4:15:34<43:49,  1.70s/it] 84%|████████▍ | 8276/9822 [4:15:36<43:41,  1.70s/it] 84%|████████▍ | 8277/9822 [4:15:37<43:44,  1.70s/it] 84%|████████▍ | 8278/9822 [4:15:39<43:39,  1.70s/it] 84%|████████▍ | 8279/9822 [4:15:41<43:36,  1.70s/it] 84%|████████▍ | 8280/9822 [4:15:43<43:30,  1.69s/it] 84%|████████▍ | 8281/9822 [4:15:44<43:30,  1.69s/it] 84%|████████▍ | 8282/9822 [4:15:46<43:30,  1.69s/it] 84%|████████▍ | 8283/9822 [4:15:48<43:24,  1.69s/it] 84%|████████▍ | 8284/9822 [4:15:49<43:25,  1.69s/it] 84%|████████▍ | 8285/9822 [4:15:51<43:24,  1.69s/it] 84%|████████▍ | 8286/9822 [4:15:53<43:25,  1.70s/it] 84%|████████▍ | 8287/9822 [4:15:54<43:20,  1.69s/it] 84%|████████▍ | 8288/9822 [4:15:56<43:19,  1.69s/it] 84%|████████▍ | 8289/9822 [4:15:58<43:15,  1.69s/it] 84%|████████▍ | 8290/9822 [4:15:59<43:13,  1.69s/it] 84%|████████▍ | 8291/9822 [4:16:01<44:00,  1.72s/it] 84%|████████▍ | 8292/9822 [4:16:03<43:40,  1.71s/it] 84%|████████▍ | 8293/9822 [4:16:05<43:34,  1.71s/it] 84%|████████▍ | 8294/9822 [4:16:06<43:17,  1.70s/it] 84%|████████▍ | 8295/9822 [4:16:08<43:11,  1.70s/it] 84%|████████▍ | 8296/9822 [4:16:10<43:03,  1.69s/it] 84%|████████▍ | 8297/9822 [4:16:11<43:03,  1.69s/it] 84%|████████▍ | 8298/9822 [4:16:13<43:01,  1.69s/it] 84%|████████▍ | 8299/9822 [4:16:15<42:52,  1.69s/it] 85%|████████▍ | 8300/9822 [4:16:16<42:53,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 15:57:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:57:03 - INFO - __main__ - ***** test Results*****
04/29/2024 15:57:03 - INFO - __main__ -   Training step = 8300
04/29/2024 15:57:03 - INFO - __main__ -  test_accuracy:0.873718887262079 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:57:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:57:08 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 15:57:08 - INFO - __main__ -   Training step = 8300
04/29/2024 15:57:08 - INFO - __main__ -  eval_accuracy:0.8531673379714391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 15:57:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 15:57:16 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 15:57:16 - INFO - __main__ -   Training step = 8300
04/29/2024 15:57:16 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 85%|████████▍ | 8301/9822 [4:16:36<2:57:46,  7.01s/it] 85%|████████▍ | 8302/9822 [4:16:38<2:17:11,  5.42s/it] 85%|████████▍ | 8303/9822 [4:16:39<1:48:49,  4.30s/it] 85%|████████▍ | 8304/9822 [4:16:41<1:28:58,  3.52s/it] 85%|████████▍ | 8305/9822 [4:16:43<1:15:04,  2.97s/it] 85%|████████▍ | 8306/9822 [4:16:44<1:05:19,  2.59s/it] 85%|████████▍ | 8307/9822 [4:16:46<58:23,  2.31s/it]   85%|████████▍ | 8308/9822 [4:16:48<53:47,  2.13s/it] 85%|████████▍ | 8309/9822 [4:16:49<50:31,  2.00s/it] 85%|████████▍ | 8310/9822 [4:16:51<48:11,  1.91s/it] 85%|████████▍ | 8311/9822 [4:16:53<46:33,  1.85s/it] 85%|████████▍ | 8312/9822 [4:16:55<45:20,  1.80s/it] 85%|████████▍ | 8313/9822 [4:16:56<44:33,  1.77s/it] 85%|████████▍ | 8314/9822 [4:16:58<43:55,  1.75s/it] 85%|████████▍ | 8315/9822 [4:17:00<44:29,  1.77s/it] 85%|████████▍ | 8316/9822 [4:17:01<43:48,  1.75s/it] 85%|████████▍ | 8317/9822 [4:17:03<43:24,  1.73s/it] 85%|████████▍ | 8318/9822 [4:17:05<43:22,  1.73s/it] 85%|████████▍ | 8319/9822 [4:17:07<43:04,  1.72s/it] 85%|████████▍ | 8320/9822 [4:17:08<42:51,  1.71s/it] 85%|████████▍ | 8321/9822 [4:17:10<42:38,  1.70s/it] 85%|████████▍ | 8322/9822 [4:17:12<42:30,  1.70s/it] 85%|████████▍ | 8323/9822 [4:17:13<42:29,  1.70s/it] 85%|████████▍ | 8324/9822 [4:17:15<42:23,  1.70s/it] 85%|████████▍ | 8325/9822 [4:17:17<42:20,  1.70s/it] 85%|████████▍ | 8326/9822 [4:17:18<42:15,  1.70s/it] 85%|████████▍ | 8327/9822 [4:17:20<42:14,  1.70s/it] 85%|████████▍ | 8328/9822 [4:17:22<42:10,  1.69s/it] 85%|████████▍ | 8329/9822 [4:17:23<42:11,  1.70s/it] 85%|████████▍ | 8330/9822 [4:17:25<42:10,  1.70s/it] 85%|████████▍ | 8331/9822 [4:17:27<42:03,  1.69s/it] 85%|████████▍ | 8332/9822 [4:17:29<41:59,  1.69s/it] 85%|████████▍ | 8333/9822 [4:17:30<41:59,  1.69s/it] 85%|████████▍ | 8334/9822 [4:17:32<42:01,  1.69s/it] 85%|████████▍ | 8335/9822 [4:17:34<42:02,  1.70s/it] 85%|████████▍ | 8336/9822 [4:17:35<42:02,  1.70s/it] 85%|████████▍ | 8337/9822 [4:17:37<42:08,  1.70s/it] 85%|████████▍ | 8338/9822 [4:17:39<42:15,  1.71s/it] 85%|████████▍ | 8339/9822 [4:17:40<42:00,  1.70s/it] 85%|████████▍ | 8340/9822 [4:17:42<41:58,  1.70s/it] 85%|████████▍ | 8341/9822 [4:17:44<42:39,  1.73s/it] 85%|████████▍ | 8342/9822 [4:17:46<41:56,  1.70s/it] 85%|████████▍ | 8343/9822 [4:17:47<41:57,  1.70s/it] 85%|████████▍ | 8344/9822 [4:17:49<41:49,  1.70s/it] 85%|████████▍ | 8345/9822 [4:17:51<41:44,  1.70s/it] 85%|████████▍ | 8346/9822 [4:17:52<41:41,  1.70s/it] 85%|████████▍ | 8347/9822 [4:17:54<41:42,  1.70s/it] 85%|████████▍ | 8348/9822 [4:17:56<42:00,  1.71s/it] 85%|████████▌ | 8349/9822 [4:17:58<41:59,  1.71s/it] 85%|████████▌ | 8350/9822 [4:17:59<41:52,  1.71s/it] 85%|████████▌ | 8351/9822 [4:18:01<41:48,  1.71s/it] 85%|████████▌ | 8352/9822 [4:18:03<41:42,  1.70s/it] 85%|████████▌ | 8353/9822 [4:18:04<41:35,  1.70s/it] 85%|████████▌ | 8354/9822 [4:18:06<41:32,  1.70s/it] 85%|████████▌ | 8355/9822 [4:18:08<41:30,  1.70s/it] 85%|████████▌ | 8356/9822 [4:18:09<41:23,  1.69s/it] 85%|████████▌ | 8357/9822 [4:18:11<41:19,  1.69s/it] 85%|████████▌ | 8358/9822 [4:18:13<41:18,  1.69s/it] 85%|████████▌ | 8359/9822 [4:18:14<41:14,  1.69s/it] 85%|████████▌ | 8360/9822 [4:18:16<41:10,  1.69s/it] 85%|████████▌ | 8361/9822 [4:18:18<41:07,  1.69s/it] 85%|████████▌ | 8362/9822 [4:18:20<41:06,  1.69s/it] 85%|████████▌ | 8363/9822 [4:18:21<41:09,  1.69s/it] 85%|████████▌ | 8364/9822 [4:18:23<41:08,  1.69s/it] 85%|████████▌ | 8365/9822 [4:18:25<41:08,  1.69s/it] 85%|████████▌ | 8366/9822 [4:18:26<41:08,  1.70s/it] 85%|████████▌ | 8367/9822 [4:18:28<41:17,  1.70s/it] 85%|████████▌ | 8368/9822 [4:18:30<41:15,  1.70s/it] 85%|████████▌ | 8369/9822 [4:18:31<41:08,  1.70s/it] 85%|████████▌ | 8370/9822 [4:18:33<41:05,  1.70s/it] 85%|████████▌ | 8371/9822 [4:18:35<41:02,  1.70s/it] 85%|████████▌ | 8372/9822 [4:18:37<41:01,  1.70s/it] 85%|████████▌ | 8373/9822 [4:18:38<40:56,  1.70s/it] 85%|████████▌ | 8374/9822 [4:18:40<41:36,  1.72s/it] 85%|████████▌ | 8375/9822 [4:18:42<41:22,  1.72s/it] 85%|████████▌ | 8376/9822 [4:18:43<41:11,  1.71s/it] 85%|████████▌ | 8377/9822 [4:18:45<41:02,  1.70s/it] 85%|████████▌ | 8378/9822 [4:18:47<40:57,  1.70s/it] 85%|████████▌ | 8379/9822 [4:18:48<40:52,  1.70s/it] 85%|████████▌ | 8380/9822 [4:18:50<40:49,  1.70s/it] 85%|████████▌ | 8381/9822 [4:18:52<40:46,  1.70s/it] 85%|████████▌ | 8382/9822 [4:18:54<40:44,  1.70s/it] 85%|████████▌ | 8383/9822 [4:18:55<40:40,  1.70s/it] 85%|████████▌ | 8384/9822 [4:18:57<40:38,  1.70s/it] 85%|████████▌ | 8385/9822 [4:18:59<40:37,  1.70s/it] 85%|████████▌ | 8386/9822 [4:19:00<40:27,  1.69s/it] 85%|████████▌ | 8387/9822 [4:19:02<40:24,  1.69s/it] 85%|████████▌ | 8388/9822 [4:19:04<40:28,  1.69s/it] 85%|████████▌ | 8389/9822 [4:19:05<40:27,  1.69s/it] 85%|████████▌ | 8390/9822 [4:19:07<40:28,  1.70s/it] 85%|████████▌ | 8391/9822 [4:19:09<40:25,  1.70s/it] 85%|████████▌ | 8392/9822 [4:19:10<40:26,  1.70s/it] 85%|████████▌ | 8393/9822 [4:19:12<40:21,  1.69s/it] 85%|████████▌ | 8394/9822 [4:19:14<40:14,  1.69s/it] 85%|████████▌ | 8395/9822 [4:19:16<40:13,  1.69s/it] 85%|████████▌ | 8396/9822 [4:19:17<40:58,  1.72s/it] 85%|████████▌ | 8397/9822 [4:19:19<40:44,  1.72s/it] 86%|████████▌ | 8398/9822 [4:19:21<40:36,  1.71s/it] 86%|████████▌ | 8399/9822 [4:19:22<40:28,  1.71s/it] 86%|████████▌ | 8400/9822 [4:19:24<40:30,  1.71s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:00:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:00:11 - INFO - __main__ - ***** test Results*****
04/29/2024 16:00:11 - INFO - __main__ -   Training step = 8400
04/29/2024 16:00:11 - INFO - __main__ -  test_accuracy:0.8755490483162518 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:00:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:00:15 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:00:15 - INFO - __main__ -   Training step = 8400
04/29/2024 16:00:15 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:00:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:00:24 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:00:24 - INFO - __main__ -   Training step = 8400
04/29/2024 16:00:24 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 86%|████████▌ | 8401/9822 [4:19:44<2:46:27,  7.03s/it] 86%|████████▌ | 8402/9822 [4:19:45<2:08:28,  5.43s/it] 86%|████████▌ | 8403/9822 [4:19:47<1:41:54,  4.31s/it] 86%|████████▌ | 8404/9822 [4:19:49<1:23:18,  3.53s/it] 86%|████████▌ | 8405/9822 [4:19:50<1:10:19,  2.98s/it] 86%|████████▌ | 8406/9822 [4:19:52<1:01:10,  2.59s/it] 86%|████████▌ | 8407/9822 [4:19:54<54:54,  2.33s/it]   86%|████████▌ | 8408/9822 [4:19:55<50:22,  2.14s/it] 86%|████████▌ | 8409/9822 [4:19:57<47:12,  2.00s/it] 86%|████████▌ | 8410/9822 [4:19:59<45:01,  1.91s/it] 86%|████████▌ | 8411/9822 [4:20:01<43:24,  1.85s/it] 86%|████████▌ | 8412/9822 [4:20:02<42:22,  1.80s/it] 86%|████████▌ | 8413/9822 [4:20:04<41:36,  1.77s/it] 86%|████████▌ | 8414/9822 [4:20:06<41:04,  1.75s/it] 86%|████████▌ | 8415/9822 [4:20:07<40:42,  1.74s/it] 86%|████████▌ | 8416/9822 [4:20:09<40:24,  1.72s/it] 86%|████████▌ | 8417/9822 [4:20:11<40:09,  1.72s/it] 86%|████████▌ | 8418/9822 [4:20:12<39:53,  1.71s/it] 86%|████████▌ | 8419/9822 [4:20:14<39:46,  1.70s/it] 86%|████████▌ | 8420/9822 [4:20:16<40:26,  1.73s/it] 86%|████████▌ | 8421/9822 [4:20:18<40:08,  1.72s/it] 86%|████████▌ | 8422/9822 [4:20:19<39:54,  1.71s/it] 86%|████████▌ | 8423/9822 [4:20:21<39:44,  1.70s/it] 86%|████████▌ | 8424/9822 [4:20:23<39:36,  1.70s/it] 86%|████████▌ | 8425/9822 [4:20:24<39:31,  1.70s/it] 86%|████████▌ | 8426/9822 [4:20:26<39:27,  1.70s/it] 86%|████████▌ | 8427/9822 [4:20:28<39:19,  1.69s/it] 86%|████████▌ | 8428/9822 [4:20:29<39:00,  1.68s/it] 86%|████████▌ | 8429/9822 [4:20:31<39:01,  1.68s/it] 86%|████████▌ | 8430/9822 [4:20:33<39:02,  1.68s/it] 86%|████████▌ | 8431/9822 [4:20:34<39:06,  1.69s/it] 86%|████████▌ | 8432/9822 [4:20:36<39:08,  1.69s/it] 86%|████████▌ | 8433/9822 [4:20:38<39:03,  1.69s/it] 86%|████████▌ | 8434/9822 [4:20:40<38:58,  1.68s/it] 86%|████████▌ | 8435/9822 [4:20:41<38:54,  1.68s/it] 86%|████████▌ | 8436/9822 [4:20:43<38:55,  1.68s/it] 86%|████████▌ | 8437/9822 [4:20:45<38:54,  1.69s/it] 86%|████████▌ | 8438/9822 [4:20:46<38:56,  1.69s/it] 86%|████████▌ | 8439/9822 [4:20:48<38:57,  1.69s/it] 86%|████████▌ | 8440/9822 [4:20:50<38:58,  1.69s/it] 86%|████████▌ | 8441/9822 [4:20:51<38:55,  1.69s/it] 86%|████████▌ | 8442/9822 [4:20:53<38:56,  1.69s/it] 86%|████████▌ | 8443/9822 [4:20:55<38:49,  1.69s/it] 86%|████████▌ | 8444/9822 [4:20:56<38:47,  1.69s/it] 86%|████████▌ | 8445/9822 [4:20:58<38:43,  1.69s/it] 86%|████████▌ | 8446/9822 [4:21:00<39:26,  1.72s/it] 86%|████████▌ | 8447/9822 [4:21:02<39:11,  1.71s/it] 86%|████████▌ | 8448/9822 [4:21:03<38:57,  1.70s/it] 86%|████████▌ | 8449/9822 [4:21:05<38:49,  1.70s/it] 86%|████████▌ | 8450/9822 [4:21:07<38:48,  1.70s/it] 86%|████████▌ | 8451/9822 [4:21:08<38:42,  1.69s/it] 86%|████████▌ | 8452/9822 [4:21:10<38:31,  1.69s/it] 86%|████████▌ | 8453/9822 [4:21:12<38:30,  1.69s/it] 86%|████████▌ | 8454/9822 [4:21:13<38:32,  1.69s/it] 86%|████████▌ | 8455/9822 [4:21:15<38:28,  1.69s/it] 86%|████████▌ | 8456/9822 [4:21:17<38:35,  1.70s/it] 86%|████████▌ | 8457/9822 [4:21:19<38:36,  1.70s/it] 86%|████████▌ | 8458/9822 [4:21:20<38:41,  1.70s/it] 86%|████████▌ | 8459/9822 [4:21:22<38:40,  1.70s/it] 86%|████████▌ | 8460/9822 [4:21:24<38:42,  1.70s/it] 86%|████████▌ | 8461/9822 [4:21:25<38:37,  1.70s/it] 86%|████████▌ | 8462/9822 [4:21:27<38:35,  1.70s/it] 86%|████████▌ | 8463/9822 [4:21:29<38:27,  1.70s/it] 86%|████████▌ | 8464/9822 [4:21:30<38:25,  1.70s/it] 86%|████████▌ | 8465/9822 [4:21:32<38:23,  1.70s/it] 86%|████████▌ | 8466/9822 [4:21:34<38:19,  1.70s/it] 86%|████████▌ | 8467/9822 [4:21:36<38:19,  1.70s/it] 86%|████████▌ | 8468/9822 [4:21:37<38:15,  1.70s/it] 86%|████████▌ | 8469/9822 [4:21:39<38:29,  1.71s/it] 86%|████████▌ | 8470/9822 [4:21:41<38:22,  1.70s/it] 86%|████████▌ | 8471/9822 [4:21:42<38:16,  1.70s/it] 86%|████████▋ | 8472/9822 [4:21:44<38:09,  1.70s/it] 86%|████████▋ | 8473/9822 [4:21:46<38:06,  1.69s/it] 86%|████████▋ | 8474/9822 [4:21:47<38:04,  1.69s/it] 86%|████████▋ | 8475/9822 [4:21:49<38:00,  1.69s/it] 86%|████████▋ | 8476/9822 [4:21:51<37:57,  1.69s/it] 86%|████████▋ | 8477/9822 [4:21:52<38:08,  1.70s/it] 86%|████████▋ | 8478/9822 [4:21:54<38:05,  1.70s/it] 86%|████████▋ | 8479/9822 [4:21:56<38:44,  1.73s/it] 86%|████████▋ | 8480/9822 [4:21:58<38:28,  1.72s/it] 86%|████████▋ | 8481/9822 [4:21:59<38:17,  1.71s/it] 86%|████████▋ | 8482/9822 [4:22:01<38:06,  1.71s/it] 86%|████████▋ | 8483/9822 [4:22:03<38:01,  1.70s/it] 86%|████████▋ | 8484/9822 [4:22:04<37:53,  1.70s/it] 86%|████████▋ | 8485/9822 [4:22:06<37:45,  1.69s/it] 86%|████████▋ | 8486/9822 [4:22:08<37:41,  1.69s/it] 86%|████████▋ | 8487/9822 [4:22:10<37:43,  1.70s/it] 86%|████████▋ | 8488/9822 [4:22:11<37:42,  1.70s/it] 86%|████████▋ | 8489/9822 [4:22:13<37:47,  1.70s/it] 86%|████████▋ | 8490/9822 [4:22:15<37:41,  1.70s/it] 86%|████████▋ | 8491/9822 [4:22:16<37:37,  1.70s/it] 86%|████████▋ | 8492/9822 [4:22:18<37:35,  1.70s/it] 86%|████████▋ | 8493/9822 [4:22:20<37:36,  1.70s/it] 86%|████████▋ | 8494/9822 [4:22:21<37:38,  1.70s/it] 86%|████████▋ | 8495/9822 [4:22:23<37:33,  1.70s/it] 86%|████████▋ | 8496/9822 [4:22:25<37:29,  1.70s/it] 87%|████████▋ | 8497/9822 [4:22:27<37:30,  1.70s/it] 87%|████████▋ | 8498/9822 [4:22:28<37:29,  1.70s/it] 87%|████████▋ | 8499/9822 [4:22:30<37:27,  1.70s/it] 87%|████████▋ | 8500/9822 [4:22:32<37:24,  1.70s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:03:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:03:18 - INFO - __main__ - ***** test Results*****
04/29/2024 16:03:18 - INFO - __main__ -   Training step = 8500
04/29/2024 16:03:18 - INFO - __main__ -  test_accuracy:0.8740849194729137 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:03:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:03:23 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:03:23 - INFO - __main__ -   Training step = 8500
04/29/2024 16:03:23 - INFO - __main__ -  eval_accuracy:0.85060417429513 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:03:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:03:32 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:03:32 - INFO - __main__ -   Training step = 8500
04/29/2024 16:03:32 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 87%|████████▋ | 8501/9822 [4:22:51<2:35:01,  7.04s/it] 87%|████████▋ | 8502/9822 [4:22:53<1:59:34,  5.44s/it] 87%|████████▋ | 8503/9822 [4:22:54<1:34:41,  4.31s/it] 87%|████████▋ | 8504/9822 [4:22:56<1:17:17,  3.52s/it] 87%|████████▋ | 8505/9822 [4:22:58<1:05:10,  2.97s/it] 87%|████████▋ | 8506/9822 [4:23:00<56:42,  2.59s/it]   87%|████████▋ | 8507/9822 [4:23:01<50:46,  2.32s/it] 87%|████████▋ | 8508/9822 [4:23:03<46:40,  2.13s/it] 87%|████████▋ | 8509/9822 [4:23:05<43:48,  2.00s/it] 87%|████████▋ | 8510/9822 [4:23:06<41:47,  1.91s/it] 87%|████████▋ | 8511/9822 [4:23:08<40:20,  1.85s/it] 87%|████████▋ | 8512/9822 [4:23:10<39:17,  1.80s/it] 87%|████████▋ | 8513/9822 [4:23:11<38:31,  1.77s/it] 87%|████████▋ | 8514/9822 [4:23:13<37:41,  1.73s/it] 87%|████████▋ | 8515/9822 [4:23:15<37:20,  1.71s/it] 87%|████████▋ | 8516/9822 [4:23:16<37:06,  1.71s/it] 87%|████████▋ | 8517/9822 [4:23:18<37:00,  1.70s/it] 87%|████████▋ | 8518/9822 [4:23:20<36:56,  1.70s/it] 87%|████████▋ | 8519/9822 [4:23:21<36:50,  1.70s/it] 87%|████████▋ | 8520/9822 [4:23:23<36:50,  1.70s/it] 87%|████████▋ | 8521/9822 [4:23:25<36:55,  1.70s/it] 87%|████████▋ | 8522/9822 [4:23:27<36:51,  1.70s/it] 87%|████████▋ | 8523/9822 [4:23:28<36:48,  1.70s/it] 87%|████████▋ | 8524/9822 [4:23:30<36:40,  1.70s/it] 87%|████████▋ | 8525/9822 [4:23:32<37:19,  1.73s/it] 87%|████████▋ | 8526/9822 [4:23:33<37:00,  1.71s/it] 87%|████████▋ | 8527/9822 [4:23:35<36:45,  1.70s/it] 87%|████████▋ | 8528/9822 [4:23:37<36:36,  1.70s/it] 87%|████████▋ | 8529/9822 [4:23:39<36:35,  1.70s/it] 87%|████████▋ | 8530/9822 [4:23:40<36:33,  1.70s/it] 87%|████████▋ | 8531/9822 [4:23:42<36:26,  1.69s/it] 87%|████████▋ | 8532/9822 [4:23:44<36:24,  1.69s/it] 87%|████████▋ | 8533/9822 [4:23:45<36:21,  1.69s/it] 87%|████████▋ | 8534/9822 [4:23:47<36:16,  1.69s/it] 87%|████████▋ | 8535/9822 [4:23:49<36:16,  1.69s/it] 87%|████████▋ | 8536/9822 [4:23:50<36:18,  1.69s/it] 87%|████████▋ | 8537/9822 [4:23:52<36:14,  1.69s/it] 87%|████████▋ | 8538/9822 [4:23:54<36:11,  1.69s/it] 87%|████████▋ | 8539/9822 [4:23:55<36:15,  1.70s/it] 87%|████████▋ | 8540/9822 [4:23:57<36:17,  1.70s/it] 87%|████████▋ | 8541/9822 [4:23:59<36:14,  1.70s/it] 87%|████████▋ | 8542/9822 [4:24:01<36:13,  1.70s/it] 87%|████████▋ | 8543/9822 [4:24:02<36:11,  1.70s/it] 87%|████████▋ | 8544/9822 [4:24:04<36:09,  1.70s/it] 87%|████████▋ | 8545/9822 [4:24:06<36:13,  1.70s/it] 87%|████████▋ | 8546/9822 [4:24:07<36:25,  1.71s/it] 87%|████████▋ | 8547/9822 [4:24:09<36:19,  1.71s/it] 87%|████████▋ | 8548/9822 [4:24:11<36:15,  1.71s/it] 87%|████████▋ | 8549/9822 [4:24:13<36:09,  1.70s/it] 87%|████████▋ | 8550/9822 [4:24:14<36:11,  1.71s/it] 87%|████████▋ | 8551/9822 [4:24:16<36:55,  1.74s/it] 87%|████████▋ | 8552/9822 [4:24:18<36:35,  1.73s/it] 87%|████████▋ | 8553/9822 [4:24:19<36:16,  1.71s/it] 87%|████████▋ | 8554/9822 [4:24:21<36:04,  1.71s/it] 87%|████████▋ | 8555/9822 [4:24:23<36:00,  1.70s/it] 87%|████████▋ | 8556/9822 [4:24:25<35:55,  1.70s/it] 87%|████████▋ | 8557/9822 [4:24:26<35:51,  1.70s/it] 87%|████████▋ | 8558/9822 [4:24:28<35:49,  1.70s/it] 87%|████████▋ | 8559/9822 [4:24:30<35:44,  1.70s/it] 87%|████████▋ | 8560/9822 [4:24:31<35:40,  1.70s/it] 87%|████████▋ | 8561/9822 [4:24:33<35:37,  1.69s/it] 87%|████████▋ | 8562/9822 [4:24:35<35:35,  1.69s/it] 87%|████████▋ | 8563/9822 [4:24:36<35:30,  1.69s/it] 87%|████████▋ | 8564/9822 [4:24:38<35:28,  1.69s/it] 87%|████████▋ | 8565/9822 [4:24:40<35:27,  1.69s/it] 87%|████████▋ | 8566/9822 [4:24:41<35:28,  1.69s/it] 87%|████████▋ | 8567/9822 [4:24:43<35:28,  1.70s/it] 87%|████████▋ | 8568/9822 [4:24:45<35:27,  1.70s/it] 87%|████████▋ | 8569/9822 [4:24:47<35:25,  1.70s/it] 87%|████████▋ | 8570/9822 [4:24:48<35:22,  1.70s/it] 87%|████████▋ | 8571/9822 [4:24:50<35:30,  1.70s/it] 87%|████████▋ | 8572/9822 [4:24:52<35:29,  1.70s/it] 87%|████████▋ | 8573/9822 [4:24:53<35:26,  1.70s/it] 87%|████████▋ | 8574/9822 [4:24:55<35:23,  1.70s/it] 87%|████████▋ | 8575/9822 [4:24:57<35:20,  1.70s/it] 87%|████████▋ | 8576/9822 [4:24:58<35:17,  1.70s/it] 87%|████████▋ | 8577/9822 [4:25:00<35:16,  1.70s/it] 87%|████████▋ | 8578/9822 [4:25:02<35:11,  1.70s/it] 87%|████████▋ | 8579/9822 [4:25:04<35:10,  1.70s/it] 87%|████████▋ | 8580/9822 [4:25:05<35:03,  1.69s/it] 87%|████████▋ | 8581/9822 [4:25:07<35:03,  1.69s/it] 87%|████████▋ | 8582/9822 [4:25:09<35:01,  1.69s/it] 87%|████████▋ | 8583/9822 [4:25:10<35:02,  1.70s/it] 87%|████████▋ | 8584/9822 [4:25:12<35:41,  1.73s/it] 87%|████████▋ | 8585/9822 [4:25:14<35:34,  1.73s/it] 87%|████████▋ | 8586/9822 [4:25:16<35:23,  1.72s/it] 87%|████████▋ | 8587/9822 [4:25:17<35:14,  1.71s/it] 87%|████████▋ | 8588/9822 [4:25:19<35:07,  1.71s/it] 87%|████████▋ | 8589/9822 [4:25:21<35:02,  1.71s/it] 87%|████████▋ | 8590/9822 [4:25:22<34:53,  1.70s/it] 87%|████████▋ | 8591/9822 [4:25:24<34:48,  1.70s/it] 87%|████████▋ | 8592/9822 [4:25:26<34:48,  1.70s/it] 87%|████████▋ | 8593/9822 [4:25:27<34:46,  1.70s/it] 87%|████████▋ | 8594/9822 [4:25:29<34:43,  1.70s/it] 88%|████████▊ | 8595/9822 [4:25:31<34:34,  1.69s/it] 88%|████████▊ | 8596/9822 [4:25:32<34:35,  1.69s/it] 88%|████████▊ | 8597/9822 [4:25:34<34:35,  1.69s/it] 88%|████████▊ | 8598/9822 [4:25:36<34:35,  1.70s/it] 88%|████████▊ | 8599/9822 [4:25:38<34:35,  1.70s/it] 88%|████████▊ | 8600/9822 [4:25:39<34:14,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:06:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:06:26 - INFO - __main__ - ***** test Results*****
04/29/2024 16:06:26 - INFO - __main__ -   Training step = 8600
04/29/2024 16:06:26 - INFO - __main__ -  test_accuracy:0.8744509516837482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:06:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:06:31 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:06:31 - INFO - __main__ -   Training step = 8600
04/29/2024 16:06:31 - INFO - __main__ -  eval_accuracy:0.8473086781398755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:06:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:06:39 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:06:39 - INFO - __main__ -   Training step = 8600
04/29/2024 16:06:39 - INFO - __main__ -  eval_accuracy:0.9139509337239107 
 88%|████████▊ | 8601/9822 [4:25:59<2:22:37,  7.01s/it] 88%|████████▊ | 8602/9822 [4:26:00<1:50:07,  5.42s/it] 88%|████████▊ | 8603/9822 [4:26:02<1:27:20,  4.30s/it] 88%|████████▊ | 8604/9822 [4:26:04<1:11:26,  3.52s/it] 88%|████████▊ | 8605/9822 [4:26:05<1:00:18,  2.97s/it] 88%|████████▊ | 8606/9822 [4:26:07<52:31,  2.59s/it]   88%|████████▊ | 8607/9822 [4:26:09<47:02,  2.32s/it] 88%|████████▊ | 8608/9822 [4:26:11<43:13,  2.14s/it] 88%|████████▊ | 8609/9822 [4:26:12<40:32,  2.01s/it] 88%|████████▊ | 8610/9822 [4:26:14<38:39,  1.91s/it] 88%|████████▊ | 8611/9822 [4:26:16<37:57,  1.88s/it] 88%|████████▊ | 8612/9822 [4:26:17<36:45,  1.82s/it] 88%|████████▊ | 8613/9822 [4:26:19<35:57,  1.78s/it] 88%|████████▊ | 8614/9822 [4:26:21<35:21,  1.76s/it] 88%|████████▊ | 8615/9822 [4:26:23<34:57,  1.74s/it] 88%|████████▊ | 8616/9822 [4:26:24<34:40,  1.73s/it] 88%|████████▊ | 8617/9822 [4:26:26<34:32,  1.72s/it] 88%|████████▊ | 8618/9822 [4:26:28<34:23,  1.71s/it] 88%|████████▊ | 8619/9822 [4:26:29<34:17,  1.71s/it] 88%|████████▊ | 8620/9822 [4:26:31<34:11,  1.71s/it] 88%|████████▊ | 8621/9822 [4:26:33<34:07,  1.70s/it] 88%|████████▊ | 8622/9822 [4:26:34<34:02,  1.70s/it] 88%|████████▊ | 8623/9822 [4:26:36<33:57,  1.70s/it] 88%|████████▊ | 8624/9822 [4:26:38<33:55,  1.70s/it] 88%|████████▊ | 8625/9822 [4:26:40<33:55,  1.70s/it] 88%|████████▊ | 8626/9822 [4:26:41<33:53,  1.70s/it] 88%|████████▊ | 8627/9822 [4:26:43<33:52,  1.70s/it] 88%|████████▊ | 8628/9822 [4:26:45<33:49,  1.70s/it] 88%|████████▊ | 8629/9822 [4:26:46<33:46,  1.70s/it] 88%|████████▊ | 8630/9822 [4:26:48<33:45,  1.70s/it] 88%|████████▊ | 8631/9822 [4:26:50<33:43,  1.70s/it] 88%|████████▊ | 8632/9822 [4:26:51<33:40,  1.70s/it] 88%|████████▊ | 8633/9822 [4:26:53<33:35,  1.70s/it] 88%|████████▊ | 8634/9822 [4:26:55<33:35,  1.70s/it] 88%|████████▊ | 8635/9822 [4:26:57<33:48,  1.71s/it] 88%|████████▊ | 8636/9822 [4:26:58<33:42,  1.71s/it] 88%|████████▊ | 8637/9822 [4:27:00<33:37,  1.70s/it] 88%|████████▊ | 8638/9822 [4:27:02<34:12,  1.73s/it] 88%|████████▊ | 8639/9822 [4:27:03<33:59,  1.72s/it] 88%|████████▊ | 8640/9822 [4:27:05<33:47,  1.72s/it] 88%|████████▊ | 8641/9822 [4:27:07<33:45,  1.72s/it] 88%|████████▊ | 8642/9822 [4:27:09<33:38,  1.71s/it] 88%|████████▊ | 8643/9822 [4:27:10<33:31,  1.71s/it] 88%|████████▊ | 8644/9822 [4:27:12<33:27,  1.70s/it] 88%|████████▊ | 8645/9822 [4:27:14<33:21,  1.70s/it] 88%|████████▊ | 8646/9822 [4:27:15<33:19,  1.70s/it] 88%|████████▊ | 8647/9822 [4:27:17<33:14,  1.70s/it] 88%|████████▊ | 8648/9822 [4:27:19<33:12,  1.70s/it] 88%|████████▊ | 8649/9822 [4:27:20<33:10,  1.70s/it] 88%|████████▊ | 8650/9822 [4:27:22<33:09,  1.70s/it] 88%|████████▊ | 8651/9822 [4:27:24<33:01,  1.69s/it] 88%|████████▊ | 8652/9822 [4:27:26<33:04,  1.70s/it] 88%|████████▊ | 8653/9822 [4:27:27<33:06,  1.70s/it] 88%|████████▊ | 8654/9822 [4:27:29<33:05,  1.70s/it] 88%|████████▊ | 8655/9822 [4:27:31<33:02,  1.70s/it] 88%|████████▊ | 8656/9822 [4:27:32<32:59,  1.70s/it] 88%|████████▊ | 8657/9822 [4:27:34<32:57,  1.70s/it] 88%|████████▊ | 8658/9822 [4:27:36<32:57,  1.70s/it] 88%|████████▊ | 8659/9822 [4:27:37<32:54,  1.70s/it] 88%|████████▊ | 8660/9822 [4:27:39<33:05,  1.71s/it] 88%|████████▊ | 8661/9822 [4:27:41<32:59,  1.70s/it] 88%|████████▊ | 8662/9822 [4:27:43<32:55,  1.70s/it] 88%|████████▊ | 8663/9822 [4:27:44<32:53,  1.70s/it] 88%|████████▊ | 8664/9822 [4:27:46<32:47,  1.70s/it] 88%|████████▊ | 8665/9822 [4:27:48<33:22,  1.73s/it] 88%|████████▊ | 8666/9822 [4:27:49<33:06,  1.72s/it] 88%|████████▊ | 8667/9822 [4:27:51<32:55,  1.71s/it] 88%|████████▊ | 8668/9822 [4:27:53<32:49,  1.71s/it] 88%|████████▊ | 8669/9822 [4:27:55<32:45,  1.70s/it] 88%|████████▊ | 8670/9822 [4:27:56<32:42,  1.70s/it] 88%|████████▊ | 8671/9822 [4:27:58<32:39,  1.70s/it] 88%|████████▊ | 8672/9822 [4:28:00<32:36,  1.70s/it] 88%|████████▊ | 8673/9822 [4:28:01<32:33,  1.70s/it] 88%|████████▊ | 8674/9822 [4:28:03<32:31,  1.70s/it] 88%|████████▊ | 8675/9822 [4:28:05<32:26,  1.70s/it] 88%|████████▊ | 8676/9822 [4:28:06<32:21,  1.69s/it] 88%|████████▊ | 8677/9822 [4:28:08<32:19,  1.69s/it] 88%|████████▊ | 8678/9822 [4:28:10<32:19,  1.70s/it] 88%|████████▊ | 8679/9822 [4:28:11<32:20,  1.70s/it] 88%|████████▊ | 8680/9822 [4:28:13<32:18,  1.70s/it] 88%|████████▊ | 8681/9822 [4:28:15<32:16,  1.70s/it] 88%|████████▊ | 8682/9822 [4:28:17<32:08,  1.69s/it] 88%|████████▊ | 8683/9822 [4:28:18<32:10,  1.70s/it] 88%|████████▊ | 8684/9822 [4:28:20<32:06,  1.69s/it] 88%|████████▊ | 8685/9822 [4:28:22<32:00,  1.69s/it] 88%|████████▊ | 8686/9822 [4:28:23<31:45,  1.68s/it] 88%|████████▊ | 8687/9822 [4:28:25<31:47,  1.68s/it] 88%|████████▊ | 8688/9822 [4:28:27<31:52,  1.69s/it] 88%|████████▊ | 8689/9822 [4:28:28<32:00,  1.70s/it] 88%|████████▊ | 8690/9822 [4:28:30<32:01,  1.70s/it] 88%|████████▊ | 8691/9822 [4:28:32<31:59,  1.70s/it] 88%|████████▊ | 8692/9822 [4:28:33<31:59,  1.70s/it] 89%|████████▊ | 8693/9822 [4:28:35<31:58,  1.70s/it] 89%|████████▊ | 8694/9822 [4:28:37<31:56,  1.70s/it] 89%|████████▊ | 8695/9822 [4:28:39<31:53,  1.70s/it] 89%|████████▊ | 8696/9822 [4:28:40<31:53,  1.70s/it] 89%|████████▊ | 8697/9822 [4:28:42<31:52,  1.70s/it] 89%|████████▊ | 8698/9822 [4:28:44<32:27,  1.73s/it] 89%|████████▊ | 8699/9822 [4:28:45<32:16,  1.72s/it] 89%|████████▊ | 8700/9822 [4:28:47<32:04,  1.71s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:09:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:09:34 - INFO - __main__ - ***** test Results*****
04/29/2024 16:09:34 - INFO - __main__ -   Training step = 8700
04/29/2024 16:09:34 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:09:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:09:39 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:09:39 - INFO - __main__ -   Training step = 8700
04/29/2024 16:09:39 - INFO - __main__ -  eval_accuracy:0.8495056755767119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:09:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:09:47 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:09:47 - INFO - __main__ -   Training step = 8700
04/29/2024 16:09:47 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 89%|████████▊ | 8701/9822 [4:29:07<2:11:24,  7.03s/it] 89%|████████▊ | 8702/9822 [4:29:08<1:41:25,  5.43s/it] 89%|████████▊ | 8703/9822 [4:29:10<1:20:26,  4.31s/it] 89%|████████▊ | 8704/9822 [4:29:12<1:05:45,  3.53s/it] 89%|████████▊ | 8705/9822 [4:29:13<55:29,  2.98s/it]   89%|████████▊ | 8706/9822 [4:29:15<48:15,  2.59s/it] 89%|████████▊ | 8707/9822 [4:29:17<43:14,  2.33s/it] 89%|████████▊ | 8708/9822 [4:29:19<39:42,  2.14s/it] 89%|████████▊ | 8709/9822 [4:29:20<37:14,  2.01s/it] 89%|████████▊ | 8710/9822 [4:29:22<35:34,  1.92s/it] 89%|████████▊ | 8711/9822 [4:29:24<34:19,  1.85s/it] 89%|████████▊ | 8712/9822 [4:29:25<33:26,  1.81s/it] 89%|████████▊ | 8713/9822 [4:29:27<32:48,  1.78s/it] 89%|████████▊ | 8714/9822 [4:29:29<32:16,  1.75s/it] 89%|████████▊ | 8715/9822 [4:29:30<31:57,  1.73s/it] 89%|████████▊ | 8716/9822 [4:29:32<31:42,  1.72s/it] 89%|████████▊ | 8717/9822 [4:29:34<31:28,  1.71s/it] 89%|████████▉ | 8718/9822 [4:29:35<31:21,  1.70s/it] 89%|████████▉ | 8719/9822 [4:29:37<31:17,  1.70s/it] 89%|████████▉ | 8720/9822 [4:29:39<31:14,  1.70s/it] 89%|████████▉ | 8721/9822 [4:29:41<31:07,  1.70s/it] 89%|████████▉ | 8722/9822 [4:29:42<31:06,  1.70s/it] 89%|████████▉ | 8723/9822 [4:29:44<31:02,  1.69s/it] 89%|████████▉ | 8724/9822 [4:29:46<31:02,  1.70s/it] 89%|████████▉ | 8725/9822 [4:29:47<30:59,  1.70s/it] 89%|████████▉ | 8726/9822 [4:29:49<30:55,  1.69s/it] 89%|████████▉ | 8727/9822 [4:29:51<30:54,  1.69s/it] 89%|████████▉ | 8728/9822 [4:29:52<30:52,  1.69s/it] 89%|████████▉ | 8729/9822 [4:29:54<30:52,  1.70s/it] 89%|████████▉ | 8730/9822 [4:29:56<30:47,  1.69s/it] 89%|████████▉ | 8731/9822 [4:29:57<30:45,  1.69s/it] 89%|████████▉ | 8732/9822 [4:29:59<30:46,  1.69s/it] 89%|████████▉ | 8733/9822 [4:30:01<30:44,  1.69s/it] 89%|████████▉ | 8734/9822 [4:30:03<31:14,  1.72s/it] 89%|████████▉ | 8735/9822 [4:30:04<31:05,  1.72s/it] 89%|████████▉ | 8736/9822 [4:30:06<30:59,  1.71s/it] 89%|████████▉ | 8737/9822 [4:30:08<30:45,  1.70s/it] 89%|████████▉ | 8738/9822 [4:30:09<30:40,  1.70s/it] 89%|████████▉ | 8739/9822 [4:30:11<30:34,  1.69s/it] 89%|████████▉ | 8740/9822 [4:30:13<30:34,  1.70s/it] 89%|████████▉ | 8741/9822 [4:30:15<30:33,  1.70s/it] 89%|████████▉ | 8742/9822 [4:30:16<30:32,  1.70s/it] 89%|████████▉ | 8743/9822 [4:30:18<30:35,  1.70s/it] 89%|████████▉ | 8744/9822 [4:30:20<30:33,  1.70s/it] 89%|████████▉ | 8745/9822 [4:30:21<30:31,  1.70s/it] 89%|████████▉ | 8746/9822 [4:30:23<30:28,  1.70s/it] 89%|████████▉ | 8747/9822 [4:30:25<30:27,  1.70s/it] 89%|████████▉ | 8748/9822 [4:30:26<30:25,  1.70s/it] 89%|████████▉ | 8749/9822 [4:30:28<30:22,  1.70s/it] 89%|████████▉ | 8750/9822 [4:30:30<30:21,  1.70s/it] 89%|████████▉ | 8751/9822 [4:30:32<30:19,  1.70s/it] 89%|████████▉ | 8752/9822 [4:30:33<30:23,  1.70s/it] 89%|████████▉ | 8753/9822 [4:30:35<30:18,  1.70s/it] 89%|████████▉ | 8754/9822 [4:30:37<30:14,  1.70s/it] 89%|████████▉ | 8755/9822 [4:30:38<30:10,  1.70s/it] 89%|████████▉ | 8756/9822 [4:30:40<30:08,  1.70s/it] 89%|████████▉ | 8757/9822 [4:30:42<30:06,  1.70s/it] 89%|████████▉ | 8758/9822 [4:30:43<30:03,  1.70s/it] 89%|████████▉ | 8759/9822 [4:30:45<29:58,  1.69s/it] 89%|████████▉ | 8760/9822 [4:30:47<29:58,  1.69s/it] 89%|████████▉ | 8761/9822 [4:30:49<30:30,  1.73s/it] 89%|████████▉ | 8762/9822 [4:30:50<30:19,  1.72s/it] 89%|████████▉ | 8763/9822 [4:30:52<30:12,  1.71s/it] 89%|████████▉ | 8764/9822 [4:30:54<30:05,  1.71s/it] 89%|████████▉ | 8765/9822 [4:30:55<30:01,  1.70s/it] 89%|████████▉ | 8766/9822 [4:30:57<29:57,  1.70s/it] 89%|████████▉ | 8767/9822 [4:30:59<29:46,  1.69s/it] 89%|████████▉ | 8768/9822 [4:31:00<29:44,  1.69s/it] 89%|████████▉ | 8769/9822 [4:31:02<29:43,  1.69s/it] 89%|████████▉ | 8770/9822 [4:31:04<29:46,  1.70s/it] 89%|████████▉ | 8771/9822 [4:31:06<29:41,  1.70s/it] 89%|████████▉ | 8772/9822 [4:31:07<29:20,  1.68s/it] 89%|████████▉ | 8773/9822 [4:31:09<29:20,  1.68s/it] 89%|████████▉ | 8774/9822 [4:31:11<29:21,  1.68s/it] 89%|████████▉ | 8775/9822 [4:31:12<29:22,  1.68s/it] 89%|████████▉ | 8776/9822 [4:31:14<29:21,  1.68s/it] 89%|████████▉ | 8777/9822 [4:31:16<29:21,  1.69s/it] 89%|████████▉ | 8778/9822 [4:31:17<29:16,  1.68s/it] 89%|████████▉ | 8779/9822 [4:31:19<29:15,  1.68s/it] 89%|████████▉ | 8780/9822 [4:31:21<29:15,  1.68s/it] 89%|████████▉ | 8781/9822 [4:31:22<29:17,  1.69s/it] 89%|████████▉ | 8782/9822 [4:31:24<29:17,  1.69s/it] 89%|████████▉ | 8783/9822 [4:31:26<29:16,  1.69s/it] 89%|████████▉ | 8784/9822 [4:31:27<29:24,  1.70s/it] 89%|████████▉ | 8785/9822 [4:31:29<29:22,  1.70s/it] 89%|████████▉ | 8786/9822 [4:31:31<29:20,  1.70s/it] 89%|████████▉ | 8787/9822 [4:31:33<29:14,  1.70s/it] 89%|████████▉ | 8788/9822 [4:31:34<29:47,  1.73s/it] 89%|████████▉ | 8789/9822 [4:31:36<29:33,  1.72s/it] 89%|████████▉ | 8790/9822 [4:31:38<29:23,  1.71s/it] 90%|████████▉ | 8791/9822 [4:31:39<29:16,  1.70s/it] 90%|████████▉ | 8792/9822 [4:31:41<29:12,  1.70s/it] 90%|████████▉ | 8793/9822 [4:31:43<29:05,  1.70s/it] 90%|████████▉ | 8794/9822 [4:31:44<29:01,  1.69s/it] 90%|████████▉ | 8795/9822 [4:31:46<28:58,  1.69s/it] 90%|████████▉ | 8796/9822 [4:31:48<28:56,  1.69s/it] 90%|████████▉ | 8797/9822 [4:31:50<28:54,  1.69s/it] 90%|████████▉ | 8798/9822 [4:31:51<28:54,  1.69s/it] 90%|████████▉ | 8799/9822 [4:31:53<28:49,  1.69s/it] 90%|████████▉ | 8800/9822 [4:31:55<28:47,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1198, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:12:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:12:41 - INFO - __main__ - ***** test Results*****
04/29/2024 16:12:41 - INFO - __main__ -   Training step = 8800
04/29/2024 16:12:41 - INFO - __main__ -  test_accuracy:0.876281112737921 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:12:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:12:46 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:12:46 - INFO - __main__ -   Training step = 8800
04/29/2024 16:12:46 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:12:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:12:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:12:54 - INFO - __main__ -   Training step = 8800
04/29/2024 16:12:54 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 90%|████████▉ | 8801/9822 [4:32:14<1:58:54,  6.99s/it] 90%|████████▉ | 8802/9822 [4:32:16<1:31:47,  5.40s/it] 90%|████████▉ | 8803/9822 [4:32:17<1:12:50,  4.29s/it] 90%|████████▉ | 8804/9822 [4:32:19<59:30,  3.51s/it]   90%|████████▉ | 8805/9822 [4:32:21<50:07,  2.96s/it] 90%|████████▉ | 8806/9822 [4:32:22<43:34,  2.57s/it] 90%|████████▉ | 8807/9822 [4:32:24<39:04,  2.31s/it] 90%|████████▉ | 8808/9822 [4:32:26<35:50,  2.12s/it] 90%|████████▉ | 8809/9822 [4:32:27<33:33,  1.99s/it] 90%|████████▉ | 8810/9822 [4:32:29<32:00,  1.90s/it] 90%|████████▉ | 8811/9822 [4:32:31<30:56,  1.84s/it] 90%|████████▉ | 8812/9822 [4:32:33<30:08,  1.79s/it] 90%|████████▉ | 8813/9822 [4:32:34<29:32,  1.76s/it] 90%|████████▉ | 8814/9822 [4:32:36<29:07,  1.73s/it] 90%|████████▉ | 8815/9822 [4:32:38<28:47,  1.72s/it] 90%|████████▉ | 8816/9822 [4:32:39<28:39,  1.71s/it] 90%|████████▉ | 8817/9822 [4:32:41<29:00,  1.73s/it] 90%|████████▉ | 8818/9822 [4:32:43<28:42,  1.72s/it] 90%|████████▉ | 8819/9822 [4:32:44<28:26,  1.70s/it] 90%|████████▉ | 8820/9822 [4:32:46<28:15,  1.69s/it] 90%|████████▉ | 8821/9822 [4:32:48<28:08,  1.69s/it] 90%|████████▉ | 8822/9822 [4:32:49<28:00,  1.68s/it] 90%|████████▉ | 8823/9822 [4:32:51<27:55,  1.68s/it] 90%|████████▉ | 8824/9822 [4:32:53<27:53,  1.68s/it] 90%|████████▉ | 8825/9822 [4:32:54<27:49,  1.67s/it] 90%|████████▉ | 8826/9822 [4:32:56<27:48,  1.68s/it] 90%|████████▉ | 8827/9822 [4:32:58<27:47,  1.68s/it] 90%|████████▉ | 8828/9822 [4:32:59<27:48,  1.68s/it] 90%|████████▉ | 8829/9822 [4:33:01<27:47,  1.68s/it] 90%|████████▉ | 8830/9822 [4:33:03<27:45,  1.68s/it] 90%|████████▉ | 8831/9822 [4:33:04<27:41,  1.68s/it] 90%|████████▉ | 8832/9822 [4:33:06<27:35,  1.67s/it] 90%|████████▉ | 8833/9822 [4:33:08<27:33,  1.67s/it] 90%|████████▉ | 8834/9822 [4:33:09<27:31,  1.67s/it] 90%|████████▉ | 8835/9822 [4:33:11<27:34,  1.68s/it] 90%|████████▉ | 8836/9822 [4:33:13<27:36,  1.68s/it] 90%|████████▉ | 8837/9822 [4:33:15<27:33,  1.68s/it] 90%|████████▉ | 8838/9822 [4:33:16<27:33,  1.68s/it] 90%|████████▉ | 8839/9822 [4:33:18<27:33,  1.68s/it] 90%|█████████ | 8840/9822 [4:33:20<27:40,  1.69s/it] 90%|█████████ | 8841/9822 [4:33:21<27:36,  1.69s/it] 90%|█████████ | 8842/9822 [4:33:23<27:35,  1.69s/it] 90%|█████████ | 8843/9822 [4:33:25<27:30,  1.69s/it] 90%|█████████ | 8844/9822 [4:33:26<27:24,  1.68s/it] 90%|█████████ | 8845/9822 [4:33:28<27:24,  1.68s/it] 90%|█████████ | 8846/9822 [4:33:30<27:22,  1.68s/it] 90%|█████████ | 8847/9822 [4:33:31<27:22,  1.68s/it] 90%|█████████ | 8848/9822 [4:33:33<27:20,  1.68s/it] 90%|█████████ | 8849/9822 [4:33:35<27:19,  1.68s/it] 90%|█████████ | 8850/9822 [4:33:37<27:46,  1.71s/it] 90%|█████████ | 8851/9822 [4:33:38<27:35,  1.71s/it] 90%|█████████ | 8852/9822 [4:33:40<27:28,  1.70s/it] 90%|█████████ | 8853/9822 [4:33:42<27:20,  1.69s/it] 90%|█████████ | 8854/9822 [4:33:43<27:13,  1.69s/it] 90%|█████████ | 8855/9822 [4:33:45<27:08,  1.68s/it] 90%|█████████ | 8856/9822 [4:33:47<27:05,  1.68s/it] 90%|█████████ | 8857/9822 [4:33:48<27:00,  1.68s/it] 90%|█████████ | 8858/9822 [4:33:50<26:45,  1.66s/it] 90%|█████████ | 8859/9822 [4:33:52<26:49,  1.67s/it] 90%|█████████ | 8860/9822 [4:33:53<26:49,  1.67s/it] 90%|█████████ | 8861/9822 [4:33:55<26:53,  1.68s/it] 90%|█████████ | 8862/9822 [4:33:57<26:52,  1.68s/it] 90%|█████████ | 8863/9822 [4:33:58<26:50,  1.68s/it] 90%|█████████ | 8864/9822 [4:34:00<27:03,  1.69s/it] 90%|█████████ | 8865/9822 [4:34:02<26:59,  1.69s/it] 90%|█████████ | 8866/9822 [4:34:03<26:55,  1.69s/it] 90%|█████████ | 8867/9822 [4:34:05<26:52,  1.69s/it] 90%|█████████ | 8868/9822 [4:34:07<26:50,  1.69s/it] 90%|█████████ | 8869/9822 [4:34:08<26:47,  1.69s/it] 90%|█████████ | 8870/9822 [4:34:10<26:45,  1.69s/it] 90%|█████████ | 8871/9822 [4:34:12<26:43,  1.69s/it] 90%|█████████ | 8872/9822 [4:34:14<27:10,  1.72s/it] 90%|█████████ | 8873/9822 [4:34:15<27:00,  1.71s/it] 90%|█████████ | 8874/9822 [4:34:17<26:51,  1.70s/it] 90%|█████████ | 8875/9822 [4:34:19<26:44,  1.69s/it] 90%|█████████ | 8876/9822 [4:34:20<26:38,  1.69s/it] 90%|█████████ | 8877/9822 [4:34:22<26:34,  1.69s/it] 90%|█████████ | 8878/9822 [4:34:24<26:32,  1.69s/it] 90%|█████████ | 8879/9822 [4:34:25<26:31,  1.69s/it] 90%|█████████ | 8880/9822 [4:34:27<26:29,  1.69s/it] 90%|█████████ | 8881/9822 [4:34:29<26:29,  1.69s/it] 90%|█████████ | 8882/9822 [4:34:31<26:27,  1.69s/it] 90%|█████████ | 8883/9822 [4:34:32<26:25,  1.69s/it] 90%|█████████ | 8884/9822 [4:34:34<26:24,  1.69s/it] 90%|█████████ | 8885/9822 [4:34:36<26:22,  1.69s/it] 90%|█████████ | 8886/9822 [4:34:37<26:20,  1.69s/it] 90%|█████████ | 8887/9822 [4:34:39<26:17,  1.69s/it] 90%|█████████ | 8888/9822 [4:34:41<26:12,  1.68s/it] 91%|█████████ | 8889/9822 [4:34:42<26:09,  1.68s/it] 91%|█████████ | 8890/9822 [4:34:44<26:07,  1.68s/it] 91%|█████████ | 8891/9822 [4:34:46<26:06,  1.68s/it] 91%|█████████ | 8892/9822 [4:34:47<26:05,  1.68s/it] 91%|█████████ | 8893/9822 [4:34:49<26:00,  1.68s/it] 91%|█████████ | 8894/9822 [4:34:51<25:55,  1.68s/it] 91%|█████████ | 8895/9822 [4:34:52<25:57,  1.68s/it] 91%|█████████ | 8896/9822 [4:34:54<25:58,  1.68s/it] 91%|█████████ | 8897/9822 [4:34:56<26:08,  1.70s/it] 91%|█████████ | 8898/9822 [4:34:57<26:01,  1.69s/it] 91%|█████████ | 8899/9822 [4:34:59<26:24,  1.72s/it] 91%|█████████ | 8900/9822 [4:35:01<26:14,  1.71s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:15:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:15:48 - INFO - __main__ - ***** test Results*****
04/29/2024 16:15:48 - INFO - __main__ -   Training step = 8900
04/29/2024 16:15:48 - INFO - __main__ -  test_accuracy:0.8748169838945827 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:15:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:15:52 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:15:52 - INFO - __main__ -   Training step = 8900
04/29/2024 16:15:52 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:16:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:16:01 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:16:01 - INFO - __main__ -   Training step = 8900
04/29/2024 16:16:01 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 91%|█████████ | 8901/9822 [4:35:20<1:47:05,  6.98s/it] 91%|█████████ | 8902/9822 [4:35:22<1:22:36,  5.39s/it] 91%|█████████ | 8903/9822 [4:35:24<1:05:30,  4.28s/it] 91%|█████████ | 8904/9822 [4:35:25<53:42,  3.51s/it]   91%|█████████ | 8905/9822 [4:35:27<45:16,  2.96s/it] 91%|█████████ | 8906/9822 [4:35:29<39:19,  2.58s/it] 91%|█████████ | 8907/9822 [4:35:30<35:12,  2.31s/it] 91%|█████████ | 8908/9822 [4:35:32<32:16,  2.12s/it] 91%|█████████ | 8909/9822 [4:35:34<30:14,  1.99s/it] 91%|█████████ | 8910/9822 [4:35:35<28:45,  1.89s/it] 91%|█████████ | 8911/9822 [4:35:37<27:46,  1.83s/it] 91%|█████████ | 8912/9822 [4:35:39<27:02,  1.78s/it] 91%|█████████ | 8913/9822 [4:35:40<26:29,  1.75s/it] 91%|█████████ | 8914/9822 [4:35:42<26:06,  1.73s/it] 91%|█████████ | 8915/9822 [4:35:44<25:51,  1.71s/it] 91%|█████████ | 8916/9822 [4:35:45<25:39,  1.70s/it] 91%|█████████ | 8917/9822 [4:35:47<25:28,  1.69s/it] 91%|█████████ | 8918/9822 [4:35:49<25:26,  1.69s/it] 91%|█████████ | 8919/9822 [4:35:50<25:25,  1.69s/it] 91%|█████████ | 8920/9822 [4:35:52<25:22,  1.69s/it] 91%|█████████ | 8921/9822 [4:35:54<25:19,  1.69s/it] 91%|█████████ | 8922/9822 [4:35:56<25:18,  1.69s/it] 91%|█████████ | 8923/9822 [4:35:57<25:16,  1.69s/it] 91%|█████████ | 8924/9822 [4:35:59<25:14,  1.69s/it] 91%|█████████ | 8925/9822 [4:36:01<25:13,  1.69s/it] 91%|█████████ | 8926/9822 [4:36:02<25:06,  1.68s/it] 91%|█████████ | 8927/9822 [4:36:04<25:33,  1.71s/it] 91%|█████████ | 8928/9822 [4:36:06<25:23,  1.70s/it] 91%|█████████ | 8929/9822 [4:36:07<25:17,  1.70s/it] 91%|█████████ | 8930/9822 [4:36:09<25:12,  1.70s/it] 91%|█████████ | 8931/9822 [4:36:11<25:08,  1.69s/it] 91%|█████████ | 8932/9822 [4:36:12<25:06,  1.69s/it] 91%|█████████ | 8933/9822 [4:36:14<25:03,  1.69s/it] 91%|█████████ | 8934/9822 [4:36:16<25:01,  1.69s/it] 91%|█████████ | 8935/9822 [4:36:18<24:58,  1.69s/it] 91%|█████████ | 8936/9822 [4:36:19<24:53,  1.69s/it] 91%|█████████ | 8937/9822 [4:36:21<24:49,  1.68s/it] 91%|█████████ | 8938/9822 [4:36:23<24:47,  1.68s/it] 91%|█████████ | 8939/9822 [4:36:24<24:46,  1.68s/it] 91%|█████████ | 8940/9822 [4:36:26<24:46,  1.68s/it] 91%|█████████ | 8941/9822 [4:36:28<24:39,  1.68s/it] 91%|█████████ | 8942/9822 [4:36:29<24:37,  1.68s/it] 91%|█████████ | 8943/9822 [4:36:31<24:36,  1.68s/it] 91%|█████████ | 8944/9822 [4:36:33<24:22,  1.67s/it] 91%|█████████ | 8945/9822 [4:36:34<24:23,  1.67s/it] 91%|█████████ | 8946/9822 [4:36:36<24:24,  1.67s/it] 91%|█████████ | 8947/9822 [4:36:38<24:24,  1.67s/it] 91%|█████████ | 8948/9822 [4:36:39<24:24,  1.68s/it] 91%|█████████ | 8949/9822 [4:36:41<24:24,  1.68s/it] 91%|█████████ | 8950/9822 [4:36:43<24:23,  1.68s/it] 91%|█████████ | 8951/9822 [4:36:44<24:23,  1.68s/it] 91%|█████████ | 8952/9822 [4:36:46<24:20,  1.68s/it] 91%|█████████ | 8953/9822 [4:36:48<24:19,  1.68s/it] 91%|█████████ | 8954/9822 [4:36:49<24:19,  1.68s/it] 91%|█████████ | 8955/9822 [4:36:51<24:14,  1.68s/it] 91%|█████████ | 8956/9822 [4:36:53<24:11,  1.68s/it] 91%|█████████ | 8957/9822 [4:36:54<24:09,  1.68s/it] 91%|█████████ | 8958/9822 [4:36:56<24:06,  1.67s/it] 91%|█████████ | 8959/9822 [4:36:58<24:05,  1.67s/it] 91%|█████████ | 8960/9822 [4:36:59<24:03,  1.67s/it] 91%|█████████ | 8961/9822 [4:37:01<24:02,  1.67s/it] 91%|█████████ | 8962/9822 [4:37:03<23:58,  1.67s/it] 91%|█████████▏| 8963/9822 [4:37:04<23:56,  1.67s/it] 91%|█████████▏| 8964/9822 [4:37:06<23:58,  1.68s/it] 91%|█████████▏| 8965/9822 [4:37:08<23:54,  1.67s/it] 91%|█████████▏| 8966/9822 [4:37:09<23:53,  1.67s/it] 91%|█████████▏| 8967/9822 [4:37:11<23:51,  1.67s/it] 91%|█████████▏| 8968/9822 [4:37:13<24:19,  1.71s/it] 91%|█████████▏| 8969/9822 [4:37:15<24:23,  1.72s/it] 91%|█████████▏| 8970/9822 [4:37:16<24:14,  1.71s/it] 91%|█████████▏| 8971/9822 [4:37:18<24:11,  1.71s/it] 91%|█████████▏| 8972/9822 [4:37:20<24:04,  1.70s/it] 91%|█████████▏| 8973/9822 [4:37:21<23:55,  1.69s/it] 91%|█████████▏| 8974/9822 [4:37:23<23:46,  1.68s/it] 91%|█████████▏| 8975/9822 [4:37:25<23:46,  1.68s/it] 91%|█████████▏| 8976/9822 [4:37:26<23:43,  1.68s/it] 91%|█████████▏| 8977/9822 [4:37:28<23:42,  1.68s/it] 91%|█████████▏| 8978/9822 [4:37:30<23:37,  1.68s/it] 91%|█████████▏| 8979/9822 [4:37:31<23:37,  1.68s/it] 91%|█████████▏| 8980/9822 [4:37:33<23:33,  1.68s/it] 91%|█████████▏| 8981/9822 [4:37:35<23:31,  1.68s/it] 91%|█████████▏| 8982/9822 [4:37:37<23:30,  1.68s/it] 91%|█████████▏| 8983/9822 [4:37:38<23:28,  1.68s/it] 91%|█████████▏| 8984/9822 [4:37:40<23:30,  1.68s/it] 91%|█████████▏| 8985/9822 [4:37:42<23:28,  1.68s/it] 91%|█████████▏| 8986/9822 [4:37:43<23:26,  1.68s/it] 91%|█████████▏| 8987/9822 [4:37:45<23:23,  1.68s/it] 92%|█████████▏| 8988/9822 [4:37:47<23:22,  1.68s/it] 92%|█████████▏| 8989/9822 [4:37:48<23:21,  1.68s/it] 92%|█████████▏| 8990/9822 [4:37:50<23:18,  1.68s/it] 92%|█████████▏| 8991/9822 [4:37:52<23:17,  1.68s/it] 92%|█████████▏| 8992/9822 [4:37:53<23:16,  1.68s/it] 92%|█████████▏| 8993/9822 [4:37:55<23:11,  1.68s/it] 92%|█████████▏| 8994/9822 [4:37:57<23:11,  1.68s/it] 92%|█████████▏| 8995/9822 [4:37:58<23:35,  1.71s/it] 92%|█████████▏| 8996/9822 [4:38:00<23:25,  1.70s/it] 92%|█████████▏| 8997/9822 [4:38:02<23:19,  1.70s/it] 92%|█████████▏| 8998/9822 [4:38:04<23:14,  1.69s/it] 92%|█████████▏| 8999/9822 [4:38:05<23:09,  1.69s/it] 92%|█████████▏| 9000/9822 [4:38:07<23:06,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:18:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:18:54 - INFO - __main__ - ***** test Results*****
04/29/2024 16:18:54 - INFO - __main__ -   Training step = 9000
04/29/2024 16:18:54 - INFO - __main__ -  test_accuracy:0.8744509516837482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:18:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:18:58 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:18:58 - INFO - __main__ -   Training step = 9000
04/29/2024 16:18:58 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:19:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:19:07 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:19:07 - INFO - __main__ -   Training step = 9000
04/29/2024 16:19:07 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 92%|█████████▏| 9001/9822 [4:38:26<1:35:10,  6.96s/it] 92%|█████████▏| 9002/9822 [4:38:28<1:13:24,  5.37s/it] 92%|█████████▏| 9003/9822 [4:38:30<58:17,  4.27s/it]   92%|█████████▏| 9004/9822 [4:38:31<47:40,  3.50s/it] 92%|█████████▏| 9005/9822 [4:38:33<40:12,  2.95s/it] 92%|█████████▏| 9006/9822 [4:38:35<34:59,  2.57s/it] 92%|█████████▏| 9007/9822 [4:38:36<31:19,  2.31s/it] 92%|█████████▏| 9008/9822 [4:38:38<28:49,  2.12s/it] 92%|█████████▏| 9009/9822 [4:38:40<26:59,  1.99s/it] 92%|█████████▏| 9010/9822 [4:38:41<25:43,  1.90s/it] 92%|█████████▏| 9011/9822 [4:38:43<24:49,  1.84s/it] 92%|█████████▏| 9012/9822 [4:38:45<24:11,  1.79s/it] 92%|█████████▏| 9013/9822 [4:38:46<23:43,  1.76s/it] 92%|█████████▏| 9014/9822 [4:38:48<23:22,  1.74s/it] 92%|█████████▏| 9015/9822 [4:38:50<23:09,  1.72s/it] 92%|█████████▏| 9016/9822 [4:38:51<22:56,  1.71s/it] 92%|█████████▏| 9017/9822 [4:38:53<22:46,  1.70s/it] 92%|█████████▏| 9018/9822 [4:38:55<22:40,  1.69s/it] 92%|█████████▏| 9019/9822 [4:38:57<23:03,  1.72s/it] 92%|█████████▏| 9020/9822 [4:38:58<22:52,  1.71s/it] 92%|█████████▏| 9021/9822 [4:39:00<22:44,  1.70s/it] 92%|█████████▏| 9022/9822 [4:39:02<22:39,  1.70s/it] 92%|█████████▏| 9023/9822 [4:39:03<22:33,  1.69s/it] 92%|█████████▏| 9024/9822 [4:39:05<22:27,  1.69s/it] 92%|█████████▏| 9025/9822 [4:39:07<22:25,  1.69s/it] 92%|█████████▏| 9026/9822 [4:39:08<22:19,  1.68s/it] 92%|█████████▏| 9027/9822 [4:39:10<22:14,  1.68s/it] 92%|█████████▏| 9028/9822 [4:39:12<22:10,  1.68s/it] 92%|█████████▏| 9029/9822 [4:39:13<22:13,  1.68s/it] 92%|█████████▏| 9030/9822 [4:39:15<22:01,  1.67s/it] 92%|█████████▏| 9031/9822 [4:39:17<22:03,  1.67s/it] 92%|█████████▏| 9032/9822 [4:39:18<22:01,  1.67s/it] 92%|█████████▏| 9033/9822 [4:39:20<22:00,  1.67s/it] 92%|█████████▏| 9034/9822 [4:39:22<22:01,  1.68s/it] 92%|█████████▏| 9035/9822 [4:39:23<22:00,  1.68s/it] 92%|█████████▏| 9036/9822 [4:39:25<22:01,  1.68s/it] 92%|█████████▏| 9037/9822 [4:39:27<22:02,  1.68s/it] 92%|█████████▏| 9038/9822 [4:39:28<22:01,  1.69s/it] 92%|█████████▏| 9039/9822 [4:39:30<22:04,  1.69s/it] 92%|█████████▏| 9040/9822 [4:39:32<22:01,  1.69s/it] 92%|█████████▏| 9041/9822 [4:39:34<22:01,  1.69s/it] 92%|█████████▏| 9042/9822 [4:39:35<22:01,  1.69s/it] 92%|█████████▏| 9043/9822 [4:39:37<21:59,  1.69s/it] 92%|█████████▏| 9044/9822 [4:39:39<22:00,  1.70s/it] 92%|█████████▏| 9045/9822 [4:39:40<22:20,  1.73s/it] 92%|█████████▏| 9046/9822 [4:39:42<22:10,  1.71s/it] 92%|█████████▏| 9047/9822 [4:39:44<21:59,  1.70s/it] 92%|█████████▏| 9048/9822 [4:39:46<21:51,  1.69s/it] 92%|█████████▏| 9049/9822 [4:39:47<21:46,  1.69s/it] 92%|█████████▏| 9050/9822 [4:39:49<21:45,  1.69s/it] 92%|█████████▏| 9051/9822 [4:39:51<21:46,  1.69s/it] 92%|█████████▏| 9052/9822 [4:39:52<21:42,  1.69s/it] 92%|█████████▏| 9053/9822 [4:39:54<21:39,  1.69s/it] 92%|█████████▏| 9054/9822 [4:39:56<21:33,  1.68s/it] 92%|█████████▏| 9055/9822 [4:39:57<21:29,  1.68s/it] 92%|█████████▏| 9056/9822 [4:39:59<21:25,  1.68s/it] 92%|█████████▏| 9057/9822 [4:40:01<21:25,  1.68s/it] 92%|█████████▏| 9058/9822 [4:40:02<21:24,  1.68s/it] 92%|█████████▏| 9059/9822 [4:40:04<21:22,  1.68s/it] 92%|█████████▏| 9060/9822 [4:40:06<21:26,  1.69s/it] 92%|█████████▏| 9061/9822 [4:40:07<21:23,  1.69s/it] 92%|█████████▏| 9062/9822 [4:40:09<21:21,  1.69s/it] 92%|█████████▏| 9063/9822 [4:40:11<21:20,  1.69s/it] 92%|█████████▏| 9064/9822 [4:40:12<21:17,  1.69s/it] 92%|█████████▏| 9065/9822 [4:40:14<21:14,  1.68s/it] 92%|█████████▏| 9066/9822 [4:40:16<21:10,  1.68s/it] 92%|█████████▏| 9067/9822 [4:40:18<21:10,  1.68s/it] 92%|█████████▏| 9068/9822 [4:40:19<21:09,  1.68s/it] 92%|█████████▏| 9069/9822 [4:40:21<21:08,  1.68s/it] 92%|█████████▏| 9070/9822 [4:40:23<21:06,  1.68s/it] 92%|█████████▏| 9071/9822 [4:40:24<21:04,  1.68s/it] 92%|█████████▏| 9072/9822 [4:40:26<21:07,  1.69s/it] 92%|█████████▏| 9073/9822 [4:40:28<21:06,  1.69s/it] 92%|█████████▏| 9074/9822 [4:40:29<21:04,  1.69s/it] 92%|█████████▏| 9075/9822 [4:40:31<21:00,  1.69s/it] 92%|█████████▏| 9076/9822 [4:40:33<20:57,  1.69s/it] 92%|█████████▏| 9077/9822 [4:40:34<20:55,  1.69s/it] 92%|█████████▏| 9078/9822 [4:40:36<21:16,  1.72s/it] 92%|█████████▏| 9079/9822 [4:40:38<21:07,  1.71s/it] 92%|█████████▏| 9080/9822 [4:40:40<20:57,  1.70s/it] 92%|█████████▏| 9081/9822 [4:40:41<21:00,  1.70s/it] 92%|█████████▏| 9082/9822 [4:40:43<20:53,  1.69s/it] 92%|█████████▏| 9083/9822 [4:40:45<20:47,  1.69s/it] 92%|█████████▏| 9084/9822 [4:40:46<20:45,  1.69s/it] 92%|█████████▏| 9085/9822 [4:40:48<20:42,  1.69s/it] 93%|█████████▎| 9086/9822 [4:40:50<20:39,  1.68s/it] 93%|█████████▎| 9087/9822 [4:40:51<20:38,  1.68s/it] 93%|█████████▎| 9088/9822 [4:40:53<20:37,  1.69s/it] 93%|█████████▎| 9089/9822 [4:40:55<20:37,  1.69s/it] 93%|█████████▎| 9090/9822 [4:40:56<20:35,  1.69s/it] 93%|█████████▎| 9091/9822 [4:40:58<20:32,  1.69s/it] 93%|█████████▎| 9092/9822 [4:41:00<20:30,  1.69s/it] 93%|█████████▎| 9093/9822 [4:41:01<20:27,  1.68s/it] 93%|█████████▎| 9094/9822 [4:41:03<20:24,  1.68s/it] 93%|█████████▎| 9095/9822 [4:41:05<20:23,  1.68s/it] 93%|█████████▎| 9096/9822 [4:41:06<20:22,  1.68s/it] 93%|█████████▎| 9097/9822 [4:41:08<20:24,  1.69s/it] 93%|█████████▎| 9098/9822 [4:41:10<20:21,  1.69s/it] 93%|█████████▎| 9099/9822 [4:41:12<20:19,  1.69s/it] 93%|█████████▎| 9100/9822 [4:41:13<20:39,  1.72s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:22:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:22:00 - INFO - __main__ - ***** test Results*****
04/29/2024 16:22:00 - INFO - __main__ -   Training step = 9100
04/29/2024 16:22:00 - INFO - __main__ -  test_accuracy:0.8751830161054173 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:22:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:22:05 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:22:05 - INFO - __main__ -   Training step = 9100
04/29/2024 16:22:05 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:22:13 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:22:13 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:22:13 - INFO - __main__ -   Training step = 9100
04/29/2024 16:22:13 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 93%|█████████▎| 9101/9822 [4:41:33<1:23:55,  6.98s/it] 93%|█████████▎| 9102/9822 [4:41:34<1:04:44,  5.40s/it] 93%|█████████▎| 9103/9822 [4:41:36<51:23,  4.29s/it]   93%|█████████▎| 9104/9822 [4:41:38<41:58,  3.51s/it] 93%|█████████▎| 9105/9822 [4:41:39<35:23,  2.96s/it] 93%|█████████▎| 9106/9822 [4:41:41<30:46,  2.58s/it] 93%|█████████▎| 9107/9822 [4:41:43<27:35,  2.32s/it] 93%|█████████▎| 9108/9822 [4:41:44<25:18,  2.13s/it] 93%|█████████▎| 9109/9822 [4:41:46<23:41,  1.99s/it] 93%|█████████▎| 9110/9822 [4:41:48<22:36,  1.91s/it] 93%|█████████▎| 9111/9822 [4:41:50<21:47,  1.84s/it] 93%|█████████▎| 9112/9822 [4:41:51<21:14,  1.80s/it] 93%|█████████▎| 9113/9822 [4:41:53<20:49,  1.76s/it] 93%|█████████▎| 9114/9822 [4:41:55<20:30,  1.74s/it] 93%|█████████▎| 9115/9822 [4:41:56<20:15,  1.72s/it] 93%|█████████▎| 9116/9822 [4:41:58<19:58,  1.70s/it] 93%|█████████▎| 9117/9822 [4:42:00<19:54,  1.69s/it] 93%|█████████▎| 9118/9822 [4:42:01<19:51,  1.69s/it] 93%|█████████▎| 9119/9822 [4:42:03<19:48,  1.69s/it] 93%|█████████▎| 9120/9822 [4:42:05<19:45,  1.69s/it] 93%|█████████▎| 9121/9822 [4:42:06<19:43,  1.69s/it] 93%|█████████▎| 9122/9822 [4:42:08<19:41,  1.69s/it] 93%|█████████▎| 9123/9822 [4:42:10<19:39,  1.69s/it] 93%|█████████▎| 9124/9822 [4:42:12<19:59,  1.72s/it] 93%|█████████▎| 9125/9822 [4:42:13<19:52,  1.71s/it] 93%|█████████▎| 9126/9822 [4:42:15<19:45,  1.70s/it] 93%|█████████▎| 9127/9822 [4:42:17<19:38,  1.70s/it] 93%|█████████▎| 9128/9822 [4:42:18<19:35,  1.69s/it] 93%|█████████▎| 9129/9822 [4:42:20<19:32,  1.69s/it] 93%|█████████▎| 9130/9822 [4:42:22<19:29,  1.69s/it] 93%|█████████▎| 9131/9822 [4:42:23<19:27,  1.69s/it] 93%|█████████▎| 9132/9822 [4:42:25<19:24,  1.69s/it] 93%|█████████▎| 9133/9822 [4:42:27<19:22,  1.69s/it] 93%|█████████▎| 9134/9822 [4:42:28<19:20,  1.69s/it] 93%|█████████▎| 9135/9822 [4:42:30<19:19,  1.69s/it] 93%|█████████▎| 9136/9822 [4:42:32<19:16,  1.69s/it] 93%|█████████▎| 9137/9822 [4:42:33<19:14,  1.69s/it] 93%|█████████▎| 9138/9822 [4:42:35<19:12,  1.69s/it] 93%|█████████▎| 9139/9822 [4:42:37<19:12,  1.69s/it] 93%|█████████▎| 9140/9822 [4:42:38<19:09,  1.69s/it] 93%|█████████▎| 9141/9822 [4:42:40<19:08,  1.69s/it] 93%|█████████▎| 9142/9822 [4:42:42<19:05,  1.68s/it] 93%|█████████▎| 9143/9822 [4:42:44<19:01,  1.68s/it] 93%|█████████▎| 9144/9822 [4:42:45<19:01,  1.68s/it] 93%|█████████▎| 9145/9822 [4:42:47<18:58,  1.68s/it] 93%|█████████▎| 9146/9822 [4:42:49<18:52,  1.68s/it] 93%|█████████▎| 9147/9822 [4:42:50<18:51,  1.68s/it] 93%|█████████▎| 9148/9822 [4:42:52<18:46,  1.67s/it] 93%|█████████▎| 9149/9822 [4:42:54<18:42,  1.67s/it] 93%|█████████▎| 9150/9822 [4:42:55<19:05,  1.70s/it] 93%|█████████▎| 9151/9822 [4:42:57<18:56,  1.69s/it] 93%|█████████▎| 9152/9822 [4:42:59<18:49,  1.69s/it] 93%|█████████▎| 9153/9822 [4:43:00<18:46,  1.68s/it] 93%|█████████▎| 9154/9822 [4:43:02<18:44,  1.68s/it] 93%|█████████▎| 9155/9822 [4:43:04<18:42,  1.68s/it] 93%|█████████▎| 9156/9822 [4:43:05<18:40,  1.68s/it] 93%|█████████▎| 9157/9822 [4:43:07<18:38,  1.68s/it] 93%|█████████▎| 9158/9822 [4:43:09<18:35,  1.68s/it] 93%|█████████▎| 9159/9822 [4:43:10<18:34,  1.68s/it] 93%|█████████▎| 9160/9822 [4:43:12<18:32,  1.68s/it] 93%|█████████▎| 9161/9822 [4:43:14<18:30,  1.68s/it] 93%|█████████▎| 9162/9822 [4:43:15<18:29,  1.68s/it] 93%|█████████▎| 9163/9822 [4:43:17<18:26,  1.68s/it] 93%|█████████▎| 9164/9822 [4:43:19<18:21,  1.67s/it] 93%|█████████▎| 9165/9822 [4:43:21<18:23,  1.68s/it] 93%|█████████▎| 9166/9822 [4:43:22<18:22,  1.68s/it] 93%|█████████▎| 9167/9822 [4:43:24<18:22,  1.68s/it] 93%|█████████▎| 9168/9822 [4:43:26<18:18,  1.68s/it] 93%|█████████▎| 9169/9822 [4:43:27<18:15,  1.68s/it] 93%|█████████▎| 9170/9822 [4:43:29<18:15,  1.68s/it] 93%|█████████▎| 9171/9822 [4:43:31<18:14,  1.68s/it] 93%|█████████▎| 9172/9822 [4:43:32<18:13,  1.68s/it] 93%|█████████▎| 9173/9822 [4:43:34<18:13,  1.68s/it] 93%|█████████▎| 9174/9822 [4:43:36<18:11,  1.68s/it] 93%|█████████▎| 9175/9822 [4:43:37<18:10,  1.69s/it] 93%|█████████▎| 9176/9822 [4:43:39<18:07,  1.68s/it] 93%|█████████▎| 9177/9822 [4:43:41<18:06,  1.68s/it] 93%|█████████▎| 9178/9822 [4:43:42<18:02,  1.68s/it] 93%|█████████▎| 9179/9822 [4:43:44<17:58,  1.68s/it] 93%|█████████▎| 9180/9822 [4:43:46<17:56,  1.68s/it] 93%|█████████▎| 9181/9822 [4:43:47<17:54,  1.68s/it] 93%|█████████▎| 9182/9822 [4:43:49<17:53,  1.68s/it] 93%|█████████▎| 9183/9822 [4:43:51<18:18,  1.72s/it] 94%|█████████▎| 9184/9822 [4:43:53<18:07,  1.70s/it] 94%|█████████▎| 9185/9822 [4:43:54<17:58,  1.69s/it] 94%|█████████▎| 9186/9822 [4:43:56<17:53,  1.69s/it] 94%|█████████▎| 9187/9822 [4:43:58<17:49,  1.68s/it] 94%|█████████▎| 9188/9822 [4:43:59<17:48,  1.68s/it] 94%|█████████▎| 9189/9822 [4:44:01<17:45,  1.68s/it] 94%|█████████▎| 9190/9822 [4:44:03<17:44,  1.68s/it] 94%|█████████▎| 9191/9822 [4:44:04<17:43,  1.69s/it] 94%|█████████▎| 9192/9822 [4:44:06<17:42,  1.69s/it] 94%|█████████▎| 9193/9822 [4:44:08<17:38,  1.68s/it] 94%|█████████▎| 9194/9822 [4:44:09<17:37,  1.68s/it] 94%|█████████▎| 9195/9822 [4:44:11<17:35,  1.68s/it] 94%|█████████▎| 9196/9822 [4:44:13<17:34,  1.68s/it] 94%|█████████▎| 9197/9822 [4:44:14<17:33,  1.69s/it] 94%|█████████▎| 9198/9822 [4:44:16<17:32,  1.69s/it] 94%|█████████▎| 9199/9822 [4:44:18<17:29,  1.69s/it] 94%|█████████▎| 9200/9822 [4:44:19<17:26,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:25:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:25:06 - INFO - __main__ - ***** test Results*****
04/29/2024 16:25:06 - INFO - __main__ -   Training step = 9200
04/29/2024 16:25:06 - INFO - __main__ -  test_accuracy:0.873718887262079 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:25:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:25:11 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:25:11 - INFO - __main__ -   Training step = 9200
04/29/2024 16:25:11 - INFO - __main__ -  eval_accuracy:0.8528011717319663 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:25:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:25:19 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:25:19 - INFO - __main__ -   Training step = 9200
04/29/2024 16:25:19 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 94%|█████████▎| 9201/9822 [4:44:39<1:11:58,  6.95s/it] 94%|█████████▎| 9202/9822 [4:44:40<55:22,  5.36s/it]   94%|█████████▎| 9203/9822 [4:44:42<43:52,  4.25s/it] 94%|█████████▎| 9204/9822 [4:44:44<35:53,  3.48s/it] 94%|█████████▎| 9205/9822 [4:44:45<30:17,  2.95s/it] 94%|█████████▎| 9206/9822 [4:44:47<26:22,  2.57s/it] 94%|█████████▎| 9207/9822 [4:44:49<23:37,  2.30s/it] 94%|█████████▎| 9208/9822 [4:44:50<21:41,  2.12s/it] 94%|█████████▍| 9209/9822 [4:44:52<20:38,  2.02s/it] 94%|█████████▍| 9210/9822 [4:44:54<19:35,  1.92s/it] 94%|█████████▍| 9211/9822 [4:44:56<18:51,  1.85s/it] 94%|█████████▍| 9212/9822 [4:44:57<18:18,  1.80s/it] 94%|█████████▍| 9213/9822 [4:44:59<17:55,  1.77s/it] 94%|█████████▍| 9214/9822 [4:45:01<17:40,  1.74s/it] 94%|█████████▍| 9215/9822 [4:45:02<17:28,  1.73s/it] 94%|█████████▍| 9216/9822 [4:45:04<17:18,  1.71s/it] 94%|█████████▍| 9217/9822 [4:45:06<17:12,  1.71s/it] 94%|█████████▍| 9218/9822 [4:45:07<17:07,  1.70s/it] 94%|█████████▍| 9219/9822 [4:45:09<17:03,  1.70s/it] 94%|█████████▍| 9220/9822 [4:45:11<17:01,  1.70s/it] 94%|█████████▍| 9221/9822 [4:45:13<16:57,  1.69s/it] 94%|█████████▍| 9222/9822 [4:45:14<16:53,  1.69s/it] 94%|█████████▍| 9223/9822 [4:45:16<16:47,  1.68s/it] 94%|█████████▍| 9224/9822 [4:45:18<16:45,  1.68s/it] 94%|█████████▍| 9225/9822 [4:45:19<16:44,  1.68s/it] 94%|█████████▍| 9226/9822 [4:45:21<16:44,  1.69s/it] 94%|█████████▍| 9227/9822 [4:45:23<16:41,  1.68s/it] 94%|█████████▍| 9228/9822 [4:45:24<16:37,  1.68s/it] 94%|█████████▍| 9229/9822 [4:45:26<16:37,  1.68s/it] 94%|█████████▍| 9230/9822 [4:45:28<16:37,  1.69s/it] 94%|█████████▍| 9231/9822 [4:45:29<16:34,  1.68s/it] 94%|█████████▍| 9232/9822 [4:45:31<16:33,  1.68s/it] 94%|█████████▍| 9233/9822 [4:45:33<16:32,  1.68s/it] 94%|█████████▍| 9234/9822 [4:45:34<16:30,  1.68s/it] 94%|█████████▍| 9235/9822 [4:45:36<16:48,  1.72s/it] 94%|█████████▍| 9236/9822 [4:45:38<16:38,  1.70s/it] 94%|█████████▍| 9237/9822 [4:45:40<16:30,  1.69s/it] 94%|█████████▍| 9238/9822 [4:45:41<16:28,  1.69s/it] 94%|█████████▍| 9239/9822 [4:45:43<16:22,  1.69s/it] 94%|█████████▍| 9240/9822 [4:45:45<16:20,  1.68s/it] 94%|█████████▍| 9241/9822 [4:45:46<16:17,  1.68s/it] 94%|█████████▍| 9242/9822 [4:45:48<16:15,  1.68s/it] 94%|█████████▍| 9243/9822 [4:45:50<16:13,  1.68s/it] 94%|█████████▍| 9244/9822 [4:45:51<16:09,  1.68s/it] 94%|█████████▍| 9245/9822 [4:45:53<16:09,  1.68s/it] 94%|█████████▍| 9246/9822 [4:45:55<16:07,  1.68s/it] 94%|█████████▍| 9247/9822 [4:45:56<16:06,  1.68s/it] 94%|█████████▍| 9248/9822 [4:45:58<16:03,  1.68s/it] 94%|█████████▍| 9249/9822 [4:46:00<16:01,  1.68s/it] 94%|█████████▍| 9250/9822 [4:46:01<16:00,  1.68s/it] 94%|█████████▍| 9251/9822 [4:46:03<15:57,  1.68s/it] 94%|█████████▍| 9252/9822 [4:46:05<15:56,  1.68s/it] 94%|█████████▍| 9253/9822 [4:46:06<15:56,  1.68s/it] 94%|█████████▍| 9254/9822 [4:46:08<15:55,  1.68s/it] 94%|█████████▍| 9255/9822 [4:46:10<15:52,  1.68s/it] 94%|█████████▍| 9256/9822 [4:46:11<15:52,  1.68s/it] 94%|█████████▍| 9257/9822 [4:46:13<15:49,  1.68s/it] 94%|█████████▍| 9258/9822 [4:46:15<15:48,  1.68s/it] 94%|█████████▍| 9259/9822 [4:46:16<15:45,  1.68s/it] 94%|█████████▍| 9260/9822 [4:46:18<15:43,  1.68s/it] 94%|█████████▍| 9261/9822 [4:46:20<15:41,  1.68s/it] 94%|█████████▍| 9262/9822 [4:46:22<15:39,  1.68s/it] 94%|█████████▍| 9263/9822 [4:46:23<15:37,  1.68s/it] 94%|█████████▍| 9264/9822 [4:46:25<15:35,  1.68s/it] 94%|█████████▍| 9265/9822 [4:46:27<15:35,  1.68s/it] 94%|█████████▍| 9266/9822 [4:46:28<15:33,  1.68s/it] 94%|█████████▍| 9267/9822 [4:46:30<15:33,  1.68s/it] 94%|█████████▍| 9268/9822 [4:46:32<15:31,  1.68s/it] 94%|█████████▍| 9269/9822 [4:46:33<15:31,  1.68s/it] 94%|█████████▍| 9270/9822 [4:46:35<15:38,  1.70s/it] 94%|█████████▍| 9271/9822 [4:46:37<15:34,  1.70s/it] 94%|█████████▍| 9272/9822 [4:46:38<15:31,  1.69s/it] 94%|█████████▍| 9273/9822 [4:46:40<15:27,  1.69s/it] 94%|█████████▍| 9274/9822 [4:46:42<15:25,  1.69s/it] 94%|█████████▍| 9275/9822 [4:46:43<15:23,  1.69s/it] 94%|█████████▍| 9276/9822 [4:46:45<15:36,  1.72s/it] 94%|█████████▍| 9277/9822 [4:46:47<15:27,  1.70s/it] 94%|█████████▍| 9278/9822 [4:46:49<15:20,  1.69s/it] 94%|█████████▍| 9279/9822 [4:46:50<15:15,  1.69s/it] 94%|█████████▍| 9280/9822 [4:46:52<15:14,  1.69s/it] 94%|█████████▍| 9281/9822 [4:46:54<15:12,  1.69s/it] 95%|█████████▍| 9282/9822 [4:46:55<15:10,  1.69s/it] 95%|█████████▍| 9283/9822 [4:46:57<15:09,  1.69s/it] 95%|█████████▍| 9284/9822 [4:46:59<15:06,  1.69s/it] 95%|█████████▍| 9285/9822 [4:47:00<15:02,  1.68s/it] 95%|█████████▍| 9286/9822 [4:47:02<14:59,  1.68s/it] 95%|█████████▍| 9287/9822 [4:47:04<14:56,  1.67s/it] 95%|█████████▍| 9288/9822 [4:47:05<14:46,  1.66s/it] 95%|█████████▍| 9289/9822 [4:47:07<14:47,  1.67s/it] 95%|█████████▍| 9290/9822 [4:47:09<14:46,  1.67s/it] 95%|█████████▍| 9291/9822 [4:47:10<14:50,  1.68s/it] 95%|█████████▍| 9292/9822 [4:47:12<14:50,  1.68s/it] 95%|█████████▍| 9293/9822 [4:47:14<14:50,  1.68s/it] 95%|█████████▍| 9294/9822 [4:47:15<14:47,  1.68s/it] 95%|█████████▍| 9295/9822 [4:47:17<14:47,  1.68s/it] 95%|█████████▍| 9296/9822 [4:47:19<14:45,  1.68s/it] 95%|█████████▍| 9297/9822 [4:47:20<14:44,  1.68s/it] 95%|█████████▍| 9298/9822 [4:47:22<14:42,  1.68s/it] 95%|█████████▍| 9299/9822 [4:47:24<14:40,  1.68s/it] 95%|█████████▍| 9300/9822 [4:47:26<14:38,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:28:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:28:12 - INFO - __main__ - ***** test Results*****
04/29/2024 16:28:12 - INFO - __main__ -   Training step = 9300
04/29/2024 16:28:12 - INFO - __main__ -  test_accuracy:0.8744509516837482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:28:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:28:17 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:28:17 - INFO - __main__ -   Training step = 9300
04/29/2024 16:28:17 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:28:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:28:25 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:28:25 - INFO - __main__ -   Training step = 9300
04/29/2024 16:28:25 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 95%|█████████▍| 9301/9822 [4:47:45<1:00:22,  6.95s/it] 95%|█████████▍| 9302/9822 [4:47:47<46:49,  5.40s/it]   95%|█████████▍| 9303/9822 [4:47:48<37:04,  4.29s/it] 95%|█████████▍| 9304/9822 [4:47:50<30:15,  3.51s/it] 95%|█████████▍| 9305/9822 [4:47:52<25:28,  2.96s/it] 95%|█████████▍| 9306/9822 [4:47:53<22:07,  2.57s/it] 95%|█████████▍| 9307/9822 [4:47:55<19:47,  2.31s/it] 95%|█████████▍| 9308/9822 [4:47:57<18:08,  2.12s/it] 95%|█████████▍| 9309/9822 [4:47:58<16:56,  1.98s/it] 95%|█████████▍| 9310/9822 [4:48:00<16:06,  1.89s/it] 95%|█████████▍| 9311/9822 [4:48:02<15:33,  1.83s/it] 95%|█████████▍| 9312/9822 [4:48:03<15:07,  1.78s/it] 95%|█████████▍| 9313/9822 [4:48:05<14:52,  1.75s/it] 95%|█████████▍| 9314/9822 [4:48:07<14:39,  1.73s/it] 95%|█████████▍| 9315/9822 [4:48:08<14:30,  1.72s/it] 95%|█████████▍| 9316/9822 [4:48:10<14:24,  1.71s/it] 95%|█████████▍| 9317/9822 [4:48:12<14:19,  1.70s/it] 95%|█████████▍| 9318/9822 [4:48:13<14:23,  1.71s/it] 95%|█████████▍| 9319/9822 [4:48:15<14:17,  1.70s/it] 95%|█████████▍| 9320/9822 [4:48:17<14:10,  1.69s/it] 95%|█████████▍| 9321/9822 [4:48:19<14:07,  1.69s/it] 95%|█████████▍| 9322/9822 [4:48:20<14:04,  1.69s/it] 95%|█████████▍| 9323/9822 [4:48:22<14:02,  1.69s/it] 95%|█████████▍| 9324/9822 [4:48:24<14:00,  1.69s/it] 95%|█████████▍| 9325/9822 [4:48:25<13:58,  1.69s/it] 95%|█████████▍| 9326/9822 [4:48:27<13:54,  1.68s/it] 95%|█████████▍| 9327/9822 [4:48:29<13:53,  1.68s/it] 95%|█████████▍| 9328/9822 [4:48:30<13:51,  1.68s/it] 95%|█████████▍| 9329/9822 [4:48:32<14:03,  1.71s/it] 95%|█████████▍| 9330/9822 [4:48:34<13:58,  1.70s/it] 95%|█████████▌| 9331/9822 [4:48:35<13:52,  1.70s/it] 95%|█████████▌| 9332/9822 [4:48:37<13:48,  1.69s/it] 95%|█████████▌| 9333/9822 [4:48:39<13:43,  1.68s/it] 95%|█████████▌| 9334/9822 [4:48:40<13:40,  1.68s/it] 95%|█████████▌| 9335/9822 [4:48:42<13:39,  1.68s/it] 95%|█████████▌| 9336/9822 [4:48:44<13:35,  1.68s/it] 95%|█████████▌| 9337/9822 [4:48:46<13:40,  1.69s/it] 95%|█████████▌| 9338/9822 [4:48:47<13:37,  1.69s/it] 95%|█████████▌| 9339/9822 [4:48:49<13:32,  1.68s/it] 95%|█████████▌| 9340/9822 [4:48:51<13:29,  1.68s/it] 95%|█████████▌| 9341/9822 [4:48:52<13:28,  1.68s/it] 95%|█████████▌| 9342/9822 [4:48:54<13:24,  1.68s/it] 95%|█████████▌| 9343/9822 [4:48:56<13:23,  1.68s/it] 95%|█████████▌| 9344/9822 [4:48:57<13:22,  1.68s/it] 95%|█████████▌| 9345/9822 [4:48:59<13:22,  1.68s/it] 95%|█████████▌| 9346/9822 [4:49:01<13:19,  1.68s/it] 95%|█████████▌| 9347/9822 [4:49:02<13:16,  1.68s/it] 95%|█████████▌| 9348/9822 [4:49:04<13:16,  1.68s/it] 95%|█████████▌| 9349/9822 [4:49:06<13:18,  1.69s/it] 95%|█████████▌| 9350/9822 [4:49:07<13:13,  1.68s/it] 95%|█████████▌| 9351/9822 [4:49:09<13:12,  1.68s/it] 95%|█████████▌| 9352/9822 [4:49:11<13:11,  1.68s/it] 95%|█████████▌| 9353/9822 [4:49:12<13:08,  1.68s/it] 95%|█████████▌| 9354/9822 [4:49:14<13:10,  1.69s/it] 95%|█████████▌| 9355/9822 [4:49:16<13:08,  1.69s/it] 95%|█████████▌| 9356/9822 [4:49:18<13:19,  1.72s/it] 95%|█████████▌| 9357/9822 [4:49:19<13:12,  1.71s/it] 95%|█████████▌| 9358/9822 [4:49:21<13:09,  1.70s/it] 95%|█████████▌| 9359/9822 [4:49:23<13:03,  1.69s/it] 95%|█████████▌| 9360/9822 [4:49:24<12:59,  1.69s/it] 95%|█████████▌| 9361/9822 [4:49:26<12:58,  1.69s/it] 95%|█████████▌| 9362/9822 [4:49:28<12:53,  1.68s/it] 95%|█████████▌| 9363/9822 [4:49:29<12:52,  1.68s/it] 95%|█████████▌| 9364/9822 [4:49:31<12:50,  1.68s/it] 95%|█████████▌| 9365/9822 [4:49:33<12:47,  1.68s/it] 95%|█████████▌| 9366/9822 [4:49:34<12:45,  1.68s/it] 95%|█████████▌| 9367/9822 [4:49:36<12:43,  1.68s/it] 95%|█████████▌| 9368/9822 [4:49:38<12:39,  1.67s/it] 95%|█████████▌| 9369/9822 [4:49:39<12:36,  1.67s/it] 95%|█████████▌| 9370/9822 [4:49:41<12:36,  1.67s/it] 95%|█████████▌| 9371/9822 [4:49:43<12:35,  1.67s/it] 95%|█████████▌| 9372/9822 [4:49:44<12:32,  1.67s/it] 95%|█████████▌| 9373/9822 [4:49:46<12:30,  1.67s/it] 95%|█████████▌| 9374/9822 [4:49:48<12:22,  1.66s/it] 95%|█████████▌| 9375/9822 [4:49:49<12:22,  1.66s/it] 95%|█████████▌| 9376/9822 [4:49:51<12:22,  1.67s/it] 95%|█████████▌| 9377/9822 [4:49:53<12:21,  1.67s/it] 95%|█████████▌| 9378/9822 [4:49:54<12:21,  1.67s/it] 95%|█████████▌| 9379/9822 [4:49:56<12:21,  1.67s/it] 95%|█████████▌| 9380/9822 [4:49:58<12:20,  1.68s/it] 96%|█████████▌| 9381/9822 [4:49:59<12:18,  1.67s/it] 96%|█████████▌| 9382/9822 [4:50:01<12:16,  1.67s/it] 96%|█████████▌| 9383/9822 [4:50:03<12:15,  1.68s/it] 96%|█████████▌| 9384/9822 [4:50:04<12:14,  1.68s/it] 96%|█████████▌| 9385/9822 [4:50:06<12:12,  1.68s/it] 96%|█████████▌| 9386/9822 [4:50:08<12:10,  1.68s/it] 96%|█████████▌| 9387/9822 [4:50:09<12:07,  1.67s/it] 96%|█████████▌| 9388/9822 [4:50:11<12:06,  1.67s/it] 96%|█████████▌| 9389/9822 [4:50:13<12:18,  1.71s/it] 96%|█████████▌| 9390/9822 [4:50:15<12:13,  1.70s/it] 96%|█████████▌| 9391/9822 [4:50:16<12:08,  1.69s/it] 96%|█████████▌| 9392/9822 [4:50:18<12:04,  1.69s/it] 96%|█████████▌| 9393/9822 [4:50:20<12:00,  1.68s/it] 96%|█████████▌| 9394/9822 [4:50:21<11:58,  1.68s/it] 96%|█████████▌| 9395/9822 [4:50:23<11:56,  1.68s/it] 96%|█████████▌| 9396/9822 [4:50:25<11:54,  1.68s/it] 96%|█████████▌| 9397/9822 [4:50:26<11:52,  1.68s/it] 96%|█████████▌| 9398/9822 [4:50:28<11:48,  1.67s/it] 96%|█████████▌| 9399/9822 [4:50:30<11:46,  1.67s/it] 96%|█████████▌| 9400/9822 [4:50:31<11:45,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:31:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:31:18 - INFO - __main__ - ***** test Results*****
04/29/2024 16:31:18 - INFO - __main__ -   Training step = 9400
04/29/2024 16:31:18 - INFO - __main__ -  test_accuracy:0.8748169838945827 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:31:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:31:23 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:31:23 - INFO - __main__ -   Training step = 9400
04/29/2024 16:31:23 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:31:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:31:31 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:31:31 - INFO - __main__ -   Training step = 9400
04/29/2024 16:31:31 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 96%|█████████▌| 9401/9822 [4:50:51<48:45,  6.95s/it] 96%|█████████▌| 9402/9822 [4:50:52<37:33,  5.36s/it] 96%|█████████▌| 9403/9822 [4:50:54<29:42,  4.25s/it] 96%|█████████▌| 9404/9822 [4:50:56<24:14,  3.48s/it] 96%|█████████▌| 9405/9822 [4:50:57<20:24,  2.94s/it] 96%|█████████▌| 9406/9822 [4:50:59<17:45,  2.56s/it] 96%|█████████▌| 9407/9822 [4:51:01<15:53,  2.30s/it] 96%|█████████▌| 9408/9822 [4:51:02<14:33,  2.11s/it] 96%|█████████▌| 9409/9822 [4:51:04<13:37,  1.98s/it] 96%|█████████▌| 9410/9822 [4:51:06<12:58,  1.89s/it] 96%|█████████▌| 9411/9822 [4:51:07<12:31,  1.83s/it] 96%|█████████▌| 9412/9822 [4:51:09<12:12,  1.79s/it] 96%|█████████▌| 9413/9822 [4:51:11<11:57,  1.75s/it] 96%|█████████▌| 9414/9822 [4:51:12<11:47,  1.74s/it] 96%|█████████▌| 9415/9822 [4:51:14<11:39,  1.72s/it] 96%|█████████▌| 9416/9822 [4:51:16<11:32,  1.71s/it] 96%|█████████▌| 9417/9822 [4:51:17<11:27,  1.70s/it] 96%|█████████▌| 9418/9822 [4:51:19<11:24,  1.70s/it] 96%|█████████▌| 9419/9822 [4:51:21<11:33,  1.72s/it] 96%|█████████▌| 9420/9822 [4:51:23<11:28,  1.71s/it] 96%|█████████▌| 9421/9822 [4:51:24<11:23,  1.70s/it] 96%|█████████▌| 9422/9822 [4:51:26<11:20,  1.70s/it] 96%|█████████▌| 9423/9822 [4:51:28<11:15,  1.69s/it] 96%|█████████▌| 9424/9822 [4:51:29<11:13,  1.69s/it] 96%|█████████▌| 9425/9822 [4:51:31<11:10,  1.69s/it] 96%|█████████▌| 9426/9822 [4:51:33<11:06,  1.68s/it] 96%|█████████▌| 9427/9822 [4:51:34<11:05,  1.69s/it] 96%|█████████▌| 9428/9822 [4:51:36<11:03,  1.68s/it] 96%|█████████▌| 9429/9822 [4:51:38<11:01,  1.68s/it] 96%|█████████▌| 9430/9822 [4:51:39<10:58,  1.68s/it] 96%|█████████▌| 9431/9822 [4:51:41<10:57,  1.68s/it] 96%|█████████▌| 9432/9822 [4:51:43<10:52,  1.67s/it] 96%|█████████▌| 9433/9822 [4:51:44<10:51,  1.67s/it] 96%|█████████▌| 9434/9822 [4:51:46<10:50,  1.68s/it] 96%|█████████▌| 9435/9822 [4:51:48<10:49,  1.68s/it] 96%|█████████▌| 9436/9822 [4:51:49<10:46,  1.68s/it] 96%|█████████▌| 9437/9822 [4:51:51<10:46,  1.68s/it] 96%|█████████▌| 9438/9822 [4:51:53<10:45,  1.68s/it] 96%|█████████▌| 9439/9822 [4:51:55<10:44,  1.68s/it] 96%|█████████▌| 9440/9822 [4:51:56<10:43,  1.68s/it] 96%|█████████▌| 9441/9822 [4:51:58<10:40,  1.68s/it] 96%|█████████▌| 9442/9822 [4:52:00<10:37,  1.68s/it] 96%|█████████▌| 9443/9822 [4:52:01<10:34,  1.67s/it] 96%|█████████▌| 9444/9822 [4:52:03<10:33,  1.67s/it] 96%|█████████▌| 9445/9822 [4:52:05<10:31,  1.67s/it] 96%|█████████▌| 9446/9822 [4:52:06<10:42,  1.71s/it] 96%|█████████▌| 9447/9822 [4:52:08<10:37,  1.70s/it] 96%|█████████▌| 9448/9822 [4:52:10<10:34,  1.70s/it] 96%|█████████▌| 9449/9822 [4:52:11<10:31,  1.69s/it] 96%|█████████▌| 9450/9822 [4:52:13<10:27,  1.69s/it] 96%|█████████▌| 9451/9822 [4:52:15<10:24,  1.68s/it] 96%|█████████▌| 9452/9822 [4:52:16<10:22,  1.68s/it] 96%|█████████▌| 9453/9822 [4:52:18<10:21,  1.69s/it] 96%|█████████▋| 9454/9822 [4:52:20<10:18,  1.68s/it] 96%|█████████▋| 9455/9822 [4:52:22<10:16,  1.68s/it] 96%|█████████▋| 9456/9822 [4:52:23<10:15,  1.68s/it] 96%|█████████▋| 9457/9822 [4:52:25<10:12,  1.68s/it] 96%|█████████▋| 9458/9822 [4:52:27<10:10,  1.68s/it] 96%|█████████▋| 9459/9822 [4:52:28<10:09,  1.68s/it] 96%|█████████▋| 9460/9822 [4:52:30<10:02,  1.66s/it] 96%|█████████▋| 9461/9822 [4:52:32<10:01,  1.67s/it] 96%|█████████▋| 9462/9822 [4:52:33<10:01,  1.67s/it] 96%|█████████▋| 9463/9822 [4:52:35<10:00,  1.67s/it] 96%|█████████▋| 9464/9822 [4:52:37<09:58,  1.67s/it] 96%|█████████▋| 9465/9822 [4:52:38<09:57,  1.67s/it] 96%|█████████▋| 9466/9822 [4:52:40<09:56,  1.68s/it] 96%|█████████▋| 9467/9822 [4:52:42<09:55,  1.68s/it] 96%|█████████▋| 9468/9822 [4:52:43<09:53,  1.68s/it] 96%|█████████▋| 9469/9822 [4:52:45<09:52,  1.68s/it] 96%|█████████▋| 9470/9822 [4:52:47<09:50,  1.68s/it] 96%|█████████▋| 9471/9822 [4:52:48<09:49,  1.68s/it] 96%|█████████▋| 9472/9822 [4:52:50<09:48,  1.68s/it] 96%|█████████▋| 9473/9822 [4:52:52<09:58,  1.72s/it] 96%|█████████▋| 9474/9822 [4:52:53<09:53,  1.71s/it] 96%|█████████▋| 9475/9822 [4:52:55<09:49,  1.70s/it] 96%|█████████▋| 9476/9822 [4:52:57<09:44,  1.69s/it] 96%|█████████▋| 9477/9822 [4:52:59<09:41,  1.69s/it] 96%|█████████▋| 9478/9822 [4:53:00<09:38,  1.68s/it] 97%|█████████▋| 9479/9822 [4:53:02<09:35,  1.68s/it] 97%|█████████▋| 9480/9822 [4:53:04<09:32,  1.67s/it] 97%|█████████▋| 9481/9822 [4:53:05<09:30,  1.67s/it] 97%|█████████▋| 9482/9822 [4:53:07<09:28,  1.67s/it] 97%|█████████▋| 9483/9822 [4:53:09<09:25,  1.67s/it] 97%|█████████▋| 9484/9822 [4:53:10<09:26,  1.67s/it] 97%|█████████▋| 9485/9822 [4:53:12<09:23,  1.67s/it] 97%|█████████▋| 9486/9822 [4:53:14<09:20,  1.67s/it] 97%|█████████▋| 9487/9822 [4:53:15<09:18,  1.67s/it] 97%|█████████▋| 9488/9822 [4:53:17<09:17,  1.67s/it] 97%|█████████▋| 9489/9822 [4:53:19<09:16,  1.67s/it] 97%|█████████▋| 9490/9822 [4:53:20<09:16,  1.68s/it] 97%|█████████▋| 9491/9822 [4:53:22<09:16,  1.68s/it] 97%|█████████▋| 9492/9822 [4:53:24<09:14,  1.68s/it] 97%|█████████▋| 9493/9822 [4:53:25<09:13,  1.68s/it] 97%|█████████▋| 9494/9822 [4:53:27<09:11,  1.68s/it] 97%|█████████▋| 9495/9822 [4:53:29<09:09,  1.68s/it] 97%|█████████▋| 9496/9822 [4:53:30<09:07,  1.68s/it] 97%|█████████▋| 9497/9822 [4:53:32<09:06,  1.68s/it] 97%|█████████▋| 9498/9822 [4:53:34<09:05,  1.68s/it] 97%|█████████▋| 9499/9822 [4:53:35<09:03,  1.68s/it] 97%|█████████▋| 9500/9822 [4:53:37<09:01,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:34:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:34:24 - INFO - __main__ - ***** test Results*****
04/29/2024 16:34:24 - INFO - __main__ -   Training step = 9500
04/29/2024 16:34:24 - INFO - __main__ -  test_accuracy:0.8751830161054173 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:34:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:34:28 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:34:28 - INFO - __main__ -   Training step = 9500
04/29/2024 16:34:28 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:34:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:34:37 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:34:37 - INFO - __main__ -   Training step = 9500
04/29/2024 16:34:37 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 97%|█████████▋| 9501/9822 [4:53:56<37:12,  6.95s/it] 97%|█████████▋| 9502/9822 [4:53:58<28:39,  5.37s/it] 97%|█████████▋| 9503/9822 [4:54:00<22:41,  4.27s/it] 97%|█████████▋| 9504/9822 [4:54:01<18:29,  3.49s/it] 97%|█████████▋| 9505/9822 [4:54:03<15:42,  2.97s/it] 97%|█████████▋| 9506/9822 [4:54:05<13:37,  2.59s/it] 97%|█████████▋| 9507/9822 [4:54:06<12:09,  2.32s/it] 97%|█████████▋| 9508/9822 [4:54:08<11:07,  2.13s/it] 97%|█████████▋| 9509/9822 [4:54:10<10:22,  1.99s/it] 97%|█████████▋| 9510/9822 [4:54:12<09:51,  1.90s/it] 97%|█████████▋| 9511/9822 [4:54:13<09:29,  1.83s/it] 97%|█████████▋| 9512/9822 [4:54:15<09:14,  1.79s/it] 97%|█████████▋| 9513/9822 [4:54:17<09:02,  1.75s/it] 97%|█████████▋| 9514/9822 [4:54:18<08:54,  1.73s/it] 97%|█████████▋| 9515/9822 [4:54:20<08:47,  1.72s/it] 97%|█████████▋| 9516/9822 [4:54:22<08:40,  1.70s/it] 97%|█████████▋| 9517/9822 [4:54:23<08:36,  1.69s/it] 97%|█████████▋| 9518/9822 [4:54:25<08:32,  1.69s/it] 97%|█████████▋| 9519/9822 [4:54:27<08:30,  1.68s/it] 97%|█████████▋| 9520/9822 [4:54:28<08:27,  1.68s/it] 97%|█████████▋| 9521/9822 [4:54:30<08:25,  1.68s/it] 97%|█████████▋| 9522/9822 [4:54:32<08:23,  1.68s/it] 97%|█████████▋| 9523/9822 [4:54:33<08:21,  1.68s/it] 97%|█████████▋| 9524/9822 [4:54:35<08:21,  1.68s/it] 97%|█████████▋| 9525/9822 [4:54:37<08:19,  1.68s/it] 97%|█████████▋| 9526/9822 [4:54:38<08:18,  1.68s/it] 97%|█████████▋| 9527/9822 [4:54:40<08:17,  1.69s/it] 97%|█████████▋| 9528/9822 [4:54:42<08:15,  1.69s/it] 97%|█████████▋| 9529/9822 [4:54:43<08:17,  1.70s/it] 97%|█████████▋| 9530/9822 [4:54:45<08:15,  1.70s/it] 97%|█████████▋| 9531/9822 [4:54:47<08:12,  1.69s/it] 97%|█████████▋| 9532/9822 [4:54:49<08:10,  1.69s/it] 97%|█████████▋| 9533/9822 [4:54:50<08:08,  1.69s/it] 97%|█████████▋| 9534/9822 [4:54:52<08:06,  1.69s/it] 97%|█████████▋| 9535/9822 [4:54:54<08:04,  1.69s/it] 97%|█████████▋| 9536/9822 [4:54:55<08:02,  1.69s/it] 97%|█████████▋| 9537/9822 [4:54:57<08:00,  1.69s/it] 97%|█████████▋| 9538/9822 [4:54:59<08:08,  1.72s/it] 97%|█████████▋| 9539/9822 [4:55:00<08:04,  1.71s/it] 97%|█████████▋| 9540/9822 [4:55:02<08:00,  1.70s/it] 97%|█████████▋| 9541/9822 [4:55:04<07:57,  1.70s/it] 97%|█████████▋| 9542/9822 [4:55:06<07:54,  1.70s/it] 97%|█████████▋| 9543/9822 [4:55:07<07:52,  1.69s/it] 97%|█████████▋| 9544/9822 [4:55:09<07:53,  1.70s/it] 97%|█████████▋| 9545/9822 [4:55:11<07:50,  1.70s/it] 97%|█████████▋| 9546/9822 [4:55:12<07:43,  1.68s/it] 97%|█████████▋| 9547/9822 [4:55:14<07:41,  1.68s/it] 97%|█████████▋| 9548/9822 [4:55:16<07:39,  1.68s/it] 97%|█████████▋| 9549/9822 [4:55:17<07:37,  1.68s/it] 97%|█████████▋| 9550/9822 [4:55:19<07:36,  1.68s/it] 97%|█████████▋| 9551/9822 [4:55:21<07:34,  1.68s/it] 97%|█████████▋| 9552/9822 [4:55:22<07:31,  1.67s/it] 97%|█████████▋| 9553/9822 [4:55:24<07:30,  1.67s/it] 97%|█████████▋| 9554/9822 [4:55:26<07:29,  1.68s/it] 97%|█████████▋| 9555/9822 [4:55:27<07:28,  1.68s/it] 97%|█████████▋| 9556/9822 [4:55:29<07:26,  1.68s/it] 97%|█████████▋| 9557/9822 [4:55:31<07:24,  1.68s/it] 97%|█████████▋| 9558/9822 [4:55:32<07:22,  1.68s/it] 97%|█████████▋| 9559/9822 [4:55:34<07:21,  1.68s/it] 97%|█████████▋| 9560/9822 [4:55:36<07:27,  1.71s/it] 97%|█████████▋| 9561/9822 [4:55:38<07:23,  1.70s/it] 97%|█████████▋| 9562/9822 [4:55:39<07:20,  1.70s/it] 97%|█████████▋| 9563/9822 [4:55:41<07:17,  1.69s/it] 97%|█████████▋| 9564/9822 [4:55:43<07:15,  1.69s/it] 97%|█████████▋| 9565/9822 [4:55:44<07:12,  1.68s/it] 97%|█████████▋| 9566/9822 [4:55:46<07:09,  1.68s/it] 97%|█████████▋| 9567/9822 [4:55:48<07:07,  1.68s/it] 97%|█████████▋| 9568/9822 [4:55:49<07:05,  1.67s/it] 97%|█████████▋| 9569/9822 [4:55:51<07:03,  1.68s/it] 97%|█████████▋| 9570/9822 [4:55:53<07:01,  1.67s/it] 97%|█████████▋| 9571/9822 [4:55:54<06:59,  1.67s/it] 97%|█████████▋| 9572/9822 [4:55:56<06:58,  1.68s/it] 97%|█████████▋| 9573/9822 [4:55:58<06:58,  1.68s/it] 97%|█████████▋| 9574/9822 [4:55:59<06:56,  1.68s/it] 97%|█████████▋| 9575/9822 [4:56:01<06:55,  1.68s/it] 97%|█████████▋| 9576/9822 [4:56:03<06:55,  1.69s/it] 98%|█████████▊| 9577/9822 [4:56:04<06:53,  1.69s/it] 98%|█████████▊| 9578/9822 [4:56:06<06:50,  1.68s/it] 98%|█████████▊| 9579/9822 [4:56:08<06:49,  1.68s/it] 98%|█████████▊| 9580/9822 [4:56:09<06:46,  1.68s/it] 98%|█████████▊| 9581/9822 [4:56:11<06:44,  1.68s/it] 98%|█████████▊| 9582/9822 [4:56:13<06:41,  1.67s/it] 98%|█████████▊| 9583/9822 [4:56:14<06:41,  1.68s/it] 98%|█████████▊| 9584/9822 [4:56:16<06:39,  1.68s/it] 98%|█████████▊| 9585/9822 [4:56:18<06:37,  1.68s/it] 98%|█████████▊| 9586/9822 [4:56:19<06:35,  1.68s/it] 98%|█████████▊| 9587/9822 [4:56:21<06:41,  1.71s/it] 98%|█████████▊| 9588/9822 [4:56:23<06:37,  1.70s/it] 98%|█████████▊| 9589/9822 [4:56:25<06:33,  1.69s/it] 98%|█████████▊| 9590/9822 [4:56:26<06:30,  1.68s/it] 98%|█████████▊| 9591/9822 [4:56:28<06:28,  1.68s/it] 98%|█████████▊| 9592/9822 [4:56:30<06:27,  1.69s/it] 98%|█████████▊| 9593/9822 [4:56:31<06:25,  1.68s/it] 98%|█████████▊| 9594/9822 [4:56:33<06:23,  1.68s/it] 98%|█████████▊| 9595/9822 [4:56:35<06:22,  1.68s/it] 98%|█████████▊| 9596/9822 [4:56:36<06:19,  1.68s/it] 98%|█████████▊| 9597/9822 [4:56:38<06:18,  1.68s/it] 98%|█████████▊| 9598/9822 [4:56:40<06:16,  1.68s/it] 98%|█████████▊| 9599/9822 [4:56:41<06:15,  1.68s/it] 98%|█████████▊| 9600/9822 [4:56:43<06:13,  1.68s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.2071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.3062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:37:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:37:30 - INFO - __main__ - ***** test Results*****
04/29/2024 16:37:30 - INFO - __main__ -   Training step = 9600
04/29/2024 16:37:30 - INFO - __main__ -  test_accuracy:0.8751830161054173 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:37:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:37:34 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:37:34 - INFO - __main__ -   Training step = 9600
04/29/2024 16:37:34 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:37:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:37:43 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:37:43 - INFO - __main__ -   Training step = 9600
04/29/2024 16:37:43 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 98%|█████████▊| 9601/9822 [4:57:02<25:38,  6.96s/it] 98%|█████████▊| 9602/9822 [4:57:04<19:43,  5.38s/it] 98%|█████████▊| 9603/9822 [4:57:06<15:34,  4.27s/it] 98%|█████████▊| 9604/9822 [4:57:07<12:41,  3.49s/it] 98%|█████████▊| 9605/9822 [4:57:09<10:39,  2.95s/it] 98%|█████████▊| 9606/9822 [4:57:11<09:14,  2.57s/it] 98%|█████████▊| 9607/9822 [4:57:12<08:14,  2.30s/it] 98%|█████████▊| 9608/9822 [4:57:14<07:32,  2.11s/it] 98%|█████████▊| 9609/9822 [4:57:16<07:02,  1.98s/it] 98%|█████████▊| 9610/9822 [4:57:18<06:41,  1.89s/it] 98%|█████████▊| 9611/9822 [4:57:19<06:26,  1.83s/it] 98%|█████████▊| 9612/9822 [4:57:21<06:15,  1.79s/it] 98%|█████████▊| 9613/9822 [4:57:23<06:07,  1.76s/it] 98%|█████████▊| 9614/9822 [4:57:24<06:01,  1.74s/it] 98%|█████████▊| 9615/9822 [4:57:26<06:02,  1.75s/it] 98%|█████████▊| 9616/9822 [4:57:28<05:57,  1.73s/it] 98%|█████████▊| 9617/9822 [4:57:29<05:52,  1.72s/it] 98%|█████████▊| 9618/9822 [4:57:31<05:48,  1.71s/it] 98%|█████████▊| 9619/9822 [4:57:33<05:44,  1.70s/it] 98%|█████████▊| 9620/9822 [4:57:34<05:42,  1.69s/it] 98%|█████████▊| 9621/9822 [4:57:36<05:39,  1.69s/it] 98%|█████████▊| 9622/9822 [4:57:38<05:36,  1.68s/it] 98%|█████████▊| 9623/9822 [4:57:39<05:34,  1.68s/it] 98%|█████████▊| 9624/9822 [4:57:41<05:33,  1.68s/it] 98%|█████████▊| 9625/9822 [4:57:43<05:31,  1.68s/it] 98%|█████████▊| 9626/9822 [4:57:45<05:29,  1.68s/it] 98%|█████████▊| 9627/9822 [4:57:46<05:27,  1.68s/it] 98%|█████████▊| 9628/9822 [4:57:48<05:24,  1.67s/it] 98%|█████████▊| 9629/9822 [4:57:50<05:23,  1.68s/it] 98%|█████████▊| 9630/9822 [4:57:51<05:21,  1.67s/it] 98%|█████████▊| 9631/9822 [4:57:53<05:19,  1.67s/it] 98%|█████████▊| 9632/9822 [4:57:55<05:15,  1.66s/it] 98%|█████████▊| 9633/9822 [4:57:56<05:15,  1.67s/it] 98%|█████████▊| 9634/9822 [4:57:58<05:14,  1.67s/it] 98%|█████████▊| 9635/9822 [4:58:00<05:13,  1.68s/it] 98%|█████████▊| 9636/9822 [4:58:01<05:12,  1.68s/it] 98%|█████████▊| 9637/9822 [4:58:03<05:11,  1.68s/it] 98%|█████████▊| 9638/9822 [4:58:05<05:10,  1.69s/it] 98%|█████████▊| 9639/9822 [4:58:06<05:08,  1.69s/it] 98%|█████████▊| 9640/9822 [4:58:08<05:06,  1.68s/it] 98%|█████████▊| 9641/9822 [4:58:10<05:04,  1.68s/it] 98%|█████████▊| 9642/9822 [4:58:11<05:03,  1.69s/it] 98%|█████████▊| 9643/9822 [4:58:13<05:01,  1.69s/it] 98%|█████████▊| 9644/9822 [4:58:15<05:00,  1.69s/it] 98%|█████████▊| 9645/9822 [4:58:16<04:58,  1.69s/it] 98%|█████████▊| 9646/9822 [4:58:18<04:56,  1.68s/it] 98%|█████████▊| 9647/9822 [4:58:20<04:54,  1.68s/it] 98%|█████████▊| 9648/9822 [4:58:22<04:52,  1.68s/it] 98%|█████████▊| 9649/9822 [4:58:23<04:51,  1.68s/it] 98%|█████████▊| 9650/9822 [4:58:25<04:49,  1.68s/it] 98%|█████████▊| 9651/9822 [4:58:27<04:47,  1.68s/it] 98%|█████████▊| 9652/9822 [4:58:28<04:46,  1.68s/it] 98%|█████████▊| 9653/9822 [4:58:30<04:44,  1.68s/it] 98%|█████████▊| 9654/9822 [4:58:32<04:42,  1.68s/it] 98%|█████████▊| 9655/9822 [4:58:33<04:40,  1.68s/it] 98%|█████████▊| 9656/9822 [4:58:35<04:44,  1.71s/it] 98%|█████████▊| 9657/9822 [4:58:37<04:41,  1.71s/it] 98%|█████████▊| 9658/9822 [4:58:38<04:38,  1.70s/it] 98%|█████████▊| 9659/9822 [4:58:40<04:36,  1.70s/it] 98%|█████████▊| 9660/9822 [4:58:42<04:34,  1.69s/it] 98%|█████████▊| 9661/9822 [4:58:43<04:32,  1.69s/it] 98%|█████████▊| 9662/9822 [4:58:45<04:30,  1.69s/it] 98%|█████████▊| 9663/9822 [4:58:47<04:28,  1.69s/it] 98%|█████████▊| 9664/9822 [4:58:49<04:26,  1.68s/it] 98%|█████████▊| 9665/9822 [4:58:50<04:24,  1.69s/it] 98%|█████████▊| 9666/9822 [4:58:52<04:23,  1.69s/it] 98%|█████████▊| 9667/9822 [4:58:54<04:21,  1.68s/it] 98%|█████████▊| 9668/9822 [4:58:55<04:18,  1.68s/it] 98%|█████████▊| 9669/9822 [4:58:57<04:16,  1.67s/it] 98%|█████████▊| 9670/9822 [4:58:59<04:14,  1.67s/it] 98%|█████████▊| 9671/9822 [4:59:00<04:13,  1.68s/it] 98%|█████████▊| 9672/9822 [4:59:02<04:11,  1.68s/it] 98%|█████████▊| 9673/9822 [4:59:04<04:09,  1.68s/it] 98%|█████████▊| 9674/9822 [4:59:05<04:08,  1.68s/it] 99%|█████████▊| 9675/9822 [4:59:07<04:06,  1.68s/it] 99%|█████████▊| 9676/9822 [4:59:09<04:04,  1.67s/it] 99%|█████████▊| 9677/9822 [4:59:10<04:02,  1.67s/it] 99%|█████████▊| 9678/9822 [4:59:12<04:01,  1.68s/it] 99%|█████████▊| 9679/9822 [4:59:14<03:59,  1.67s/it] 99%|█████████▊| 9680/9822 [4:59:15<03:57,  1.68s/it] 99%|█████████▊| 9681/9822 [4:59:17<03:56,  1.68s/it] 99%|█████████▊| 9682/9822 [4:59:19<03:54,  1.67s/it] 99%|█████████▊| 9683/9822 [4:59:21<03:57,  1.71s/it] 99%|█████████▊| 9684/9822 [4:59:22<03:54,  1.70s/it] 99%|█████████▊| 9685/9822 [4:59:24<03:51,  1.69s/it] 99%|█████████▊| 9686/9822 [4:59:26<03:49,  1.68s/it] 99%|█████████▊| 9687/9822 [4:59:27<03:46,  1.68s/it] 99%|█████████▊| 9688/9822 [4:59:29<03:44,  1.68s/it] 99%|█████████▊| 9689/9822 [4:59:31<03:43,  1.68s/it] 99%|█████████▊| 9690/9822 [4:59:32<03:41,  1.68s/it] 99%|█████████▊| 9691/9822 [4:59:34<03:40,  1.68s/it] 99%|█████████▊| 9692/9822 [4:59:36<03:38,  1.68s/it] 99%|█████████▊| 9693/9822 [4:59:37<03:36,  1.68s/it] 99%|█████████▊| 9694/9822 [4:59:39<03:34,  1.68s/it] 99%|█████████▊| 9695/9822 [4:59:41<03:33,  1.68s/it] 99%|█████████▊| 9696/9822 [4:59:42<03:30,  1.67s/it] 99%|█████████▊| 9697/9822 [4:59:44<03:29,  1.67s/it] 99%|█████████▊| 9698/9822 [4:59:46<03:27,  1.67s/it] 99%|█████████▊| 9699/9822 [4:59:47<03:25,  1.67s/it] 99%|█████████▉| 9700/9822 [4:59:49<03:23,  1.67s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:40:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:40:36 - INFO - __main__ - ***** test Results*****
04/29/2024 16:40:36 - INFO - __main__ -   Training step = 9700
04/29/2024 16:40:36 - INFO - __main__ -  test_accuracy:0.8751830161054173 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:40:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:40:40 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:40:40 - INFO - __main__ -   Training step = 9700
04/29/2024 16:40:40 - INFO - __main__ -  eval_accuracy:0.8517026730135482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:40:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:40:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:40:49 - INFO - __main__ -   Training step = 9700
04/29/2024 16:40:49 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 99%|█████████▉| 9701/9822 [5:00:08<14:00,  6.95s/it] 99%|█████████▉| 9702/9822 [5:00:10<10:44,  5.37s/it] 99%|█████████▉| 9703/9822 [5:00:12<08:26,  4.26s/it] 99%|█████████▉| 9704/9822 [5:00:13<06:50,  3.48s/it] 99%|█████████▉| 9705/9822 [5:00:15<05:43,  2.94s/it] 99%|█████████▉| 9706/9822 [5:00:17<04:56,  2.56s/it] 99%|█████████▉| 9707/9822 [5:00:18<04:23,  2.29s/it] 99%|█████████▉| 9708/9822 [5:00:20<04:00,  2.11s/it] 99%|█████████▉| 9709/9822 [5:00:22<03:47,  2.01s/it] 99%|█████████▉| 9710/9822 [5:00:23<03:34,  1.91s/it] 99%|█████████▉| 9711/9822 [5:00:25<03:23,  1.84s/it] 99%|█████████▉| 9712/9822 [5:00:27<03:16,  1.78s/it] 99%|█████████▉| 9713/9822 [5:00:28<03:10,  1.75s/it] 99%|█████████▉| 9714/9822 [5:00:30<03:06,  1.73s/it] 99%|█████████▉| 9715/9822 [5:00:32<03:03,  1.71s/it] 99%|█████████▉| 9716/9822 [5:00:33<03:00,  1.70s/it] 99%|█████████▉| 9717/9822 [5:00:35<02:58,  1.70s/it] 99%|█████████▉| 9718/9822 [5:00:37<02:54,  1.68s/it] 99%|█████████▉| 9719/9822 [5:00:38<02:53,  1.68s/it] 99%|█████████▉| 9720/9822 [5:00:40<02:52,  1.69s/it] 99%|█████████▉| 9721/9822 [5:00:42<02:50,  1.69s/it] 99%|█████████▉| 9722/9822 [5:00:43<02:48,  1.68s/it] 99%|█████████▉| 9723/9822 [5:00:45<02:46,  1.68s/it] 99%|█████████▉| 9724/9822 [5:00:47<02:45,  1.68s/it] 99%|█████████▉| 9725/9822 [5:00:49<02:43,  1.69s/it] 99%|█████████▉| 9726/9822 [5:00:50<02:41,  1.68s/it] 99%|█████████▉| 9727/9822 [5:00:52<02:39,  1.68s/it] 99%|█████████▉| 9728/9822 [5:00:54<02:37,  1.68s/it] 99%|█████████▉| 9729/9822 [5:00:55<02:36,  1.68s/it] 99%|█████████▉| 9730/9822 [5:00:57<02:35,  1.69s/it] 99%|█████████▉| 9731/9822 [5:00:59<02:33,  1.69s/it] 99%|█████████▉| 9732/9822 [5:01:00<02:31,  1.69s/it] 99%|█████████▉| 9733/9822 [5:01:02<02:30,  1.69s/it] 99%|█████████▉| 9734/9822 [5:01:04<02:28,  1.69s/it] 99%|█████████▉| 9735/9822 [5:01:05<02:26,  1.69s/it] 99%|█████████▉| 9736/9822 [5:01:07<02:27,  1.72s/it] 99%|█████████▉| 9737/9822 [5:01:09<02:24,  1.71s/it] 99%|█████████▉| 9738/9822 [5:01:11<02:22,  1.70s/it] 99%|█████████▉| 9739/9822 [5:01:12<02:20,  1.70s/it] 99%|█████████▉| 9740/9822 [5:01:14<02:18,  1.69s/it] 99%|█████████▉| 9741/9822 [5:01:16<02:16,  1.68s/it] 99%|█████████▉| 9742/9822 [5:01:17<02:14,  1.68s/it] 99%|█████████▉| 9743/9822 [5:01:19<02:13,  1.68s/it] 99%|█████████▉| 9744/9822 [5:01:21<02:11,  1.68s/it] 99%|█████████▉| 9745/9822 [5:01:22<02:09,  1.68s/it] 99%|█████████▉| 9746/9822 [5:01:24<02:07,  1.68s/it] 99%|█████████▉| 9747/9822 [5:01:26<02:06,  1.68s/it] 99%|█████████▉| 9748/9822 [5:01:27<02:04,  1.68s/it] 99%|█████████▉| 9749/9822 [5:01:29<02:02,  1.68s/it] 99%|█████████▉| 9750/9822 [5:01:31<02:00,  1.68s/it] 99%|█████████▉| 9751/9822 [5:01:32<01:59,  1.68s/it] 99%|█████████▉| 9752/9822 [5:01:34<01:58,  1.70s/it] 99%|█████████▉| 9753/9822 [5:01:36<01:57,  1.70s/it] 99%|█████████▉| 9754/9822 [5:01:38<01:55,  1.69s/it] 99%|█████████▉| 9755/9822 [5:01:39<01:54,  1.70s/it] 99%|█████████▉| 9756/9822 [5:01:41<01:52,  1.70s/it] 99%|█████████▉| 9757/9822 [5:01:43<01:50,  1.70s/it] 99%|█████████▉| 9758/9822 [5:01:44<01:48,  1.69s/it] 99%|█████████▉| 9759/9822 [5:01:46<01:46,  1.69s/it] 99%|█████████▉| 9760/9822 [5:01:48<01:44,  1.69s/it] 99%|█████████▉| 9761/9822 [5:01:49<01:42,  1.68s/it] 99%|█████████▉| 9762/9822 [5:01:51<01:41,  1.68s/it] 99%|█████████▉| 9763/9822 [5:01:53<01:39,  1.68s/it] 99%|█████████▉| 9764/9822 [5:01:54<01:37,  1.69s/it] 99%|█████████▉| 9765/9822 [5:01:56<01:36,  1.69s/it] 99%|█████████▉| 9766/9822 [5:01:58<01:34,  1.69s/it] 99%|█████████▉| 9767/9822 [5:01:59<01:32,  1.69s/it] 99%|█████████▉| 9768/9822 [5:02:01<01:31,  1.69s/it] 99%|█████████▉| 9769/9822 [5:02:03<01:31,  1.72s/it] 99%|█████████▉| 9770/9822 [5:02:05<01:28,  1.71s/it] 99%|█████████▉| 9771/9822 [5:02:06<01:26,  1.70s/it] 99%|█████████▉| 9772/9822 [5:02:08<01:24,  1.70s/it]100%|█████████▉| 9773/9822 [5:02:10<01:22,  1.69s/it]100%|█████████▉| 9774/9822 [5:02:11<01:21,  1.69s/it]100%|█████████▉| 9775/9822 [5:02:13<01:19,  1.69s/it]100%|█████████▉| 9776/9822 [5:02:15<01:17,  1.69s/it]100%|█████████▉| 9777/9822 [5:02:16<01:15,  1.69s/it]100%|█████████▉| 9778/9822 [5:02:18<01:14,  1.69s/it]100%|█████████▉| 9779/9822 [5:02:20<01:12,  1.68s/it]100%|█████████▉| 9780/9822 [5:02:21<01:10,  1.69s/it]100%|█████████▉| 9781/9822 [5:02:23<01:09,  1.69s/it]100%|█████████▉| 9782/9822 [5:02:25<01:07,  1.69s/it]100%|█████████▉| 9783/9822 [5:02:27<01:05,  1.68s/it]100%|█████████▉| 9784/9822 [5:02:28<01:04,  1.69s/it]100%|█████████▉| 9785/9822 [5:02:30<01:02,  1.68s/it]100%|█████████▉| 9786/9822 [5:02:32<01:00,  1.69s/it]100%|█████████▉| 9787/9822 [5:02:33<00:58,  1.69s/it]100%|█████████▉| 9788/9822 [5:02:35<00:57,  1.69s/it]100%|█████████▉| 9789/9822 [5:02:37<00:55,  1.69s/it]100%|█████████▉| 9790/9822 [5:02:38<00:53,  1.69s/it]100%|█████████▉| 9791/9822 [5:02:40<00:53,  1.72s/it]100%|█████████▉| 9792/9822 [5:02:42<00:51,  1.71s/it]100%|█████████▉| 9793/9822 [5:02:43<00:49,  1.70s/it]100%|█████████▉| 9794/9822 [5:02:45<00:47,  1.70s/it]100%|█████████▉| 9795/9822 [5:02:47<00:45,  1.69s/it]100%|█████████▉| 9796/9822 [5:02:49<00:43,  1.69s/it]100%|█████████▉| 9797/9822 [5:02:50<00:42,  1.69s/it]100%|█████████▉| 9798/9822 [5:02:52<00:40,  1.69s/it]100%|█████████▉| 9799/9822 [5:02:54<00:38,  1.69s/it]100%|█████████▉| 9800/9822 [5:02:55<00:37,  1.69s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1933, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:43:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:43:42 - INFO - __main__ - ***** test Results*****
04/29/2024 16:43:42 - INFO - __main__ -   Training step = 9800
04/29/2024 16:43:42 - INFO - __main__ -  test_accuracy:0.8748169838945827 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:43:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:43:47 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:43:47 - INFO - __main__ -   Training step = 9800
04/29/2024 16:43:47 - INFO - __main__ -  eval_accuracy:0.8509703405346027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:43:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:43:55 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:43:55 - INFO - __main__ -   Training step = 9800
04/29/2024 16:43:55 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
100%|█████████▉| 9801/9822 [5:03:15<02:26,  6.96s/it]100%|█████████▉| 9802/9822 [5:03:16<01:47,  5.37s/it]100%|█████████▉| 9803/9822 [5:03:18<01:20,  4.26s/it]100%|█████████▉| 9804/9822 [5:03:20<01:02,  3.47s/it]100%|█████████▉| 9805/9822 [5:03:21<00:49,  2.93s/it]100%|█████████▉| 9806/9822 [5:03:23<00:40,  2.56s/it]100%|█████████▉| 9807/9822 [5:03:25<00:34,  2.30s/it]100%|█████████▉| 9808/9822 [5:03:26<00:29,  2.11s/it]100%|█████████▉| 9809/9822 [5:03:28<00:25,  1.99s/it]100%|█████████▉| 9810/9822 [5:03:30<00:22,  1.90s/it]100%|█████████▉| 9811/9822 [5:03:31<00:20,  1.83s/it]100%|█████████▉| 9812/9822 [5:03:33<00:17,  1.79s/it]100%|█████████▉| 9813/9822 [5:03:35<00:15,  1.76s/it]100%|█████████▉| 9814/9822 [5:03:36<00:13,  1.73s/it]100%|█████████▉| 9815/9822 [5:03:38<00:12,  1.75s/it]100%|█████████▉| 9816/9822 [5:03:40<00:10,  1.73s/it]100%|█████████▉| 9817/9822 [5:03:42<00:08,  1.72s/it]100%|█████████▉| 9818/9822 [5:03:43<00:06,  1.71s/it]100%|█████████▉| 9819/9822 [5:03:45<00:05,  1.70s/it]100%|█████████▉| 9820/9822 [5:03:47<00:03,  1.69s/it]100%|█████████▉| 9821/9822 [5:03:48<00:01,  1.69s/it]100%|██████████| 9822/9822 [5:03:49<00:00,  1.53s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>)
04/29/2024 16:44:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:44:36 - INFO - __main__ - ***** test Results*****
04/29/2024 16:44:36 - INFO - __main__ -   Training step = 9822
04/29/2024 16:44:36 - INFO - __main__ -  test_accuracy:0.8748169838945827 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:44:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:44:41 - INFO - __main__ - ***** Evaluation Results*****
04/29/2024 16:44:41 - INFO - __main__ -   Training step = 9822
04/29/2024 16:44:41 - INFO - __main__ -  eval_accuracy:0.8509703405346027 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.85463200292933}
test:
{'accuracy': 0.8726207906295754}
04/29/2024 16:44:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
04/29/2024 16:44:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
04/29/2024 16:44:49 - INFO - __main__ -   Training step = 9822
04/29/2024 16:44:49 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
100%|██████████| 9822/9822 [5:04:07<00:00,  1.86s/it]