01/07/2024 21:39:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
01/07/2024 21:39:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=100,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/runs/Jan07_21-39-45_ubuntu-gd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=32,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
Overwrite dataset info from restored data version if exists.
01/07/2024 21:40:35 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834
01/07/2024 21:40:35 - INFO - datasets.info - Loading Dataset info from /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834
Found cached dataset glue (/mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834)
01/07/2024 21:40:35 - INFO - datasets.builder - Found cached dataset glue (/mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834)
Loading Dataset info from /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834
01/07/2024 21:40:35 - INFO - datasets.info - Loading Dataset info from /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834
[INFO|configuration_utils.py:646] 2024-01-07 21:40:35,769 >> loading configuration file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/config.json
[INFO|configuration_utils.py:684] 2024-01-07 21:40:35,770 >> Model config BertConfig {
  "_name_or_path": "/mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/",
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "qnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1703] 2024-01-07 21:40:35,771 >> Didn't find file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1784] 2024-01-07 21:40:35,771 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/vocab.txt
[INFO|tokenization_utils_base.py:1784] 2024-01-07 21:40:35,771 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/tokenizer.json
[INFO|tokenization_utils_base.py:1784] 2024-01-07 21:40:35,771 >> loading file None
[INFO|tokenization_utils_base.py:1784] 2024-01-07 21:40:35,771 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/special_tokens_map.json
[INFO|tokenization_utils_base.py:1784] 2024-01-07 21:40:35,771 >> loading file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/tokenizer_config.json
[INFO|modeling_utils.py:1429] 2024-01-07 21:40:35,795 >> loading weights file /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/pytorch_model.bin
[WARNING|modeling_utils.py:1693] 2024-01-07 21:40:36,396 >> Some weights of the model checkpoint at /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/ were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2024-01-07 21:40:36,396 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /mnt/zhanyuliang/data/nlp_data/theseus/BertFineTrain/download/qnli/bert-base-uncased-qnli/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.
[INFO|configuration_utils.py:648] 2024-01-07 21:40:36,798 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/zhanyuliang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:684] 2024-01-07 21:40:36,799 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "qnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2024-01-07 21:40:37,196 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/zhanyuliang/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1693] 2024-01-07 21:40:38,157 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2024-01-07 21:40:38,158 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-d8176ac8a38cde53.arrow
01/07/2024 21:40:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-d8176ac8a38cde53.arrow
Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-6496a66a36781013.arrow
01/07/2024 21:40:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-6496a66a36781013.arrow
Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-379b1c2e8d568bb6.arrow
01/07/2024 21:40:38 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /mnt/zhanyuliang/cache/huggingface/datasets/downloads/glue/qnli/0.0.0/9b3dd4eac28be834/cache-379b1c2e8d568bb6.arrow
01/07/2024 21:40:38 - INFO - __main__ - The following columns  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, question, sentence.
01/07/2024 21:40:38 - INFO - __main__ - Sample 83810 of the training set: {'label': 0, 'input_ids': [101, 2054, 9887, 1997, 1996, 26189, 2050, 2020, 2149, 1998, 20996, 2243, 3629, 2025, 4810, 2000, 5047, 1029, 102, 2006, 2676, 2281, 2012, 1996, 4759, 2789, 2392, 1010, 1037, 1057, 1012, 1055, 1012, 5504, 3939, 2407, 17604, 4337, 2136, 1006, 1017, 1010, 2199, 3548, 1007, 1998, 1996, 1057, 1012, 1055, 1012, 3083, 3884, 2407, 1006, 2260, 1010, 2199, 1516, 2321, 1010, 2199, 9622, 1007, 2020, 4895, 28139, 19362, 2098, 2005, 1996, 26189, 2050, 6280, 2390, 2177, 1005, 1055, 2093, 1011, 4013, 21558, 4372, 6895, 21769, 3672, 9887, 2012, 1996, 2645, 1997, 16480, 11493, 8071, 1010, 2021, 2027, 3266, 2000, 4019, 2104, 2250, 2486, 1998, 1060, 3650, 2490, 2543, 1517, 12167, 2007, 2070, 2321, 1010, 2199, 7268, 8664, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
01/07/2024 21:40:38 - INFO - __main__ - Sample 14592 of the training set: {'label': 1, 'input_ids': [101, 2129, 2106, 1996, 6811, 2111, 2514, 2055, 26099, 1005, 1055, 3433, 2000, 1996, 2886, 1029, 102, 26099, 2106, 2025, 2514, 2008, 1996, 6811, 2390, 2001, 3201, 2005, 1037, 13111, 1998, 2106, 2025, 2128, 9080, 13143, 23689, 6590, 11272, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
01/07/2024 21:40:38 - INFO - __main__ - Sample 3278 of the training set: {'label': 0, 'input_ids': [101, 2054, 2785, 1997, 2948, 2024, 5214, 1997, 2635, 2125, 20018, 1029, 102, 2348, 2358, 4492, 2140, 2948, 2024, 5214, 1997, 2635, 2125, 20018, 2013, 1037, 3962, 2006, 1996, 5877, 1010, 2478, 1996, 13276, 1998, 1037, 2770, 2707, 2003, 2521, 2062, 4762, 8114, 1998, 14245, 1037, 11907, 4888, 3635, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
#########################################################################################################
分解前参数量：66956546
#########################################################################################################
bert.encoder.layer.0.attention.self.query.weight
bert.encoder.layer.0.attention.self.key.weight
bert.encoder.layer.0.attention.self.value.weight
bert.encoder.layer.0.attention.output.dense.weight
bert.encoder.layer.0.intermediate.dense.weight
bert.encoder.layer.0.output.dense.weight
bert.encoder.layer.1.attention.self.query.weight
bert.encoder.layer.1.attention.self.key.weight
bert.encoder.layer.1.attention.self.value.weight
bert.encoder.layer.1.attention.output.dense.weight
bert.encoder.layer.1.intermediate.dense.weight
bert.encoder.layer.1.output.dense.weight
bert.encoder.layer.2.attention.self.query.weight
bert.encoder.layer.2.attention.self.key.weight
bert.encoder.layer.2.attention.self.value.weight
bert.encoder.layer.2.attention.output.dense.weight
bert.encoder.layer.2.intermediate.dense.weight
bert.encoder.layer.2.output.dense.weight
bert.encoder.layer.3.attention.self.query.weight
bert.encoder.layer.3.attention.self.key.weight
bert.encoder.layer.3.attention.self.value.weight
bert.encoder.layer.3.attention.output.dense.weight
bert.encoder.layer.3.intermediate.dense.weight
bert.encoder.layer.3.output.dense.weight
bert.encoder.layer.4.attention.self.query.weight
bert.encoder.layer.4.attention.self.key.weight
bert.encoder.layer.4.attention.self.value.weight
bert.encoder.layer.4.attention.output.dense.weight
bert.encoder.layer.4.intermediate.dense.weight
bert.encoder.layer.4.output.dense.weight
bert.encoder.layer.5.attention.self.query.weight
bert.encoder.layer.5.attention.self.key.weight
bert.encoder.layer.5.attention.self.value.weight
bert.encoder.layer.5.attention.output.dense.weight
bert.encoder.layer.5.intermediate.dense.weight
bert.encoder.layer.5.output.dense.weight
#########################################################################################################
分解后参数量：281947394
#########################################################################################################
01/07/2024 21:47:40 - INFO - __main__ - ***** Running training *****
01/07/2024 21:47:40 - INFO - __main__ -   Num examples = 104743
01/07/2024 21:47:40 - INFO - __main__ -   Num Epochs = 3
01/07/2024 21:47:40 - INFO - __main__ -   Instantaneous batch size per device = 32
01/07/2024 21:47:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
01/07/2024 21:47:40 - INFO - __main__ -   Gradient Accumulation steps = 1
01/07/2024 21:47:40 - INFO - __main__ -   Total optimization steps = 9822
run_glue_mpo_laterloss.py:658: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("glue", data_args.task_name)
/home/zhanyuliang/miniconda3/envs/lgtm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/9822 [00:00<?, ?it/s]/home/zhanyuliang/Project/DistillingMPO/OPF/LGTM/utils_glue.py:132: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/9822 [00:01<5:19:13,  1.95s/it]  0%|          | 2/9822 [00:03<4:24:14,  1.61s/it]  0%|          | 3/9822 [00:04<4:07:53,  1.51s/it]  0%|          | 4/9822 [00:06<3:59:43,  1.47s/it]  0%|          | 5/9822 [00:07<3:54:41,  1.43s/it]  0%|          | 6/9822 [00:08<3:51:48,  1.42s/it]  0%|          | 7/9822 [00:10<3:50:24,  1.41s/it]  0%|          | 8/9822 [00:11<3:49:12,  1.40s/it]  0%|          | 9/9822 [00:13<3:48:10,  1.40s/it]  0%|          | 10/9822 [00:14<3:47:37,  1.39s/it]  0%|          | 11/9822 [00:15<3:48:05,  1.39s/it]  0%|          | 12/9822 [00:17<3:47:38,  1.39s/it]  0%|          | 13/9822 [00:18<3:47:24,  1.39s/it]  0%|          | 14/9822 [00:19<3:47:32,  1.39s/it]  0%|          | 15/9822 [00:21<3:48:03,  1.40s/it]  0%|          | 16/9822 [00:22<3:48:08,  1.40s/it]  0%|          | 17/9822 [00:24<3:52:16,  1.42s/it]  0%|          | 18/9822 [00:25<3:51:28,  1.42s/it]  0%|          | 19/9822 [00:27<3:50:40,  1.41s/it]  0%|          | 20/9822 [00:28<3:50:43,  1.41s/it]  0%|          | 21/9822 [00:29<3:50:09,  1.41s/it]  0%|          | 22/9822 [00:31<3:49:56,  1.41s/it]  0%|          | 23/9822 [00:32<3:50:11,  1.41s/it]  0%|          | 24/9822 [00:34<3:52:00,  1.42s/it]  0%|          | 25/9822 [00:35<3:52:04,  1.42s/it]  0%|          | 26/9822 [00:36<3:51:15,  1.42s/it]  0%|          | 27/9822 [00:38<3:50:45,  1.41s/it]  0%|          | 28/9822 [00:39<3:52:14,  1.42s/it]  0%|          | 29/9822 [00:41<3:51:46,  1.42s/it]  0%|          | 30/9822 [00:42<3:51:56,  1.42s/it]  0%|          | 31/9822 [00:44<3:52:06,  1.42s/it]  0%|          | 32/9822 [00:45<3:52:23,  1.42s/it]  0%|          | 33/9822 [00:46<3:52:05,  1.42s/it]  0%|          | 34/9822 [00:48<3:51:48,  1.42s/it]  0%|          | 35/9822 [00:49<3:51:38,  1.42s/it]  0%|          | 36/9822 [00:51<3:51:40,  1.42s/it]  0%|          | 37/9822 [00:52<3:52:23,  1.42s/it]  0%|          | 38/9822 [00:54<3:52:36,  1.43s/it]  0%|          | 39/9822 [00:55<3:52:44,  1.43s/it]  0%|          | 40/9822 [00:56<3:54:35,  1.44s/it]  0%|          | 41/9822 [00:58<3:55:49,  1.45s/it]  0%|          | 42/9822 [00:59<3:55:07,  1.44s/it]  0%|          | 43/9822 [01:01<3:54:53,  1.44s/it]  0%|          | 44/9822 [01:02<3:53:17,  1.43s/it]  0%|          | 45/9822 [01:04<3:52:36,  1.43s/it]  0%|          | 46/9822 [01:05<3:53:45,  1.43s/it]  0%|          | 47/9822 [01:07<3:57:43,  1.46s/it]  0%|          | 48/9822 [01:08<3:55:55,  1.45s/it]  0%|          | 49/9822 [01:09<3:54:19,  1.44s/it]  1%|          | 50/9822 [01:11<3:53:10,  1.43s/it]  1%|          | 51/9822 [01:12<3:53:26,  1.43s/it]  1%|          | 52/9822 [01:14<3:52:43,  1.43s/it]  1%|          | 53/9822 [01:15<3:52:18,  1.43s/it]  1%|          | 54/9822 [01:17<3:52:04,  1.43s/it]  1%|          | 55/9822 [01:18<3:51:48,  1.42s/it]  1%|          | 56/9822 [01:19<3:53:10,  1.43s/it]  1%|          | 57/9822 [01:21<3:52:20,  1.43s/it]  1%|          | 58/9822 [01:22<3:51:44,  1.42s/it]  1%|          | 59/9822 [01:24<3:51:46,  1.42s/it]  1%|          | 60/9822 [01:25<3:52:38,  1.43s/it]  1%|          | 61/9822 [01:27<3:52:44,  1.43s/it]  1%|          | 62/9822 [01:28<3:52:38,  1.43s/it]  1%|          | 63/9822 [01:29<3:52:35,  1.43s/it]  1%|          | 64/9822 [01:31<3:52:20,  1.43s/it]  1%|          | 65/9822 [01:32<3:52:51,  1.43s/it]  1%|          | 66/9822 [01:34<3:52:39,  1.43s/it]  1%|          | 67/9822 [01:35<3:52:38,  1.43s/it]  1%|          | 68/9822 [01:37<3:52:59,  1.43s/it]  1%|          | 69/9822 [01:38<3:52:12,  1.43s/it]  1%|          | 70/9822 [01:39<3:52:20,  1.43s/it]  1%|          | 71/9822 [01:41<3:52:13,  1.43s/it]  1%|          | 72/9822 [01:42<3:52:10,  1.43s/it]  1%|          | 73/9822 [01:44<3:51:19,  1.42s/it]  1%|          | 74/9822 [01:45<3:51:06,  1.42s/it]  1%|          | 75/9822 [01:47<3:51:54,  1.43s/it]  1%|          | 76/9822 [01:48<3:51:45,  1.43s/it]  1%|          | 77/9822 [01:49<3:51:32,  1.43s/it]  1%|          | 78/9822 [01:51<3:51:53,  1.43s/it]  1%|          | 79/9822 [01:52<3:55:33,  1.45s/it]  1%|          | 80/9822 [01:54<3:54:39,  1.45s/it]  1%|          | 81/9822 [01:55<3:54:01,  1.44s/it]  1%|          | 82/9822 [01:57<3:53:11,  1.44s/it]  1%|          | 83/9822 [01:58<3:53:13,  1.44s/it]  1%|          | 84/9822 [01:59<3:52:23,  1.43s/it]  1%|          | 85/9822 [02:01<3:51:54,  1.43s/it]  1%|          | 86/9822 [02:02<3:48:50,  1.41s/it]  1%|          | 87/9822 [02:04<3:49:39,  1.42s/it]  1%|          | 88/9822 [02:05<3:50:03,  1.42s/it]  1%|          | 89/9822 [02:07<3:50:15,  1.42s/it]  1%|          | 90/9822 [02:08<3:51:19,  1.43s/it]  1%|          | 91/9822 [02:09<3:51:20,  1.43s/it]  1%|          | 92/9822 [02:11<3:51:25,  1.43s/it]  1%|          | 93/9822 [02:12<3:51:27,  1.43s/it]  1%|          | 94/9822 [02:14<3:51:51,  1.43s/it]  1%|          | 95/9822 [02:15<3:51:31,  1.43s/it]  1%|          | 96/9822 [02:17<3:51:43,  1.43s/it]  1%|          | 97/9822 [02:18<3:51:35,  1.43s/it]  1%|          | 98/9822 [02:19<3:51:29,  1.43s/it]  1%|          | 99/9822 [02:21<3:51:45,  1.43s/it]  1%|          | 100/9822 [02:22<3:52:27,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 21:50:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:50:09 - INFO - __main__ - ***** test Results*****
01/07/2024 21:50:09 - INFO - __main__ -   Training step = 100
01/07/2024 21:50:09 - INFO - __main__ -  test_accuracy:0.7814787701317716 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:50:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:50:15 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 21:50:15 - INFO - __main__ -   Training step = 100
01/07/2024 21:50:15 - INFO - __main__ -  eval_accuracy:0.7781032588795314 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 21:50:15,686 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 21:50:15,686 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 21:50:15,723 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 21:50:17,292 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.7781032588795314}
test:
{'accuracy': 0.7814787701317716}
01/07/2024 21:50:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:50:22 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 21:50:22 - INFO - __main__ -   Training step = 100
01/07/2024 21:50:22 - INFO - __main__ -  eval_accuracy:0.8136213841083852 
  1%|          | 101/9822 [02:42<18:50:06,  6.98s/it]  1%|          | 102/9822 [02:44<14:20:21,  5.31s/it]  1%|          | 103/9822 [02:45<11:11:04,  4.14s/it]  1%|          | 104/9822 [02:46<8:59:43,  3.33s/it]   1%|          | 105/9822 [02:48<7:27:25,  2.76s/it]  1%|          | 106/9822 [02:49<6:22:24,  2.36s/it]  1%|          | 107/9822 [02:51<5:40:50,  2.11s/it]  1%|          | 108/9822 [02:52<5:07:32,  1.90s/it]  1%|          | 109/9822 [02:54<4:44:05,  1.75s/it]  1%|          | 110/9822 [02:55<4:28:37,  1.66s/it]  1%|          | 111/9822 [02:57<4:17:20,  1.59s/it]  1%|          | 112/9822 [02:58<4:10:00,  1.54s/it]  1%|          | 113/9822 [02:59<4:03:55,  1.51s/it]  1%|          | 114/9822 [03:01<4:00:11,  1.48s/it]  1%|          | 115/9822 [03:02<3:57:38,  1.47s/it]  1%|          | 116/9822 [03:04<3:55:50,  1.46s/it]  1%|          | 117/9822 [03:05<3:54:04,  1.45s/it]  1%|          | 118/9822 [03:07<3:53:04,  1.44s/it]  1%|          | 119/9822 [03:08<3:52:30,  1.44s/it]  1%|          | 120/9822 [03:09<3:51:50,  1.43s/it]  1%|          | 121/9822 [03:11<3:51:25,  1.43s/it]  1%|          | 122/9822 [03:12<3:51:22,  1.43s/it]  1%|▏         | 123/9822 [03:14<3:50:48,  1.43s/it]  1%|▏         | 124/9822 [03:15<3:50:42,  1.43s/it]  1%|▏         | 125/9822 [03:17<3:50:39,  1.43s/it]  1%|▏         | 126/9822 [03:18<3:50:59,  1.43s/it]  1%|▏         | 127/9822 [03:19<3:51:05,  1.43s/it]  1%|▏         | 128/9822 [03:21<3:50:47,  1.43s/it]  1%|▏         | 129/9822 [03:22<3:50:56,  1.43s/it]  1%|▏         | 130/9822 [03:24<3:51:04,  1.43s/it]  1%|▏         | 131/9822 [03:25<3:51:02,  1.43s/it]  1%|▏         | 132/9822 [03:27<3:50:38,  1.43s/it]  1%|▏         | 133/9822 [03:28<3:50:25,  1.43s/it]  1%|▏         | 134/9822 [03:29<3:50:46,  1.43s/it]  1%|▏         | 135/9822 [03:31<3:50:49,  1.43s/it]  1%|▏         | 136/9822 [03:32<3:50:36,  1.43s/it]  1%|▏         | 137/9822 [03:34<3:51:32,  1.43s/it]  1%|▏         | 138/9822 [03:35<3:53:12,  1.44s/it]  1%|▏         | 139/9822 [03:37<3:56:15,  1.46s/it]  1%|▏         | 140/9822 [03:38<3:54:26,  1.45s/it]  1%|▏         | 141/9822 [03:40<3:55:06,  1.46s/it]  1%|▏         | 142/9822 [03:41<3:53:32,  1.45s/it]  1%|▏         | 143/9822 [03:42<3:52:38,  1.44s/it]  1%|▏         | 144/9822 [03:44<3:52:35,  1.44s/it]  1%|▏         | 145/9822 [03:45<3:51:50,  1.44s/it]  1%|▏         | 146/9822 [03:47<3:51:29,  1.44s/it]  1%|▏         | 147/9822 [03:48<3:51:32,  1.44s/it]  2%|▏         | 148/9822 [03:50<3:51:30,  1.44s/it]  2%|▏         | 149/9822 [03:51<3:50:50,  1.43s/it]  2%|▏         | 150/9822 [03:52<3:50:32,  1.43s/it]  2%|▏         | 151/9822 [03:54<3:51:10,  1.43s/it]  2%|▏         | 152/9822 [03:55<3:50:50,  1.43s/it]  2%|▏         | 153/9822 [03:57<3:50:44,  1.43s/it]  2%|▏         | 154/9822 [03:58<3:50:25,  1.43s/it]  2%|▏         | 155/9822 [04:00<3:50:24,  1.43s/it]  2%|▏         | 156/9822 [04:01<3:50:30,  1.43s/it]  2%|▏         | 157/9822 [04:03<3:51:13,  1.44s/it]  2%|▏         | 158/9822 [04:04<3:51:03,  1.43s/it]  2%|▏         | 159/9822 [04:05<3:51:06,  1.44s/it]  2%|▏         | 160/9822 [04:07<3:50:38,  1.43s/it]  2%|▏         | 161/9822 [04:08<3:51:11,  1.44s/it]  2%|▏         | 162/9822 [04:10<3:50:46,  1.43s/it]  2%|▏         | 163/9822 [04:11<3:50:16,  1.43s/it]  2%|▏         | 164/9822 [04:13<3:50:19,  1.43s/it]  2%|▏         | 165/9822 [04:14<3:50:54,  1.43s/it]  2%|▏         | 166/9822 [04:15<3:51:02,  1.44s/it]  2%|▏         | 167/9822 [04:17<3:51:00,  1.44s/it]  2%|▏         | 168/9822 [04:18<3:50:57,  1.44s/it]  2%|▏         | 169/9822 [04:20<3:54:42,  1.46s/it]  2%|▏         | 170/9822 [04:21<3:53:41,  1.45s/it]  2%|▏         | 171/9822 [04:23<3:52:26,  1.45s/it]  2%|▏         | 172/9822 [04:24<3:49:24,  1.43s/it]  2%|▏         | 173/9822 [04:25<3:49:40,  1.43s/it]  2%|▏         | 174/9822 [04:27<3:49:23,  1.43s/it]  2%|▏         | 175/9822 [04:28<3:49:43,  1.43s/it]  2%|▏         | 176/9822 [04:30<3:49:43,  1.43s/it]  2%|▏         | 177/9822 [04:31<3:51:25,  1.44s/it]  2%|▏         | 178/9822 [04:33<3:50:59,  1.44s/it]  2%|▏         | 179/9822 [04:34<3:50:57,  1.44s/it]  2%|▏         | 180/9822 [04:36<3:50:26,  1.43s/it]  2%|▏         | 181/9822 [04:37<3:51:24,  1.44s/it]  2%|▏         | 182/9822 [04:38<3:51:22,  1.44s/it]  2%|▏         | 183/9822 [04:40<3:50:35,  1.44s/it]  2%|▏         | 184/9822 [04:41<3:50:54,  1.44s/it]  2%|▏         | 185/9822 [04:43<3:50:56,  1.44s/it]  2%|▏         | 186/9822 [04:44<3:50:09,  1.43s/it]  2%|▏         | 187/9822 [04:46<3:49:39,  1.43s/it]  2%|▏         | 188/9822 [04:47<3:49:13,  1.43s/it]  2%|▏         | 189/9822 [04:48<3:50:13,  1.43s/it]  2%|▏         | 190/9822 [04:50<3:49:33,  1.43s/it]  2%|▏         | 191/9822 [04:51<3:49:16,  1.43s/it]  2%|▏         | 192/9822 [04:53<3:48:51,  1.43s/it]  2%|▏         | 193/9822 [04:54<3:48:20,  1.42s/it]  2%|▏         | 194/9822 [04:56<3:49:22,  1.43s/it]  2%|▏         | 195/9822 [04:57<3:49:59,  1.43s/it]  2%|▏         | 196/9822 [04:58<3:49:32,  1.43s/it]  2%|▏         | 197/9822 [05:00<3:49:21,  1.43s/it]  2%|▏         | 198/9822 [05:01<3:50:03,  1.43s/it]  2%|▏         | 199/9822 [05:03<3:49:15,  1.43s/it]  2%|▏         | 200/9822 [05:04<3:49:15,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 21:52:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:52:51 - INFO - __main__ - ***** test Results*****
01/07/2024 21:52:51 - INFO - __main__ -   Training step = 200
01/07/2024 21:52:51 - INFO - __main__ -  test_accuracy:0.7445095168374817 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:52:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:52:57 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 21:52:57 - INFO - __main__ -   Training step = 200
01/07/2024 21:52:57 - INFO - __main__ -  eval_accuracy:0.7272061515928231 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.7781032588795314}
test:
{'accuracy': 0.7814787701317716}
01/07/2024 21:53:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:53:02 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 21:53:02 - INFO - __main__ -   Training step = 200
01/07/2024 21:53:02 - INFO - __main__ -  eval_accuracy:0.7854265836689858 
  2%|▏         | 201/9822 [05:23<17:26:41,  6.53s/it]  2%|▏         | 202/9822 [05:24<13:21:35,  5.00s/it]  2%|▏         | 203/9822 [05:25<10:31:12,  3.94s/it]  2%|▏         | 204/9822 [05:27<8:30:48,  3.19s/it]   2%|▏         | 205/9822 [05:28<7:06:00,  2.66s/it]  2%|▏         | 206/9822 [05:30<6:06:23,  2.29s/it]  2%|▏         | 207/9822 [05:31<5:25:01,  2.03s/it]  2%|▏         | 208/9822 [05:33<4:55:57,  1.85s/it]  2%|▏         | 209/9822 [05:34<4:35:45,  1.72s/it]  2%|▏         | 210/9822 [05:35<4:22:14,  1.64s/it]  2%|▏         | 211/9822 [05:37<4:12:23,  1.58s/it]  2%|▏         | 212/9822 [05:38<4:05:17,  1.53s/it]  2%|▏         | 213/9822 [05:40<4:01:19,  1.51s/it]  2%|▏         | 214/9822 [05:41<3:57:58,  1.49s/it]  2%|▏         | 215/9822 [05:43<3:57:20,  1.48s/it]  2%|▏         | 216/9822 [05:44<3:55:37,  1.47s/it]  2%|▏         | 217/9822 [05:46<3:53:40,  1.46s/it]  2%|▏         | 218/9822 [05:47<3:51:56,  1.45s/it]  2%|▏         | 219/9822 [05:48<3:50:27,  1.44s/it]  2%|▏         | 220/9822 [05:50<3:49:49,  1.44s/it]  2%|▏         | 221/9822 [05:51<3:49:16,  1.43s/it]  2%|▏         | 222/9822 [05:53<3:48:43,  1.43s/it]  2%|▏         | 223/9822 [05:54<3:49:04,  1.43s/it]  2%|▏         | 224/9822 [05:56<3:48:26,  1.43s/it]  2%|▏         | 225/9822 [05:57<3:48:06,  1.43s/it]  2%|▏         | 226/9822 [05:58<3:48:17,  1.43s/it]  2%|▏         | 227/9822 [06:00<3:48:46,  1.43s/it]  2%|▏         | 228/9822 [06:01<3:49:29,  1.44s/it]  2%|▏         | 229/9822 [06:03<3:48:47,  1.43s/it]  2%|▏         | 230/9822 [06:04<3:49:17,  1.43s/it]  2%|▏         | 231/9822 [06:06<3:49:23,  1.44s/it]  2%|▏         | 232/9822 [06:07<3:49:52,  1.44s/it]  2%|▏         | 233/9822 [06:09<3:53:29,  1.46s/it]  2%|▏         | 234/9822 [06:10<3:51:41,  1.45s/it]  2%|▏         | 235/9822 [06:11<3:51:27,  1.45s/it]  2%|▏         | 236/9822 [06:13<3:51:18,  1.45s/it]  2%|▏         | 237/9822 [06:14<3:51:24,  1.45s/it]  2%|▏         | 238/9822 [06:16<3:51:08,  1.45s/it]  2%|▏         | 239/9822 [06:17<3:49:59,  1.44s/it]  2%|▏         | 240/9822 [06:19<3:48:52,  1.43s/it]  2%|▏         | 241/9822 [06:20<3:49:02,  1.43s/it]  2%|▏         | 242/9822 [06:21<3:48:50,  1.43s/it]  2%|▏         | 243/9822 [06:23<3:48:44,  1.43s/it]  2%|▏         | 244/9822 [06:24<3:48:53,  1.43s/it]  2%|▏         | 245/9822 [06:26<3:48:22,  1.43s/it]  3%|▎         | 246/9822 [06:27<3:48:53,  1.43s/it]  3%|▎         | 247/9822 [06:29<3:48:57,  1.43s/it]  3%|▎         | 248/9822 [06:30<3:49:18,  1.44s/it]  3%|▎         | 249/9822 [06:31<3:49:07,  1.44s/it]  3%|▎         | 250/9822 [06:33<3:48:23,  1.43s/it]  3%|▎         | 251/9822 [06:34<3:48:34,  1.43s/it]  3%|▎         | 252/9822 [06:36<3:48:27,  1.43s/it]  3%|▎         | 253/9822 [06:37<3:48:06,  1.43s/it]  3%|▎         | 254/9822 [06:39<3:47:35,  1.43s/it]  3%|▎         | 255/9822 [06:40<3:47:07,  1.42s/it]  3%|▎         | 256/9822 [06:41<3:47:19,  1.43s/it]  3%|▎         | 257/9822 [06:43<3:46:53,  1.42s/it]  3%|▎         | 258/9822 [06:44<3:44:27,  1.41s/it]  3%|▎         | 259/9822 [06:46<3:45:39,  1.42s/it]  3%|▎         | 260/9822 [06:47<3:46:20,  1.42s/it]  3%|▎         | 261/9822 [06:49<3:47:04,  1.42s/it]  3%|▎         | 262/9822 [06:50<3:47:09,  1.43s/it]  3%|▎         | 263/9822 [06:51<3:47:24,  1.43s/it]  3%|▎         | 264/9822 [06:53<3:50:37,  1.45s/it]  3%|▎         | 265/9822 [06:54<3:53:44,  1.47s/it]  3%|▎         | 266/9822 [06:56<3:51:57,  1.46s/it]  3%|▎         | 267/9822 [06:57<3:50:44,  1.45s/it]  3%|▎         | 268/9822 [06:59<3:50:00,  1.44s/it]  3%|▎         | 269/9822 [07:00<3:49:01,  1.44s/it]  3%|▎         | 270/9822 [07:02<3:48:58,  1.44s/it]  3%|▎         | 271/9822 [07:03<3:48:19,  1.43s/it]  3%|▎         | 272/9822 [07:04<3:48:41,  1.44s/it]  3%|▎         | 273/9822 [07:06<3:48:40,  1.44s/it]  3%|▎         | 274/9822 [07:07<3:48:24,  1.44s/it]  3%|▎         | 275/9822 [07:09<3:48:08,  1.43s/it]  3%|▎         | 276/9822 [07:10<3:48:07,  1.43s/it]  3%|▎         | 277/9822 [07:12<3:47:52,  1.43s/it]  3%|▎         | 278/9822 [07:13<3:48:07,  1.43s/it]  3%|▎         | 279/9822 [07:14<3:47:37,  1.43s/it]  3%|▎         | 280/9822 [07:16<3:47:27,  1.43s/it]  3%|▎         | 281/9822 [07:17<3:47:43,  1.43s/it]  3%|▎         | 282/9822 [07:19<3:47:21,  1.43s/it]  3%|▎         | 283/9822 [07:20<3:47:57,  1.43s/it]  3%|▎         | 284/9822 [07:22<3:47:29,  1.43s/it]  3%|▎         | 285/9822 [07:23<3:47:14,  1.43s/it]  3%|▎         | 286/9822 [07:25<3:47:46,  1.43s/it]  3%|▎         | 287/9822 [07:26<3:47:48,  1.43s/it]  3%|▎         | 288/9822 [07:27<3:47:09,  1.43s/it]  3%|▎         | 289/9822 [07:29<3:47:49,  1.43s/it]  3%|▎         | 290/9822 [07:30<3:51:30,  1.46s/it]  3%|▎         | 291/9822 [07:32<3:49:39,  1.45s/it]  3%|▎         | 292/9822 [07:33<3:48:33,  1.44s/it]  3%|▎         | 293/9822 [07:35<3:48:31,  1.44s/it]  3%|▎         | 294/9822 [07:36<3:48:17,  1.44s/it]  3%|▎         | 295/9822 [07:37<3:47:42,  1.43s/it]  3%|▎         | 296/9822 [07:39<3:47:45,  1.43s/it]  3%|▎         | 297/9822 [07:40<3:47:53,  1.44s/it]  3%|▎         | 298/9822 [07:42<3:47:47,  1.44s/it]  3%|▎         | 299/9822 [07:43<3:47:51,  1.44s/it]  3%|▎         | 300/9822 [07:45<3:47:49,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 21:55:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:55:31 - INFO - __main__ - ***** test Results*****
01/07/2024 21:55:31 - INFO - __main__ -   Training step = 300
01/07/2024 21:55:31 - INFO - __main__ -  test_accuracy:0.8074670571010248 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:55:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:55:38 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 21:55:38 - INFO - __main__ -   Training step = 300
01/07/2024 21:55:38 - INFO - __main__ -  eval_accuracy:0.816550714024167 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 21:55:38,073 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 21:55:38,073 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 21:55:38,110 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 21:55:39,696 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.816550714024167}
test:
{'accuracy': 0.8074670571010248}
01/07/2024 21:55:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:55:44 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 21:55:44 - INFO - __main__ -   Training step = 300
01/07/2024 21:55:44 - INFO - __main__ -  eval_accuracy:0.85463200292933 
  3%|▎         | 301/9822 [08:05<18:30:13,  7.00s/it]  3%|▎         | 302/9822 [08:06<14:05:12,  5.33s/it]  3%|▎         | 303/9822 [08:07<10:59:51,  4.16s/it]  3%|▎         | 304/9822 [08:09<8:50:25,  3.34s/it]   3%|▎         | 305/9822 [08:10<7:19:05,  2.77s/it]  3%|▎         | 306/9822 [08:12<6:15:06,  2.37s/it]  3%|▎         | 307/9822 [08:13<5:30:32,  2.08s/it]  3%|▎         | 308/9822 [08:15<4:59:15,  1.89s/it]  3%|▎         | 309/9822 [08:16<4:37:25,  1.75s/it]  3%|▎         | 310/9822 [08:17<4:21:58,  1.65s/it]  3%|▎         | 311/9822 [08:19<4:11:09,  1.58s/it]  3%|▎         | 312/9822 [08:20<4:03:49,  1.54s/it]  3%|▎         | 313/9822 [08:22<3:58:44,  1.51s/it]  3%|▎         | 314/9822 [08:23<3:55:13,  1.48s/it]  3%|▎         | 315/9822 [08:25<3:53:29,  1.47s/it]  3%|▎         | 316/9822 [08:26<3:51:23,  1.46s/it]  3%|▎         | 317/9822 [08:28<3:50:01,  1.45s/it]  3%|▎         | 318/9822 [08:29<3:49:11,  1.45s/it]  3%|▎         | 319/9822 [08:30<3:48:16,  1.44s/it]  3%|▎         | 320/9822 [08:32<3:47:23,  1.44s/it]  3%|▎         | 321/9822 [08:33<3:51:24,  1.46s/it]  3%|▎         | 322/9822 [08:35<3:49:49,  1.45s/it]  3%|▎         | 323/9822 [08:36<3:50:33,  1.46s/it]  3%|▎         | 324/9822 [08:38<3:48:59,  1.45s/it]  3%|▎         | 325/9822 [08:39<3:48:46,  1.45s/it]  3%|▎         | 326/9822 [08:41<3:49:00,  1.45s/it]  3%|▎         | 327/9822 [08:42<3:47:42,  1.44s/it]  3%|▎         | 328/9822 [08:43<3:46:59,  1.43s/it]  3%|▎         | 329/9822 [08:45<3:46:42,  1.43s/it]  3%|▎         | 330/9822 [08:46<3:46:55,  1.43s/it]  3%|▎         | 331/9822 [08:48<3:46:45,  1.43s/it]  3%|▎         | 332/9822 [08:49<3:46:43,  1.43s/it]  3%|▎         | 333/9822 [08:51<3:46:47,  1.43s/it]  3%|▎         | 334/9822 [08:52<3:47:11,  1.44s/it]  3%|▎         | 335/9822 [08:53<3:47:08,  1.44s/it]  3%|▎         | 336/9822 [08:55<3:47:09,  1.44s/it]  3%|▎         | 337/9822 [08:56<3:47:33,  1.44s/it]  3%|▎         | 338/9822 [08:58<3:47:28,  1.44s/it]  3%|▎         | 339/9822 [08:59<3:47:04,  1.44s/it]  3%|▎         | 340/9822 [09:01<3:47:25,  1.44s/it]  3%|▎         | 341/9822 [09:02<3:46:54,  1.44s/it]  3%|▎         | 342/9822 [09:03<3:46:38,  1.43s/it]  3%|▎         | 343/9822 [09:05<3:46:42,  1.43s/it]  4%|▎         | 344/9822 [09:06<3:44:25,  1.42s/it]  4%|▎         | 345/9822 [09:08<3:45:30,  1.43s/it]  4%|▎         | 346/9822 [09:09<3:50:59,  1.46s/it]  4%|▎         | 347/9822 [09:11<3:51:21,  1.47s/it]  4%|▎         | 348/9822 [09:12<3:49:26,  1.45s/it]  4%|▎         | 349/9822 [09:14<3:50:14,  1.46s/it]  4%|▎         | 350/9822 [09:15<3:48:48,  1.45s/it]  4%|▎         | 351/9822 [09:17<3:48:25,  1.45s/it]  4%|▎         | 352/9822 [09:18<3:48:02,  1.44s/it]  4%|▎         | 353/9822 [09:19<3:47:38,  1.44s/it]  4%|▎         | 354/9822 [09:21<3:47:27,  1.44s/it]  4%|▎         | 355/9822 [09:22<3:47:10,  1.44s/it]  4%|▎         | 356/9822 [09:24<3:46:14,  1.43s/it]  4%|▎         | 357/9822 [09:25<3:46:00,  1.43s/it]  4%|▎         | 358/9822 [09:27<3:46:00,  1.43s/it]  4%|▎         | 359/9822 [09:28<3:46:22,  1.44s/it]  4%|▎         | 360/9822 [09:29<3:48:42,  1.45s/it]  4%|▎         | 361/9822 [09:31<3:47:34,  1.44s/it]  4%|▎         | 362/9822 [09:32<3:46:39,  1.44s/it]  4%|▎         | 363/9822 [09:34<3:46:42,  1.44s/it]  4%|▎         | 364/9822 [09:35<3:47:33,  1.44s/it]  4%|▎         | 365/9822 [09:37<3:46:36,  1.44s/it]  4%|▎         | 366/9822 [09:38<3:46:11,  1.44s/it]  4%|▎         | 367/9822 [09:40<3:45:46,  1.43s/it]  4%|▎         | 368/9822 [09:41<3:45:31,  1.43s/it]  4%|▍         | 369/9822 [09:42<3:45:19,  1.43s/it]  4%|▍         | 370/9822 [09:44<3:45:25,  1.43s/it]  4%|▍         | 371/9822 [09:45<3:45:15,  1.43s/it]  4%|▍         | 372/9822 [09:47<3:45:13,  1.43s/it]  4%|▍         | 373/9822 [09:48<3:44:41,  1.43s/it]  4%|▍         | 374/9822 [09:50<3:44:37,  1.43s/it]  4%|▍         | 375/9822 [09:51<3:45:27,  1.43s/it]  4%|▍         | 376/9822 [09:52<3:46:22,  1.44s/it]  4%|▍         | 377/9822 [09:54<3:48:47,  1.45s/it]  4%|▍         | 378/9822 [09:55<3:53:41,  1.48s/it]  4%|▍         | 379/9822 [09:57<3:51:23,  1.47s/it]  4%|▍         | 380/9822 [09:58<3:49:13,  1.46s/it]  4%|▍         | 381/9822 [10:00<3:47:18,  1.44s/it]  4%|▍         | 382/9822 [10:01<3:46:25,  1.44s/it]  4%|▍         | 383/9822 [10:03<3:45:45,  1.44s/it]  4%|▍         | 384/9822 [10:04<3:44:56,  1.43s/it]  4%|▍         | 385/9822 [10:05<3:44:46,  1.43s/it]  4%|▍         | 386/9822 [10:07<3:44:31,  1.43s/it]  4%|▍         | 387/9822 [10:08<3:44:11,  1.43s/it]  4%|▍         | 388/9822 [10:10<3:43:36,  1.42s/it]  4%|▍         | 389/9822 [10:11<3:43:45,  1.42s/it]  4%|▍         | 390/9822 [10:13<3:44:23,  1.43s/it]  4%|▍         | 391/9822 [10:14<3:45:07,  1.43s/it]  4%|▍         | 392/9822 [10:15<3:45:16,  1.43s/it]  4%|▍         | 393/9822 [10:17<3:45:13,  1.43s/it]  4%|▍         | 394/9822 [10:18<3:45:01,  1.43s/it]  4%|▍         | 395/9822 [10:20<3:44:26,  1.43s/it]  4%|▍         | 396/9822 [10:21<3:45:36,  1.44s/it]  4%|▍         | 397/9822 [10:23<3:45:05,  1.43s/it]  4%|▍         | 398/9822 [10:24<3:44:53,  1.43s/it]  4%|▍         | 399/9822 [10:25<3:44:42,  1.43s/it]  4%|▍         | 400/9822 [10:27<3:44:46,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 21:58:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:58:14 - INFO - __main__ - ***** test Results*****
01/07/2024 21:58:14 - INFO - __main__ -   Training step = 400
01/07/2024 21:58:14 - INFO - __main__ -  test_accuracy:0.8118594436310396 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:58:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:58:20 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 21:58:20 - INFO - __main__ -   Training step = 400
01/07/2024 21:58:20 - INFO - __main__ -  eval_accuracy:0.8176492127425852 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 21:58:20,287 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 21:58:20,287 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 21:58:20,323 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 21:58:21,970 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8176492127425852}
test:
{'accuracy': 0.8118594436310396}
01/07/2024 21:58:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 21:58:26 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 21:58:26 - INFO - __main__ -   Training step = 400
01/07/2024 21:58:26 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
  4%|▍         | 401/9822 [10:47<18:19:22,  7.00s/it]  4%|▍         | 402/9822 [10:48<13:57:09,  5.33s/it]  4%|▍         | 403/9822 [10:50<10:53:09,  4.16s/it]  4%|▍         | 404/9822 [10:51<8:46:27,  3.35s/it]   4%|▍         | 405/9822 [10:53<7:15:59,  2.78s/it]  4%|▍         | 406/9822 [10:54<6:12:42,  2.37s/it]  4%|▍         | 407/9822 [10:56<5:28:26,  2.09s/it]  4%|▍         | 408/9822 [10:57<5:00:26,  1.91s/it]  4%|▍         | 409/9822 [10:58<4:38:06,  1.77s/it]  4%|▍         | 410/9822 [11:00<4:22:15,  1.67s/it]  4%|▍         | 411/9822 [11:01<4:11:04,  1.60s/it]  4%|▍         | 412/9822 [11:03<4:03:18,  1.55s/it]  4%|▍         | 413/9822 [11:04<3:58:04,  1.52s/it]  4%|▍         | 414/9822 [11:06<3:53:36,  1.49s/it]  4%|▍         | 415/9822 [11:07<3:51:26,  1.48s/it]  4%|▍         | 416/9822 [11:09<3:49:20,  1.46s/it]  4%|▍         | 417/9822 [11:10<3:48:03,  1.45s/it]  4%|▍         | 418/9822 [11:11<3:47:00,  1.45s/it]  4%|▍         | 419/9822 [11:13<3:45:53,  1.44s/it]  4%|▍         | 420/9822 [11:14<3:45:18,  1.44s/it]  4%|▍         | 421/9822 [11:16<3:44:44,  1.43s/it]  4%|▍         | 422/9822 [11:17<3:44:33,  1.43s/it]  4%|▍         | 423/9822 [11:19<3:44:27,  1.43s/it]  4%|▍         | 424/9822 [11:20<3:44:16,  1.43s/it]  4%|▍         | 425/9822 [11:21<3:44:12,  1.43s/it]  4%|▍         | 426/9822 [11:23<3:44:44,  1.44s/it]  4%|▍         | 427/9822 [11:24<3:44:37,  1.43s/it]  4%|▍         | 428/9822 [11:26<3:44:36,  1.43s/it]  4%|▍         | 429/9822 [11:27<3:44:37,  1.43s/it]  4%|▍         | 430/9822 [11:28<3:41:34,  1.42s/it]  4%|▍         | 431/9822 [11:30<3:41:55,  1.42s/it]  4%|▍         | 432/9822 [11:31<3:42:34,  1.42s/it]  4%|▍         | 433/9822 [11:33<3:42:58,  1.42s/it]  4%|▍         | 434/9822 [11:34<3:43:05,  1.43s/it]  4%|▍         | 435/9822 [11:36<3:43:31,  1.43s/it]  4%|▍         | 436/9822 [11:37<3:43:01,  1.43s/it]  4%|▍         | 437/9822 [11:38<3:43:01,  1.43s/it]  4%|▍         | 438/9822 [11:40<3:43:18,  1.43s/it]  4%|▍         | 439/9822 [11:41<3:44:42,  1.44s/it]  4%|▍         | 440/9822 [11:43<3:50:35,  1.47s/it]  4%|▍         | 441/9822 [11:44<3:48:32,  1.46s/it]  5%|▍         | 442/9822 [11:46<3:48:04,  1.46s/it]  5%|▍         | 443/9822 [11:47<3:48:22,  1.46s/it]  5%|▍         | 444/9822 [11:49<3:47:43,  1.46s/it]  5%|▍         | 445/9822 [11:50<3:46:21,  1.45s/it]  5%|▍         | 446/9822 [11:52<3:45:23,  1.44s/it]  5%|▍         | 447/9822 [11:53<3:44:47,  1.44s/it]  5%|▍         | 448/9822 [11:54<3:44:54,  1.44s/it]  5%|▍         | 449/9822 [11:56<3:44:41,  1.44s/it]  5%|▍         | 450/9822 [11:57<3:44:08,  1.44s/it]  5%|▍         | 451/9822 [11:59<3:43:56,  1.43s/it]  5%|▍         | 452/9822 [12:00<3:43:41,  1.43s/it]  5%|▍         | 453/9822 [12:02<3:43:16,  1.43s/it]  5%|▍         | 454/9822 [12:03<3:43:22,  1.43s/it]  5%|▍         | 455/9822 [12:04<3:43:06,  1.43s/it]  5%|▍         | 456/9822 [12:06<3:42:39,  1.43s/it]  5%|▍         | 457/9822 [12:07<3:42:56,  1.43s/it]  5%|▍         | 458/9822 [12:09<3:42:32,  1.43s/it]  5%|▍         | 459/9822 [12:10<3:43:32,  1.43s/it]  5%|▍         | 460/9822 [12:12<3:43:39,  1.43s/it]  5%|▍         | 461/9822 [12:13<3:43:36,  1.43s/it]  5%|▍         | 462/9822 [12:15<3:45:09,  1.44s/it]  5%|▍         | 463/9822 [12:16<3:44:03,  1.44s/it]  5%|▍         | 464/9822 [12:17<3:43:44,  1.43s/it]  5%|▍         | 465/9822 [12:19<3:44:18,  1.44s/it]  5%|▍         | 466/9822 [12:20<3:43:37,  1.43s/it]  5%|▍         | 467/9822 [12:22<3:44:01,  1.44s/it]  5%|▍         | 468/9822 [12:23<3:45:23,  1.45s/it]  5%|▍         | 469/9822 [12:25<3:44:59,  1.44s/it]  5%|▍         | 470/9822 [12:26<3:44:09,  1.44s/it]  5%|▍         | 471/9822 [12:27<3:43:40,  1.44s/it]  5%|▍         | 472/9822 [12:29<3:46:58,  1.46s/it]  5%|▍         | 473/9822 [12:30<3:46:27,  1.45s/it]  5%|▍         | 474/9822 [12:32<3:46:26,  1.45s/it]  5%|▍         | 475/9822 [12:33<3:46:10,  1.45s/it]  5%|▍         | 476/9822 [12:35<3:45:15,  1.45s/it]  5%|▍         | 477/9822 [12:36<3:44:12,  1.44s/it]  5%|▍         | 478/9822 [12:38<3:43:21,  1.43s/it]  5%|▍         | 479/9822 [12:39<3:43:00,  1.43s/it]  5%|▍         | 480/9822 [12:40<3:42:56,  1.43s/it]  5%|▍         | 481/9822 [12:42<3:43:22,  1.43s/it]  5%|▍         | 482/9822 [12:43<3:44:01,  1.44s/it]  5%|▍         | 483/9822 [12:45<3:43:57,  1.44s/it]  5%|▍         | 484/9822 [12:46<3:43:41,  1.44s/it]  5%|▍         | 485/9822 [12:48<3:42:55,  1.43s/it]  5%|▍         | 486/9822 [12:49<3:42:54,  1.43s/it]  5%|▍         | 487/9822 [12:51<3:43:54,  1.44s/it]  5%|▍         | 488/9822 [12:52<3:43:20,  1.44s/it]  5%|▍         | 489/9822 [12:53<3:42:49,  1.43s/it]  5%|▍         | 490/9822 [12:55<3:42:31,  1.43s/it]  5%|▍         | 491/9822 [12:56<3:42:30,  1.43s/it]  5%|▌         | 492/9822 [12:58<3:42:28,  1.43s/it]  5%|▌         | 493/9822 [12:59<3:42:39,  1.43s/it]  5%|▌         | 494/9822 [13:01<3:42:23,  1.43s/it]  5%|▌         | 495/9822 [13:02<3:43:17,  1.44s/it]  5%|▌         | 496/9822 [13:03<3:43:20,  1.44s/it]  5%|▌         | 497/9822 [13:05<3:47:42,  1.47s/it]  5%|▌         | 498/9822 [13:06<3:46:27,  1.46s/it]  5%|▌         | 499/9822 [13:08<3:44:56,  1.45s/it]  5%|▌         | 500/9822 [13:09<3:43:37,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:00:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:00:56 - INFO - __main__ - ***** test Results*****
01/07/2024 22:00:56 - INFO - __main__ -   Training step = 500
01/07/2024 22:00:56 - INFO - __main__ -  test_accuracy:0.8235724743777453 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:01:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:01:02 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:01:02 - INFO - __main__ -   Training step = 500
01/07/2024 22:01:02 - INFO - __main__ -  eval_accuracy:0.8260710362504577 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:01:02,633 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:01:02,633 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:01:02,669 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:01:04,272 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8260710362504577}
test:
{'accuracy': 0.8235724743777453}
01/07/2024 22:01:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:01:08 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:01:08 - INFO - __main__ -   Training step = 500
01/07/2024 22:01:08 - INFO - __main__ -  eval_accuracy:0.857195166605639 
  5%|▌         | 501/9822 [13:29<18:06:00,  6.99s/it]  5%|▌         | 502/9822 [13:31<13:47:11,  5.33s/it]  5%|▌         | 503/9822 [13:32<10:46:46,  4.16s/it]  5%|▌         | 504/9822 [13:33<8:39:24,  3.34s/it]   5%|▌         | 505/9822 [13:35<7:11:44,  2.78s/it]  5%|▌         | 506/9822 [13:36<6:08:37,  2.37s/it]  5%|▌         | 507/9822 [13:38<5:24:52,  2.09s/it]  5%|▌         | 508/9822 [13:39<4:54:12,  1.90s/it]  5%|▌         | 509/9822 [13:41<4:32:20,  1.75s/it]  5%|▌         | 510/9822 [13:42<4:17:01,  1.66s/it]  5%|▌         | 511/9822 [13:44<4:06:40,  1.59s/it]  5%|▌         | 512/9822 [13:45<3:59:03,  1.54s/it]  5%|▌         | 513/9822 [13:46<3:54:02,  1.51s/it]  5%|▌         | 514/9822 [13:48<3:50:36,  1.49s/it]  5%|▌         | 515/9822 [13:49<3:47:51,  1.47s/it]  5%|▌         | 516/9822 [13:51<3:43:45,  1.44s/it]  5%|▌         | 517/9822 [13:52<3:43:25,  1.44s/it]  5%|▌         | 518/9822 [13:54<3:42:32,  1.44s/it]  5%|▌         | 519/9822 [13:55<3:42:25,  1.43s/it]  5%|▌         | 520/9822 [13:56<3:42:48,  1.44s/it]  5%|▌         | 521/9822 [13:58<3:42:15,  1.43s/it]  5%|▌         | 522/9822 [13:59<3:42:27,  1.44s/it]  5%|▌         | 523/9822 [14:01<3:42:53,  1.44s/it]  5%|▌         | 524/9822 [14:02<3:42:34,  1.44s/it]  5%|▌         | 525/9822 [14:04<3:42:14,  1.43s/it]  5%|▌         | 526/9822 [14:05<3:44:03,  1.45s/it]  5%|▌         | 527/9822 [14:06<3:43:10,  1.44s/it]  5%|▌         | 528/9822 [14:08<3:42:28,  1.44s/it]  5%|▌         | 529/9822 [14:09<3:41:45,  1.43s/it]  5%|▌         | 530/9822 [14:11<3:41:44,  1.43s/it]  5%|▌         | 531/9822 [14:12<3:41:13,  1.43s/it]  5%|▌         | 532/9822 [14:14<3:41:26,  1.43s/it]  5%|▌         | 533/9822 [14:15<3:41:25,  1.43s/it]  5%|▌         | 534/9822 [14:16<3:41:17,  1.43s/it]  5%|▌         | 535/9822 [14:18<3:41:20,  1.43s/it]  5%|▌         | 536/9822 [14:19<3:41:27,  1.43s/it]  5%|▌         | 537/9822 [14:21<3:42:09,  1.44s/it]  5%|▌         | 538/9822 [14:22<3:42:15,  1.44s/it]  5%|▌         | 539/9822 [14:24<3:43:03,  1.44s/it]  5%|▌         | 540/9822 [14:25<3:42:22,  1.44s/it]  6%|▌         | 541/9822 [14:27<3:42:16,  1.44s/it]  6%|▌         | 542/9822 [14:28<3:41:52,  1.43s/it]  6%|▌         | 543/9822 [14:29<3:42:06,  1.44s/it]  6%|▌         | 544/9822 [14:31<3:42:07,  1.44s/it]  6%|▌         | 545/9822 [14:32<3:44:37,  1.45s/it]  6%|▌         | 546/9822 [14:34<3:44:38,  1.45s/it]  6%|▌         | 547/9822 [14:35<3:46:00,  1.46s/it]  6%|▌         | 548/9822 [14:37<3:45:10,  1.46s/it]  6%|▌         | 549/9822 [14:38<3:44:11,  1.45s/it]  6%|▌         | 550/9822 [14:40<3:43:54,  1.45s/it]  6%|▌         | 551/9822 [14:41<3:44:42,  1.45s/it]  6%|▌         | 552/9822 [14:42<3:43:44,  1.45s/it]  6%|▌         | 553/9822 [14:44<3:42:52,  1.44s/it]  6%|▌         | 554/9822 [14:45<3:42:40,  1.44s/it]  6%|▌         | 555/9822 [14:47<3:42:45,  1.44s/it]  6%|▌         | 556/9822 [14:48<3:42:03,  1.44s/it]  6%|▌         | 557/9822 [14:50<3:42:08,  1.44s/it]  6%|▌         | 558/9822 [14:51<3:42:35,  1.44s/it]  6%|▌         | 559/9822 [14:53<3:41:50,  1.44s/it]  6%|▌         | 560/9822 [14:54<3:42:17,  1.44s/it]  6%|▌         | 561/9822 [14:55<3:42:00,  1.44s/it]  6%|▌         | 562/9822 [14:57<3:41:37,  1.44s/it]  6%|▌         | 563/9822 [14:58<3:41:23,  1.43s/it]  6%|▌         | 564/9822 [15:00<3:41:12,  1.43s/it]  6%|▌         | 565/9822 [15:01<3:42:52,  1.44s/it]  6%|▌         | 566/9822 [15:03<3:45:58,  1.46s/it]  6%|▌         | 567/9822 [15:04<3:44:02,  1.45s/it]  6%|▌         | 568/9822 [15:06<3:43:00,  1.45s/it]  6%|▌         | 569/9822 [15:07<3:42:07,  1.44s/it]  6%|▌         | 570/9822 [15:08<3:41:52,  1.44s/it]  6%|▌         | 571/9822 [15:10<3:41:43,  1.44s/it]  6%|▌         | 572/9822 [15:11<3:41:42,  1.44s/it]  6%|▌         | 573/9822 [15:13<3:41:32,  1.44s/it]  6%|▌         | 574/9822 [15:14<3:41:12,  1.44s/it]  6%|▌         | 575/9822 [15:16<3:41:29,  1.44s/it]  6%|▌         | 576/9822 [15:17<3:41:06,  1.43s/it]  6%|▌         | 577/9822 [15:18<3:40:33,  1.43s/it]  6%|▌         | 578/9822 [15:20<3:41:12,  1.44s/it]  6%|▌         | 579/9822 [15:21<3:41:01,  1.43s/it]  6%|▌         | 580/9822 [15:23<3:41:15,  1.44s/it]  6%|▌         | 581/9822 [15:24<3:41:21,  1.44s/it]  6%|▌         | 582/9822 [15:26<3:42:02,  1.44s/it]  6%|▌         | 583/9822 [15:27<3:41:53,  1.44s/it]  6%|▌         | 584/9822 [15:29<3:41:32,  1.44s/it]  6%|▌         | 585/9822 [15:30<3:41:47,  1.44s/it]  6%|▌         | 586/9822 [15:31<3:41:22,  1.44s/it]  6%|▌         | 587/9822 [15:33<3:41:19,  1.44s/it]  6%|▌         | 588/9822 [15:34<3:41:57,  1.44s/it]  6%|▌         | 589/9822 [15:36<3:41:11,  1.44s/it]  6%|▌         | 590/9822 [15:37<3:41:29,  1.44s/it]  6%|▌         | 591/9822 [15:39<3:40:52,  1.44s/it]  6%|▌         | 592/9822 [15:40<3:40:10,  1.43s/it]  6%|▌         | 593/9822 [15:41<3:40:06,  1.43s/it]  6%|▌         | 594/9822 [15:43<3:40:26,  1.43s/it]  6%|▌         | 595/9822 [15:44<3:40:24,  1.43s/it]  6%|▌         | 596/9822 [15:46<3:40:16,  1.43s/it]  6%|▌         | 597/9822 [15:47<3:43:50,  1.46s/it]  6%|▌         | 598/9822 [15:49<3:42:17,  1.45s/it]  6%|▌         | 599/9822 [15:50<3:41:51,  1.44s/it]  6%|▌         | 600/9822 [15:52<3:41:48,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:03:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:03:38 - INFO - __main__ - ***** test Results*****
01/07/2024 22:03:38 - INFO - __main__ -   Training step = 600
01/07/2024 22:03:38 - INFO - __main__ -  test_accuracy:0.8114934114202049 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:03:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:03:44 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:03:44 - INFO - __main__ -   Training step = 600
01/07/2024 22:03:44 - INFO - __main__ -  eval_accuracy:0.8143537165873307 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8260710362504577}
test:
{'accuracy': 0.8235724743777453}
01/07/2024 22:03:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:03:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:03:49 - INFO - __main__ -   Training step = 600
01/07/2024 22:03:49 - INFO - __main__ -  eval_accuracy:0.8593921640424753 
  6%|▌         | 601/9822 [16:10<16:41:08,  6.51s/it]  6%|▌         | 602/9822 [16:11<12:44:09,  4.97s/it]  6%|▌         | 603/9822 [16:13<10:00:18,  3.91s/it]  6%|▌         | 604/9822 [16:14<8:06:21,  3.17s/it]   6%|▌         | 605/9822 [16:16<6:45:53,  2.64s/it]  6%|▌         | 606/9822 [16:17<5:49:30,  2.28s/it]  6%|▌         | 607/9822 [16:18<5:10:15,  2.02s/it]  6%|▌         | 608/9822 [16:20<4:43:52,  1.85s/it]  6%|▌         | 609/9822 [16:21<4:27:50,  1.74s/it]  6%|▌         | 610/9822 [16:23<4:15:43,  1.67s/it]  6%|▌         | 611/9822 [16:24<4:05:42,  1.60s/it]  6%|▌         | 612/9822 [16:26<3:58:56,  1.56s/it]  6%|▌         | 613/9822 [16:27<3:53:27,  1.52s/it]  6%|▋         | 614/9822 [16:29<3:49:14,  1.49s/it]  6%|▋         | 615/9822 [16:30<3:46:31,  1.48s/it]  6%|▋         | 616/9822 [16:31<3:44:45,  1.46s/it]  6%|▋         | 617/9822 [16:33<3:42:45,  1.45s/it]  6%|▋         | 618/9822 [16:34<3:41:56,  1.45s/it]  6%|▋         | 619/9822 [16:36<3:41:35,  1.44s/it]  6%|▋         | 620/9822 [16:37<3:41:13,  1.44s/it]  6%|▋         | 621/9822 [16:39<3:41:02,  1.44s/it]  6%|▋         | 622/9822 [16:40<3:40:27,  1.44s/it]  6%|▋         | 623/9822 [16:42<3:40:37,  1.44s/it]  6%|▋         | 624/9822 [16:43<3:40:24,  1.44s/it]  6%|▋         | 625/9822 [16:44<3:39:55,  1.43s/it]  6%|▋         | 626/9822 [16:46<3:40:20,  1.44s/it]  6%|▋         | 627/9822 [16:47<3:39:47,  1.43s/it]  6%|▋         | 628/9822 [16:49<3:40:09,  1.44s/it]  6%|▋         | 629/9822 [16:50<3:43:35,  1.46s/it]  6%|▋         | 630/9822 [16:52<3:42:10,  1.45s/it]  6%|▋         | 631/9822 [16:53<3:41:17,  1.44s/it]  6%|▋         | 632/9822 [16:54<3:40:22,  1.44s/it]  6%|▋         | 633/9822 [16:56<3:40:35,  1.44s/it]  6%|▋         | 634/9822 [16:57<3:40:52,  1.44s/it]  6%|▋         | 635/9822 [16:59<3:40:17,  1.44s/it]  6%|▋         | 636/9822 [17:00<3:40:43,  1.44s/it]  6%|▋         | 637/9822 [17:02<3:40:24,  1.44s/it]  6%|▋         | 638/9822 [17:03<3:40:03,  1.44s/it]  7%|▋         | 639/9822 [17:05<3:39:28,  1.43s/it]  7%|▋         | 640/9822 [17:06<3:39:38,  1.44s/it]  7%|▋         | 641/9822 [17:07<3:40:57,  1.44s/it]  7%|▋         | 642/9822 [17:09<3:40:20,  1.44s/it]  7%|▋         | 643/9822 [17:10<3:40:21,  1.44s/it]  7%|▋         | 644/9822 [17:12<3:40:20,  1.44s/it]  7%|▋         | 645/9822 [17:13<3:40:02,  1.44s/it]  7%|▋         | 646/9822 [17:15<3:39:56,  1.44s/it]  7%|▋         | 647/9822 [17:16<3:40:14,  1.44s/it]  7%|▋         | 648/9822 [17:18<3:39:48,  1.44s/it]  7%|▋         | 649/9822 [17:19<3:39:58,  1.44s/it]  7%|▋         | 650/9822 [17:20<3:39:50,  1.44s/it]  7%|▋         | 651/9822 [17:22<3:39:46,  1.44s/it]  7%|▋         | 652/9822 [17:23<3:39:38,  1.44s/it]  7%|▋         | 653/9822 [17:25<3:41:34,  1.45s/it]  7%|▋         | 654/9822 [17:26<3:40:56,  1.45s/it]  7%|▋         | 655/9822 [17:28<3:40:15,  1.44s/it]  7%|▋         | 656/9822 [17:29<3:40:17,  1.44s/it]  7%|▋         | 657/9822 [17:31<3:41:31,  1.45s/it]  7%|▋         | 658/9822 [17:32<3:40:18,  1.44s/it]  7%|▋         | 659/9822 [17:33<3:39:39,  1.44s/it]  7%|▋         | 660/9822 [17:35<3:39:10,  1.44s/it]  7%|▋         | 661/9822 [17:36<3:42:55,  1.46s/it]  7%|▋         | 662/9822 [17:38<3:41:24,  1.45s/it]  7%|▋         | 663/9822 [17:39<3:40:35,  1.45s/it]  7%|▋         | 664/9822 [17:41<3:41:05,  1.45s/it]  7%|▋         | 665/9822 [17:42<3:40:13,  1.44s/it]  7%|▋         | 666/9822 [17:44<3:40:32,  1.45s/it]  7%|▋         | 667/9822 [17:45<3:40:24,  1.44s/it]  7%|▋         | 668/9822 [17:46<3:41:31,  1.45s/it]  7%|▋         | 669/9822 [17:48<3:40:26,  1.45s/it]  7%|▋         | 670/9822 [17:49<3:39:13,  1.44s/it]  7%|▋         | 671/9822 [17:51<3:39:04,  1.44s/it]  7%|▋         | 672/9822 [17:52<3:38:23,  1.43s/it]  7%|▋         | 673/9822 [17:54<3:38:21,  1.43s/it]  7%|▋         | 674/9822 [17:55<3:38:35,  1.43s/it]  7%|▋         | 675/9822 [17:56<3:38:22,  1.43s/it]  7%|▋         | 676/9822 [17:58<3:37:58,  1.43s/it]  7%|▋         | 677/9822 [17:59<3:38:41,  1.43s/it]  7%|▋         | 678/9822 [18:01<3:38:25,  1.43s/it]  7%|▋         | 679/9822 [18:02<3:38:09,  1.43s/it]  7%|▋         | 680/9822 [18:04<3:37:39,  1.43s/it]  7%|▋         | 681/9822 [18:05<3:37:38,  1.43s/it]  7%|▋         | 682/9822 [18:06<3:38:13,  1.43s/it]  7%|▋         | 683/9822 [18:08<3:38:06,  1.43s/it]  7%|▋         | 684/9822 [18:09<3:37:53,  1.43s/it]  7%|▋         | 685/9822 [18:11<3:37:39,  1.43s/it]  7%|▋         | 686/9822 [18:12<3:41:22,  1.45s/it]  7%|▋         | 687/9822 [18:14<3:39:48,  1.44s/it]  7%|▋         | 688/9822 [18:15<3:37:20,  1.43s/it]  7%|▋         | 689/9822 [18:16<3:37:20,  1.43s/it]  7%|▋         | 690/9822 [18:18<3:39:54,  1.44s/it]  7%|▋         | 691/9822 [18:19<3:38:44,  1.44s/it]  7%|▋         | 692/9822 [18:21<3:38:16,  1.43s/it]  7%|▋         | 693/9822 [18:22<3:37:50,  1.43s/it]  7%|▋         | 694/9822 [18:24<3:38:05,  1.43s/it]  7%|▋         | 695/9822 [18:25<3:38:04,  1.43s/it]  7%|▋         | 696/9822 [18:27<3:38:09,  1.43s/it]  7%|▋         | 697/9822 [18:28<3:37:54,  1.43s/it]  7%|▋         | 698/9822 [18:29<3:37:43,  1.43s/it]  7%|▋         | 699/9822 [18:31<3:38:01,  1.43s/it]  7%|▋         | 700/9822 [18:32<3:37:44,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:06:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:06:19 - INFO - __main__ - ***** test Results*****
01/07/2024 22:06:19 - INFO - __main__ -   Training step = 700
01/07/2024 22:06:19 - INFO - __main__ -  test_accuracy:0.8275988286969254 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:06:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:06:25 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:06:25 - INFO - __main__ -   Training step = 700
01/07/2024 22:06:25 - INFO - __main__ -  eval_accuracy:0.8242402050530941 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8260710362504577}
test:
{'accuracy': 0.8235724743777453}
01/07/2024 22:06:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:06:30 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:06:30 - INFO - __main__ -   Training step = 700
01/07/2024 22:06:30 - INFO - __main__ -  eval_accuracy:0.8626876601977298 
  7%|▋         | 701/9822 [18:51<16:29:18,  6.51s/it]  7%|▋         | 702/9822 [18:52<12:38:23,  4.99s/it]  7%|▋         | 703/9822 [18:54<9:55:48,  3.92s/it]   7%|▋         | 704/9822 [18:55<8:02:21,  3.17s/it]  7%|▋         | 705/9822 [18:56<6:43:15,  2.65s/it]  7%|▋         | 706/9822 [18:58<5:47:33,  2.29s/it]  7%|▋         | 707/9822 [18:59<5:09:12,  2.04s/it]  7%|▋         | 708/9822 [19:01<4:41:53,  1.86s/it]  7%|▋         | 709/9822 [19:02<4:22:47,  1.73s/it]  7%|▋         | 710/9822 [19:04<4:09:11,  1.64s/it]  7%|▋         | 711/9822 [19:05<3:59:23,  1.58s/it]  7%|▋         | 712/9822 [19:06<3:53:16,  1.54s/it]  7%|▋         | 713/9822 [19:08<3:48:51,  1.51s/it]  7%|▋         | 714/9822 [19:09<3:45:22,  1.48s/it]  7%|▋         | 715/9822 [19:11<3:47:29,  1.50s/it]  7%|▋         | 716/9822 [19:12<3:44:33,  1.48s/it]  7%|▋         | 717/9822 [19:14<3:42:59,  1.47s/it]  7%|▋         | 718/9822 [19:15<3:41:28,  1.46s/it]  7%|▋         | 719/9822 [19:17<3:39:53,  1.45s/it]  7%|▋         | 720/9822 [19:18<3:38:57,  1.44s/it]  7%|▋         | 721/9822 [19:19<3:39:39,  1.45s/it]  7%|▋         | 722/9822 [19:21<3:40:20,  1.45s/it]  7%|▋         | 723/9822 [19:22<3:39:58,  1.45s/it]  7%|▋         | 724/9822 [19:24<3:40:14,  1.45s/it]  7%|▋         | 725/9822 [19:25<3:39:07,  1.45s/it]  7%|▋         | 726/9822 [19:27<3:39:10,  1.45s/it]  7%|▋         | 727/9822 [19:28<3:38:50,  1.44s/it]  7%|▋         | 728/9822 [19:30<3:38:10,  1.44s/it]  7%|▋         | 729/9822 [19:31<3:37:38,  1.44s/it]  7%|▋         | 730/9822 [19:32<3:37:38,  1.44s/it]  7%|▋         | 731/9822 [19:34<3:37:28,  1.44s/it]  7%|▋         | 732/9822 [19:35<3:37:18,  1.43s/it]  7%|▋         | 733/9822 [19:37<3:36:41,  1.43s/it]  7%|▋         | 734/9822 [19:38<3:36:20,  1.43s/it]  7%|▋         | 735/9822 [19:40<3:36:10,  1.43s/it]  7%|▋         | 736/9822 [19:41<3:36:27,  1.43s/it]  8%|▊         | 737/9822 [19:42<3:36:27,  1.43s/it]  8%|▊         | 738/9822 [19:44<3:36:21,  1.43s/it]  8%|▊         | 739/9822 [19:45<3:36:35,  1.43s/it]  8%|▊         | 740/9822 [19:47<3:36:24,  1.43s/it]  8%|▊         | 741/9822 [19:48<3:36:40,  1.43s/it]  8%|▊         | 742/9822 [19:50<3:36:54,  1.43s/it]  8%|▊         | 743/9822 [19:51<3:37:04,  1.43s/it]  8%|▊         | 744/9822 [19:52<3:36:55,  1.43s/it]  8%|▊         | 745/9822 [19:54<3:37:30,  1.44s/it]  8%|▊         | 746/9822 [19:55<3:37:12,  1.44s/it]  8%|▊         | 747/9822 [19:57<3:40:50,  1.46s/it]  8%|▊         | 748/9822 [19:58<3:39:01,  1.45s/it]  8%|▊         | 749/9822 [20:00<3:38:35,  1.45s/it]  8%|▊         | 750/9822 [20:01<3:37:42,  1.44s/it]  8%|▊         | 751/9822 [20:03<3:37:14,  1.44s/it]  8%|▊         | 752/9822 [20:04<3:37:46,  1.44s/it]  8%|▊         | 753/9822 [20:05<3:37:19,  1.44s/it]  8%|▊         | 754/9822 [20:07<3:36:31,  1.43s/it]  8%|▊         | 755/9822 [20:08<3:36:25,  1.43s/it]  8%|▊         | 756/9822 [20:10<3:36:31,  1.43s/it]  8%|▊         | 757/9822 [20:11<3:35:56,  1.43s/it]  8%|▊         | 758/9822 [20:13<3:35:35,  1.43s/it]  8%|▊         | 759/9822 [20:14<3:35:27,  1.43s/it]  8%|▊         | 760/9822 [20:15<3:35:37,  1.43s/it]  8%|▊         | 761/9822 [20:17<3:35:35,  1.43s/it]  8%|▊         | 762/9822 [20:18<3:35:39,  1.43s/it]  8%|▊         | 763/9822 [20:20<3:35:45,  1.43s/it]  8%|▊         | 764/9822 [20:21<3:36:06,  1.43s/it]  8%|▊         | 765/9822 [20:23<3:35:55,  1.43s/it]  8%|▊         | 766/9822 [20:24<3:35:31,  1.43s/it]  8%|▊         | 767/9822 [20:25<3:35:30,  1.43s/it]  8%|▊         | 768/9822 [20:27<3:36:50,  1.44s/it]  8%|▊         | 769/9822 [20:28<3:36:33,  1.44s/it]  8%|▊         | 770/9822 [20:30<3:36:18,  1.43s/it]  8%|▊         | 771/9822 [20:31<3:36:26,  1.43s/it]  8%|▊         | 772/9822 [20:33<3:36:12,  1.43s/it]  8%|▊         | 773/9822 [20:34<3:36:48,  1.44s/it]  8%|▊         | 774/9822 [20:35<3:34:51,  1.42s/it]  8%|▊         | 775/9822 [20:37<3:35:12,  1.43s/it]  8%|▊         | 776/9822 [20:38<3:35:33,  1.43s/it]  8%|▊         | 777/9822 [20:40<3:35:41,  1.43s/it]  8%|▊         | 778/9822 [20:41<3:36:17,  1.43s/it]  8%|▊         | 779/9822 [20:43<3:39:17,  1.45s/it]  8%|▊         | 780/9822 [20:44<3:38:06,  1.45s/it]  8%|▊         | 781/9822 [20:46<3:37:37,  1.44s/it]  8%|▊         | 782/9822 [20:47<3:37:16,  1.44s/it]  8%|▊         | 783/9822 [20:48<3:36:36,  1.44s/it]  8%|▊         | 784/9822 [20:50<3:36:52,  1.44s/it]  8%|▊         | 785/9822 [20:51<3:36:54,  1.44s/it]  8%|▊         | 786/9822 [20:53<3:36:23,  1.44s/it]  8%|▊         | 787/9822 [20:54<3:35:57,  1.43s/it]  8%|▊         | 788/9822 [20:56<3:35:51,  1.43s/it]  8%|▊         | 789/9822 [20:57<3:36:19,  1.44s/it]  8%|▊         | 790/9822 [20:59<3:36:23,  1.44s/it]  8%|▊         | 791/9822 [21:00<3:36:19,  1.44s/it]  8%|▊         | 792/9822 [21:01<3:35:56,  1.43s/it]  8%|▊         | 793/9822 [21:03<3:35:47,  1.43s/it]  8%|▊         | 794/9822 [21:04<3:35:52,  1.43s/it]  8%|▊         | 795/9822 [21:06<3:36:07,  1.44s/it]  8%|▊         | 796/9822 [21:07<3:36:30,  1.44s/it]  8%|▊         | 797/9822 [21:09<3:35:59,  1.44s/it]  8%|▊         | 798/9822 [21:10<3:38:14,  1.45s/it]  8%|▊         | 799/9822 [21:11<3:36:47,  1.44s/it]  8%|▊         | 800/9822 [21:13<3:36:19,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:09:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:09:00 - INFO - __main__ - ***** test Results*****
01/07/2024 22:09:00 - INFO - __main__ -   Training step = 800
01/07/2024 22:09:00 - INFO - __main__ -  test_accuracy:0.8382137628111274 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:09:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:09:06 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:09:06 - INFO - __main__ -   Training step = 800
01/07/2024 22:09:06 - INFO - __main__ -  eval_accuracy:0.8275357012083486 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:09:06,327 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:09:06,327 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:09:06,364 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:09:07,929 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8275357012083486}
test:
{'accuracy': 0.8382137628111274}
01/07/2024 22:09:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:09:12 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:09:12 - INFO - __main__ -   Training step = 800
01/07/2024 22:09:12 - INFO - __main__ -  eval_accuracy:0.8773343097766386 
  8%|▊         | 801/9822 [21:33<17:31:05,  6.99s/it]  8%|▊         | 802/9822 [21:34<13:20:26,  5.32s/it]  8%|▊         | 803/9822 [21:36<10:24:49,  4.16s/it]  8%|▊         | 804/9822 [21:37<8:25:12,  3.36s/it]   8%|▊         | 805/9822 [21:39<6:58:01,  2.78s/it]  8%|▊         | 806/9822 [21:40<5:57:10,  2.38s/it]  8%|▊         | 807/9822 [21:42<5:14:29,  2.09s/it]  8%|▊         | 808/9822 [21:43<4:45:16,  1.90s/it]  8%|▊         | 809/9822 [21:44<4:24:01,  1.76s/it]  8%|▊         | 810/9822 [21:46<4:09:28,  1.66s/it]  8%|▊         | 811/9822 [21:47<3:58:49,  1.59s/it]  8%|▊         | 812/9822 [21:49<3:51:55,  1.54s/it]  8%|▊         | 813/9822 [21:50<3:46:38,  1.51s/it]  8%|▊         | 814/9822 [21:52<3:43:43,  1.49s/it]  8%|▊         | 815/9822 [21:53<3:40:57,  1.47s/it]  8%|▊         | 816/9822 [21:54<3:41:07,  1.47s/it]  8%|▊         | 817/9822 [21:56<3:39:25,  1.46s/it]  8%|▊         | 818/9822 [21:57<3:37:56,  1.45s/it]  8%|▊         | 819/9822 [21:59<3:36:42,  1.44s/it]  8%|▊         | 820/9822 [22:00<3:35:57,  1.44s/it]  8%|▊         | 821/9822 [22:02<3:35:32,  1.44s/it]  8%|▊         | 822/9822 [22:03<3:34:44,  1.43s/it]  8%|▊         | 823/9822 [22:04<3:34:17,  1.43s/it]  8%|▊         | 824/9822 [22:06<3:34:05,  1.43s/it]  8%|▊         | 825/9822 [22:07<3:34:12,  1.43s/it]  8%|▊         | 826/9822 [22:09<3:34:31,  1.43s/it]  8%|▊         | 827/9822 [22:10<3:34:08,  1.43s/it]  8%|▊         | 828/9822 [22:12<3:34:40,  1.43s/it]  8%|▊         | 829/9822 [22:13<3:34:27,  1.43s/it]  8%|▊         | 830/9822 [22:14<3:35:15,  1.44s/it]  8%|▊         | 831/9822 [22:16<3:35:37,  1.44s/it]  8%|▊         | 832/9822 [22:17<3:34:50,  1.43s/it]  8%|▊         | 833/9822 [22:19<3:34:09,  1.43s/it]  8%|▊         | 834/9822 [22:20<3:34:28,  1.43s/it]  9%|▊         | 835/9822 [22:22<3:37:54,  1.45s/it]  9%|▊         | 836/9822 [22:23<3:37:09,  1.45s/it]  9%|▊         | 837/9822 [22:25<3:35:41,  1.44s/it]  9%|▊         | 838/9822 [22:26<3:35:05,  1.44s/it]  9%|▊         | 839/9822 [22:27<3:35:08,  1.44s/it]  9%|▊         | 840/9822 [22:29<3:35:04,  1.44s/it]  9%|▊         | 841/9822 [22:30<3:34:50,  1.44s/it]  9%|▊         | 842/9822 [22:32<3:34:30,  1.43s/it]  9%|▊         | 843/9822 [22:33<3:34:15,  1.43s/it]  9%|▊         | 844/9822 [22:35<3:34:21,  1.43s/it]  9%|▊         | 845/9822 [22:36<3:34:07,  1.43s/it]  9%|▊         | 846/9822 [22:37<3:33:39,  1.43s/it]  9%|▊         | 847/9822 [22:39<3:33:45,  1.43s/it]  9%|▊         | 848/9822 [22:40<3:33:33,  1.43s/it]  9%|▊         | 849/9822 [22:42<3:33:20,  1.43s/it]  9%|▊         | 850/9822 [22:43<3:34:31,  1.43s/it]  9%|▊         | 851/9822 [22:45<3:34:48,  1.44s/it]  9%|▊         | 852/9822 [22:46<3:35:43,  1.44s/it]  9%|▊         | 853/9822 [22:48<3:37:33,  1.46s/it]  9%|▊         | 854/9822 [22:49<3:36:51,  1.45s/it]  9%|▊         | 855/9822 [22:50<3:36:21,  1.45s/it]  9%|▊         | 856/9822 [22:52<3:36:16,  1.45s/it]  9%|▊         | 857/9822 [22:53<3:36:18,  1.45s/it]  9%|▊         | 858/9822 [22:55<3:35:40,  1.44s/it]  9%|▊         | 859/9822 [22:56<3:35:12,  1.44s/it]  9%|▉         | 860/9822 [22:58<3:36:23,  1.45s/it]  9%|▉         | 861/9822 [22:59<3:36:33,  1.45s/it]  9%|▉         | 862/9822 [23:01<3:38:03,  1.46s/it]  9%|▉         | 863/9822 [23:02<3:36:44,  1.45s/it]  9%|▉         | 864/9822 [23:03<3:36:01,  1.45s/it]  9%|▉         | 865/9822 [23:05<3:35:00,  1.44s/it]  9%|▉         | 866/9822 [23:06<3:34:37,  1.44s/it]  9%|▉         | 867/9822 [23:08<3:34:28,  1.44s/it]  9%|▉         | 868/9822 [23:09<3:34:22,  1.44s/it]  9%|▉         | 869/9822 [23:11<3:34:46,  1.44s/it]  9%|▉         | 870/9822 [23:12<3:34:39,  1.44s/it]  9%|▉         | 871/9822 [23:14<3:35:11,  1.44s/it]  9%|▉         | 872/9822 [23:15<3:37:17,  1.46s/it]  9%|▉         | 873/9822 [23:17<3:38:44,  1.47s/it]  9%|▉         | 874/9822 [23:18<3:37:48,  1.46s/it]  9%|▉         | 875/9822 [23:19<3:37:16,  1.46s/it]  9%|▉         | 876/9822 [23:21<3:37:00,  1.46s/it]  9%|▉         | 877/9822 [23:22<3:36:35,  1.45s/it]  9%|▉         | 878/9822 [23:24<3:35:13,  1.44s/it]  9%|▉         | 879/9822 [23:25<3:35:07,  1.44s/it]  9%|▉         | 880/9822 [23:27<3:35:19,  1.44s/it]  9%|▉         | 881/9822 [23:28<3:35:34,  1.45s/it]  9%|▉         | 882/9822 [23:30<3:35:49,  1.45s/it]  9%|▉         | 883/9822 [23:31<3:35:55,  1.45s/it]  9%|▉         | 884/9822 [23:32<3:36:17,  1.45s/it]  9%|▉         | 885/9822 [23:34<3:38:16,  1.47s/it]  9%|▉         | 886/9822 [23:35<3:38:27,  1.47s/it]  9%|▉         | 887/9822 [23:37<3:37:33,  1.46s/it]  9%|▉         | 888/9822 [23:38<3:36:35,  1.45s/it]  9%|▉         | 889/9822 [23:40<3:35:46,  1.45s/it]  9%|▉         | 890/9822 [23:41<3:35:32,  1.45s/it]  9%|▉         | 891/9822 [23:43<3:34:31,  1.44s/it]  9%|▉         | 892/9822 [23:44<3:37:55,  1.46s/it]  9%|▉         | 893/9822 [23:46<3:36:55,  1.46s/it]  9%|▉         | 894/9822 [23:47<3:35:49,  1.45s/it]  9%|▉         | 895/9822 [23:48<3:34:37,  1.44s/it]  9%|▉         | 896/9822 [23:50<3:34:32,  1.44s/it]  9%|▉         | 897/9822 [23:51<3:35:00,  1.45s/it]  9%|▉         | 898/9822 [23:53<3:34:42,  1.44s/it]  9%|▉         | 899/9822 [23:54<3:34:18,  1.44s/it]  9%|▉         | 900/9822 [23:56<3:33:41,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:11:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:11:42 - INFO - __main__ - ***** test Results*****
01/07/2024 22:11:42 - INFO - __main__ -   Training step = 900
01/07/2024 22:11:42 - INFO - __main__ -  test_accuracy:0.8275988286969254 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:11:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:11:49 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:11:49 - INFO - __main__ -   Training step = 900
01/07/2024 22:11:49 - INFO - __main__ -  eval_accuracy:0.8213108751373124 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8275357012083486}
test:
{'accuracy': 0.8382137628111274}
01/07/2024 22:11:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:11:53 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:11:53 - INFO - __main__ -   Training step = 900
01/07/2024 22:11:53 - INFO - __main__ -  eval_accuracy:0.8725741486634933 
  9%|▉         | 901/9822 [24:14<16:06:45,  6.50s/it]  9%|▉         | 902/9822 [24:15<12:21:05,  4.98s/it]  9%|▉         | 903/9822 [24:17<9:43:09,  3.92s/it]   9%|▉         | 904/9822 [24:18<7:52:40,  3.18s/it]  9%|▉         | 905/9822 [24:20<6:35:19,  2.66s/it]  9%|▉         | 906/9822 [24:21<5:40:17,  2.29s/it]  9%|▉         | 907/9822 [24:23<5:02:14,  2.03s/it]  9%|▉         | 908/9822 [24:24<4:35:22,  1.85s/it]  9%|▉         | 909/9822 [24:25<4:17:15,  1.73s/it]  9%|▉         | 910/9822 [24:27<4:03:32,  1.64s/it]  9%|▉         | 911/9822 [24:28<3:53:54,  1.57s/it]  9%|▉         | 912/9822 [24:30<3:47:33,  1.53s/it]  9%|▉         | 913/9822 [24:31<3:43:02,  1.50s/it]  9%|▉         | 914/9822 [24:33<3:40:56,  1.49s/it]  9%|▉         | 915/9822 [24:34<3:40:06,  1.48s/it]  9%|▉         | 916/9822 [24:36<3:38:15,  1.47s/it]  9%|▉         | 917/9822 [24:37<3:36:21,  1.46s/it]  9%|▉         | 918/9822 [24:38<3:35:23,  1.45s/it]  9%|▉         | 919/9822 [24:40<3:34:44,  1.45s/it]  9%|▉         | 920/9822 [24:41<3:33:39,  1.44s/it]  9%|▉         | 921/9822 [24:43<3:33:03,  1.44s/it]  9%|▉         | 922/9822 [24:44<3:32:50,  1.43s/it]  9%|▉         | 923/9822 [24:46<3:33:18,  1.44s/it]  9%|▉         | 924/9822 [24:47<3:33:08,  1.44s/it]  9%|▉         | 925/9822 [24:48<3:34:17,  1.45s/it]  9%|▉         | 926/9822 [24:50<3:33:34,  1.44s/it]  9%|▉         | 927/9822 [24:51<3:33:00,  1.44s/it]  9%|▉         | 928/9822 [24:53<3:33:03,  1.44s/it]  9%|▉         | 929/9822 [24:54<3:33:01,  1.44s/it]  9%|▉         | 930/9822 [24:56<3:36:11,  1.46s/it]  9%|▉         | 931/9822 [24:57<3:34:41,  1.45s/it]  9%|▉         | 932/9822 [24:59<3:34:09,  1.45s/it]  9%|▉         | 933/9822 [25:00<3:33:35,  1.44s/it] 10%|▉         | 934/9822 [25:01<3:33:37,  1.44s/it] 10%|▉         | 935/9822 [25:03<3:33:58,  1.44s/it] 10%|▉         | 936/9822 [25:04<3:33:17,  1.44s/it] 10%|▉         | 937/9822 [25:06<3:32:54,  1.44s/it] 10%|▉         | 938/9822 [25:07<3:32:33,  1.44s/it] 10%|▉         | 939/9822 [25:09<3:32:03,  1.43s/it] 10%|▉         | 940/9822 [25:10<3:32:57,  1.44s/it] 10%|▉         | 941/9822 [25:12<3:33:18,  1.44s/it] 10%|▉         | 942/9822 [25:13<3:32:32,  1.44s/it] 10%|▉         | 943/9822 [25:14<3:32:38,  1.44s/it] 10%|▉         | 944/9822 [25:16<3:32:04,  1.43s/it] 10%|▉         | 945/9822 [25:17<3:32:25,  1.44s/it] 10%|▉         | 946/9822 [25:19<3:30:01,  1.42s/it] 10%|▉         | 947/9822 [25:20<3:30:35,  1.42s/it] 10%|▉         | 948/9822 [25:22<3:30:32,  1.42s/it] 10%|▉         | 949/9822 [25:23<3:31:14,  1.43s/it] 10%|▉         | 950/9822 [25:24<3:31:33,  1.43s/it] 10%|▉         | 951/9822 [25:26<3:31:04,  1.43s/it] 10%|▉         | 952/9822 [25:27<3:31:13,  1.43s/it] 10%|▉         | 953/9822 [25:29<3:31:19,  1.43s/it] 10%|▉         | 954/9822 [25:30<3:31:08,  1.43s/it] 10%|▉         | 955/9822 [25:32<3:31:05,  1.43s/it] 10%|▉         | 956/9822 [25:33<3:31:12,  1.43s/it] 10%|▉         | 957/9822 [25:34<3:31:18,  1.43s/it] 10%|▉         | 958/9822 [25:36<3:31:24,  1.43s/it] 10%|▉         | 959/9822 [25:37<3:31:07,  1.43s/it] 10%|▉         | 960/9822 [25:39<3:31:19,  1.43s/it] 10%|▉         | 961/9822 [25:40<3:31:50,  1.43s/it] 10%|▉         | 962/9822 [25:42<3:36:56,  1.47s/it] 10%|▉         | 963/9822 [25:43<3:34:49,  1.45s/it] 10%|▉         | 964/9822 [25:45<3:34:07,  1.45s/it] 10%|▉         | 965/9822 [25:46<3:33:06,  1.44s/it] 10%|▉         | 966/9822 [25:47<3:32:23,  1.44s/it] 10%|▉         | 967/9822 [25:49<3:32:15,  1.44s/it] 10%|▉         | 968/9822 [25:50<3:31:42,  1.43s/it] 10%|▉         | 969/9822 [25:52<3:31:36,  1.43s/it] 10%|▉         | 970/9822 [25:53<3:31:40,  1.43s/it] 10%|▉         | 971/9822 [25:55<3:31:34,  1.43s/it] 10%|▉         | 972/9822 [25:56<3:31:05,  1.43s/it] 10%|▉         | 973/9822 [25:57<3:30:40,  1.43s/it] 10%|▉         | 974/9822 [25:59<3:31:01,  1.43s/it] 10%|▉         | 975/9822 [26:00<3:32:57,  1.44s/it] 10%|▉         | 976/9822 [26:02<3:32:08,  1.44s/it] 10%|▉         | 977/9822 [26:03<3:31:30,  1.43s/it] 10%|▉         | 978/9822 [26:05<3:32:52,  1.44s/it] 10%|▉         | 979/9822 [26:06<3:34:12,  1.45s/it] 10%|▉         | 980/9822 [26:08<3:33:45,  1.45s/it] 10%|▉         | 981/9822 [26:09<3:33:03,  1.45s/it] 10%|▉         | 982/9822 [26:10<3:32:50,  1.44s/it] 10%|█         | 983/9822 [26:12<3:32:13,  1.44s/it] 10%|█         | 984/9822 [26:13<3:31:16,  1.43s/it] 10%|█         | 985/9822 [26:15<3:30:40,  1.43s/it] 10%|█         | 986/9822 [26:16<3:30:29,  1.43s/it] 10%|█         | 987/9822 [26:18<3:35:12,  1.46s/it] 10%|█         | 988/9822 [26:19<3:36:24,  1.47s/it] 10%|█         | 989/9822 [26:21<3:35:17,  1.46s/it] 10%|█         | 990/9822 [26:22<3:33:50,  1.45s/it] 10%|█         | 991/9822 [26:23<3:32:28,  1.44s/it] 10%|█         | 992/9822 [26:25<3:32:05,  1.44s/it] 10%|█         | 993/9822 [26:26<3:31:17,  1.44s/it] 10%|█         | 994/9822 [26:28<3:30:54,  1.43s/it] 10%|█         | 995/9822 [26:29<3:30:34,  1.43s/it] 10%|█         | 996/9822 [26:31<3:30:30,  1.43s/it] 10%|█         | 997/9822 [26:32<3:30:55,  1.43s/it] 10%|█         | 998/9822 [26:33<3:30:44,  1.43s/it] 10%|█         | 999/9822 [26:35<3:31:13,  1.44s/it] 10%|█         | 1000/9822 [26:36<3:31:05,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:14:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:14:23 - INFO - __main__ - ***** test Results*****
01/07/2024 22:14:23 - INFO - __main__ -   Training step = 1000
01/07/2024 22:14:23 - INFO - __main__ -  test_accuracy:0.8433382137628112 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:14:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:14:29 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:14:29 - INFO - __main__ -   Training step = 1000
01/07/2024 22:14:29 - INFO - __main__ -  eval_accuracy:0.8304650311241304 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:14:29,746 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:14:29,746 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:14:29,786 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:14:31,388 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8304650311241304}
test:
{'accuracy': 0.8433382137628112}
01/07/2024 22:14:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:14:36 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:14:36 - INFO - __main__ -   Training step = 1000
01/07/2024 22:14:36 - INFO - __main__ -  eval_accuracy:0.8766019772976932 
 10%|█         | 1001/9822 [26:56<17:08:33,  7.00s/it] 10%|█         | 1002/9822 [26:58<13:03:09,  5.33s/it] 10%|█         | 1003/9822 [26:59<10:11:17,  4.16s/it] 10%|█         | 1004/9822 [27:01<8:11:23,  3.34s/it]  10%|█         | 1005/9822 [27:02<6:47:02,  2.77s/it] 10%|█         | 1006/9822 [27:03<5:48:44,  2.37s/it] 10%|█         | 1007/9822 [27:05<5:07:17,  2.09s/it] 10%|█         | 1008/9822 [27:06<4:38:18,  1.89s/it] 10%|█         | 1009/9822 [27:08<4:18:05,  1.76s/it] 10%|█         | 1010/9822 [27:09<4:04:16,  1.66s/it] 10%|█         | 1011/9822 [27:11<3:54:08,  1.59s/it] 10%|█         | 1012/9822 [27:12<3:46:59,  1.55s/it] 10%|█         | 1013/9822 [27:14<3:41:33,  1.51s/it] 10%|█         | 1014/9822 [27:15<3:38:04,  1.49s/it] 10%|█         | 1015/9822 [27:16<3:35:39,  1.47s/it] 10%|█         | 1016/9822 [27:18<3:34:03,  1.46s/it] 10%|█         | 1017/9822 [27:19<3:32:34,  1.45s/it] 10%|█         | 1018/9822 [27:21<3:35:06,  1.47s/it] 10%|█         | 1019/9822 [27:22<3:33:24,  1.45s/it] 10%|█         | 1020/9822 [27:24<3:32:22,  1.45s/it] 10%|█         | 1021/9822 [27:25<3:31:47,  1.44s/it] 10%|█         | 1022/9822 [27:26<3:31:48,  1.44s/it] 10%|█         | 1023/9822 [27:28<3:30:55,  1.44s/it] 10%|█         | 1024/9822 [27:29<3:30:38,  1.44s/it] 10%|█         | 1025/9822 [27:31<3:30:41,  1.44s/it] 10%|█         | 1026/9822 [27:32<3:31:01,  1.44s/it] 10%|█         | 1027/9822 [27:34<3:30:03,  1.43s/it] 10%|█         | 1028/9822 [27:35<3:30:28,  1.44s/it] 10%|█         | 1029/9822 [27:37<3:30:03,  1.43s/it] 10%|█         | 1030/9822 [27:38<3:29:31,  1.43s/it] 10%|█         | 1031/9822 [27:39<3:29:08,  1.43s/it] 11%|█         | 1032/9822 [27:41<3:27:06,  1.41s/it] 11%|█         | 1033/9822 [27:42<3:28:05,  1.42s/it] 11%|█         | 1034/9822 [27:44<3:29:00,  1.43s/it] 11%|█         | 1035/9822 [27:45<3:29:48,  1.43s/it] 11%|█         | 1036/9822 [27:47<3:29:57,  1.43s/it] 11%|█         | 1037/9822 [27:48<3:29:25,  1.43s/it] 11%|█         | 1038/9822 [27:49<3:29:07,  1.43s/it] 11%|█         | 1039/9822 [27:51<3:28:57,  1.43s/it] 11%|█         | 1040/9822 [27:52<3:28:52,  1.43s/it] 11%|█         | 1041/9822 [27:54<3:28:52,  1.43s/it] 11%|█         | 1042/9822 [27:55<3:30:51,  1.44s/it] 11%|█         | 1043/9822 [27:57<3:34:18,  1.46s/it] 11%|█         | 1044/9822 [27:58<3:32:23,  1.45s/it] 11%|█         | 1045/9822 [27:59<3:31:02,  1.44s/it] 11%|█         | 1046/9822 [28:01<3:30:16,  1.44s/it] 11%|█         | 1047/9822 [28:02<3:30:08,  1.44s/it] 11%|█         | 1048/9822 [28:04<3:29:39,  1.43s/it] 11%|█         | 1049/9822 [28:05<3:29:56,  1.44s/it] 11%|█         | 1050/9822 [28:07<3:29:37,  1.43s/it] 11%|█         | 1051/9822 [28:08<3:29:00,  1.43s/it] 11%|█         | 1052/9822 [28:09<3:28:41,  1.43s/it] 11%|█         | 1053/9822 [28:11<3:29:00,  1.43s/it] 11%|█         | 1054/9822 [28:12<3:28:39,  1.43s/it] 11%|█         | 1055/9822 [28:14<3:28:23,  1.43s/it] 11%|█         | 1056/9822 [28:15<3:29:26,  1.43s/it] 11%|█         | 1057/9822 [28:17<3:29:55,  1.44s/it] 11%|█         | 1058/9822 [28:18<3:29:12,  1.43s/it] 11%|█         | 1059/9822 [28:20<3:28:56,  1.43s/it] 11%|█         | 1060/9822 [28:21<3:29:50,  1.44s/it] 11%|█         | 1061/9822 [28:22<3:30:58,  1.44s/it] 11%|█         | 1062/9822 [28:24<3:30:30,  1.44s/it] 11%|█         | 1063/9822 [28:25<3:29:51,  1.44s/it] 11%|█         | 1064/9822 [28:27<3:29:48,  1.44s/it] 11%|█         | 1065/9822 [28:28<3:30:05,  1.44s/it] 11%|█         | 1066/9822 [28:30<3:29:46,  1.44s/it] 11%|█         | 1067/9822 [28:31<3:29:12,  1.43s/it] 11%|█         | 1068/9822 [28:32<3:28:46,  1.43s/it] 11%|█         | 1069/9822 [28:34<3:28:47,  1.43s/it] 11%|█         | 1070/9822 [28:35<3:29:03,  1.43s/it] 11%|█         | 1071/9822 [28:37<3:29:50,  1.44s/it] 11%|█         | 1072/9822 [28:38<3:31:28,  1.45s/it] 11%|█         | 1073/9822 [28:40<3:30:26,  1.44s/it] 11%|█         | 1074/9822 [28:41<3:29:57,  1.44s/it] 11%|█         | 1075/9822 [28:43<3:33:02,  1.46s/it] 11%|█         | 1076/9822 [28:44<3:31:35,  1.45s/it] 11%|█         | 1077/9822 [28:45<3:30:32,  1.44s/it] 11%|█         | 1078/9822 [28:47<3:29:52,  1.44s/it] 11%|█         | 1079/9822 [28:48<3:32:01,  1.46s/it] 11%|█         | 1080/9822 [28:50<3:30:45,  1.45s/it] 11%|█         | 1081/9822 [28:51<3:31:46,  1.45s/it] 11%|█         | 1082/9822 [28:53<3:30:32,  1.45s/it] 11%|█         | 1083/9822 [28:54<3:29:42,  1.44s/it] 11%|█         | 1084/9822 [28:56<3:29:03,  1.44s/it] 11%|█         | 1085/9822 [28:57<3:29:20,  1.44s/it] 11%|█         | 1086/9822 [28:58<3:28:35,  1.43s/it] 11%|█         | 1087/9822 [29:00<3:28:12,  1.43s/it] 11%|█         | 1088/9822 [29:01<3:29:04,  1.44s/it] 11%|█         | 1089/9822 [29:03<3:29:45,  1.44s/it] 11%|█         | 1090/9822 [29:04<3:28:52,  1.44s/it] 11%|█         | 1091/9822 [29:06<3:29:07,  1.44s/it] 11%|█         | 1092/9822 [29:07<3:28:59,  1.44s/it] 11%|█         | 1093/9822 [29:08<3:28:38,  1.43s/it] 11%|█         | 1094/9822 [29:10<3:28:00,  1.43s/it] 11%|█         | 1095/9822 [29:11<3:28:47,  1.44s/it] 11%|█         | 1096/9822 [29:13<3:28:00,  1.43s/it] 11%|█         | 1097/9822 [29:14<3:28:08,  1.43s/it] 11%|█         | 1098/9822 [29:16<3:28:17,  1.43s/it] 11%|█         | 1099/9822 [29:17<3:28:13,  1.43s/it] 11%|█         | 1100/9822 [29:19<3:28:32,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:17:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:17:05 - INFO - __main__ - ***** test Results*****
01/07/2024 22:17:05 - INFO - __main__ -   Training step = 1100
01/07/2024 22:17:05 - INFO - __main__ -  test_accuracy:0.8184480234260615 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:17:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:17:11 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:17:11 - INFO - __main__ -   Training step = 1100
01/07/2024 22:17:11 - INFO - __main__ -  eval_accuracy:0.8246063712925669 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8304650311241304}
test:
{'accuracy': 0.8433382137628112}
01/07/2024 22:17:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:17:16 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:17:16 - INFO - __main__ -   Training step = 1100
01/07/2024 22:17:16 - INFO - __main__ -  eval_accuracy:0.872940314902966 
 11%|█         | 1101/9822 [29:37<15:46:35,  6.51s/it] 11%|█         | 1102/9822 [29:38<12:04:53,  4.99s/it] 11%|█         | 1103/9822 [29:40<9:29:14,  3.92s/it]  11%|█         | 1104/9822 [29:41<7:40:16,  3.17s/it] 11%|█▏        | 1105/9822 [29:43<6:24:37,  2.65s/it] 11%|█▏        | 1106/9822 [29:44<5:31:37,  2.28s/it] 11%|█▏        | 1107/9822 [29:45<4:54:32,  2.03s/it] 11%|█▏        | 1108/9822 [29:47<4:31:59,  1.87s/it] 11%|█▏        | 1109/9822 [29:48<4:14:20,  1.75s/it] 11%|█▏        | 1110/9822 [29:50<3:59:47,  1.65s/it] 11%|█▏        | 1111/9822 [29:51<3:50:08,  1.59s/it] 11%|█▏        | 1112/9822 [29:53<3:43:35,  1.54s/it] 11%|█▏        | 1113/9822 [29:54<3:40:15,  1.52s/it] 11%|█▏        | 1114/9822 [29:56<3:36:31,  1.49s/it] 11%|█▏        | 1115/9822 [29:57<3:34:02,  1.47s/it] 11%|█▏        | 1116/9822 [29:58<3:32:04,  1.46s/it] 11%|█▏        | 1117/9822 [30:00<3:30:54,  1.45s/it] 11%|█▏        | 1118/9822 [30:01<3:27:33,  1.43s/it] 11%|█▏        | 1119/9822 [30:03<3:27:50,  1.43s/it] 11%|█▏        | 1120/9822 [30:04<3:27:36,  1.43s/it] 11%|█▏        | 1121/9822 [30:06<3:27:19,  1.43s/it] 11%|█▏        | 1122/9822 [30:07<3:27:24,  1.43s/it] 11%|█▏        | 1123/9822 [30:08<3:27:23,  1.43s/it] 11%|█▏        | 1124/9822 [30:10<3:27:24,  1.43s/it] 11%|█▏        | 1125/9822 [30:11<3:27:38,  1.43s/it] 11%|█▏        | 1126/9822 [30:13<3:28:21,  1.44s/it] 11%|█▏        | 1127/9822 [30:14<3:28:30,  1.44s/it] 11%|█▏        | 1128/9822 [30:16<3:28:23,  1.44s/it] 11%|█▏        | 1129/9822 [30:17<3:28:20,  1.44s/it] 12%|█▏        | 1130/9822 [30:18<3:27:55,  1.44s/it] 12%|█▏        | 1131/9822 [30:20<3:27:10,  1.43s/it] 12%|█▏        | 1132/9822 [30:21<3:27:11,  1.43s/it] 12%|█▏        | 1133/9822 [30:23<3:27:27,  1.43s/it] 12%|█▏        | 1134/9822 [30:24<3:28:05,  1.44s/it] 12%|█▏        | 1135/9822 [30:26<3:27:22,  1.43s/it] 12%|█▏        | 1136/9822 [30:27<3:26:58,  1.43s/it] 12%|█▏        | 1137/9822 [30:29<3:27:09,  1.43s/it] 12%|█▏        | 1138/9822 [30:30<3:27:07,  1.43s/it] 12%|█▏        | 1139/9822 [30:31<3:30:25,  1.45s/it] 12%|█▏        | 1140/9822 [30:33<3:29:19,  1.45s/it] 12%|█▏        | 1141/9822 [30:34<3:28:25,  1.44s/it] 12%|█▏        | 1142/9822 [30:36<3:28:04,  1.44s/it] 12%|█▏        | 1143/9822 [30:37<3:27:48,  1.44s/it] 12%|█▏        | 1144/9822 [30:39<3:27:19,  1.43s/it] 12%|█▏        | 1145/9822 [30:40<3:27:17,  1.43s/it] 12%|█▏        | 1146/9822 [30:41<3:27:10,  1.43s/it] 12%|█▏        | 1147/9822 [30:43<3:26:53,  1.43s/it] 12%|█▏        | 1148/9822 [30:44<3:27:11,  1.43s/it] 12%|█▏        | 1149/9822 [30:46<3:27:15,  1.43s/it] 12%|█▏        | 1150/9822 [30:47<3:27:40,  1.44s/it] 12%|█▏        | 1151/9822 [30:49<3:27:38,  1.44s/it] 12%|█▏        | 1152/9822 [30:50<3:27:49,  1.44s/it] 12%|█▏        | 1153/9822 [30:52<3:27:21,  1.44s/it] 12%|█▏        | 1154/9822 [30:53<3:27:37,  1.44s/it] 12%|█▏        | 1155/9822 [30:54<3:27:04,  1.43s/it] 12%|█▏        | 1156/9822 [30:56<3:27:12,  1.43s/it] 12%|█▏        | 1157/9822 [30:57<3:27:26,  1.44s/it] 12%|█▏        | 1158/9822 [30:59<3:27:14,  1.44s/it] 12%|█▏        | 1159/9822 [31:00<3:27:11,  1.44s/it] 12%|█▏        | 1160/9822 [31:02<3:27:35,  1.44s/it] 12%|█▏        | 1161/9822 [31:03<3:27:09,  1.44s/it] 12%|█▏        | 1162/9822 [31:04<3:26:39,  1.43s/it] 12%|█▏        | 1163/9822 [31:06<3:26:37,  1.43s/it] 12%|█▏        | 1164/9822 [31:07<3:29:33,  1.45s/it] 12%|█▏        | 1165/9822 [31:09<3:29:38,  1.45s/it] 12%|█▏        | 1166/9822 [31:10<3:29:05,  1.45s/it] 12%|█▏        | 1167/9822 [31:12<3:28:11,  1.44s/it] 12%|█▏        | 1168/9822 [31:13<3:27:44,  1.44s/it] 12%|█▏        | 1169/9822 [31:15<3:27:21,  1.44s/it] 12%|█▏        | 1170/9822 [31:16<3:26:57,  1.44s/it] 12%|█▏        | 1171/9822 [31:17<3:26:26,  1.43s/it] 12%|█▏        | 1172/9822 [31:19<3:26:15,  1.43s/it] 12%|█▏        | 1173/9822 [31:20<3:26:32,  1.43s/it] 12%|█▏        | 1174/9822 [31:22<3:26:21,  1.43s/it] 12%|█▏        | 1175/9822 [31:23<3:26:21,  1.43s/it] 12%|█▏        | 1176/9822 [31:25<3:26:10,  1.43s/it] 12%|█▏        | 1177/9822 [31:26<3:26:12,  1.43s/it] 12%|█▏        | 1178/9822 [31:27<3:26:12,  1.43s/it] 12%|█▏        | 1179/9822 [31:29<3:26:41,  1.43s/it] 12%|█▏        | 1180/9822 [31:30<3:27:28,  1.44s/it] 12%|█▏        | 1181/9822 [31:32<3:27:41,  1.44s/it] 12%|█▏        | 1182/9822 [31:33<3:27:19,  1.44s/it] 12%|█▏        | 1183/9822 [31:35<3:27:56,  1.44s/it] 12%|█▏        | 1184/9822 [31:36<3:27:32,  1.44s/it] 12%|█▏        | 1185/9822 [31:38<3:27:30,  1.44s/it] 12%|█▏        | 1186/9822 [31:39<3:27:04,  1.44s/it] 12%|█▏        | 1187/9822 [31:40<3:27:19,  1.44s/it] 12%|█▏        | 1188/9822 [31:42<3:26:46,  1.44s/it] 12%|█▏        | 1189/9822 [31:43<3:26:45,  1.44s/it] 12%|█▏        | 1190/9822 [31:45<3:26:42,  1.44s/it] 12%|█▏        | 1191/9822 [31:46<3:26:51,  1.44s/it] 12%|█▏        | 1192/9822 [31:48<3:26:43,  1.44s/it] 12%|█▏        | 1193/9822 [31:49<3:26:11,  1.43s/it] 12%|█▏        | 1194/9822 [31:50<3:26:19,  1.43s/it] 12%|█▏        | 1195/9822 [31:52<3:26:22,  1.44s/it] 12%|█▏        | 1196/9822 [31:53<3:30:34,  1.46s/it] 12%|█▏        | 1197/9822 [31:55<3:31:19,  1.47s/it] 12%|█▏        | 1198/9822 [31:56<3:30:10,  1.46s/it] 12%|█▏        | 1199/9822 [31:58<3:28:50,  1.45s/it] 12%|█▏        | 1200/9822 [31:59<3:27:55,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:19:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:19:46 - INFO - __main__ - ***** test Results*****
01/07/2024 22:19:46 - INFO - __main__ -   Training step = 1200
01/07/2024 22:19:46 - INFO - __main__ -  test_accuracy:0.8400439238653001 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:19:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:19:52 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:19:52 - INFO - __main__ -   Training step = 1200
01/07/2024 22:19:52 - INFO - __main__ -  eval_accuracy:0.8315635298425486 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:19:52,618 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:19:52,618 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:19:52,655 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:19:54,231 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8315635298425486}
test:
{'accuracy': 0.8400439238653001}
01/07/2024 22:19:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:19:58 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:19:58 - INFO - __main__ -   Training step = 1200
01/07/2024 22:19:58 - INFO - __main__ -  eval_accuracy:0.8864884657634566 
 12%|█▏        | 1201/9822 [32:19<16:46:22,  7.00s/it] 12%|█▏        | 1202/9822 [32:21<12:46:15,  5.33s/it] 12%|█▏        | 1203/9822 [32:22<9:58:00,  4.16s/it]  12%|█▏        | 1204/9822 [32:23<7:58:07,  3.33s/it] 12%|█▏        | 1205/9822 [32:25<6:36:04,  2.76s/it] 12%|█▏        | 1206/9822 [32:26<5:38:40,  2.36s/it] 12%|█▏        | 1207/9822 [32:28<4:58:33,  2.08s/it] 12%|█▏        | 1208/9822 [32:29<4:30:07,  1.88s/it] 12%|█▏        | 1209/9822 [32:31<4:11:22,  1.75s/it] 12%|█▏        | 1210/9822 [32:32<3:57:17,  1.65s/it] 12%|█▏        | 1211/9822 [32:33<3:47:21,  1.58s/it] 12%|█▏        | 1212/9822 [32:35<3:40:12,  1.53s/it] 12%|█▏        | 1213/9822 [32:36<3:36:40,  1.51s/it] 12%|█▏        | 1214/9822 [32:38<3:33:10,  1.49s/it] 12%|█▏        | 1215/9822 [32:39<3:31:17,  1.47s/it] 12%|█▏        | 1216/9822 [32:41<3:29:06,  1.46s/it] 12%|█▏        | 1217/9822 [32:42<3:27:50,  1.45s/it] 12%|█▏        | 1218/9822 [32:43<3:26:32,  1.44s/it] 12%|█▏        | 1219/9822 [32:45<3:27:34,  1.45s/it] 12%|█▏        | 1220/9822 [32:46<3:28:26,  1.45s/it] 12%|█▏        | 1221/9822 [32:48<3:27:35,  1.45s/it] 12%|█▏        | 1222/9822 [32:49<3:27:17,  1.45s/it] 12%|█▏        | 1223/9822 [32:51<3:30:32,  1.47s/it] 12%|█▏        | 1224/9822 [32:52<3:28:41,  1.46s/it] 12%|█▏        | 1225/9822 [32:54<3:27:41,  1.45s/it] 12%|█▏        | 1226/9822 [32:55<3:26:50,  1.44s/it] 12%|█▏        | 1227/9822 [32:56<3:25:54,  1.44s/it] 13%|█▎        | 1228/9822 [32:58<3:25:50,  1.44s/it] 13%|█▎        | 1229/9822 [32:59<3:25:55,  1.44s/it] 13%|█▎        | 1230/9822 [33:01<3:26:00,  1.44s/it] 13%|█▎        | 1231/9822 [33:02<3:25:29,  1.44s/it] 13%|█▎        | 1232/9822 [33:04<3:25:25,  1.43s/it] 13%|█▎        | 1233/9822 [33:05<3:25:47,  1.44s/it] 13%|█▎        | 1234/9822 [33:07<3:25:54,  1.44s/it] 13%|█▎        | 1235/9822 [33:08<3:25:26,  1.44s/it] 13%|█▎        | 1236/9822 [33:09<3:24:53,  1.43s/it] 13%|█▎        | 1237/9822 [33:11<3:25:17,  1.43s/it] 13%|█▎        | 1238/9822 [33:12<3:24:59,  1.43s/it] 13%|█▎        | 1239/9822 [33:14<3:25:19,  1.44s/it] 13%|█▎        | 1240/9822 [33:15<3:25:22,  1.44s/it] 13%|█▎        | 1241/9822 [33:17<3:25:15,  1.44s/it] 13%|█▎        | 1242/9822 [33:18<3:25:42,  1.44s/it] 13%|█▎        | 1243/9822 [33:19<3:25:56,  1.44s/it] 13%|█▎        | 1244/9822 [33:21<3:25:15,  1.44s/it] 13%|█▎        | 1245/9822 [33:22<3:24:53,  1.43s/it] 13%|█▎        | 1246/9822 [33:24<3:24:38,  1.43s/it] 13%|█▎        | 1247/9822 [33:25<3:24:29,  1.43s/it] 13%|█▎        | 1248/9822 [33:27<3:24:02,  1.43s/it] 13%|█▎        | 1249/9822 [33:28<3:23:52,  1.43s/it] 13%|█▎        | 1250/9822 [33:29<3:23:58,  1.43s/it] 13%|█▎        | 1251/9822 [33:31<3:24:28,  1.43s/it] 13%|█▎        | 1252/9822 [33:32<3:24:51,  1.43s/it] 13%|█▎        | 1253/9822 [33:34<3:25:43,  1.44s/it] 13%|█▎        | 1254/9822 [33:35<3:25:41,  1.44s/it] 13%|█▎        | 1255/9822 [33:37<3:28:44,  1.46s/it] 13%|█▎        | 1256/9822 [33:38<3:27:31,  1.45s/it] 13%|█▎        | 1257/9822 [33:40<3:25:59,  1.44s/it] 13%|█▎        | 1258/9822 [33:41<3:25:35,  1.44s/it] 13%|█▎        | 1259/9822 [33:42<3:25:18,  1.44s/it] 13%|█▎        | 1260/9822 [33:44<3:24:37,  1.43s/it] 13%|█▎        | 1261/9822 [33:45<3:24:25,  1.43s/it] 13%|█▎        | 1262/9822 [33:47<3:24:04,  1.43s/it] 13%|█▎        | 1263/9822 [33:48<3:24:03,  1.43s/it] 13%|█▎        | 1264/9822 [33:50<3:23:33,  1.43s/it] 13%|█▎        | 1265/9822 [33:51<3:23:45,  1.43s/it] 13%|█▎        | 1266/9822 [33:52<3:23:23,  1.43s/it] 13%|█▎        | 1267/9822 [33:54<3:23:53,  1.43s/it] 13%|█▎        | 1268/9822 [33:55<3:24:15,  1.43s/it] 13%|█▎        | 1269/9822 [33:57<3:23:40,  1.43s/it] 13%|█▎        | 1270/9822 [33:58<3:23:28,  1.43s/it] 13%|█▎        | 1271/9822 [34:00<3:23:45,  1.43s/it] 13%|█▎        | 1272/9822 [34:01<3:23:39,  1.43s/it] 13%|█▎        | 1273/9822 [34:02<3:23:47,  1.43s/it] 13%|█▎        | 1274/9822 [34:04<3:24:29,  1.44s/it] 13%|█▎        | 1275/9822 [34:05<3:24:04,  1.43s/it] 13%|█▎        | 1276/9822 [34:07<3:23:43,  1.43s/it] 13%|█▎        | 1277/9822 [34:08<3:23:26,  1.43s/it] 13%|█▎        | 1278/9822 [34:10<3:23:31,  1.43s/it] 13%|█▎        | 1279/9822 [34:11<3:24:03,  1.43s/it] 13%|█▎        | 1280/9822 [34:12<3:23:42,  1.43s/it] 13%|█▎        | 1281/9822 [34:14<3:23:19,  1.43s/it] 13%|█▎        | 1282/9822 [34:15<3:22:55,  1.43s/it] 13%|█▎        | 1283/9822 [34:17<3:23:07,  1.43s/it] 13%|█▎        | 1284/9822 [34:18<3:23:08,  1.43s/it] 13%|█▎        | 1285/9822 [34:20<3:23:18,  1.43s/it] 13%|█▎        | 1286/9822 [34:21<3:23:47,  1.43s/it] 13%|█▎        | 1287/9822 [34:23<3:27:29,  1.46s/it] 13%|█▎        | 1288/9822 [34:24<3:26:09,  1.45s/it] 13%|█▎        | 1289/9822 [34:25<3:25:05,  1.44s/it] 13%|█▎        | 1290/9822 [34:27<3:22:31,  1.42s/it] 13%|█▎        | 1291/9822 [34:28<3:23:01,  1.43s/it] 13%|█▎        | 1292/9822 [34:30<3:23:03,  1.43s/it] 13%|█▎        | 1293/9822 [34:31<3:22:48,  1.43s/it] 13%|█▎        | 1294/9822 [34:33<3:22:50,  1.43s/it] 13%|█▎        | 1295/9822 [34:34<3:22:43,  1.43s/it] 13%|█▎        | 1296/9822 [34:35<3:23:09,  1.43s/it] 13%|█▎        | 1297/9822 [34:37<3:23:03,  1.43s/it] 13%|█▎        | 1298/9822 [34:38<3:22:53,  1.43s/it] 13%|█▎        | 1299/9822 [34:40<3:23:30,  1.43s/it] 13%|█▎        | 1300/9822 [34:41<3:23:48,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:22:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:22:28 - INFO - __main__ - ***** test Results*****
01/07/2024 22:22:28 - INFO - __main__ -   Training step = 1300
01/07/2024 22:22:28 - INFO - __main__ -  test_accuracy:0.835285505124451 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:22:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:22:34 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:22:34 - INFO - __main__ -   Training step = 1300
01/07/2024 22:22:34 - INFO - __main__ -  eval_accuracy:0.8333943610399122 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:22:34,554 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:22:34,554 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:22:34,589 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:22:36,267 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8333943610399122}
test:
{'accuracy': 0.835285505124451}
01/07/2024 22:22:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:22:40 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:22:40 - INFO - __main__ -   Training step = 1300
01/07/2024 22:22:40 - INFO - __main__ -  eval_accuracy:0.8861222995239839 
 13%|█▎        | 1301/9822 [35:01<16:37:51,  7.03s/it] 13%|█▎        | 1302/9822 [35:03<12:39:12,  5.35s/it] 13%|█▎        | 1303/9822 [35:04<9:52:33,  4.17s/it]  13%|█▎        | 1304/9822 [35:05<7:55:21,  3.35s/it] 13%|█▎        | 1305/9822 [35:07<6:33:16,  2.77s/it] 13%|█▎        | 1306/9822 [35:08<5:35:46,  2.37s/it] 13%|█▎        | 1307/9822 [35:10<4:56:16,  2.09s/it] 13%|█▎        | 1308/9822 [35:11<4:28:26,  1.89s/it] 13%|█▎        | 1309/9822 [35:13<4:08:54,  1.75s/it] 13%|█▎        | 1310/9822 [35:14<3:54:55,  1.66s/it] 13%|█▎        | 1311/9822 [35:15<3:45:26,  1.59s/it] 13%|█▎        | 1312/9822 [35:17<3:39:02,  1.54s/it] 13%|█▎        | 1313/9822 [35:18<3:34:59,  1.52s/it] 13%|█▎        | 1314/9822 [35:20<3:31:21,  1.49s/it] 13%|█▎        | 1315/9822 [35:21<3:28:46,  1.47s/it] 13%|█▎        | 1316/9822 [35:23<3:26:59,  1.46s/it] 13%|█▎        | 1317/9822 [35:24<3:25:32,  1.45s/it] 13%|█▎        | 1318/9822 [35:26<3:28:14,  1.47s/it] 13%|█▎        | 1319/9822 [35:27<3:26:25,  1.46s/it] 13%|█▎        | 1320/9822 [35:28<3:25:28,  1.45s/it] 13%|█▎        | 1321/9822 [35:30<3:24:25,  1.44s/it] 13%|█▎        | 1322/9822 [35:31<3:23:50,  1.44s/it] 13%|█▎        | 1323/9822 [35:33<3:23:31,  1.44s/it] 13%|█▎        | 1324/9822 [35:34<3:23:46,  1.44s/it] 13%|█▎        | 1325/9822 [35:36<3:23:47,  1.44s/it] 14%|█▎        | 1326/9822 [35:37<3:23:34,  1.44s/it] 14%|█▎        | 1327/9822 [35:39<3:23:21,  1.44s/it] 14%|█▎        | 1328/9822 [35:40<3:22:56,  1.43s/it] 14%|█▎        | 1329/9822 [35:41<3:22:43,  1.43s/it] 14%|█▎        | 1330/9822 [35:43<3:23:31,  1.44s/it] 14%|█▎        | 1331/9822 [35:44<3:23:40,  1.44s/it] 14%|█▎        | 1332/9822 [35:46<3:23:09,  1.44s/it] 14%|█▎        | 1333/9822 [35:47<3:23:05,  1.44s/it] 14%|█▎        | 1334/9822 [35:49<3:22:50,  1.43s/it] 14%|█▎        | 1335/9822 [35:50<3:23:40,  1.44s/it] 14%|█▎        | 1336/9822 [35:51<3:23:15,  1.44s/it] 14%|█▎        | 1337/9822 [35:53<3:22:25,  1.43s/it] 14%|█▎        | 1338/9822 [35:54<3:22:40,  1.43s/it] 14%|█▎        | 1339/9822 [35:56<3:22:48,  1.43s/it] 14%|█▎        | 1340/9822 [35:57<3:23:19,  1.44s/it] 14%|█▎        | 1341/9822 [35:59<3:23:12,  1.44s/it] 14%|█▎        | 1342/9822 [36:00<3:23:27,  1.44s/it] 14%|█▎        | 1343/9822 [36:02<3:24:05,  1.44s/it] 14%|█▎        | 1344/9822 [36:03<3:24:29,  1.45s/it] 14%|█▎        | 1345/9822 [36:04<3:26:18,  1.46s/it] 14%|█▎        | 1346/9822 [36:06<3:27:33,  1.47s/it] 14%|█▎        | 1347/9822 [36:07<3:27:26,  1.47s/it] 14%|█▎        | 1348/9822 [36:09<3:25:54,  1.46s/it] 14%|█▎        | 1349/9822 [36:10<3:28:05,  1.47s/it] 14%|█▎        | 1350/9822 [36:12<3:26:43,  1.46s/it] 14%|█▍        | 1351/9822 [36:13<3:25:48,  1.46s/it] 14%|█▍        | 1352/9822 [36:15<3:24:32,  1.45s/it] 14%|█▍        | 1353/9822 [36:16<3:25:00,  1.45s/it] 14%|█▍        | 1354/9822 [36:18<3:24:07,  1.45s/it] 14%|█▍        | 1355/9822 [36:19<3:23:57,  1.45s/it] 14%|█▍        | 1356/9822 [36:20<3:23:03,  1.44s/it] 14%|█▍        | 1357/9822 [36:22<3:22:48,  1.44s/it] 14%|█▍        | 1358/9822 [36:23<3:22:02,  1.43s/it] 14%|█▍        | 1359/9822 [36:25<3:21:23,  1.43s/it] 14%|█▍        | 1360/9822 [36:26<3:21:51,  1.43s/it] 14%|█▍        | 1361/9822 [36:28<3:21:41,  1.43s/it] 14%|█▍        | 1362/9822 [36:29<3:21:41,  1.43s/it] 14%|█▍        | 1363/9822 [36:30<3:22:10,  1.43s/it] 14%|█▍        | 1364/9822 [36:32<3:22:07,  1.43s/it] 14%|█▍        | 1365/9822 [36:33<3:22:01,  1.43s/it] 14%|█▍        | 1366/9822 [36:35<3:21:57,  1.43s/it] 14%|█▍        | 1367/9822 [36:36<3:22:36,  1.44s/it] 14%|█▍        | 1368/9822 [36:38<3:22:16,  1.44s/it] 14%|█▍        | 1369/9822 [36:39<3:22:39,  1.44s/it] 14%|█▍        | 1370/9822 [36:40<3:22:18,  1.44s/it] 14%|█▍        | 1371/9822 [36:42<3:22:17,  1.44s/it] 14%|█▍        | 1372/9822 [36:43<3:23:11,  1.44s/it] 14%|█▍        | 1373/9822 [36:45<3:25:22,  1.46s/it] 14%|█▍        | 1374/9822 [36:46<3:30:37,  1.50s/it] 14%|█▍        | 1375/9822 [36:48<3:29:46,  1.49s/it] 14%|█▍        | 1376/9822 [36:49<3:25:48,  1.46s/it] 14%|█▍        | 1377/9822 [36:51<3:25:15,  1.46s/it] 14%|█▍        | 1378/9822 [36:52<3:24:43,  1.45s/it] 14%|█▍        | 1379/9822 [36:54<3:23:55,  1.45s/it] 14%|█▍        | 1380/9822 [36:55<3:23:38,  1.45s/it] 14%|█▍        | 1381/9822 [36:57<3:23:54,  1.45s/it] 14%|█▍        | 1382/9822 [36:58<3:23:46,  1.45s/it] 14%|█▍        | 1383/9822 [36:59<3:23:29,  1.45s/it] 14%|█▍        | 1384/9822 [37:01<3:23:11,  1.44s/it] 14%|█▍        | 1385/9822 [37:02<3:22:46,  1.44s/it] 14%|█▍        | 1386/9822 [37:04<3:24:27,  1.45s/it] 14%|█▍        | 1387/9822 [37:05<3:24:07,  1.45s/it] 14%|█▍        | 1388/9822 [37:07<3:23:09,  1.45s/it] 14%|█▍        | 1389/9822 [37:08<3:22:00,  1.44s/it] 14%|█▍        | 1390/9822 [37:10<3:21:18,  1.43s/it] 14%|█▍        | 1391/9822 [37:11<3:20:50,  1.43s/it] 14%|█▍        | 1392/9822 [37:12<3:21:15,  1.43s/it] 14%|█▍        | 1393/9822 [37:14<3:21:51,  1.44s/it] 14%|█▍        | 1394/9822 [37:15<3:22:09,  1.44s/it] 14%|█▍        | 1395/9822 [37:17<3:22:10,  1.44s/it] 14%|█▍        | 1396/9822 [37:18<3:24:07,  1.45s/it] 14%|█▍        | 1397/9822 [37:20<3:25:47,  1.47s/it] 14%|█▍        | 1398/9822 [37:21<3:26:47,  1.47s/it] 14%|█▍        | 1399/9822 [37:23<3:25:44,  1.47s/it] 14%|█▍        | 1400/9822 [37:24<3:24:43,  1.46s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:25:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:25:11 - INFO - __main__ - ***** test Results*****
01/07/2024 22:25:11 - INFO - __main__ -   Training step = 1400
01/07/2024 22:25:11 - INFO - __main__ -  test_accuracy:0.8462664714494875 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:25:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:25:17 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:25:17 - INFO - __main__ -   Training step = 1400
01/07/2024 22:25:17 - INFO - __main__ -  eval_accuracy:0.834859025997803 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:25:17,495 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:25:17,495 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:25:17,532 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:25:19,147 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.834859025997803}
test:
{'accuracy': 0.8462664714494875}
01/07/2024 22:25:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:25:23 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:25:23 - INFO - __main__ -   Training step = 1400
01/07/2024 22:25:23 - INFO - __main__ -  eval_accuracy:0.8809959721713658 
 14%|█▍        | 1401/9822 [37:44<16:24:45,  7.02s/it] 14%|█▍        | 1402/9822 [37:46<12:29:42,  5.34s/it] 14%|█▍        | 1403/9822 [37:47<9:44:45,  4.17s/it]  14%|█▍        | 1404/9822 [37:48<7:49:45,  3.35s/it] 14%|█▍        | 1405/9822 [37:50<6:32:50,  2.80s/it] 14%|█▍        | 1406/9822 [37:51<5:34:53,  2.39s/it] 14%|█▍        | 1407/9822 [37:53<4:54:36,  2.10s/it] 14%|█▍        | 1408/9822 [37:54<4:26:14,  1.90s/it] 14%|█▍        | 1409/9822 [37:56<4:06:27,  1.76s/it] 14%|█▍        | 1410/9822 [37:57<3:52:29,  1.66s/it] 14%|█▍        | 1411/9822 [37:58<3:43:45,  1.60s/it] 14%|█▍        | 1412/9822 [38:00<3:36:32,  1.54s/it] 14%|█▍        | 1413/9822 [38:01<3:32:27,  1.52s/it] 14%|█▍        | 1414/9822 [38:03<3:29:25,  1.49s/it] 14%|█▍        | 1415/9822 [38:04<3:26:19,  1.47s/it] 14%|█▍        | 1416/9822 [38:06<3:24:58,  1.46s/it] 14%|█▍        | 1417/9822 [38:07<3:23:21,  1.45s/it] 14%|█▍        | 1418/9822 [38:09<3:22:29,  1.45s/it] 14%|█▍        | 1419/9822 [38:10<3:21:51,  1.44s/it] 14%|█▍        | 1420/9822 [38:11<3:21:37,  1.44s/it] 14%|█▍        | 1421/9822 [38:13<3:21:01,  1.44s/it] 14%|█▍        | 1422/9822 [38:14<3:20:26,  1.43s/it] 14%|█▍        | 1423/9822 [38:16<3:20:21,  1.43s/it] 14%|█▍        | 1424/9822 [38:17<3:20:05,  1.43s/it] 15%|█▍        | 1425/9822 [38:19<3:20:08,  1.43s/it] 15%|█▍        | 1426/9822 [38:20<3:20:09,  1.43s/it] 15%|█▍        | 1427/9822 [38:21<3:20:09,  1.43s/it] 15%|█▍        | 1428/9822 [38:23<3:20:39,  1.43s/it] 15%|█▍        | 1429/9822 [38:24<3:20:41,  1.43s/it] 15%|█▍        | 1430/9822 [38:26<3:24:17,  1.46s/it] 15%|█▍        | 1431/9822 [38:27<3:22:42,  1.45s/it] 15%|█▍        | 1432/9822 [38:29<3:21:54,  1.44s/it] 15%|█▍        | 1433/9822 [38:30<3:21:50,  1.44s/it] 15%|█▍        | 1434/9822 [38:32<3:21:15,  1.44s/it] 15%|█▍        | 1435/9822 [38:33<3:21:37,  1.44s/it] 15%|█▍        | 1436/9822 [38:34<3:21:34,  1.44s/it] 15%|█▍        | 1437/9822 [38:36<3:21:25,  1.44s/it] 15%|█▍        | 1438/9822 [38:37<3:20:55,  1.44s/it] 15%|█▍        | 1439/9822 [38:39<3:21:11,  1.44s/it] 15%|█▍        | 1440/9822 [38:40<3:21:36,  1.44s/it] 15%|█▍        | 1441/9822 [38:42<3:22:25,  1.45s/it] 15%|█▍        | 1442/9822 [38:43<3:21:57,  1.45s/it] 15%|█▍        | 1443/9822 [38:45<3:21:35,  1.44s/it] 15%|█▍        | 1444/9822 [38:46<3:20:52,  1.44s/it] 15%|█▍        | 1445/9822 [38:47<3:20:37,  1.44s/it] 15%|█▍        | 1446/9822 [38:49<3:20:20,  1.44s/it] 15%|█▍        | 1447/9822 [38:50<3:19:57,  1.43s/it] 15%|█▍        | 1448/9822 [38:52<3:20:22,  1.44s/it] 15%|█▍        | 1449/9822 [38:53<3:20:28,  1.44s/it] 15%|█▍        | 1450/9822 [38:55<3:20:15,  1.44s/it] 15%|█▍        | 1451/9822 [38:56<3:20:16,  1.44s/it] 15%|█▍        | 1452/9822 [38:57<3:21:37,  1.45s/it] 15%|█▍        | 1453/9822 [38:59<3:21:09,  1.44s/it] 15%|█▍        | 1454/9822 [39:00<3:20:16,  1.44s/it] 15%|█▍        | 1455/9822 [39:02<3:19:57,  1.43s/it] 15%|█▍        | 1456/9822 [39:03<3:19:22,  1.43s/it] 15%|█▍        | 1457/9822 [39:05<3:19:49,  1.43s/it] 15%|█▍        | 1458/9822 [39:06<3:19:32,  1.43s/it] 15%|█▍        | 1459/9822 [39:07<3:19:16,  1.43s/it] 15%|█▍        | 1460/9822 [39:09<3:19:11,  1.43s/it] 15%|█▍        | 1461/9822 [39:10<3:19:34,  1.43s/it] 15%|█▍        | 1462/9822 [39:12<3:20:43,  1.44s/it] 15%|█▍        | 1463/9822 [39:13<3:20:32,  1.44s/it] 15%|█▍        | 1464/9822 [39:15<3:20:19,  1.44s/it] 15%|█▍        | 1465/9822 [39:16<3:20:15,  1.44s/it] 15%|█▍        | 1466/9822 [39:18<3:20:33,  1.44s/it] 15%|█▍        | 1467/9822 [39:19<3:20:04,  1.44s/it] 15%|█▍        | 1468/9822 [39:20<3:20:01,  1.44s/it] 15%|█▍        | 1469/9822 [39:22<3:20:01,  1.44s/it] 15%|█▍        | 1470/9822 [39:23<3:19:57,  1.44s/it] 15%|█▍        | 1471/9822 [39:25<3:19:22,  1.43s/it] 15%|█▍        | 1472/9822 [39:26<3:19:31,  1.43s/it] 15%|█▍        | 1473/9822 [39:28<3:19:24,  1.43s/it] 15%|█▌        | 1474/9822 [39:29<3:19:15,  1.43s/it] 15%|█▌        | 1475/9822 [39:30<3:19:28,  1.43s/it] 15%|█▌        | 1476/9822 [39:32<3:19:34,  1.43s/it] 15%|█▌        | 1477/9822 [39:33<3:19:25,  1.43s/it] 15%|█▌        | 1478/9822 [39:35<3:18:59,  1.43s/it] 15%|█▌        | 1479/9822 [39:36<3:18:51,  1.43s/it] 15%|█▌        | 1480/9822 [39:38<3:18:53,  1.43s/it] 15%|█▌        | 1481/9822 [39:39<3:19:17,  1.43s/it] 15%|█▌        | 1482/9822 [39:40<3:19:29,  1.44s/it] 15%|█▌        | 1483/9822 [39:42<3:19:12,  1.43s/it] 15%|█▌        | 1484/9822 [39:43<3:19:05,  1.43s/it] 15%|█▌        | 1485/9822 [39:45<3:18:50,  1.43s/it] 15%|█▌        | 1486/9822 [39:46<3:18:56,  1.43s/it] 15%|█▌        | 1487/9822 [39:48<3:18:30,  1.43s/it] 15%|█▌        | 1488/9822 [39:49<3:19:01,  1.43s/it] 15%|█▌        | 1489/9822 [39:50<3:18:37,  1.43s/it] 15%|█▌        | 1490/9822 [39:52<3:18:48,  1.43s/it] 15%|█▌        | 1491/9822 [39:53<3:18:24,  1.43s/it] 15%|█▌        | 1492/9822 [39:55<3:18:33,  1.43s/it] 15%|█▌        | 1493/9822 [39:56<3:20:29,  1.44s/it] 15%|█▌        | 1494/9822 [39:58<3:23:58,  1.47s/it] 15%|█▌        | 1495/9822 [39:59<3:22:59,  1.46s/it] 15%|█▌        | 1496/9822 [40:01<3:22:27,  1.46s/it] 15%|█▌        | 1497/9822 [40:02<3:22:08,  1.46s/it] 15%|█▌        | 1498/9822 [40:04<3:22:11,  1.46s/it] 15%|█▌        | 1499/9822 [40:05<3:22:42,  1.46s/it] 15%|█▌        | 1500/9822 [40:06<3:22:14,  1.46s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:27:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:27:53 - INFO - __main__ - ***** test Results*****
01/07/2024 22:27:53 - INFO - __main__ -   Training step = 1500
01/07/2024 22:27:53 - INFO - __main__ -  test_accuracy:0.8316251830161054 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:27:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:27:59 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:27:59 - INFO - __main__ -   Training step = 1500
01/07/2024 22:27:59 - INFO - __main__ -  eval_accuracy:0.8253387037715123 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.834859025997803}
test:
{'accuracy': 0.8462664714494875}
01/07/2024 22:28:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:28:04 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:28:04 - INFO - __main__ -   Training step = 1500
01/07/2024 22:28:04 - INFO - __main__ -  eval_accuracy:0.8740388136213841 
 15%|█▌        | 1501/9822 [40:25<15:04:44,  6.52s/it] 15%|█▌        | 1502/9822 [40:26<11:33:37,  5.00s/it] 15%|█▌        | 1503/9822 [40:28<9:05:34,  3.93s/it]  15%|█▌        | 1504/9822 [40:29<7:22:00,  3.19s/it] 15%|█▌        | 1505/9822 [40:31<6:09:34,  2.67s/it] 15%|█▌        | 1506/9822 [40:32<5:18:46,  2.30s/it] 15%|█▌        | 1507/9822 [40:34<4:43:13,  2.04s/it] 15%|█▌        | 1508/9822 [40:35<4:18:24,  1.86s/it] 15%|█▌        | 1509/9822 [40:36<4:00:43,  1.74s/it] 15%|█▌        | 1510/9822 [40:38<3:48:06,  1.65s/it] 15%|█▌        | 1511/9822 [40:39<3:40:18,  1.59s/it] 15%|█▌        | 1512/9822 [40:41<3:36:40,  1.56s/it] 15%|█▌        | 1513/9822 [40:42<3:33:07,  1.54s/it] 15%|█▌        | 1514/9822 [40:44<3:29:16,  1.51s/it] 15%|█▌        | 1515/9822 [40:45<3:26:37,  1.49s/it] 15%|█▌        | 1516/9822 [40:47<3:24:33,  1.48s/it] 15%|█▌        | 1517/9822 [40:48<3:23:12,  1.47s/it] 15%|█▌        | 1518/9822 [40:50<3:22:19,  1.46s/it] 15%|█▌        | 1519/9822 [40:51<3:21:47,  1.46s/it] 15%|█▌        | 1520/9822 [40:52<3:21:28,  1.46s/it] 15%|█▌        | 1521/9822 [40:54<3:21:09,  1.45s/it] 15%|█▌        | 1522/9822 [40:55<3:20:58,  1.45s/it] 16%|█▌        | 1523/9822 [40:57<3:20:45,  1.45s/it] 16%|█▌        | 1524/9822 [40:58<3:23:46,  1.47s/it] 16%|█▌        | 1525/9822 [41:00<3:21:39,  1.46s/it] 16%|█▌        | 1526/9822 [41:01<3:21:28,  1.46s/it] 16%|█▌        | 1527/9822 [41:03<3:21:48,  1.46s/it] 16%|█▌        | 1528/9822 [41:04<3:21:10,  1.46s/it] 16%|█▌        | 1529/9822 [41:06<3:20:19,  1.45s/it] 16%|█▌        | 1530/9822 [41:07<3:20:05,  1.45s/it] 16%|█▌        | 1531/9822 [41:08<3:22:07,  1.46s/it] 16%|█▌        | 1532/9822 [41:10<3:20:58,  1.45s/it] 16%|█▌        | 1533/9822 [41:11<3:20:02,  1.45s/it] 16%|█▌        | 1534/9822 [41:13<3:19:25,  1.44s/it] 16%|█▌        | 1535/9822 [41:14<3:20:33,  1.45s/it] 16%|█▌        | 1536/9822 [41:16<3:19:44,  1.45s/it] 16%|█▌        | 1537/9822 [41:17<3:19:07,  1.44s/it] 16%|█▌        | 1538/9822 [41:19<3:18:21,  1.44s/it] 16%|█▌        | 1539/9822 [41:20<3:18:09,  1.44s/it] 16%|█▌        | 1540/9822 [41:21<3:18:07,  1.44s/it] 16%|█▌        | 1541/9822 [41:23<3:18:23,  1.44s/it] 16%|█▌        | 1542/9822 [41:24<3:18:39,  1.44s/it] 16%|█▌        | 1543/9822 [41:26<3:18:11,  1.44s/it] 16%|█▌        | 1544/9822 [41:27<3:18:00,  1.44s/it] 16%|█▌        | 1545/9822 [41:29<3:17:59,  1.44s/it] 16%|█▌        | 1546/9822 [41:30<3:18:29,  1.44s/it] 16%|█▌        | 1547/9822 [41:31<3:18:27,  1.44s/it] 16%|█▌        | 1548/9822 [41:33<3:16:04,  1.42s/it] 16%|█▌        | 1549/9822 [41:34<3:16:22,  1.42s/it] 16%|█▌        | 1550/9822 [41:36<3:17:17,  1.43s/it] 16%|█▌        | 1551/9822 [41:37<3:18:45,  1.44s/it] 16%|█▌        | 1552/9822 [41:39<3:18:36,  1.44s/it] 16%|█▌        | 1553/9822 [41:40<3:17:51,  1.44s/it] 16%|█▌        | 1554/9822 [41:42<3:21:10,  1.46s/it] 16%|█▌        | 1555/9822 [41:43<3:20:16,  1.45s/it] 16%|█▌        | 1556/9822 [41:44<3:19:32,  1.45s/it] 16%|█▌        | 1557/9822 [41:46<3:19:13,  1.45s/it] 16%|█▌        | 1558/9822 [41:47<3:18:31,  1.44s/it] 16%|█▌        | 1559/9822 [41:49<3:17:44,  1.44s/it] 16%|█▌        | 1560/9822 [41:50<3:17:25,  1.43s/it] 16%|█▌        | 1561/9822 [41:52<3:16:56,  1.43s/it] 16%|█▌        | 1562/9822 [41:53<3:17:16,  1.43s/it] 16%|█▌        | 1563/9822 [41:54<3:17:09,  1.43s/it] 16%|█▌        | 1564/9822 [41:56<3:17:01,  1.43s/it] 16%|█▌        | 1565/9822 [41:57<3:17:04,  1.43s/it] 16%|█▌        | 1566/9822 [41:59<3:17:02,  1.43s/it] 16%|█▌        | 1567/9822 [42:00<3:17:17,  1.43s/it] 16%|█▌        | 1568/9822 [42:02<3:17:31,  1.44s/it] 16%|█▌        | 1569/9822 [42:03<3:17:11,  1.43s/it] 16%|█▌        | 1570/9822 [42:04<3:17:06,  1.43s/it] 16%|█▌        | 1571/9822 [42:06<3:17:14,  1.43s/it] 16%|█▌        | 1572/9822 [42:07<3:17:22,  1.44s/it] 16%|█▌        | 1573/9822 [42:09<3:17:10,  1.43s/it] 16%|█▌        | 1574/9822 [42:10<3:16:45,  1.43s/it] 16%|█▌        | 1575/9822 [42:12<3:16:57,  1.43s/it] 16%|█▌        | 1576/9822 [42:13<3:17:24,  1.44s/it] 16%|█▌        | 1577/9822 [42:15<3:17:29,  1.44s/it] 16%|█▌        | 1578/9822 [42:16<3:17:56,  1.44s/it] 16%|█▌        | 1579/9822 [42:17<3:18:01,  1.44s/it] 16%|█▌        | 1580/9822 [42:19<3:17:31,  1.44s/it] 16%|█▌        | 1581/9822 [42:20<3:17:33,  1.44s/it] 16%|█▌        | 1582/9822 [42:22<3:17:04,  1.44s/it] 16%|█▌        | 1583/9822 [42:23<3:16:44,  1.43s/it] 16%|█▌        | 1584/9822 [42:25<3:17:14,  1.44s/it] 16%|█▌        | 1585/9822 [42:26<3:16:52,  1.43s/it] 16%|█▌        | 1586/9822 [42:28<3:21:33,  1.47s/it] 16%|█▌        | 1587/9822 [42:29<3:20:03,  1.46s/it] 16%|█▌        | 1588/9822 [42:30<3:19:05,  1.45s/it] 16%|█▌        | 1589/9822 [42:32<3:17:58,  1.44s/it] 16%|█▌        | 1590/9822 [42:33<3:17:49,  1.44s/it] 16%|█▌        | 1591/9822 [42:35<3:17:44,  1.44s/it] 16%|█▌        | 1592/9822 [42:36<3:17:44,  1.44s/it] 16%|█▌        | 1593/9822 [42:38<3:17:26,  1.44s/it] 16%|█▌        | 1594/9822 [42:39<3:17:57,  1.44s/it] 16%|█▌        | 1595/9822 [42:41<3:17:34,  1.44s/it] 16%|█▌        | 1596/9822 [42:42<3:17:08,  1.44s/it] 16%|█▋        | 1597/9822 [42:43<3:17:23,  1.44s/it] 16%|█▋        | 1598/9822 [42:45<3:16:51,  1.44s/it] 16%|█▋        | 1599/9822 [42:46<3:17:47,  1.44s/it] 16%|█▋        | 1600/9822 [42:48<3:17:17,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:30:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:30:35 - INFO - __main__ - ***** test Results*****
01/07/2024 22:30:35 - INFO - __main__ -   Training step = 1600
01/07/2024 22:30:35 - INFO - __main__ -  test_accuracy:0.8532210834553441 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:30:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:30:41 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:30:41 - INFO - __main__ -   Training step = 1600
01/07/2024 22:30:41 - INFO - __main__ -  eval_accuracy:0.8410838520688393 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:30:41,110 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:30:41,110 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:30:41,147 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:30:42,741 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8532210834553441}
01/07/2024 22:30:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:30:47 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:30:47 - INFO - __main__ -   Training step = 1600
01/07/2024 22:30:47 - INFO - __main__ -  eval_accuracy:0.8890516294397657 
 16%|█▋        | 1601/9822 [43:08<15:58:05,  6.99s/it] 16%|█▋        | 1602/9822 [43:09<12:09:17,  5.32s/it] 16%|█▋        | 1603/9822 [43:11<9:29:24,  4.16s/it]  16%|█▋        | 1604/9822 [43:12<7:37:15,  3.34s/it] 16%|█▋        | 1605/9822 [43:13<6:18:17,  2.76s/it] 16%|█▋        | 1606/9822 [43:15<5:23:28,  2.36s/it] 16%|█▋        | 1607/9822 [43:16<4:45:14,  2.08s/it] 16%|█▋        | 1608/9822 [43:18<4:18:09,  1.89s/it] 16%|█▋        | 1609/9822 [43:19<3:59:30,  1.75s/it] 16%|█▋        | 1610/9822 [43:21<3:46:11,  1.65s/it] 16%|█▋        | 1611/9822 [43:22<3:36:58,  1.59s/it] 16%|█▋        | 1612/9822 [43:23<3:31:15,  1.54s/it] 16%|█▋        | 1613/9822 [43:25<3:28:17,  1.52s/it] 16%|█▋        | 1614/9822 [43:26<3:25:04,  1.50s/it] 16%|█▋        | 1615/9822 [43:28<3:22:01,  1.48s/it] 16%|█▋        | 1616/9822 [43:29<3:24:13,  1.49s/it] 16%|█▋        | 1617/9822 [43:31<3:21:48,  1.48s/it] 16%|█▋        | 1618/9822 [43:32<3:20:20,  1.47s/it] 16%|█▋        | 1619/9822 [43:34<3:18:31,  1.45s/it] 16%|█▋        | 1620/9822 [43:35<3:17:34,  1.45s/it] 17%|█▋        | 1621/9822 [43:36<3:16:55,  1.44s/it] 17%|█▋        | 1622/9822 [43:38<3:16:08,  1.44s/it] 17%|█▋        | 1623/9822 [43:39<3:16:08,  1.44s/it] 17%|█▋        | 1624/9822 [43:41<3:15:42,  1.43s/it] 17%|█▋        | 1625/9822 [43:42<3:15:40,  1.43s/it] 17%|█▋        | 1626/9822 [43:44<3:15:39,  1.43s/it] 17%|█▋        | 1627/9822 [43:45<3:15:25,  1.43s/it] 17%|█▋        | 1628/9822 [43:46<3:16:10,  1.44s/it] 17%|█▋        | 1629/9822 [43:48<3:17:10,  1.44s/it] 17%|█▋        | 1630/9822 [43:49<3:16:21,  1.44s/it] 17%|█▋        | 1631/9822 [43:51<3:16:01,  1.44s/it] 17%|█▋        | 1632/9822 [43:52<3:16:14,  1.44s/it] 17%|█▋        | 1633/9822 [43:54<3:15:48,  1.43s/it] 17%|█▋        | 1634/9822 [43:55<3:13:26,  1.42s/it] 17%|█▋        | 1635/9822 [43:56<3:13:52,  1.42s/it] 17%|█▋        | 1636/9822 [43:58<3:14:14,  1.42s/it] 17%|█▋        | 1637/9822 [43:59<3:14:21,  1.42s/it] 17%|█▋        | 1638/9822 [44:01<3:14:27,  1.43s/it] 17%|█▋        | 1639/9822 [44:02<3:14:23,  1.43s/it] 17%|█▋        | 1640/9822 [44:04<3:14:30,  1.43s/it] 17%|█▋        | 1641/9822 [44:05<3:15:31,  1.43s/it] 17%|█▋        | 1642/9822 [44:06<3:15:33,  1.43s/it] 17%|█▋        | 1643/9822 [44:08<3:15:31,  1.43s/it] 17%|█▋        | 1644/9822 [44:09<3:15:43,  1.44s/it] 17%|█▋        | 1645/9822 [44:11<3:17:17,  1.45s/it] 17%|█▋        | 1646/9822 [44:12<3:16:27,  1.44s/it] 17%|█▋        | 1647/9822 [44:14<3:15:31,  1.44s/it] 17%|█▋        | 1648/9822 [44:15<3:18:24,  1.46s/it] 17%|█▋        | 1649/9822 [44:17<3:17:04,  1.45s/it] 17%|█▋        | 1650/9822 [44:18<3:16:31,  1.44s/it] 17%|█▋        | 1651/9822 [44:19<3:17:29,  1.45s/it] 17%|█▋        | 1652/9822 [44:21<3:16:23,  1.44s/it] 17%|█▋        | 1653/9822 [44:22<3:16:25,  1.44s/it] 17%|█▋        | 1654/9822 [44:24<3:16:00,  1.44s/it] 17%|█▋        | 1655/9822 [44:25<3:16:05,  1.44s/it] 17%|█▋        | 1656/9822 [44:27<3:16:23,  1.44s/it] 17%|█▋        | 1657/9822 [44:28<3:15:56,  1.44s/it] 17%|█▋        | 1658/9822 [44:30<3:15:32,  1.44s/it] 17%|█▋        | 1659/9822 [44:31<3:15:58,  1.44s/it] 17%|█▋        | 1660/9822 [44:32<3:15:31,  1.44s/it] 17%|█▋        | 1661/9822 [44:34<3:15:30,  1.44s/it] 17%|█▋        | 1662/9822 [44:35<3:15:39,  1.44s/it] 17%|█▋        | 1663/9822 [44:37<3:15:15,  1.44s/it] 17%|█▋        | 1664/9822 [44:38<3:14:55,  1.43s/it] 17%|█▋        | 1665/9822 [44:40<3:15:04,  1.43s/it] 17%|█▋        | 1666/9822 [44:41<3:14:54,  1.43s/it] 17%|█▋        | 1667/9822 [44:42<3:14:41,  1.43s/it] 17%|█▋        | 1668/9822 [44:44<3:14:26,  1.43s/it] 17%|█▋        | 1669/9822 [44:45<3:14:14,  1.43s/it] 17%|█▋        | 1670/9822 [44:47<3:14:27,  1.43s/it] 17%|█▋        | 1671/9822 [44:48<3:14:35,  1.43s/it] 17%|█▋        | 1672/9822 [44:50<3:14:34,  1.43s/it] 17%|█▋        | 1673/9822 [44:51<3:15:10,  1.44s/it] 17%|█▋        | 1674/9822 [44:52<3:15:22,  1.44s/it] 17%|█▋        | 1675/9822 [44:54<3:15:20,  1.44s/it] 17%|█▋        | 1676/9822 [44:55<3:14:46,  1.43s/it] 17%|█▋        | 1677/9822 [44:57<3:14:22,  1.43s/it] 17%|█▋        | 1678/9822 [44:58<3:14:28,  1.43s/it] 17%|█▋        | 1679/9822 [45:00<3:14:31,  1.43s/it] 17%|█▋        | 1680/9822 [45:01<3:18:03,  1.46s/it] 17%|█▋        | 1681/9822 [45:03<3:16:40,  1.45s/it] 17%|█▋        | 1682/9822 [45:04<3:15:48,  1.44s/it] 17%|█▋        | 1683/9822 [45:05<3:15:50,  1.44s/it] 17%|█▋        | 1684/9822 [45:07<3:15:02,  1.44s/it] 17%|█▋        | 1685/9822 [45:08<3:14:57,  1.44s/it] 17%|█▋        | 1686/9822 [45:10<3:14:35,  1.44s/it] 17%|█▋        | 1687/9822 [45:11<3:14:18,  1.43s/it] 17%|█▋        | 1688/9822 [45:13<3:14:26,  1.43s/it] 17%|█▋        | 1689/9822 [45:14<3:14:04,  1.43s/it] 17%|█▋        | 1690/9822 [45:16<3:14:31,  1.44s/it] 17%|█▋        | 1691/9822 [45:17<3:14:30,  1.44s/it] 17%|█▋        | 1692/9822 [45:18<3:14:23,  1.43s/it] 17%|█▋        | 1693/9822 [45:20<3:14:11,  1.43s/it] 17%|█▋        | 1694/9822 [45:21<3:14:19,  1.43s/it] 17%|█▋        | 1695/9822 [45:23<3:14:00,  1.43s/it] 17%|█▋        | 1696/9822 [45:24<3:13:54,  1.43s/it] 17%|█▋        | 1697/9822 [45:26<3:14:09,  1.43s/it] 17%|█▋        | 1698/9822 [45:27<3:14:14,  1.43s/it] 17%|█▋        | 1699/9822 [45:28<3:13:53,  1.43s/it] 17%|█▋        | 1700/9822 [45:30<3:13:33,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:33:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:33:17 - INFO - __main__ - ***** test Results*****
01/07/2024 22:33:17 - INFO - __main__ -   Training step = 1700
01/07/2024 22:33:17 - INFO - __main__ -  test_accuracy:0.8565153733528551 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:33:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:33:23 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:33:23 - INFO - __main__ -   Training step = 1700
01/07/2024 22:33:23 - INFO - __main__ -  eval_accuracy:0.8392530208714757 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8532210834553441}
01/07/2024 22:33:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:33:27 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:33:27 - INFO - __main__ -   Training step = 1700
01/07/2024 22:33:27 - INFO - __main__ -  eval_accuracy:0.888685463200293 
 17%|█▋        | 1701/9822 [45:48<14:40:15,  6.50s/it] 17%|█▋        | 1702/9822 [45:50<11:14:56,  4.99s/it] 17%|█▋        | 1703/9822 [45:51<8:51:21,  3.93s/it]  17%|█▋        | 1704/9822 [45:53<7:10:30,  3.18s/it] 17%|█▋        | 1705/9822 [45:54<5:59:18,  2.66s/it] 17%|█▋        | 1706/9822 [45:55<5:09:40,  2.29s/it] 17%|█▋        | 1707/9822 [45:57<4:37:55,  2.05s/it] 17%|█▋        | 1708/9822 [45:58<4:12:49,  1.87s/it] 17%|█▋        | 1709/9822 [46:00<3:54:51,  1.74s/it] 17%|█▋        | 1710/9822 [46:01<3:42:18,  1.64s/it] 17%|█▋        | 1711/9822 [46:03<3:33:49,  1.58s/it] 17%|█▋        | 1712/9822 [46:04<3:27:27,  1.53s/it] 17%|█▋        | 1713/9822 [46:05<3:23:01,  1.50s/it] 17%|█▋        | 1714/9822 [46:07<3:19:56,  1.48s/it] 17%|█▋        | 1715/9822 [46:08<3:17:38,  1.46s/it] 17%|█▋        | 1716/9822 [46:10<3:16:03,  1.45s/it] 17%|█▋        | 1717/9822 [46:11<3:15:48,  1.45s/it] 17%|█▋        | 1718/9822 [46:13<3:14:45,  1.44s/it] 18%|█▊        | 1719/9822 [46:14<3:13:57,  1.44s/it] 18%|█▊        | 1720/9822 [46:15<3:11:26,  1.42s/it] 18%|█▊        | 1721/9822 [46:17<3:11:52,  1.42s/it] 18%|█▊        | 1722/9822 [46:18<3:12:06,  1.42s/it] 18%|█▊        | 1723/9822 [46:20<3:12:07,  1.42s/it] 18%|█▊        | 1724/9822 [46:21<3:12:11,  1.42s/it] 18%|█▊        | 1725/9822 [46:23<3:12:20,  1.43s/it] 18%|█▊        | 1726/9822 [46:24<3:12:58,  1.43s/it] 18%|█▊        | 1727/9822 [46:25<3:13:26,  1.43s/it] 18%|█▊        | 1728/9822 [46:27<3:13:36,  1.44s/it] 18%|█▊        | 1729/9822 [46:28<3:12:56,  1.43s/it] 18%|█▊        | 1730/9822 [46:30<3:12:37,  1.43s/it] 18%|█▊        | 1731/9822 [46:31<3:12:59,  1.43s/it] 18%|█▊        | 1732/9822 [46:33<3:13:02,  1.43s/it] 18%|█▊        | 1733/9822 [46:34<3:12:49,  1.43s/it] 18%|█▊        | 1734/9822 [46:35<3:13:40,  1.44s/it] 18%|█▊        | 1735/9822 [46:37<3:13:35,  1.44s/it] 18%|█▊        | 1736/9822 [46:38<3:13:02,  1.43s/it] 18%|█▊        | 1737/9822 [46:40<3:16:56,  1.46s/it] 18%|█▊        | 1738/9822 [46:41<3:15:51,  1.45s/it] 18%|█▊        | 1739/9822 [46:43<3:15:00,  1.45s/it] 18%|█▊        | 1740/9822 [46:44<3:14:56,  1.45s/it] 18%|█▊        | 1741/9822 [46:46<3:14:36,  1.44s/it] 18%|█▊        | 1742/9822 [46:47<3:14:13,  1.44s/it] 18%|█▊        | 1743/9822 [46:48<3:13:51,  1.44s/it] 18%|█▊        | 1744/9822 [46:50<3:13:17,  1.44s/it] 18%|█▊        | 1745/9822 [46:51<3:13:30,  1.44s/it] 18%|█▊        | 1746/9822 [46:53<3:13:24,  1.44s/it] 18%|█▊        | 1747/9822 [46:54<3:12:54,  1.43s/it] 18%|█▊        | 1748/9822 [46:56<3:12:42,  1.43s/it] 18%|█▊        | 1749/9822 [46:57<3:12:25,  1.43s/it] 18%|█▊        | 1750/9822 [46:58<3:12:05,  1.43s/it] 18%|█▊        | 1751/9822 [47:00<3:12:40,  1.43s/it] 18%|█▊        | 1752/9822 [47:01<3:12:47,  1.43s/it] 18%|█▊        | 1753/9822 [47:03<3:12:35,  1.43s/it] 18%|█▊        | 1754/9822 [47:04<3:12:26,  1.43s/it] 18%|█▊        | 1755/9822 [47:06<3:12:24,  1.43s/it] 18%|█▊        | 1756/9822 [47:07<3:12:08,  1.43s/it] 18%|█▊        | 1757/9822 [47:08<3:12:18,  1.43s/it] 18%|█▊        | 1758/9822 [47:10<3:12:36,  1.43s/it] 18%|█▊        | 1759/9822 [47:11<3:12:43,  1.43s/it] 18%|█▊        | 1760/9822 [47:13<3:12:31,  1.43s/it] 18%|█▊        | 1761/9822 [47:14<3:12:15,  1.43s/it] 18%|█▊        | 1762/9822 [47:16<3:11:59,  1.43s/it] 18%|█▊        | 1763/9822 [47:17<3:11:54,  1.43s/it] 18%|█▊        | 1764/9822 [47:19<3:11:54,  1.43s/it] 18%|█▊        | 1765/9822 [47:20<3:13:13,  1.44s/it] 18%|█▊        | 1766/9822 [47:21<3:12:51,  1.44s/it] 18%|█▊        | 1767/9822 [47:23<3:12:50,  1.44s/it] 18%|█▊        | 1768/9822 [47:24<3:12:34,  1.43s/it] 18%|█▊        | 1769/9822 [47:26<3:15:33,  1.46s/it] 18%|█▊        | 1770/9822 [47:27<3:14:50,  1.45s/it] 18%|█▊        | 1771/9822 [47:29<3:14:04,  1.45s/it] 18%|█▊        | 1772/9822 [47:30<3:13:07,  1.44s/it] 18%|█▊        | 1773/9822 [47:32<3:12:55,  1.44s/it] 18%|█▊        | 1774/9822 [47:33<3:12:51,  1.44s/it] 18%|█▊        | 1775/9822 [47:34<3:13:26,  1.44s/it] 18%|█▊        | 1776/9822 [47:36<3:13:53,  1.45s/it] 18%|█▊        | 1777/9822 [47:37<3:13:41,  1.44s/it] 18%|█▊        | 1778/9822 [47:39<3:12:56,  1.44s/it] 18%|█▊        | 1779/9822 [47:40<3:12:53,  1.44s/it] 18%|█▊        | 1780/9822 [47:42<3:13:14,  1.44s/it] 18%|█▊        | 1781/9822 [47:43<3:12:44,  1.44s/it] 18%|█▊        | 1782/9822 [47:44<3:13:09,  1.44s/it] 18%|█▊        | 1783/9822 [47:46<3:13:19,  1.44s/it] 18%|█▊        | 1784/9822 [47:47<3:13:20,  1.44s/it] 18%|█▊        | 1785/9822 [47:49<3:13:13,  1.44s/it] 18%|█▊        | 1786/9822 [47:50<3:12:48,  1.44s/it] 18%|█▊        | 1787/9822 [47:52<3:12:13,  1.44s/it] 18%|█▊        | 1788/9822 [47:53<3:12:35,  1.44s/it] 18%|█▊        | 1789/9822 [47:55<3:12:25,  1.44s/it] 18%|█▊        | 1790/9822 [47:56<3:12:46,  1.44s/it] 18%|█▊        | 1791/9822 [47:57<3:12:23,  1.44s/it] 18%|█▊        | 1792/9822 [47:59<3:12:01,  1.43s/it] 18%|█▊        | 1793/9822 [48:00<3:12:05,  1.44s/it] 18%|█▊        | 1794/9822 [48:02<3:12:10,  1.44s/it] 18%|█▊        | 1795/9822 [48:03<3:11:46,  1.43s/it] 18%|█▊        | 1796/9822 [48:05<3:11:27,  1.43s/it] 18%|█▊        | 1797/9822 [48:06<3:11:45,  1.43s/it] 18%|█▊        | 1798/9822 [48:07<3:12:00,  1.44s/it] 18%|█▊        | 1799/9822 [48:09<3:12:22,  1.44s/it] 18%|█▊        | 1800/9822 [48:10<3:11:58,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:35:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:35:57 - INFO - __main__ - ***** test Results*****
01/07/2024 22:35:57 - INFO - __main__ -   Training step = 1800
01/07/2024 22:35:57 - INFO - __main__ -  test_accuracy:0.8506588579795022 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:36:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:36:03 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:36:03 - INFO - __main__ -   Training step = 1800
01/07/2024 22:36:03 - INFO - __main__ -  eval_accuracy:0.8396191871109484 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8532210834553441}
01/07/2024 22:36:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:36:08 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:36:08 - INFO - __main__ -   Training step = 1800
01/07/2024 22:36:08 - INFO - __main__ -  eval_accuracy:0.8897839619187111 
 18%|█▊        | 1801/9822 [48:29<14:31:02,  6.52s/it] 18%|█▊        | 1802/9822 [48:30<11:10:29,  5.02s/it] 18%|█▊        | 1803/9822 [48:32<8:46:45,  3.94s/it]  18%|█▊        | 1804/9822 [48:33<7:06:53,  3.19s/it] 18%|█▊        | 1805/9822 [48:35<5:56:26,  2.67s/it] 18%|█▊        | 1806/9822 [48:36<5:04:51,  2.28s/it] 18%|█▊        | 1807/9822 [48:37<4:30:40,  2.03s/it] 18%|█▊        | 1808/9822 [48:39<4:07:24,  1.85s/it] 18%|█▊        | 1809/9822 [48:40<3:50:09,  1.72s/it] 18%|█▊        | 1810/9822 [48:42<3:38:03,  1.63s/it] 18%|█▊        | 1811/9822 [48:43<3:31:51,  1.59s/it] 18%|█▊        | 1812/9822 [48:45<3:26:02,  1.54s/it] 18%|█▊        | 1813/9822 [48:46<3:21:35,  1.51s/it] 18%|█▊        | 1814/9822 [48:47<3:18:06,  1.48s/it] 18%|█▊        | 1815/9822 [48:49<3:15:54,  1.47s/it] 18%|█▊        | 1816/9822 [48:50<3:14:42,  1.46s/it] 18%|█▊        | 1817/9822 [48:52<3:13:59,  1.45s/it] 19%|█▊        | 1818/9822 [48:53<3:12:55,  1.45s/it] 19%|█▊        | 1819/9822 [48:55<3:12:34,  1.44s/it] 19%|█▊        | 1820/9822 [48:56<3:12:10,  1.44s/it] 19%|█▊        | 1821/9822 [48:57<3:11:41,  1.44s/it] 19%|█▊        | 1822/9822 [48:59<3:11:17,  1.43s/it] 19%|█▊        | 1823/9822 [49:00<3:10:46,  1.43s/it] 19%|█▊        | 1824/9822 [49:02<3:10:24,  1.43s/it] 19%|█▊        | 1825/9822 [49:03<3:10:00,  1.43s/it] 19%|█▊        | 1826/9822 [49:05<3:10:16,  1.43s/it] 19%|█▊        | 1827/9822 [49:06<3:10:25,  1.43s/it] 19%|█▊        | 1828/9822 [49:07<3:10:08,  1.43s/it] 19%|█▊        | 1829/9822 [49:09<3:10:03,  1.43s/it] 19%|█▊        | 1830/9822 [49:10<3:10:20,  1.43s/it] 19%|█▊        | 1831/9822 [49:12<3:10:05,  1.43s/it] 19%|█▊        | 1832/9822 [49:13<3:10:21,  1.43s/it] 19%|█▊        | 1833/9822 [49:15<3:13:36,  1.45s/it] 19%|█▊        | 1834/9822 [49:16<3:12:39,  1.45s/it] 19%|█▊        | 1835/9822 [49:18<3:12:14,  1.44s/it] 19%|█▊        | 1836/9822 [49:19<3:12:33,  1.45s/it] 19%|█▊        | 1837/9822 [49:20<3:11:45,  1.44s/it] 19%|█▊        | 1838/9822 [49:22<3:11:11,  1.44s/it] 19%|█▊        | 1839/9822 [49:23<3:11:53,  1.44s/it] 19%|█▊        | 1840/9822 [49:25<3:11:29,  1.44s/it] 19%|█▊        | 1841/9822 [49:26<3:11:39,  1.44s/it] 19%|█▉        | 1842/9822 [49:28<3:11:30,  1.44s/it] 19%|█▉        | 1843/9822 [49:29<3:11:11,  1.44s/it] 19%|█▉        | 1844/9822 [49:31<3:11:29,  1.44s/it] 19%|█▉        | 1845/9822 [49:32<3:11:03,  1.44s/it] 19%|█▉        | 1846/9822 [49:33<3:10:29,  1.43s/it] 19%|█▉        | 1847/9822 [49:35<3:10:24,  1.43s/it] 19%|█▉        | 1848/9822 [49:36<3:10:28,  1.43s/it] 19%|█▉        | 1849/9822 [49:38<3:10:26,  1.43s/it] 19%|█▉        | 1850/9822 [49:39<3:10:13,  1.43s/it] 19%|█▉        | 1851/9822 [49:41<3:10:19,  1.43s/it] 19%|█▉        | 1852/9822 [49:42<3:10:01,  1.43s/it] 19%|█▉        | 1853/9822 [49:43<3:10:29,  1.43s/it] 19%|█▉        | 1854/9822 [49:45<3:11:03,  1.44s/it] 19%|█▉        | 1855/9822 [49:46<3:11:41,  1.44s/it] 19%|█▉        | 1856/9822 [49:48<3:11:09,  1.44s/it] 19%|█▉        | 1857/9822 [49:49<3:10:25,  1.43s/it] 19%|█▉        | 1858/9822 [49:51<3:14:01,  1.46s/it] 19%|█▉        | 1859/9822 [49:52<3:13:07,  1.46s/it] 19%|█▉        | 1860/9822 [49:54<3:12:12,  1.45s/it] 19%|█▉        | 1861/9822 [49:55<3:11:51,  1.45s/it] 19%|█▉        | 1862/9822 [49:56<3:11:57,  1.45s/it] 19%|█▉        | 1863/9822 [49:58<3:12:16,  1.45s/it] 19%|█▉        | 1864/9822 [49:59<3:12:29,  1.45s/it] 19%|█▉        | 1865/9822 [50:01<3:12:21,  1.45s/it] 19%|█▉        | 1866/9822 [50:02<3:12:12,  1.45s/it] 19%|█▉        | 1867/9822 [50:04<3:12:40,  1.45s/it] 19%|█▉        | 1868/9822 [50:05<3:13:33,  1.46s/it] 19%|█▉        | 1869/9822 [50:07<3:12:50,  1.45s/it] 19%|█▉        | 1870/9822 [50:08<3:12:36,  1.45s/it] 19%|█▉        | 1871/9822 [50:10<3:14:06,  1.46s/it] 19%|█▉        | 1872/9822 [50:11<3:13:27,  1.46s/it] 19%|█▉        | 1873/9822 [50:12<3:12:42,  1.45s/it] 19%|█▉        | 1874/9822 [50:14<3:12:08,  1.45s/it] 19%|█▉        | 1875/9822 [50:15<3:11:57,  1.45s/it] 19%|█▉        | 1876/9822 [50:17<3:12:04,  1.45s/it] 19%|█▉        | 1877/9822 [50:18<3:11:45,  1.45s/it] 19%|█▉        | 1878/9822 [50:20<3:11:44,  1.45s/it] 19%|█▉        | 1879/9822 [50:21<3:12:08,  1.45s/it] 19%|█▉        | 1880/9822 [50:23<3:13:35,  1.46s/it] 19%|█▉        | 1881/9822 [50:24<3:12:26,  1.45s/it] 19%|█▉        | 1882/9822 [50:26<3:11:37,  1.45s/it] 19%|█▉        | 1883/9822 [50:27<3:11:07,  1.44s/it] 19%|█▉        | 1884/9822 [50:28<3:11:04,  1.44s/it] 19%|█▉        | 1885/9822 [50:30<3:11:30,  1.45s/it] 19%|█▉        | 1886/9822 [50:31<3:11:22,  1.45s/it] 19%|█▉        | 1887/9822 [50:33<3:11:22,  1.45s/it] 19%|█▉        | 1888/9822 [50:34<3:11:04,  1.44s/it] 19%|█▉        | 1889/9822 [50:36<3:10:42,  1.44s/it] 19%|█▉        | 1890/9822 [50:37<3:15:45,  1.48s/it] 19%|█▉        | 1891/9822 [50:39<3:14:57,  1.47s/it] 19%|█▉        | 1892/9822 [50:40<3:11:19,  1.45s/it] 19%|█▉        | 1893/9822 [50:41<3:12:12,  1.45s/it] 19%|█▉        | 1894/9822 [50:43<3:11:37,  1.45s/it] 19%|█▉        | 1895/9822 [50:44<3:11:04,  1.45s/it] 19%|█▉        | 1896/9822 [50:46<3:10:46,  1.44s/it] 19%|█▉        | 1897/9822 [50:47<3:10:19,  1.44s/it] 19%|█▉        | 1898/9822 [50:49<3:10:15,  1.44s/it] 19%|█▉        | 1899/9822 [50:50<3:10:25,  1.44s/it] 19%|█▉        | 1900/9822 [50:52<3:10:19,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:38:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:38:38 - INFO - __main__ - ***** test Results*****
01/07/2024 22:38:38 - INFO - __main__ -   Training step = 1900
01/07/2024 22:38:38 - INFO - __main__ -  test_accuracy:0.8554172767203514 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:38:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:38:44 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:38:44 - INFO - __main__ -   Training step = 1900
01/07/2024 22:38:44 - INFO - __main__ -  eval_accuracy:0.8410838520688393 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8410838520688393}
test:
{'accuracy': 0.8532210834553441}
01/07/2024 22:38:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:38:49 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:38:49 - INFO - __main__ -   Training step = 1900
01/07/2024 22:38:49 - INFO - __main__ -  eval_accuracy:0.8908824606371293 
 19%|█▉        | 1901/9822 [51:10<14:19:17,  6.51s/it] 19%|█▉        | 1902/9822 [51:11<10:58:29,  4.99s/it] 19%|█▉        | 1903/9822 [51:13<8:37:46,  3.92s/it]  19%|█▉        | 1904/9822 [51:14<6:59:29,  3.18s/it] 19%|█▉        | 1905/9822 [51:16<5:51:28,  2.66s/it] 19%|█▉        | 1906/9822 [51:17<5:02:51,  2.30s/it] 19%|█▉        | 1907/9822 [51:19<4:28:52,  2.04s/it] 19%|█▉        | 1908/9822 [51:20<4:04:56,  1.86s/it] 19%|█▉        | 1909/9822 [51:21<3:48:21,  1.73s/it] 19%|█▉        | 1910/9822 [51:23<3:36:45,  1.64s/it] 19%|█▉        | 1911/9822 [51:24<3:28:05,  1.58s/it] 19%|█▉        | 1912/9822 [51:26<3:22:28,  1.54s/it] 19%|█▉        | 1913/9822 [51:27<3:18:35,  1.51s/it] 19%|█▉        | 1914/9822 [51:29<3:16:09,  1.49s/it] 19%|█▉        | 1915/9822 [51:30<3:13:27,  1.47s/it] 20%|█▉        | 1916/9822 [51:31<3:12:13,  1.46s/it] 20%|█▉        | 1917/9822 [51:33<3:10:52,  1.45s/it] 20%|█▉        | 1918/9822 [51:34<3:10:20,  1.44s/it] 20%|█▉        | 1919/9822 [51:36<3:09:44,  1.44s/it] 20%|█▉        | 1920/9822 [51:37<3:09:06,  1.44s/it] 20%|█▉        | 1921/9822 [51:39<3:09:25,  1.44s/it] 20%|█▉        | 1922/9822 [51:40<3:09:13,  1.44s/it] 20%|█▉        | 1923/9822 [51:42<3:09:19,  1.44s/it] 20%|█▉        | 1924/9822 [51:43<3:08:59,  1.44s/it] 20%|█▉        | 1925/9822 [51:44<3:08:28,  1.43s/it] 20%|█▉        | 1926/9822 [51:46<3:08:24,  1.43s/it] 20%|█▉        | 1927/9822 [51:47<3:08:20,  1.43s/it] 20%|█▉        | 1928/9822 [51:49<3:11:55,  1.46s/it] 20%|█▉        | 1929/9822 [51:50<3:11:14,  1.45s/it] 20%|█▉        | 1930/9822 [51:52<3:11:56,  1.46s/it] 20%|█▉        | 1931/9822 [51:53<3:11:19,  1.45s/it] 20%|█▉        | 1932/9822 [51:55<3:10:43,  1.45s/it] 20%|█▉        | 1933/9822 [51:56<3:10:08,  1.45s/it] 20%|█▉        | 1934/9822 [51:57<3:10:16,  1.45s/it] 20%|█▉        | 1935/9822 [51:59<3:09:45,  1.44s/it] 20%|█▉        | 1936/9822 [52:00<3:09:16,  1.44s/it] 20%|█▉        | 1937/9822 [52:02<3:08:44,  1.44s/it] 20%|█▉        | 1938/9822 [52:03<3:08:25,  1.43s/it] 20%|█▉        | 1939/9822 [52:05<3:08:00,  1.43s/it] 20%|█▉        | 1940/9822 [52:06<3:08:00,  1.43s/it] 20%|█▉        | 1941/9822 [52:08<3:10:21,  1.45s/it] 20%|█▉        | 1942/9822 [52:09<3:12:06,  1.46s/it] 20%|█▉        | 1943/9822 [52:10<3:12:19,  1.46s/it] 20%|█▉        | 1944/9822 [52:12<3:10:53,  1.45s/it] 20%|█▉        | 1945/9822 [52:13<3:10:28,  1.45s/it] 20%|█▉        | 1946/9822 [52:15<3:10:03,  1.45s/it] 20%|█▉        | 1947/9822 [52:16<3:09:23,  1.44s/it] 20%|█▉        | 1948/9822 [52:18<3:09:16,  1.44s/it] 20%|█▉        | 1949/9822 [52:19<3:08:58,  1.44s/it] 20%|█▉        | 1950/9822 [52:21<3:08:28,  1.44s/it] 20%|█▉        | 1951/9822 [52:22<3:08:15,  1.44s/it] 20%|█▉        | 1952/9822 [52:23<3:08:12,  1.43s/it] 20%|█▉        | 1953/9822 [52:25<3:08:26,  1.44s/it] 20%|█▉        | 1954/9822 [52:26<3:07:51,  1.43s/it] 20%|█▉        | 1955/9822 [52:28<3:07:36,  1.43s/it] 20%|█▉        | 1956/9822 [52:29<3:08:13,  1.44s/it] 20%|█▉        | 1957/9822 [52:31<3:07:56,  1.43s/it] 20%|█▉        | 1958/9822 [52:32<3:07:59,  1.43s/it] 20%|█▉        | 1959/9822 [52:33<3:07:46,  1.43s/it] 20%|█▉        | 1960/9822 [52:35<3:11:24,  1.46s/it] 20%|█▉        | 1961/9822 [52:36<3:11:50,  1.46s/it] 20%|█▉        | 1962/9822 [52:38<3:10:38,  1.46s/it] 20%|█▉        | 1963/9822 [52:39<3:09:36,  1.45s/it] 20%|█▉        | 1964/9822 [52:41<3:09:13,  1.44s/it] 20%|██        | 1965/9822 [52:42<3:08:48,  1.44s/it] 20%|██        | 1966/9822 [52:44<3:08:19,  1.44s/it] 20%|██        | 1967/9822 [52:45<3:08:13,  1.44s/it] 20%|██        | 1968/9822 [52:46<3:08:18,  1.44s/it] 20%|██        | 1969/9822 [52:48<3:08:02,  1.44s/it] 20%|██        | 1970/9822 [52:49<3:07:47,  1.43s/it] 20%|██        | 1971/9822 [52:51<3:07:41,  1.43s/it] 20%|██        | 1972/9822 [52:52<3:07:42,  1.43s/it] 20%|██        | 1973/9822 [52:54<3:07:41,  1.43s/it] 20%|██        | 1974/9822 [52:55<3:07:30,  1.43s/it] 20%|██        | 1975/9822 [52:56<3:07:42,  1.44s/it] 20%|██        | 1976/9822 [52:58<3:07:47,  1.44s/it] 20%|██        | 1977/9822 [52:59<3:07:52,  1.44s/it] 20%|██        | 1978/9822 [53:01<3:06:52,  1.43s/it] 20%|██        | 1979/9822 [53:02<3:07:04,  1.43s/it] 20%|██        | 1980/9822 [53:04<3:07:06,  1.43s/it] 20%|██        | 1981/9822 [53:05<3:08:40,  1.44s/it] 20%|██        | 1982/9822 [53:07<3:08:12,  1.44s/it] 20%|██        | 1983/9822 [53:08<3:07:27,  1.43s/it] 20%|██        | 1984/9822 [53:09<3:07:08,  1.43s/it] 20%|██        | 1985/9822 [53:11<3:10:19,  1.46s/it] 20%|██        | 1986/9822 [53:12<3:09:34,  1.45s/it] 20%|██        | 1987/9822 [53:14<3:08:34,  1.44s/it] 20%|██        | 1988/9822 [53:15<3:07:37,  1.44s/it] 20%|██        | 1989/9822 [53:17<3:07:12,  1.43s/it] 20%|██        | 1990/9822 [53:18<3:07:29,  1.44s/it] 20%|██        | 1991/9822 [53:20<3:07:49,  1.44s/it] 20%|██        | 1992/9822 [53:21<3:08:07,  1.44s/it] 20%|██        | 1993/9822 [53:22<3:08:02,  1.44s/it] 20%|██        | 1994/9822 [53:24<3:07:28,  1.44s/it] 20%|██        | 1995/9822 [53:25<3:06:55,  1.43s/it] 20%|██        | 1996/9822 [53:27<3:06:45,  1.43s/it] 20%|██        | 1997/9822 [53:28<3:06:24,  1.43s/it] 20%|██        | 1998/9822 [53:30<3:07:04,  1.43s/it] 20%|██        | 1999/9822 [53:31<3:07:20,  1.44s/it] 20%|██        | 2000/9822 [53:32<3:06:47,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:41:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:41:19 - INFO - __main__ - ***** test Results*****
01/07/2024 22:41:19 - INFO - __main__ -   Training step = 2000
01/07/2024 22:41:19 - INFO - __main__ -  test_accuracy:0.8587115666178624 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:41:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:41:25 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:41:25 - INFO - __main__ -   Training step = 2000
01/07/2024 22:41:25 - INFO - __main__ -  eval_accuracy:0.8436470157451483 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:41:25,840 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:41:25,840 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:41:25,876 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:41:27,465 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8436470157451483}
test:
{'accuracy': 0.8587115666178624}
01/07/2024 22:41:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:41:32 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:41:32 - INFO - __main__ -   Training step = 2000
01/07/2024 22:41:32 - INFO - __main__ -  eval_accuracy:0.8923471255950202 
 20%|██        | 2001/9822 [53:52<15:10:50,  6.99s/it] 20%|██        | 2002/9822 [53:54<11:33:20,  5.32s/it] 20%|██        | 2003/9822 [53:55<9:01:23,  4.15s/it]  20%|██        | 2004/9822 [53:57<7:15:09,  3.34s/it] 20%|██        | 2005/9822 [53:58<6:00:31,  2.77s/it] 20%|██        | 2006/9822 [54:00<5:08:11,  2.37s/it] 20%|██        | 2007/9822 [54:01<4:32:06,  2.09s/it] 20%|██        | 2008/9822 [54:02<4:06:26,  1.89s/it] 20%|██        | 2009/9822 [54:04<3:47:54,  1.75s/it] 20%|██        | 2010/9822 [54:05<3:35:08,  1.65s/it] 20%|██        | 2011/9822 [54:07<3:25:58,  1.58s/it] 20%|██        | 2012/9822 [54:08<3:20:06,  1.54s/it] 20%|██        | 2013/9822 [54:10<3:15:43,  1.50s/it] 21%|██        | 2014/9822 [54:11<3:14:42,  1.50s/it] 21%|██        | 2015/9822 [54:12<3:11:47,  1.47s/it] 21%|██        | 2016/9822 [54:14<3:09:36,  1.46s/it] 21%|██        | 2017/9822 [54:15<3:08:14,  1.45s/it] 21%|██        | 2018/9822 [54:17<3:08:57,  1.45s/it] 21%|██        | 2019/9822 [54:18<3:08:35,  1.45s/it] 21%|██        | 2020/9822 [54:20<3:07:59,  1.45s/it] 21%|██        | 2021/9822 [54:21<3:09:04,  1.45s/it] 21%|██        | 2022/9822 [54:23<3:08:11,  1.45s/it] 21%|██        | 2023/9822 [54:24<3:07:25,  1.44s/it] 21%|██        | 2024/9822 [54:25<3:08:15,  1.45s/it] 21%|██        | 2025/9822 [54:27<3:07:39,  1.44s/it] 21%|██        | 2026/9822 [54:28<3:06:59,  1.44s/it] 21%|██        | 2027/9822 [54:30<3:06:38,  1.44s/it] 21%|██        | 2028/9822 [54:31<3:06:44,  1.44s/it] 21%|██        | 2029/9822 [54:33<3:06:50,  1.44s/it] 21%|██        | 2030/9822 [54:34<3:06:49,  1.44s/it] 21%|██        | 2031/9822 [54:35<3:06:21,  1.44s/it] 21%|██        | 2032/9822 [54:37<3:06:23,  1.44s/it] 21%|██        | 2033/9822 [54:38<3:06:21,  1.44s/it] 21%|██        | 2034/9822 [54:40<3:06:03,  1.43s/it] 21%|██        | 2035/9822 [54:41<3:06:57,  1.44s/it] 21%|██        | 2036/9822 [54:43<3:08:29,  1.45s/it] 21%|██        | 2037/9822 [54:44<3:07:33,  1.45s/it] 21%|██        | 2038/9822 [54:46<3:07:15,  1.44s/it] 21%|██        | 2039/9822 [54:47<3:07:10,  1.44s/it] 21%|██        | 2040/9822 [54:48<3:06:26,  1.44s/it] 21%|██        | 2041/9822 [54:50<3:06:16,  1.44s/it] 21%|██        | 2042/9822 [54:51<3:05:40,  1.43s/it] 21%|██        | 2043/9822 [54:53<3:05:38,  1.43s/it] 21%|██        | 2044/9822 [54:54<3:05:37,  1.43s/it] 21%|██        | 2045/9822 [54:56<3:05:47,  1.43s/it] 21%|██        | 2046/9822 [54:57<3:05:40,  1.43s/it] 21%|██        | 2047/9822 [54:58<3:06:03,  1.44s/it] 21%|██        | 2048/9822 [55:00<3:06:19,  1.44s/it] 21%|██        | 2049/9822 [55:01<3:06:22,  1.44s/it] 21%|██        | 2050/9822 [55:03<3:06:25,  1.44s/it] 21%|██        | 2051/9822 [55:04<3:06:51,  1.44s/it] 21%|██        | 2052/9822 [55:06<3:06:34,  1.44s/it] 21%|██        | 2053/9822 [55:07<3:05:56,  1.44s/it] 21%|██        | 2054/9822 [55:09<3:09:05,  1.46s/it] 21%|██        | 2055/9822 [55:10<3:07:58,  1.45s/it] 21%|██        | 2056/9822 [55:11<3:07:21,  1.45s/it] 21%|██        | 2057/9822 [55:13<3:07:21,  1.45s/it] 21%|██        | 2058/9822 [55:14<3:06:51,  1.44s/it] 21%|██        | 2059/9822 [55:16<3:06:11,  1.44s/it] 21%|██        | 2060/9822 [55:17<3:06:40,  1.44s/it] 21%|██        | 2061/9822 [55:19<3:07:02,  1.45s/it] 21%|██        | 2062/9822 [55:20<3:06:55,  1.45s/it] 21%|██        | 2063/9822 [55:22<3:06:30,  1.44s/it] 21%|██        | 2064/9822 [55:23<3:04:43,  1.43s/it] 21%|██        | 2065/9822 [55:24<3:06:06,  1.44s/it] 21%|██        | 2066/9822 [55:26<3:05:48,  1.44s/it] 21%|██        | 2067/9822 [55:27<3:05:44,  1.44s/it] 21%|██        | 2068/9822 [55:29<3:06:09,  1.44s/it] 21%|██        | 2069/9822 [55:30<3:05:59,  1.44s/it] 21%|██        | 2070/9822 [55:32<3:07:24,  1.45s/it] 21%|██        | 2071/9822 [55:33<3:06:34,  1.44s/it] 21%|██        | 2072/9822 [55:35<3:06:01,  1.44s/it] 21%|██        | 2073/9822 [55:36<3:05:23,  1.44s/it] 21%|██        | 2074/9822 [55:37<3:04:49,  1.43s/it] 21%|██        | 2075/9822 [55:39<3:04:24,  1.43s/it] 21%|██        | 2076/9822 [55:40<3:04:58,  1.43s/it] 21%|██        | 2077/9822 [55:42<3:04:47,  1.43s/it] 21%|██        | 2078/9822 [55:43<3:04:42,  1.43s/it] 21%|██        | 2079/9822 [55:45<3:05:06,  1.43s/it] 21%|██        | 2080/9822 [55:46<3:05:07,  1.43s/it] 21%|██        | 2081/9822 [55:47<3:04:55,  1.43s/it] 21%|██        | 2082/9822 [55:49<3:06:26,  1.45s/it] 21%|██        | 2083/9822 [55:50<3:06:14,  1.44s/it] 21%|██        | 2084/9822 [55:52<3:05:58,  1.44s/it] 21%|██        | 2085/9822 [55:53<3:08:36,  1.46s/it] 21%|██        | 2086/9822 [55:55<3:07:43,  1.46s/it] 21%|██        | 2087/9822 [55:56<3:06:58,  1.45s/it] 21%|██▏       | 2088/9822 [55:58<3:06:18,  1.45s/it] 21%|██▏       | 2089/9822 [55:59<3:05:45,  1.44s/it] 21%|██▏       | 2090/9822 [56:00<3:05:18,  1.44s/it] 21%|██▏       | 2091/9822 [56:02<3:04:52,  1.43s/it] 21%|██▏       | 2092/9822 [56:03<3:04:46,  1.43s/it] 21%|██▏       | 2093/9822 [56:05<3:04:41,  1.43s/it] 21%|██▏       | 2094/9822 [56:06<3:04:17,  1.43s/it] 21%|██▏       | 2095/9822 [56:08<3:04:17,  1.43s/it] 21%|██▏       | 2096/9822 [56:09<3:03:58,  1.43s/it] 21%|██▏       | 2097/9822 [56:10<3:04:21,  1.43s/it] 21%|██▏       | 2098/9822 [56:12<3:04:37,  1.43s/it] 21%|██▏       | 2099/9822 [56:13<3:04:12,  1.43s/it] 21%|██▏       | 2100/9822 [56:15<3:04:13,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:44:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:44:02 - INFO - __main__ - ***** test Results*****
01/07/2024 22:44:02 - INFO - __main__ -   Training step = 2100
01/07/2024 22:44:02 - INFO - __main__ -  test_accuracy:0.849194729136164 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:44:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:44:08 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:44:08 - INFO - __main__ -   Training step = 2100
01/07/2024 22:44:08 - INFO - __main__ -  eval_accuracy:0.8462101794214574 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:44:08,174 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:44:08,174 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:44:08,211 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:44:09,793 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8462101794214574}
test:
{'accuracy': 0.849194729136164}
01/07/2024 22:44:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:44:14 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:44:14 - INFO - __main__ -   Training step = 2100
01/07/2024 22:44:14 - INFO - __main__ -  eval_accuracy:0.8952764555108019 
 21%|██▏       | 2101/9822 [56:35<14:59:18,  6.99s/it] 21%|██▏       | 2102/9822 [56:36<11:24:28,  5.32s/it] 21%|██▏       | 2103/9822 [56:38<8:54:19,  4.15s/it]  21%|██▏       | 2104/9822 [56:39<7:08:49,  3.33s/it] 21%|██▏       | 2105/9822 [56:40<5:56:13,  2.77s/it] 21%|██▏       | 2106/9822 [56:42<5:04:30,  2.37s/it] 21%|██▏       | 2107/9822 [56:43<4:28:21,  2.09s/it] 21%|██▏       | 2108/9822 [56:45<4:03:04,  1.89s/it] 21%|██▏       | 2109/9822 [56:46<3:45:51,  1.76s/it] 21%|██▏       | 2110/9822 [56:48<3:33:31,  1.66s/it] 21%|██▏       | 2111/9822 [56:49<3:24:39,  1.59s/it] 22%|██▏       | 2112/9822 [56:50<3:18:09,  1.54s/it] 22%|██▏       | 2113/9822 [56:52<3:16:57,  1.53s/it] 22%|██▏       | 2114/9822 [56:53<3:12:44,  1.50s/it] 22%|██▏       | 2115/9822 [56:55<3:10:06,  1.48s/it] 22%|██▏       | 2116/9822 [56:56<3:08:15,  1.47s/it] 22%|██▏       | 2117/9822 [56:58<3:07:00,  1.46s/it] 22%|██▏       | 2118/9822 [56:59<3:07:41,  1.46s/it] 22%|██▏       | 2119/9822 [57:01<3:06:16,  1.45s/it] 22%|██▏       | 2120/9822 [57:02<3:05:30,  1.45s/it] 22%|██▏       | 2121/9822 [57:03<3:04:34,  1.44s/it] 22%|██▏       | 2122/9822 [57:05<3:03:54,  1.43s/it] 22%|██▏       | 2123/9822 [57:06<3:04:41,  1.44s/it] 22%|██▏       | 2124/9822 [57:08<3:04:04,  1.43s/it] 22%|██▏       | 2125/9822 [57:09<3:04:07,  1.44s/it] 22%|██▏       | 2126/9822 [57:11<3:04:14,  1.44s/it] 22%|██▏       | 2127/9822 [57:12<3:04:07,  1.44s/it] 22%|██▏       | 2128/9822 [57:13<3:03:39,  1.43s/it] 22%|██▏       | 2129/9822 [57:15<3:03:33,  1.43s/it] 22%|██▏       | 2130/9822 [57:16<3:03:39,  1.43s/it] 22%|██▏       | 2131/9822 [57:18<3:03:54,  1.43s/it] 22%|██▏       | 2132/9822 [57:19<3:04:13,  1.44s/it] 22%|██▏       | 2133/9822 [57:21<3:03:54,  1.44s/it] 22%|██▏       | 2134/9822 [57:22<3:03:30,  1.43s/it] 22%|██▏       | 2135/9822 [57:24<3:02:58,  1.43s/it] 22%|██▏       | 2136/9822 [57:25<3:03:09,  1.43s/it] 22%|██▏       | 2137/9822 [57:26<3:03:00,  1.43s/it] 22%|██▏       | 2138/9822 [57:28<3:02:54,  1.43s/it] 22%|██▏       | 2139/9822 [57:29<3:02:41,  1.43s/it] 22%|██▏       | 2140/9822 [57:31<3:02:58,  1.43s/it] 22%|██▏       | 2141/9822 [57:32<3:02:51,  1.43s/it] 22%|██▏       | 2142/9822 [57:34<3:02:35,  1.43s/it] 22%|██▏       | 2143/9822 [57:35<3:02:45,  1.43s/it] 22%|██▏       | 2144/9822 [57:36<3:06:17,  1.46s/it] 22%|██▏       | 2145/9822 [57:38<3:05:03,  1.45s/it] 22%|██▏       | 2146/9822 [57:39<3:04:26,  1.44s/it] 22%|██▏       | 2147/9822 [57:41<3:03:46,  1.44s/it] 22%|██▏       | 2148/9822 [57:42<3:03:55,  1.44s/it] 22%|██▏       | 2149/9822 [57:44<3:03:42,  1.44s/it] 22%|██▏       | 2150/9822 [57:45<3:01:45,  1.42s/it] 22%|██▏       | 2151/9822 [57:46<3:02:27,  1.43s/it] 22%|██▏       | 2152/9822 [57:48<3:03:03,  1.43s/it] 22%|██▏       | 2153/9822 [57:49<3:02:42,  1.43s/it] 22%|██▏       | 2154/9822 [57:51<3:02:35,  1.43s/it] 22%|██▏       | 2155/9822 [57:52<3:02:18,  1.43s/it] 22%|██▏       | 2156/9822 [57:54<3:02:09,  1.43s/it] 22%|██▏       | 2157/9822 [57:55<3:02:35,  1.43s/it] 22%|██▏       | 2158/9822 [57:56<3:03:03,  1.43s/it] 22%|██▏       | 2159/9822 [57:58<3:03:03,  1.43s/it] 22%|██▏       | 2160/9822 [57:59<3:03:07,  1.43s/it] 22%|██▏       | 2161/9822 [58:01<3:03:15,  1.44s/it] 22%|██▏       | 2162/9822 [58:02<3:03:16,  1.44s/it] 22%|██▏       | 2163/9822 [58:04<3:02:50,  1.43s/it] 22%|██▏       | 2164/9822 [58:05<3:02:57,  1.43s/it] 22%|██▏       | 2165/9822 [58:06<3:02:41,  1.43s/it] 22%|██▏       | 2166/9822 [58:08<3:02:47,  1.43s/it] 22%|██▏       | 2167/9822 [58:09<3:02:30,  1.43s/it] 22%|██▏       | 2168/9822 [58:11<3:02:25,  1.43s/it] 22%|██▏       | 2169/9822 [58:12<3:05:29,  1.45s/it] 22%|██▏       | 2170/9822 [58:14<3:04:09,  1.44s/it] 22%|██▏       | 2171/9822 [58:15<3:03:46,  1.44s/it] 22%|██▏       | 2172/9822 [58:17<3:03:15,  1.44s/it] 22%|██▏       | 2173/9822 [58:18<3:03:29,  1.44s/it] 22%|██▏       | 2174/9822 [58:19<3:03:05,  1.44s/it] 22%|██▏       | 2175/9822 [58:21<3:02:47,  1.43s/it] 22%|██▏       | 2176/9822 [58:22<3:02:27,  1.43s/it] 22%|██▏       | 2177/9822 [58:24<3:01:59,  1.43s/it] 22%|██▏       | 2178/9822 [58:25<3:01:40,  1.43s/it] 22%|██▏       | 2179/9822 [58:27<3:02:00,  1.43s/it] 22%|██▏       | 2180/9822 [58:28<3:01:51,  1.43s/it] 22%|██▏       | 2181/9822 [58:29<3:01:45,  1.43s/it] 22%|██▏       | 2182/9822 [58:31<3:01:44,  1.43s/it] 22%|██▏       | 2183/9822 [58:32<3:02:01,  1.43s/it] 22%|██▏       | 2184/9822 [58:34<3:01:45,  1.43s/it] 22%|██▏       | 2185/9822 [58:35<3:01:46,  1.43s/it] 22%|██▏       | 2186/9822 [58:37<3:02:17,  1.43s/it] 22%|██▏       | 2187/9822 [58:38<3:02:10,  1.43s/it] 22%|██▏       | 2188/9822 [58:39<3:02:04,  1.43s/it] 22%|██▏       | 2189/9822 [58:41<3:02:06,  1.43s/it] 22%|██▏       | 2190/9822 [58:42<3:02:29,  1.43s/it] 22%|██▏       | 2191/9822 [58:44<3:01:52,  1.43s/it] 22%|██▏       | 2192/9822 [58:45<3:02:29,  1.44s/it] 22%|██▏       | 2193/9822 [58:47<3:02:41,  1.44s/it] 22%|██▏       | 2194/9822 [58:48<3:02:20,  1.43s/it] 22%|██▏       | 2195/9822 [58:49<3:02:09,  1.43s/it] 22%|██▏       | 2196/9822 [58:51<3:02:06,  1.43s/it] 22%|██▏       | 2197/9822 [58:52<3:02:09,  1.43s/it] 22%|██▏       | 2198/9822 [58:54<3:02:10,  1.43s/it] 22%|██▏       | 2199/9822 [58:55<3:01:54,  1.43s/it] 22%|██▏       | 2200/9822 [58:57<3:02:14,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:46:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:46:43 - INFO - __main__ - ***** test Results*****
01/07/2024 22:46:43 - INFO - __main__ -   Training step = 2200
01/07/2024 22:46:43 - INFO - __main__ -  test_accuracy:0.8612737920937042 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:46:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:46:50 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:46:50 - INFO - __main__ -   Training step = 2200
01/07/2024 22:46:50 - INFO - __main__ -  eval_accuracy:0.8454778469425119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8462101794214574}
test:
{'accuracy': 0.849194729136164}
01/07/2024 22:46:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:46:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:46:54 - INFO - __main__ -   Training step = 2200
01/07/2024 22:46:54 - INFO - __main__ -  eval_accuracy:0.90040278286342 
 22%|██▏       | 2201/9822 [59:15<13:48:54,  6.53s/it] 22%|██▏       | 2202/9822 [59:17<10:35:22,  5.00s/it] 22%|██▏       | 2203/9822 [59:18<8:20:42,  3.94s/it]  22%|██▏       | 2204/9822 [59:19<6:44:53,  3.19s/it] 22%|██▏       | 2205/9822 [59:21<5:37:56,  2.66s/it] 22%|██▏       | 2206/9822 [59:22<4:51:35,  2.30s/it] 22%|██▏       | 2207/9822 [59:24<4:18:20,  2.04s/it] 22%|██▏       | 2208/9822 [59:25<3:55:20,  1.85s/it] 22%|██▏       | 2209/9822 [59:27<3:39:24,  1.73s/it] 23%|██▎       | 2210/9822 [59:28<3:28:04,  1.64s/it] 23%|██▎       | 2211/9822 [59:29<3:20:17,  1.58s/it] 23%|██▎       | 2212/9822 [59:31<3:14:32,  1.53s/it] 23%|██▎       | 2213/9822 [59:32<3:10:50,  1.50s/it] 23%|██▎       | 2214/9822 [59:34<3:07:42,  1.48s/it] 23%|██▎       | 2215/9822 [59:35<3:05:46,  1.47s/it] 23%|██▎       | 2216/9822 [59:37<3:04:31,  1.46s/it] 23%|██▎       | 2217/9822 [59:38<3:04:56,  1.46s/it] 23%|██▎       | 2218/9822 [59:40<3:03:48,  1.45s/it] 23%|██▎       | 2219/9822 [59:41<3:02:44,  1.44s/it] 23%|██▎       | 2220/9822 [59:42<3:02:02,  1.44s/it] 23%|██▎       | 2221/9822 [59:44<3:02:18,  1.44s/it] 23%|██▎       | 2222/9822 [59:45<3:02:07,  1.44s/it] 23%|██▎       | 2223/9822 [59:47<3:02:00,  1.44s/it] 23%|██▎       | 2224/9822 [59:48<3:02:05,  1.44s/it] 23%|██▎       | 2225/9822 [59:50<3:02:00,  1.44s/it] 23%|██▎       | 2226/9822 [59:51<3:01:34,  1.43s/it] 23%|██▎       | 2227/9822 [59:52<3:01:13,  1.43s/it] 23%|██▎       | 2228/9822 [59:54<3:01:06,  1.43s/it] 23%|██▎       | 2229/9822 [59:55<3:01:13,  1.43s/it] 23%|██▎       | 2230/9822 [59:57<3:01:38,  1.44s/it] 23%|██▎       | 2231/9822 [59:58<3:01:21,  1.43s/it] 23%|██▎       | 2232/9822 [1:00:00<3:01:10,  1.43s/it] 23%|██▎       | 2233/9822 [1:00:01<3:03:56,  1.45s/it] 23%|██▎       | 2234/9822 [1:00:03<3:03:14,  1.45s/it] 23%|██▎       | 2235/9822 [1:00:04<3:02:32,  1.44s/it] 23%|██▎       | 2236/9822 [1:00:05<3:00:04,  1.42s/it] 23%|██▎       | 2237/9822 [1:00:07<3:00:37,  1.43s/it] 23%|██▎       | 2238/9822 [1:00:08<3:01:40,  1.44s/it] 23%|██▎       | 2239/9822 [1:00:10<3:02:16,  1.44s/it] 23%|██▎       | 2240/9822 [1:00:11<3:02:36,  1.45s/it] 23%|██▎       | 2241/9822 [1:00:13<3:02:41,  1.45s/it] 23%|██▎       | 2242/9822 [1:00:14<3:03:03,  1.45s/it] 23%|██▎       | 2243/9822 [1:00:15<3:02:46,  1.45s/it] 23%|██▎       | 2244/9822 [1:00:17<3:03:51,  1.46s/it] 23%|██▎       | 2245/9822 [1:00:18<3:02:58,  1.45s/it] 23%|██▎       | 2246/9822 [1:00:20<3:02:21,  1.44s/it] 23%|██▎       | 2247/9822 [1:00:21<3:02:18,  1.44s/it] 23%|██▎       | 2248/9822 [1:00:23<3:01:46,  1.44s/it] 23%|██▎       | 2249/9822 [1:00:24<3:01:17,  1.44s/it] 23%|██▎       | 2250/9822 [1:00:26<3:01:05,  1.43s/it] 23%|██▎       | 2251/9822 [1:00:27<3:01:00,  1.43s/it] 23%|██▎       | 2252/9822 [1:00:28<3:00:54,  1.43s/it] 23%|██▎       | 2253/9822 [1:00:30<3:00:30,  1.43s/it] 23%|██▎       | 2254/9822 [1:00:31<3:00:34,  1.43s/it] 23%|██▎       | 2255/9822 [1:00:33<3:01:05,  1.44s/it] 23%|██▎       | 2256/9822 [1:00:34<3:00:58,  1.44s/it] 23%|██▎       | 2257/9822 [1:00:36<3:00:28,  1.43s/it] 23%|██▎       | 2258/9822 [1:00:37<3:01:03,  1.44s/it] 23%|██▎       | 2259/9822 [1:00:38<3:01:46,  1.44s/it] 23%|██▎       | 2260/9822 [1:00:40<3:01:35,  1.44s/it] 23%|██▎       | 2261/9822 [1:00:41<3:01:41,  1.44s/it] 23%|██▎       | 2262/9822 [1:00:43<3:01:13,  1.44s/it] 23%|██▎       | 2263/9822 [1:00:44<3:00:51,  1.44s/it] 23%|██▎       | 2264/9822 [1:00:46<3:00:59,  1.44s/it] 23%|██▎       | 2265/9822 [1:00:47<3:03:53,  1.46s/it] 23%|██▎       | 2266/9822 [1:00:49<3:02:49,  1.45s/it] 23%|██▎       | 2267/9822 [1:00:50<3:02:34,  1.45s/it] 23%|██▎       | 2268/9822 [1:00:52<3:03:31,  1.46s/it] 23%|██▎       | 2269/9822 [1:00:53<3:02:57,  1.45s/it] 23%|██▎       | 2270/9822 [1:00:54<3:02:25,  1.45s/it] 23%|██▎       | 2271/9822 [1:00:56<3:02:12,  1.45s/it] 23%|██▎       | 2272/9822 [1:00:57<3:01:23,  1.44s/it] 23%|██▎       | 2273/9822 [1:00:59<3:00:54,  1.44s/it] 23%|██▎       | 2274/9822 [1:01:00<3:00:35,  1.44s/it] 23%|██▎       | 2275/9822 [1:01:02<3:00:11,  1.43s/it] 23%|██▎       | 2276/9822 [1:01:03<3:00:13,  1.43s/it] 23%|██▎       | 2277/9822 [1:01:04<3:00:53,  1.44s/it] 23%|██▎       | 2278/9822 [1:01:06<3:02:42,  1.45s/it] 23%|██▎       | 2279/9822 [1:01:07<3:01:59,  1.45s/it] 23%|██▎       | 2280/9822 [1:01:09<3:02:01,  1.45s/it] 23%|██▎       | 2281/9822 [1:01:10<3:02:01,  1.45s/it] 23%|██▎       | 2282/9822 [1:01:12<3:01:38,  1.45s/it] 23%|██▎       | 2283/9822 [1:01:13<3:01:59,  1.45s/it] 23%|██▎       | 2284/9822 [1:01:15<3:01:39,  1.45s/it] 23%|██▎       | 2285/9822 [1:01:16<3:01:19,  1.44s/it] 23%|██▎       | 2286/9822 [1:01:17<3:00:53,  1.44s/it] 23%|██▎       | 2287/9822 [1:01:19<3:00:39,  1.44s/it] 23%|██▎       | 2288/9822 [1:01:20<3:00:42,  1.44s/it] 23%|██▎       | 2289/9822 [1:01:22<3:02:01,  1.45s/it] 23%|██▎       | 2290/9822 [1:01:23<3:04:56,  1.47s/it] 23%|██▎       | 2291/9822 [1:01:25<3:03:28,  1.46s/it] 23%|██▎       | 2292/9822 [1:01:26<3:02:23,  1.45s/it] 23%|██▎       | 2293/9822 [1:01:28<3:01:33,  1.45s/it] 23%|██▎       | 2294/9822 [1:01:29<3:00:37,  1.44s/it] 23%|██▎       | 2295/9822 [1:01:31<3:01:12,  1.44s/it] 23%|██▎       | 2296/9822 [1:01:32<3:00:47,  1.44s/it] 23%|██▎       | 2297/9822 [1:01:33<3:00:18,  1.44s/it] 23%|██▎       | 2298/9822 [1:01:35<3:00:04,  1.44s/it] 23%|██▎       | 2299/9822 [1:01:36<2:59:48,  1.43s/it] 23%|██▎       | 2300/9822 [1:01:38<2:59:27,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:49:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:49:25 - INFO - __main__ - ***** test Results*****
01/07/2024 22:49:25 - INFO - __main__ -   Training step = 2300
01/07/2024 22:49:25 - INFO - __main__ -  test_accuracy:0.845900439238653 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:49:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:49:31 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:49:31 - INFO - __main__ -   Training step = 2300
01/07/2024 22:49:31 - INFO - __main__ -  eval_accuracy:0.841450018308312 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8462101794214574}
test:
{'accuracy': 0.849194729136164}
01/07/2024 22:49:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:49:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:49:35 - INFO - __main__ -   Training step = 2300
01/07/2024 22:49:35 - INFO - __main__ -  eval_accuracy:0.8982057854265837 
 23%|██▎       | 2301/9822 [1:01:56<13:35:39,  6.51s/it] 23%|██▎       | 2302/9822 [1:01:57<10:24:41,  4.98s/it] 23%|██▎       | 2303/9822 [1:01:59<8:11:00,  3.92s/it]  23%|██▎       | 2304/9822 [1:02:00<6:37:12,  3.17s/it] 23%|██▎       | 2305/9822 [1:02:02<5:31:55,  2.65s/it] 23%|██▎       | 2306/9822 [1:02:03<4:46:04,  2.28s/it] 23%|██▎       | 2307/9822 [1:02:05<4:14:15,  2.03s/it] 23%|██▎       | 2308/9822 [1:02:06<3:51:22,  1.85s/it] 24%|██▎       | 2309/9822 [1:02:07<3:35:45,  1.72s/it] 24%|██▎       | 2310/9822 [1:02:09<3:24:27,  1.63s/it] 24%|██▎       | 2311/9822 [1:02:10<3:16:50,  1.57s/it] 24%|██▎       | 2312/9822 [1:02:12<3:11:32,  1.53s/it] 24%|██▎       | 2313/9822 [1:02:13<3:07:39,  1.50s/it] 24%|██▎       | 2314/9822 [1:02:15<3:04:57,  1.48s/it] 24%|██▎       | 2315/9822 [1:02:16<3:02:57,  1.46s/it] 24%|██▎       | 2316/9822 [1:02:17<3:01:59,  1.45s/it] 24%|██▎       | 2317/9822 [1:02:19<3:01:22,  1.45s/it] 24%|██▎       | 2318/9822 [1:02:20<3:00:20,  1.44s/it] 24%|██▎       | 2319/9822 [1:02:22<3:03:27,  1.47s/it] 24%|██▎       | 2320/9822 [1:02:23<3:02:41,  1.46s/it] 24%|██▎       | 2321/9822 [1:02:25<3:01:35,  1.45s/it] 24%|██▎       | 2322/9822 [1:02:26<2:59:01,  1.43s/it] 24%|██▎       | 2323/9822 [1:02:28<2:59:23,  1.44s/it] 24%|██▎       | 2324/9822 [1:02:29<2:59:17,  1.43s/it] 24%|██▎       | 2325/9822 [1:02:30<2:59:05,  1.43s/it] 24%|██▎       | 2326/9822 [1:02:32<2:59:13,  1.43s/it] 24%|██▎       | 2327/9822 [1:02:33<2:58:39,  1.43s/it] 24%|██▎       | 2328/9822 [1:02:35<2:59:19,  1.44s/it] 24%|██▎       | 2329/9822 [1:02:36<2:59:03,  1.43s/it] 24%|██▎       | 2330/9822 [1:02:38<2:59:17,  1.44s/it] 24%|██▎       | 2331/9822 [1:02:39<2:59:32,  1.44s/it] 24%|██▎       | 2332/9822 [1:02:40<2:59:27,  1.44s/it] 24%|██▍       | 2333/9822 [1:02:42<2:59:09,  1.44s/it] 24%|██▍       | 2334/9822 [1:02:43<2:59:11,  1.44s/it] 24%|██▍       | 2335/9822 [1:02:45<3:01:14,  1.45s/it] 24%|██▍       | 2336/9822 [1:02:46<3:00:44,  1.45s/it] 24%|██▍       | 2337/9822 [1:02:48<3:00:23,  1.45s/it] 24%|██▍       | 2338/9822 [1:02:49<3:00:26,  1.45s/it] 24%|██▍       | 2339/9822 [1:02:51<3:00:00,  1.44s/it] 24%|██▍       | 2340/9822 [1:02:52<2:59:45,  1.44s/it] 24%|██▍       | 2341/9822 [1:02:53<2:59:41,  1.44s/it] 24%|██▍       | 2342/9822 [1:02:55<2:59:43,  1.44s/it] 24%|██▍       | 2343/9822 [1:02:56<2:59:22,  1.44s/it] 24%|██▍       | 2344/9822 [1:02:58<2:59:03,  1.44s/it] 24%|██▍       | 2345/9822 [1:02:59<2:58:42,  1.43s/it] 24%|██▍       | 2346/9822 [1:03:01<2:58:26,  1.43s/it] 24%|██▍       | 2347/9822 [1:03:02<2:59:05,  1.44s/it] 24%|██▍       | 2348/9822 [1:03:04<2:59:08,  1.44s/it] 24%|██▍       | 2349/9822 [1:03:05<2:58:57,  1.44s/it] 24%|██▍       | 2350/9822 [1:03:06<2:58:49,  1.44s/it] 24%|██▍       | 2351/9822 [1:03:08<3:01:34,  1.46s/it] 24%|██▍       | 2352/9822 [1:03:09<3:00:37,  1.45s/it] 24%|██▍       | 2353/9822 [1:03:11<3:00:10,  1.45s/it] 24%|██▍       | 2354/9822 [1:03:12<3:00:01,  1.45s/it] 24%|██▍       | 2355/9822 [1:03:14<2:59:34,  1.44s/it] 24%|██▍       | 2356/9822 [1:03:15<2:59:21,  1.44s/it] 24%|██▍       | 2357/9822 [1:03:17<3:00:18,  1.45s/it] 24%|██▍       | 2358/9822 [1:03:18<2:59:53,  1.45s/it] 24%|██▍       | 2359/9822 [1:03:19<2:59:21,  1.44s/it] 24%|██▍       | 2360/9822 [1:03:21<2:59:37,  1.44s/it] 24%|██▍       | 2361/9822 [1:03:22<2:58:53,  1.44s/it] 24%|██▍       | 2362/9822 [1:03:24<2:58:10,  1.43s/it] 24%|██▍       | 2363/9822 [1:03:25<2:59:59,  1.45s/it] 24%|██▍       | 2364/9822 [1:03:27<2:59:04,  1.44s/it] 24%|██▍       | 2365/9822 [1:03:28<2:59:01,  1.44s/it] 24%|██▍       | 2366/9822 [1:03:30<2:58:48,  1.44s/it] 24%|██▍       | 2367/9822 [1:03:31<2:58:30,  1.44s/it] 24%|██▍       | 2368/9822 [1:03:32<2:57:52,  1.43s/it] 24%|██▍       | 2369/9822 [1:03:34<2:57:41,  1.43s/it] 24%|██▍       | 2370/9822 [1:03:35<2:57:59,  1.43s/it] 24%|██▍       | 2371/9822 [1:03:37<2:57:36,  1.43s/it] 24%|██▍       | 2372/9822 [1:03:38<2:58:07,  1.43s/it] 24%|██▍       | 2373/9822 [1:03:40<2:58:30,  1.44s/it] 24%|██▍       | 2374/9822 [1:03:41<2:58:31,  1.44s/it] 24%|██▍       | 2375/9822 [1:03:42<2:58:27,  1.44s/it] 24%|██▍       | 2376/9822 [1:03:44<2:58:56,  1.44s/it] 24%|██▍       | 2377/9822 [1:03:45<2:58:49,  1.44s/it] 24%|██▍       | 2378/9822 [1:03:47<2:58:56,  1.44s/it] 24%|██▍       | 2379/9822 [1:03:48<2:59:58,  1.45s/it] 24%|██▍       | 2380/9822 [1:03:50<2:59:36,  1.45s/it] 24%|██▍       | 2381/9822 [1:03:51<2:59:17,  1.45s/it] 24%|██▍       | 2382/9822 [1:03:53<2:58:41,  1.44s/it] 24%|██▍       | 2383/9822 [1:03:54<3:01:26,  1.46s/it] 24%|██▍       | 2384/9822 [1:03:55<3:00:25,  1.46s/it] 24%|██▍       | 2385/9822 [1:03:57<2:59:03,  1.44s/it] 24%|██▍       | 2386/9822 [1:03:58<2:59:15,  1.45s/it] 24%|██▍       | 2387/9822 [1:04:00<3:00:31,  1.46s/it] 24%|██▍       | 2388/9822 [1:04:01<3:01:35,  1.47s/it] 24%|██▍       | 2389/9822 [1:04:03<2:59:59,  1.45s/it] 24%|██▍       | 2390/9822 [1:04:04<2:59:31,  1.45s/it] 24%|██▍       | 2391/9822 [1:04:06<2:59:15,  1.45s/it] 24%|██▍       | 2392/9822 [1:04:07<2:59:23,  1.45s/it] 24%|██▍       | 2393/9822 [1:04:09<2:59:12,  1.45s/it] 24%|██▍       | 2394/9822 [1:04:10<2:58:56,  1.45s/it] 24%|██▍       | 2395/9822 [1:04:11<2:58:40,  1.44s/it] 24%|██▍       | 2396/9822 [1:04:13<2:57:56,  1.44s/it] 24%|██▍       | 2397/9822 [1:04:14<2:58:41,  1.44s/it] 24%|██▍       | 2398/9822 [1:04:16<2:59:52,  1.45s/it] 24%|██▍       | 2399/9822 [1:04:17<3:01:11,  1.46s/it] 24%|██▍       | 2400/9822 [1:04:19<3:02:01,  1.47s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:52:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:52:06 - INFO - __main__ - ***** test Results*****
01/07/2024 22:52:06 - INFO - __main__ -   Training step = 2400
01/07/2024 22:52:06 - INFO - __main__ -  test_accuracy:0.8642020497803806 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:52:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:52:12 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:52:12 - INFO - __main__ -   Training step = 2400
01/07/2024 22:52:12 - INFO - __main__ -  eval_accuracy:0.8487733430977664 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 22:52:12,152 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 22:52:12,152 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 22:52:12,189 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 22:52:13,782 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8487733430977664}
test:
{'accuracy': 0.8642020497803806}
01/07/2024 22:52:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:52:18 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:52:18 - INFO - __main__ -   Training step = 2400
01/07/2024 22:52:18 - INFO - __main__ -  eval_accuracy:0.897839619187111 
 24%|██▍       | 2401/9822 [1:04:39<14:27:22,  7.01s/it] 24%|██▍       | 2402/9822 [1:04:40<11:00:48,  5.34s/it] 24%|██▍       | 2403/9822 [1:04:42<8:35:40,  4.17s/it]  24%|██▍       | 2404/9822 [1:04:43<6:55:54,  3.36s/it] 24%|██▍       | 2405/9822 [1:04:45<5:45:15,  2.79s/it] 24%|██▍       | 2406/9822 [1:04:46<4:54:54,  2.39s/it] 25%|██▍       | 2407/9822 [1:04:47<4:19:12,  2.10s/it] 25%|██▍       | 2408/9822 [1:04:49<3:52:51,  1.88s/it] 25%|██▍       | 2409/9822 [1:04:50<3:36:06,  1.75s/it] 25%|██▍       | 2410/9822 [1:04:52<3:25:12,  1.66s/it] 25%|██▍       | 2411/9822 [1:04:53<3:17:17,  1.60s/it] 25%|██▍       | 2412/9822 [1:04:55<3:10:57,  1.55s/it] 25%|██▍       | 2413/9822 [1:04:56<3:10:06,  1.54s/it] 25%|██▍       | 2414/9822 [1:04:57<3:05:58,  1.51s/it] 25%|██▍       | 2415/9822 [1:04:59<3:03:03,  1.48s/it] 25%|██▍       | 2416/9822 [1:05:00<3:01:15,  1.47s/it] 25%|██▍       | 2417/9822 [1:05:02<3:00:03,  1.46s/it] 25%|██▍       | 2418/9822 [1:05:03<2:59:30,  1.45s/it] 25%|██▍       | 2419/9822 [1:05:05<2:58:25,  1.45s/it] 25%|██▍       | 2420/9822 [1:05:06<2:57:53,  1.44s/it] 25%|██▍       | 2421/9822 [1:05:08<2:57:46,  1.44s/it] 25%|██▍       | 2422/9822 [1:05:09<2:57:36,  1.44s/it] 25%|██▍       | 2423/9822 [1:05:10<2:57:50,  1.44s/it] 25%|██▍       | 2424/9822 [1:05:12<2:57:27,  1.44s/it] 25%|██▍       | 2425/9822 [1:05:13<2:58:31,  1.45s/it] 25%|██▍       | 2426/9822 [1:05:15<2:57:46,  1.44s/it] 25%|██▍       | 2427/9822 [1:05:16<2:57:09,  1.44s/it] 25%|██▍       | 2428/9822 [1:05:18<2:57:03,  1.44s/it] 25%|██▍       | 2429/9822 [1:05:19<2:56:45,  1.43s/it] 25%|██▍       | 2430/9822 [1:05:20<2:56:25,  1.43s/it] 25%|██▍       | 2431/9822 [1:05:22<2:56:10,  1.43s/it] 25%|██▍       | 2432/9822 [1:05:23<2:56:59,  1.44s/it] 25%|██▍       | 2433/9822 [1:05:25<2:57:00,  1.44s/it] 25%|██▍       | 2434/9822 [1:05:26<2:56:50,  1.44s/it] 25%|██▍       | 2435/9822 [1:05:28<2:56:34,  1.43s/it] 25%|██▍       | 2436/9822 [1:05:29<2:56:21,  1.43s/it] 25%|██▍       | 2437/9822 [1:05:31<2:56:29,  1.43s/it] 25%|██▍       | 2438/9822 [1:05:32<2:56:30,  1.43s/it] 25%|██▍       | 2439/9822 [1:05:33<2:56:08,  1.43s/it] 25%|██▍       | 2440/9822 [1:05:35<2:57:09,  1.44s/it] 25%|██▍       | 2441/9822 [1:05:36<2:56:52,  1.44s/it] 25%|██▍       | 2442/9822 [1:05:38<2:56:59,  1.44s/it] 25%|██▍       | 2443/9822 [1:05:39<2:57:19,  1.44s/it] 25%|██▍       | 2444/9822 [1:05:41<2:56:49,  1.44s/it] 25%|██▍       | 2445/9822 [1:05:42<2:59:45,  1.46s/it] 25%|██▍       | 2446/9822 [1:05:44<2:59:02,  1.46s/it] 25%|██▍       | 2447/9822 [1:05:45<2:58:11,  1.45s/it] 25%|██▍       | 2448/9822 [1:05:46<2:58:01,  1.45s/it] 25%|██▍       | 2449/9822 [1:05:48<2:57:15,  1.44s/it] 25%|██▍       | 2450/9822 [1:05:49<2:57:22,  1.44s/it] 25%|██▍       | 2451/9822 [1:05:51<2:56:31,  1.44s/it] 25%|██▍       | 2452/9822 [1:05:52<2:56:35,  1.44s/it] 25%|██▍       | 2453/9822 [1:05:54<2:56:05,  1.43s/it] 25%|██▍       | 2454/9822 [1:05:55<2:55:56,  1.43s/it] 25%|██▍       | 2455/9822 [1:05:56<2:55:39,  1.43s/it] 25%|██▌       | 2456/9822 [1:05:58<2:55:42,  1.43s/it] 25%|██▌       | 2457/9822 [1:05:59<2:55:42,  1.43s/it] 25%|██▌       | 2458/9822 [1:06:01<2:55:56,  1.43s/it] 25%|██▌       | 2459/9822 [1:06:02<2:55:40,  1.43s/it] 25%|██▌       | 2460/9822 [1:06:04<2:55:26,  1.43s/it] 25%|██▌       | 2461/9822 [1:06:05<2:55:58,  1.43s/it] 25%|██▌       | 2462/9822 [1:06:06<2:56:10,  1.44s/it] 25%|██▌       | 2463/9822 [1:06:08<2:56:05,  1.44s/it] 25%|██▌       | 2464/9822 [1:06:09<2:56:13,  1.44s/it] 25%|██▌       | 2465/9822 [1:06:11<2:56:45,  1.44s/it] 25%|██▌       | 2466/9822 [1:06:12<2:55:58,  1.44s/it] 25%|██▌       | 2467/9822 [1:06:14<2:55:57,  1.44s/it] 25%|██▌       | 2468/9822 [1:06:15<2:56:29,  1.44s/it] 25%|██▌       | 2469/9822 [1:06:17<2:56:19,  1.44s/it] 25%|██▌       | 2470/9822 [1:06:18<2:58:44,  1.46s/it] 25%|██▌       | 2471/9822 [1:06:19<2:57:46,  1.45s/it] 25%|██▌       | 2472/9822 [1:06:21<2:56:54,  1.44s/it] 25%|██▌       | 2473/9822 [1:06:22<2:56:25,  1.44s/it] 25%|██▌       | 2474/9822 [1:06:24<2:56:08,  1.44s/it] 25%|██▌       | 2475/9822 [1:06:25<2:55:53,  1.44s/it] 25%|██▌       | 2476/9822 [1:06:27<2:56:01,  1.44s/it] 25%|██▌       | 2477/9822 [1:06:28<2:55:55,  1.44s/it] 25%|██▌       | 2478/9822 [1:06:30<2:55:50,  1.44s/it] 25%|██▌       | 2479/9822 [1:06:31<2:55:19,  1.43s/it] 25%|██▌       | 2480/9822 [1:06:32<2:55:37,  1.44s/it] 25%|██▌       | 2481/9822 [1:06:34<2:55:44,  1.44s/it] 25%|██▌       | 2482/9822 [1:06:35<2:55:44,  1.44s/it] 25%|██▌       | 2483/9822 [1:06:37<2:55:35,  1.44s/it] 25%|██▌       | 2484/9822 [1:06:38<2:55:24,  1.43s/it] 25%|██▌       | 2485/9822 [1:06:40<2:55:13,  1.43s/it] 25%|██▌       | 2486/9822 [1:06:41<2:55:26,  1.43s/it] 25%|██▌       | 2487/9822 [1:06:42<2:55:21,  1.43s/it] 25%|██▌       | 2488/9822 [1:06:44<2:55:26,  1.44s/it] 25%|██▌       | 2489/9822 [1:06:45<2:56:59,  1.45s/it] 25%|██▌       | 2490/9822 [1:06:47<2:55:56,  1.44s/it] 25%|██▌       | 2491/9822 [1:06:48<2:55:37,  1.44s/it] 25%|██▌       | 2492/9822 [1:06:50<2:54:57,  1.43s/it] 25%|██▌       | 2493/9822 [1:06:51<2:55:19,  1.44s/it] 25%|██▌       | 2494/9822 [1:06:52<2:53:46,  1.42s/it] 25%|██▌       | 2495/9822 [1:06:54<2:55:18,  1.44s/it] 25%|██▌       | 2496/9822 [1:06:55<2:54:46,  1.43s/it] 25%|██▌       | 2497/9822 [1:06:57<2:54:57,  1.43s/it] 25%|██▌       | 2498/9822 [1:06:58<2:54:49,  1.43s/it] 25%|██▌       | 2499/9822 [1:07:00<2:54:35,  1.43s/it] 25%|██▌       | 2500/9822 [1:07:01<2:54:46,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:54:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:54:48 - INFO - __main__ - ***** test Results*****
01/07/2024 22:54:48 - INFO - __main__ -   Training step = 2500
01/07/2024 22:54:48 - INFO - __main__ -  test_accuracy:0.8469985358711567 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:54:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:54:54 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:54:54 - INFO - __main__ -   Training step = 2500
01/07/2024 22:54:54 - INFO - __main__ -  eval_accuracy:0.8410838520688393 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8487733430977664}
test:
{'accuracy': 0.8642020497803806}
01/07/2024 22:54:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:54:59 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:54:59 - INFO - __main__ -   Training step = 2500
01/07/2024 22:54:59 - INFO - __main__ -  eval_accuracy:0.884657634566093 
 25%|██▌       | 2501/9822 [1:07:19<13:11:47,  6.49s/it] 25%|██▌       | 2502/9822 [1:07:21<10:09:13,  4.99s/it] 25%|██▌       | 2503/9822 [1:07:22<7:58:16,  3.92s/it]  25%|██▌       | 2504/9822 [1:07:24<6:26:35,  3.17s/it] 26%|██▌       | 2505/9822 [1:07:25<5:22:34,  2.65s/it] 26%|██▌       | 2506/9822 [1:07:27<4:37:42,  2.28s/it] 26%|██▌       | 2507/9822 [1:07:28<4:06:09,  2.02s/it] 26%|██▌       | 2508/9822 [1:07:29<3:45:16,  1.85s/it] 26%|██▌       | 2509/9822 [1:07:31<3:32:06,  1.74s/it] 26%|██▌       | 2510/9822 [1:07:32<3:21:58,  1.66s/it] 26%|██▌       | 2511/9822 [1:07:34<3:13:38,  1.59s/it] 26%|██▌       | 2512/9822 [1:07:35<3:08:30,  1.55s/it] 26%|██▌       | 2513/9822 [1:07:37<3:04:37,  1.52s/it] 26%|██▌       | 2514/9822 [1:07:38<3:01:41,  1.49s/it] 26%|██▌       | 2515/9822 [1:07:40<3:01:14,  1.49s/it] 26%|██▌       | 2516/9822 [1:07:41<2:59:09,  1.47s/it] 26%|██▌       | 2517/9822 [1:07:42<2:58:03,  1.46s/it] 26%|██▌       | 2518/9822 [1:07:44<2:57:17,  1.46s/it] 26%|██▌       | 2519/9822 [1:07:45<2:56:56,  1.45s/it] 26%|██▌       | 2520/9822 [1:07:47<2:56:44,  1.45s/it] 26%|██▌       | 2521/9822 [1:07:48<2:56:15,  1.45s/it] 26%|██▌       | 2522/9822 [1:07:50<2:55:45,  1.44s/it] 26%|██▌       | 2523/9822 [1:07:51<2:55:52,  1.45s/it] 26%|██▌       | 2524/9822 [1:07:53<2:57:14,  1.46s/it] 26%|██▌       | 2525/9822 [1:07:54<2:56:26,  1.45s/it] 26%|██▌       | 2526/9822 [1:07:55<2:55:48,  1.45s/it] 26%|██▌       | 2527/9822 [1:07:57<2:55:22,  1.44s/it] 26%|██▌       | 2528/9822 [1:07:58<2:55:08,  1.44s/it] 26%|██▌       | 2529/9822 [1:08:00<2:55:03,  1.44s/it] 26%|██▌       | 2530/9822 [1:08:01<2:55:17,  1.44s/it] 26%|██▌       | 2531/9822 [1:08:03<2:54:45,  1.44s/it] 26%|██▌       | 2532/9822 [1:08:04<2:54:44,  1.44s/it] 26%|██▌       | 2533/9822 [1:08:06<2:54:56,  1.44s/it] 26%|██▌       | 2534/9822 [1:08:07<2:58:18,  1.47s/it] 26%|██▌       | 2535/9822 [1:08:09<2:57:35,  1.46s/it] 26%|██▌       | 2536/9822 [1:08:10<2:56:36,  1.45s/it] 26%|██▌       | 2537/9822 [1:08:11<2:55:58,  1.45s/it] 26%|██▌       | 2538/9822 [1:08:13<2:55:06,  1.44s/it] 26%|██▌       | 2539/9822 [1:08:14<2:54:48,  1.44s/it] 26%|██▌       | 2540/9822 [1:08:16<2:54:26,  1.44s/it] 26%|██▌       | 2541/9822 [1:08:17<2:54:30,  1.44s/it] 26%|██▌       | 2542/9822 [1:08:19<2:54:03,  1.43s/it] 26%|██▌       | 2543/9822 [1:08:20<2:53:53,  1.43s/it] 26%|██▌       | 2544/9822 [1:08:21<2:53:44,  1.43s/it] 26%|██▌       | 2545/9822 [1:08:23<2:54:01,  1.43s/it] 26%|██▌       | 2546/9822 [1:08:24<2:53:53,  1.43s/it] 26%|██▌       | 2547/9822 [1:08:26<2:53:36,  1.43s/it] 26%|██▌       | 2548/9822 [1:08:27<2:53:45,  1.43s/it] 26%|██▌       | 2549/9822 [1:08:29<2:53:22,  1.43s/it] 26%|██▌       | 2550/9822 [1:08:30<2:53:41,  1.43s/it] 26%|██▌       | 2551/9822 [1:08:31<2:53:34,  1.43s/it] 26%|██▌       | 2552/9822 [1:08:33<2:53:29,  1.43s/it] 26%|██▌       | 2553/9822 [1:08:34<2:53:14,  1.43s/it] 26%|██▌       | 2554/9822 [1:08:36<2:53:32,  1.43s/it] 26%|██▌       | 2555/9822 [1:08:37<2:53:29,  1.43s/it] 26%|██▌       | 2556/9822 [1:08:39<2:53:22,  1.43s/it] 26%|██▌       | 2557/9822 [1:08:40<2:53:31,  1.43s/it] 26%|██▌       | 2558/9822 [1:08:41<2:53:07,  1.43s/it] 26%|██▌       | 2559/9822 [1:08:43<2:53:12,  1.43s/it] 26%|██▌       | 2560/9822 [1:08:44<2:53:01,  1.43s/it] 26%|██▌       | 2561/9822 [1:08:46<2:53:21,  1.43s/it] 26%|██▌       | 2562/9822 [1:08:47<2:53:14,  1.43s/it] 26%|██▌       | 2563/9822 [1:08:49<2:53:40,  1.44s/it] 26%|██▌       | 2564/9822 [1:08:50<2:54:09,  1.44s/it] 26%|██▌       | 2565/9822 [1:08:52<2:54:57,  1.45s/it] 26%|██▌       | 2566/9822 [1:08:53<2:57:27,  1.47s/it] 26%|██▌       | 2567/9822 [1:08:55<2:55:59,  1.46s/it] 26%|██▌       | 2568/9822 [1:08:56<2:54:49,  1.45s/it] 26%|██▌       | 2569/9822 [1:08:57<2:54:19,  1.44s/it] 26%|██▌       | 2570/9822 [1:08:59<2:54:05,  1.44s/it] 26%|██▌       | 2571/9822 [1:09:00<2:53:27,  1.44s/it] 26%|██▌       | 2572/9822 [1:09:02<2:53:27,  1.44s/it] 26%|██▌       | 2573/9822 [1:09:03<2:53:52,  1.44s/it] 26%|██▌       | 2574/9822 [1:09:05<2:53:36,  1.44s/it] 26%|██▌       | 2575/9822 [1:09:06<2:53:48,  1.44s/it] 26%|██▌       | 2576/9822 [1:09:07<2:53:17,  1.43s/it] 26%|██▌       | 2577/9822 [1:09:09<2:52:41,  1.43s/it] 26%|██▌       | 2578/9822 [1:09:10<2:53:06,  1.43s/it] 26%|██▋       | 2579/9822 [1:09:12<2:53:04,  1.43s/it] 26%|██▋       | 2580/9822 [1:09:13<2:51:09,  1.42s/it] 26%|██▋       | 2581/9822 [1:09:15<2:51:51,  1.42s/it] 26%|██▋       | 2582/9822 [1:09:16<2:52:04,  1.43s/it] 26%|██▋       | 2583/9822 [1:09:17<2:52:46,  1.43s/it] 26%|██▋       | 2584/9822 [1:09:19<2:53:20,  1.44s/it] 26%|██▋       | 2585/9822 [1:09:20<2:53:16,  1.44s/it] 26%|██▋       | 2586/9822 [1:09:22<2:54:09,  1.44s/it] 26%|██▋       | 2587/9822 [1:09:23<2:54:42,  1.45s/it] 26%|██▋       | 2588/9822 [1:09:25<2:54:22,  1.45s/it] 26%|██▋       | 2589/9822 [1:09:26<2:54:19,  1.45s/it] 26%|██▋       | 2590/9822 [1:09:28<2:54:21,  1.45s/it] 26%|██▋       | 2591/9822 [1:09:29<2:56:43,  1.47s/it] 26%|██▋       | 2592/9822 [1:09:30<2:55:18,  1.45s/it] 26%|██▋       | 2593/9822 [1:09:32<2:54:50,  1.45s/it] 26%|██▋       | 2594/9822 [1:09:33<2:54:21,  1.45s/it] 26%|██▋       | 2595/9822 [1:09:35<2:53:43,  1.44s/it] 26%|██▋       | 2596/9822 [1:09:36<2:53:17,  1.44s/it] 26%|██▋       | 2597/9822 [1:09:38<2:53:16,  1.44s/it] 26%|██▋       | 2598/9822 [1:09:39<2:52:46,  1.44s/it] 26%|██▋       | 2599/9822 [1:09:41<2:52:46,  1.44s/it] 26%|██▋       | 2600/9822 [1:09:42<2:52:36,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 22:57:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:57:29 - INFO - __main__ - ***** test Results*****
01/07/2024 22:57:29 - INFO - __main__ -   Training step = 2600
01/07/2024 22:57:29 - INFO - __main__ -  test_accuracy:0.863103953147877 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:57:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:57:35 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 22:57:35 - INFO - __main__ -   Training step = 2600
01/07/2024 22:57:35 - INFO - __main__ -  eval_accuracy:0.8454778469425119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8487733430977664}
test:
{'accuracy': 0.8642020497803806}
01/07/2024 22:57:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 22:57:40 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 22:57:40 - INFO - __main__ -   Training step = 2600
01/07/2024 22:57:40 - INFO - __main__ -  eval_accuracy:0.9011351153423655 
 26%|██▋       | 2601/9822 [1:10:00<13:03:19,  6.51s/it] 26%|██▋       | 2602/9822 [1:10:02<10:00:05,  4.99s/it] 27%|██▋       | 2603/9822 [1:10:03<7:51:46,  3.92s/it]  27%|██▋       | 2604/9822 [1:10:05<6:22:04,  3.18s/it] 27%|██▋       | 2605/9822 [1:10:06<5:18:57,  2.65s/it] 27%|██▋       | 2606/9822 [1:10:07<4:35:19,  2.29s/it] 27%|██▋       | 2607/9822 [1:10:09<4:05:03,  2.04s/it] 27%|██▋       | 2608/9822 [1:10:10<3:45:03,  1.87s/it] 27%|██▋       | 2609/9822 [1:10:12<3:29:30,  1.74s/it] 27%|██▋       | 2610/9822 [1:10:13<3:20:20,  1.67s/it] 27%|██▋       | 2611/9822 [1:10:15<3:13:32,  1.61s/it] 27%|██▋       | 2612/9822 [1:10:16<3:07:42,  1.56s/it] 27%|██▋       | 2613/9822 [1:10:18<3:03:12,  1.52s/it] 27%|██▋       | 2614/9822 [1:10:19<3:00:01,  1.50s/it] 27%|██▋       | 2615/9822 [1:10:21<2:57:39,  1.48s/it] 27%|██▋       | 2616/9822 [1:10:22<2:56:13,  1.47s/it] 27%|██▋       | 2617/9822 [1:10:24<2:58:14,  1.48s/it] 27%|██▋       | 2618/9822 [1:10:25<2:57:01,  1.47s/it] 27%|██▋       | 2619/9822 [1:10:26<2:55:30,  1.46s/it] 27%|██▋       | 2620/9822 [1:10:28<2:54:22,  1.45s/it] 27%|██▋       | 2621/9822 [1:10:29<2:53:28,  1.45s/it] 27%|██▋       | 2622/9822 [1:10:31<2:53:23,  1.44s/it] 27%|██▋       | 2623/9822 [1:10:32<2:52:58,  1.44s/it] 27%|██▋       | 2624/9822 [1:10:34<2:52:57,  1.44s/it] 27%|██▋       | 2625/9822 [1:10:35<2:53:01,  1.44s/it] 27%|██▋       | 2626/9822 [1:10:36<2:52:56,  1.44s/it] 27%|██▋       | 2627/9822 [1:10:38<2:52:42,  1.44s/it] 27%|██▋       | 2628/9822 [1:10:39<2:52:21,  1.44s/it] 27%|██▋       | 2629/9822 [1:10:41<2:52:10,  1.44s/it] 27%|██▋       | 2630/9822 [1:10:42<2:51:59,  1.43s/it] 27%|██▋       | 2631/9822 [1:10:44<2:51:46,  1.43s/it] 27%|██▋       | 2632/9822 [1:10:45<2:51:42,  1.43s/it] 27%|██▋       | 2633/9822 [1:10:47<2:51:26,  1.43s/it] 27%|██▋       | 2634/9822 [1:10:48<2:51:12,  1.43s/it] 27%|██▋       | 2635/9822 [1:10:49<2:51:25,  1.43s/it] 27%|██▋       | 2636/9822 [1:10:51<2:51:37,  1.43s/it] 27%|██▋       | 2637/9822 [1:10:52<2:51:55,  1.44s/it] 27%|██▋       | 2638/9822 [1:10:54<2:52:07,  1.44s/it] 27%|██▋       | 2639/9822 [1:10:55<2:51:55,  1.44s/it] 27%|██▋       | 2640/9822 [1:10:57<2:51:47,  1.44s/it] 27%|██▋       | 2641/9822 [1:10:58<2:51:39,  1.43s/it] 27%|██▋       | 2642/9822 [1:10:59<2:51:08,  1.43s/it] 27%|██▋       | 2643/9822 [1:11:01<2:51:18,  1.43s/it] 27%|██▋       | 2644/9822 [1:11:02<2:51:34,  1.43s/it] 27%|██▋       | 2645/9822 [1:11:04<2:51:28,  1.43s/it] 27%|██▋       | 2646/9822 [1:11:05<2:51:31,  1.43s/it] 27%|██▋       | 2647/9822 [1:11:07<2:51:33,  1.43s/it] 27%|██▋       | 2648/9822 [1:11:08<2:51:36,  1.44s/it] 27%|██▋       | 2649/9822 [1:11:10<2:54:15,  1.46s/it] 27%|██▋       | 2650/9822 [1:11:11<2:53:12,  1.45s/it] 27%|██▋       | 2651/9822 [1:11:12<2:52:27,  1.44s/it] 27%|██▋       | 2652/9822 [1:11:14<2:51:54,  1.44s/it] 27%|██▋       | 2653/9822 [1:11:15<2:51:45,  1.44s/it] 27%|██▋       | 2654/9822 [1:11:17<2:51:53,  1.44s/it] 27%|██▋       | 2655/9822 [1:11:18<2:51:39,  1.44s/it] 27%|██▋       | 2656/9822 [1:11:20<2:52:04,  1.44s/it] 27%|██▋       | 2657/9822 [1:11:21<2:51:36,  1.44s/it] 27%|██▋       | 2658/9822 [1:11:22<2:51:14,  1.43s/it] 27%|██▋       | 2659/9822 [1:11:24<2:51:41,  1.44s/it] 27%|██▋       | 2660/9822 [1:11:25<2:51:27,  1.44s/it] 27%|██▋       | 2661/9822 [1:11:27<2:51:37,  1.44s/it] 27%|██▋       | 2662/9822 [1:11:28<2:52:15,  1.44s/it] 27%|██▋       | 2663/9822 [1:11:30<2:52:41,  1.45s/it] 27%|██▋       | 2664/9822 [1:11:31<2:52:09,  1.44s/it] 27%|██▋       | 2665/9822 [1:11:33<2:51:23,  1.44s/it] 27%|██▋       | 2666/9822 [1:11:34<2:49:33,  1.42s/it] 27%|██▋       | 2667/9822 [1:11:35<2:50:34,  1.43s/it] 27%|██▋       | 2668/9822 [1:11:37<2:50:47,  1.43s/it] 27%|██▋       | 2669/9822 [1:11:38<2:50:32,  1.43s/it] 27%|██▋       | 2670/9822 [1:11:40<2:51:12,  1.44s/it] 27%|██▋       | 2671/9822 [1:11:41<2:51:22,  1.44s/it] 27%|██▋       | 2672/9822 [1:11:43<2:50:59,  1.43s/it] 27%|██▋       | 2673/9822 [1:11:44<2:50:47,  1.43s/it] 27%|██▋       | 2674/9822 [1:11:45<2:51:03,  1.44s/it] 27%|██▋       | 2675/9822 [1:11:47<2:51:57,  1.44s/it] 27%|██▋       | 2676/9822 [1:11:48<2:51:21,  1.44s/it] 27%|██▋       | 2677/9822 [1:11:50<2:52:15,  1.45s/it] 27%|██▋       | 2678/9822 [1:11:51<2:52:06,  1.45s/it] 27%|██▋       | 2679/9822 [1:11:53<2:51:19,  1.44s/it] 27%|██▋       | 2680/9822 [1:11:54<2:52:10,  1.45s/it] 27%|██▋       | 2681/9822 [1:11:56<2:54:32,  1.47s/it] 27%|██▋       | 2682/9822 [1:11:57<2:53:28,  1.46s/it] 27%|██▋       | 2683/9822 [1:11:58<2:52:23,  1.45s/it] 27%|██▋       | 2684/9822 [1:12:00<2:51:46,  1.44s/it] 27%|██▋       | 2685/9822 [1:12:01<2:51:33,  1.44s/it] 27%|██▋       | 2686/9822 [1:12:03<2:51:31,  1.44s/it] 27%|██▋       | 2687/9822 [1:12:04<2:51:14,  1.44s/it] 27%|██▋       | 2688/9822 [1:12:06<2:50:46,  1.44s/it] 27%|██▋       | 2689/9822 [1:12:07<2:50:35,  1.43s/it] 27%|██▋       | 2690/9822 [1:12:09<2:50:52,  1.44s/it] 27%|██▋       | 2691/9822 [1:12:10<2:50:36,  1.44s/it] 27%|██▋       | 2692/9822 [1:12:11<2:50:45,  1.44s/it] 27%|██▋       | 2693/9822 [1:12:13<2:50:37,  1.44s/it] 27%|██▋       | 2694/9822 [1:12:14<2:50:21,  1.43s/it] 27%|██▋       | 2695/9822 [1:12:16<2:50:29,  1.44s/it] 27%|██▋       | 2696/9822 [1:12:17<2:52:07,  1.45s/it] 27%|██▋       | 2697/9822 [1:12:19<2:51:20,  1.44s/it] 27%|██▋       | 2698/9822 [1:12:20<2:51:17,  1.44s/it] 27%|██▋       | 2699/9822 [1:12:21<2:50:47,  1.44s/it] 27%|██▋       | 2700/9822 [1:12:23<2:50:31,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0321, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:00:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:00:10 - INFO - __main__ - ***** test Results*****
01/07/2024 23:00:10 - INFO - __main__ -   Training step = 2700
01/07/2024 23:00:10 - INFO - __main__ -  test_accuracy:0.863103953147877 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:00:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:00:16 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:00:16 - INFO - __main__ -   Training step = 2700
01/07/2024 23:00:16 - INFO - __main__ -  eval_accuracy:0.8469425119004028 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8487733430977664}
test:
{'accuracy': 0.8642020497803806}
01/07/2024 23:00:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:00:21 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:00:21 - INFO - __main__ -   Training step = 2700
01/07/2024 23:00:21 - INFO - __main__ -  eval_accuracy:0.9036982790186745 
 27%|██▋       | 2701/9822 [1:12:41<12:53:59,  6.52s/it] 28%|██▊       | 2702/9822 [1:12:43<9:53:35,  5.00s/it]  28%|██▊       | 2703/9822 [1:12:44<7:46:45,  3.93s/it] 28%|██▊       | 2704/9822 [1:12:46<6:17:43,  3.18s/it] 28%|██▊       | 2705/9822 [1:12:47<5:15:30,  2.66s/it] 28%|██▊       | 2706/9822 [1:12:49<4:31:43,  2.29s/it] 28%|██▊       | 2707/9822 [1:12:50<4:01:08,  2.03s/it] 28%|██▊       | 2708/9822 [1:12:51<3:42:58,  1.88s/it] 28%|██▊       | 2709/9822 [1:12:53<3:26:50,  1.74s/it] 28%|██▊       | 2710/9822 [1:12:54<3:16:07,  1.65s/it] 28%|██▊       | 2711/9822 [1:12:56<3:07:47,  1.58s/it] 28%|██▊       | 2712/9822 [1:12:57<3:02:27,  1.54s/it] 28%|██▊       | 2713/9822 [1:12:59<2:58:36,  1.51s/it] 28%|██▊       | 2714/9822 [1:13:00<2:55:46,  1.48s/it] 28%|██▊       | 2715/9822 [1:13:01<2:53:31,  1.47s/it] 28%|██▊       | 2716/9822 [1:13:03<2:52:27,  1.46s/it] 28%|██▊       | 2717/9822 [1:13:04<2:51:46,  1.45s/it] 28%|██▊       | 2718/9822 [1:13:06<2:50:46,  1.44s/it] 28%|██▊       | 2719/9822 [1:13:07<2:50:46,  1.44s/it] 28%|██▊       | 2720/9822 [1:13:09<2:50:47,  1.44s/it] 28%|██▊       | 2721/9822 [1:13:10<2:50:33,  1.44s/it] 28%|██▊       | 2722/9822 [1:13:12<2:50:04,  1.44s/it] 28%|██▊       | 2723/9822 [1:13:13<2:50:06,  1.44s/it] 28%|██▊       | 2724/9822 [1:13:14<2:49:35,  1.43s/it] 28%|██▊       | 2725/9822 [1:13:16<2:49:22,  1.43s/it] 28%|██▊       | 2726/9822 [1:13:17<2:49:47,  1.44s/it] 28%|██▊       | 2727/9822 [1:13:19<2:50:12,  1.44s/it] 28%|██▊       | 2728/9822 [1:13:20<2:51:38,  1.45s/it] 28%|██▊       | 2729/9822 [1:13:22<2:51:29,  1.45s/it] 28%|██▊       | 2730/9822 [1:13:23<2:51:12,  1.45s/it] 28%|██▊       | 2731/9822 [1:13:25<2:50:59,  1.45s/it] 28%|██▊       | 2732/9822 [1:13:26<2:50:20,  1.44s/it] 28%|██▊       | 2733/9822 [1:13:27<2:49:48,  1.44s/it] 28%|██▊       | 2734/9822 [1:13:29<2:50:18,  1.44s/it] 28%|██▊       | 2735/9822 [1:13:30<2:50:30,  1.44s/it] 28%|██▊       | 2736/9822 [1:13:32<2:50:24,  1.44s/it] 28%|██▊       | 2737/9822 [1:13:33<2:50:07,  1.44s/it] 28%|██▊       | 2738/9822 [1:13:35<2:52:42,  1.46s/it] 28%|██▊       | 2739/9822 [1:13:36<2:51:29,  1.45s/it] 28%|██▊       | 2740/9822 [1:13:38<2:51:07,  1.45s/it] 28%|██▊       | 2741/9822 [1:13:39<2:50:48,  1.45s/it] 28%|██▊       | 2742/9822 [1:13:40<2:50:02,  1.44s/it] 28%|██▊       | 2743/9822 [1:13:42<2:49:45,  1.44s/it] 28%|██▊       | 2744/9822 [1:13:43<2:49:27,  1.44s/it] 28%|██▊       | 2745/9822 [1:13:45<2:49:11,  1.43s/it] 28%|██▊       | 2746/9822 [1:13:46<2:49:03,  1.43s/it] 28%|██▊       | 2747/9822 [1:13:48<2:49:14,  1.44s/it] 28%|██▊       | 2748/9822 [1:13:49<2:49:14,  1.44s/it] 28%|██▊       | 2749/9822 [1:13:50<2:49:00,  1.43s/it] 28%|██▊       | 2750/9822 [1:13:52<2:49:17,  1.44s/it] 28%|██▊       | 2751/9822 [1:13:53<2:49:42,  1.44s/it] 28%|██▊       | 2752/9822 [1:13:55<2:49:07,  1.44s/it] 28%|██▊       | 2753/9822 [1:13:56<2:49:13,  1.44s/it] 28%|██▊       | 2754/9822 [1:13:58<2:49:22,  1.44s/it] 28%|██▊       | 2755/9822 [1:13:59<2:49:04,  1.44s/it] 28%|██▊       | 2756/9822 [1:14:00<2:48:42,  1.43s/it] 28%|██▊       | 2757/9822 [1:14:02<2:48:42,  1.43s/it] 28%|██▊       | 2758/9822 [1:14:03<2:48:36,  1.43s/it] 28%|██▊       | 2759/9822 [1:14:05<2:48:39,  1.43s/it] 28%|██▊       | 2760/9822 [1:14:06<2:48:25,  1.43s/it] 28%|██▊       | 2761/9822 [1:14:08<2:48:14,  1.43s/it] 28%|██▊       | 2762/9822 [1:14:09<2:48:27,  1.43s/it] 28%|██▊       | 2763/9822 [1:14:11<2:49:03,  1.44s/it] 28%|██▊       | 2764/9822 [1:14:12<2:49:41,  1.44s/it] 28%|██▊       | 2765/9822 [1:14:13<2:49:13,  1.44s/it] 28%|██▊       | 2766/9822 [1:14:15<2:49:08,  1.44s/it] 28%|██▊       | 2767/9822 [1:14:16<2:49:32,  1.44s/it] 28%|██▊       | 2768/9822 [1:14:18<2:49:14,  1.44s/it] 28%|██▊       | 2769/9822 [1:14:19<2:48:49,  1.44s/it] 28%|██▊       | 2770/9822 [1:14:21<2:51:12,  1.46s/it] 28%|██▊       | 2771/9822 [1:14:22<2:50:00,  1.45s/it] 28%|██▊       | 2772/9822 [1:14:24<2:49:40,  1.44s/it] 28%|██▊       | 2773/9822 [1:14:25<2:49:11,  1.44s/it] 28%|██▊       | 2774/9822 [1:14:26<2:48:24,  1.43s/it] 28%|██▊       | 2775/9822 [1:14:28<2:48:24,  1.43s/it] 28%|██▊       | 2776/9822 [1:14:29<2:48:45,  1.44s/it] 28%|██▊       | 2777/9822 [1:14:31<2:48:59,  1.44s/it] 28%|██▊       | 2778/9822 [1:14:32<2:48:48,  1.44s/it] 28%|██▊       | 2779/9822 [1:14:34<2:48:46,  1.44s/it] 28%|██▊       | 2780/9822 [1:14:35<2:48:59,  1.44s/it] 28%|██▊       | 2781/9822 [1:14:36<2:49:24,  1.44s/it] 28%|██▊       | 2782/9822 [1:14:38<2:49:08,  1.44s/it] 28%|██▊       | 2783/9822 [1:14:39<2:49:18,  1.44s/it] 28%|██▊       | 2784/9822 [1:14:41<2:49:05,  1.44s/it] 28%|██▊       | 2785/9822 [1:14:42<2:48:25,  1.44s/it] 28%|██▊       | 2786/9822 [1:14:44<2:48:17,  1.44s/it] 28%|██▊       | 2787/9822 [1:14:45<2:48:17,  1.44s/it] 28%|██▊       | 2788/9822 [1:14:47<2:48:06,  1.43s/it] 28%|██▊       | 2789/9822 [1:14:48<2:48:05,  1.43s/it] 28%|██▊       | 2790/9822 [1:14:49<2:47:47,  1.43s/it] 28%|██▊       | 2791/9822 [1:14:51<2:47:57,  1.43s/it] 28%|██▊       | 2792/9822 [1:14:52<2:47:54,  1.43s/it] 28%|██▊       | 2793/9822 [1:14:54<2:47:30,  1.43s/it] 28%|██▊       | 2794/9822 [1:14:55<2:47:43,  1.43s/it] 28%|██▊       | 2795/9822 [1:14:57<2:47:55,  1.43s/it] 28%|██▊       | 2796/9822 [1:14:58<2:48:13,  1.44s/it] 28%|██▊       | 2797/9822 [1:14:59<2:47:53,  1.43s/it] 28%|██▊       | 2798/9822 [1:15:01<2:48:04,  1.44s/it] 28%|██▊       | 2799/9822 [1:15:02<2:47:48,  1.43s/it] 29%|██▊       | 2800/9822 [1:15:04<2:47:42,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:02:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:02:51 - INFO - __main__ - ***** test Results*****
01/07/2024 23:02:51 - INFO - __main__ -   Training step = 2800
01/07/2024 23:02:51 - INFO - __main__ -  test_accuracy:0.8634699853587116 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:02:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:02:57 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:02:57 - INFO - __main__ -   Training step = 2800
01/07/2024 23:02:57 - INFO - __main__ -  eval_accuracy:0.8487733430977664 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8487733430977664}
test:
{'accuracy': 0.8642020497803806}
01/07/2024 23:03:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:03:01 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:03:01 - INFO - __main__ -   Training step = 2800
01/07/2024 23:03:01 - INFO - __main__ -  eval_accuracy:0.9036982790186745 
 29%|██▊       | 2801/9822 [1:15:22<12:42:32,  6.52s/it] 29%|██▊       | 2802/9822 [1:15:24<9:46:46,  5.02s/it]  29%|██▊       | 2803/9822 [1:15:25<7:41:30,  3.95s/it] 29%|██▊       | 2804/9822 [1:15:26<6:13:37,  3.19s/it] 29%|██▊       | 2805/9822 [1:15:28<5:12:04,  2.67s/it] 29%|██▊       | 2806/9822 [1:15:29<4:28:46,  2.30s/it] 29%|██▊       | 2807/9822 [1:15:31<3:58:37,  2.04s/it] 29%|██▊       | 2808/9822 [1:15:32<3:37:32,  1.86s/it] 29%|██▊       | 2809/9822 [1:15:34<3:22:56,  1.74s/it] 29%|██▊       | 2810/9822 [1:15:35<3:12:06,  1.64s/it] 29%|██▊       | 2811/9822 [1:15:37<3:04:31,  1.58s/it] 29%|██▊       | 2812/9822 [1:15:38<2:59:28,  1.54s/it] 29%|██▊       | 2813/9822 [1:15:39<2:55:54,  1.51s/it] 29%|██▊       | 2814/9822 [1:15:41<2:53:26,  1.48s/it] 29%|██▊       | 2815/9822 [1:15:42<2:51:25,  1.47s/it] 29%|██▊       | 2816/9822 [1:15:44<2:50:30,  1.46s/it] 29%|██▊       | 2817/9822 [1:15:45<2:49:30,  1.45s/it] 29%|██▊       | 2818/9822 [1:15:47<2:48:50,  1.45s/it] 29%|██▊       | 2819/9822 [1:15:48<2:48:57,  1.45s/it] 29%|██▊       | 2820/9822 [1:15:49<2:48:24,  1.44s/it] 29%|██▊       | 2821/9822 [1:15:51<2:48:11,  1.44s/it] 29%|██▊       | 2822/9822 [1:15:52<2:48:08,  1.44s/it] 29%|██▊       | 2823/9822 [1:15:54<2:48:17,  1.44s/it] 29%|██▉       | 2824/9822 [1:15:55<2:48:27,  1.44s/it] 29%|██▉       | 2825/9822 [1:15:57<2:47:45,  1.44s/it] 29%|██▉       | 2826/9822 [1:15:58<2:47:38,  1.44s/it] 29%|██▉       | 2827/9822 [1:16:00<2:49:03,  1.45s/it] 29%|██▉       | 2828/9822 [1:16:01<2:48:47,  1.45s/it] 29%|██▉       | 2829/9822 [1:16:02<2:48:14,  1.44s/it] 29%|██▉       | 2830/9822 [1:16:04<2:48:09,  1.44s/it] 29%|██▉       | 2831/9822 [1:16:05<2:47:44,  1.44s/it] 29%|██▉       | 2832/9822 [1:16:07<2:47:40,  1.44s/it] 29%|██▉       | 2833/9822 [1:16:08<2:48:12,  1.44s/it] 29%|██▉       | 2834/9822 [1:16:10<2:50:40,  1.47s/it] 29%|██▉       | 2835/9822 [1:16:11<2:49:57,  1.46s/it] 29%|██▉       | 2836/9822 [1:16:13<2:49:26,  1.46s/it] 29%|██▉       | 2837/9822 [1:16:14<2:48:57,  1.45s/it] 29%|██▉       | 2838/9822 [1:16:15<2:46:54,  1.43s/it] 29%|██▉       | 2839/9822 [1:16:17<2:46:42,  1.43s/it] 29%|██▉       | 2840/9822 [1:16:18<2:46:34,  1.43s/it] 29%|██▉       | 2841/9822 [1:16:20<2:47:07,  1.44s/it] 29%|██▉       | 2842/9822 [1:16:21<2:47:36,  1.44s/it] 29%|██▉       | 2843/9822 [1:16:23<2:47:51,  1.44s/it] 29%|██▉       | 2844/9822 [1:16:24<2:47:32,  1.44s/it] 29%|██▉       | 2845/9822 [1:16:26<2:47:20,  1.44s/it] 29%|██▉       | 2846/9822 [1:16:27<2:47:43,  1.44s/it] 29%|██▉       | 2847/9822 [1:16:28<2:47:52,  1.44s/it] 29%|██▉       | 2848/9822 [1:16:30<2:47:39,  1.44s/it] 29%|██▉       | 2849/9822 [1:16:31<2:48:14,  1.45s/it] 29%|██▉       | 2850/9822 [1:16:33<2:47:37,  1.44s/it] 29%|██▉       | 2851/9822 [1:16:34<2:47:05,  1.44s/it] 29%|██▉       | 2852/9822 [1:16:36<2:46:40,  1.43s/it] 29%|██▉       | 2853/9822 [1:16:37<2:46:38,  1.43s/it] 29%|██▉       | 2854/9822 [1:16:38<2:46:18,  1.43s/it] 29%|██▉       | 2855/9822 [1:16:40<2:46:07,  1.43s/it] 29%|██▉       | 2856/9822 [1:16:41<2:45:53,  1.43s/it] 29%|██▉       | 2857/9822 [1:16:43<2:45:42,  1.43s/it] 29%|██▉       | 2858/9822 [1:16:44<2:45:31,  1.43s/it] 29%|██▉       | 2859/9822 [1:16:46<2:48:32,  1.45s/it] 29%|██▉       | 2860/9822 [1:16:47<2:47:53,  1.45s/it] 29%|██▉       | 2861/9822 [1:16:49<2:47:01,  1.44s/it] 29%|██▉       | 2862/9822 [1:16:50<2:46:50,  1.44s/it] 29%|██▉       | 2863/9822 [1:16:51<2:46:25,  1.43s/it] 29%|██▉       | 2864/9822 [1:16:53<2:46:26,  1.44s/it] 29%|██▉       | 2865/9822 [1:16:54<2:46:14,  1.43s/it] 29%|██▉       | 2866/9822 [1:16:56<2:46:31,  1.44s/it] 29%|██▉       | 2867/9822 [1:16:57<2:46:43,  1.44s/it] 29%|██▉       | 2868/9822 [1:16:59<2:47:23,  1.44s/it] 29%|██▉       | 2869/9822 [1:17:00<2:46:42,  1.44s/it] 29%|██▉       | 2870/9822 [1:17:01<2:46:02,  1.43s/it] 29%|██▉       | 2871/9822 [1:17:03<2:45:43,  1.43s/it] 29%|██▉       | 2872/9822 [1:17:04<2:46:27,  1.44s/it] 29%|██▉       | 2873/9822 [1:17:06<2:46:13,  1.44s/it] 29%|██▉       | 2874/9822 [1:17:07<2:46:30,  1.44s/it] 29%|██▉       | 2875/9822 [1:17:09<2:46:30,  1.44s/it] 29%|██▉       | 2876/9822 [1:17:10<2:46:12,  1.44s/it] 29%|██▉       | 2877/9822 [1:17:12<2:45:56,  1.43s/it] 29%|██▉       | 2878/9822 [1:17:13<2:45:51,  1.43s/it] 29%|██▉       | 2879/9822 [1:17:14<2:46:03,  1.44s/it] 29%|██▉       | 2880/9822 [1:17:16<2:45:57,  1.43s/it] 29%|██▉       | 2881/9822 [1:17:17<2:46:15,  1.44s/it] 29%|██▉       | 2882/9822 [1:17:19<2:45:32,  1.43s/it] 29%|██▉       | 2883/9822 [1:17:20<2:45:26,  1.43s/it] 29%|██▉       | 2884/9822 [1:17:22<2:45:49,  1.43s/it] 29%|██▉       | 2885/9822 [1:17:23<2:47:49,  1.45s/it] 29%|██▉       | 2886/9822 [1:17:25<2:49:08,  1.46s/it] 29%|██▉       | 2887/9822 [1:17:26<2:48:26,  1.46s/it] 29%|██▉       | 2888/9822 [1:17:27<2:47:28,  1.45s/it] 29%|██▉       | 2889/9822 [1:17:29<2:46:56,  1.44s/it] 29%|██▉       | 2890/9822 [1:17:30<2:47:09,  1.45s/it] 29%|██▉       | 2891/9822 [1:17:32<2:49:18,  1.47s/it] 29%|██▉       | 2892/9822 [1:17:33<2:48:24,  1.46s/it] 29%|██▉       | 2893/9822 [1:17:35<2:47:35,  1.45s/it] 29%|██▉       | 2894/9822 [1:17:36<2:47:01,  1.45s/it] 29%|██▉       | 2895/9822 [1:17:38<2:46:26,  1.44s/it] 29%|██▉       | 2896/9822 [1:17:39<2:46:31,  1.44s/it] 29%|██▉       | 2897/9822 [1:17:40<2:45:51,  1.44s/it] 30%|██▉       | 2898/9822 [1:17:42<2:46:04,  1.44s/it] 30%|██▉       | 2899/9822 [1:17:43<2:46:06,  1.44s/it] 30%|██▉       | 2900/9822 [1:17:45<2:46:35,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:05:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:05:32 - INFO - __main__ - ***** test Results*****
01/07/2024 23:05:32 - INFO - __main__ -   Training step = 2900
01/07/2024 23:05:32 - INFO - __main__ -  test_accuracy:0.8605417276720352 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:05:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:05:38 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:05:38 - INFO - __main__ -   Training step = 2900
01/07/2024 23:05:38 - INFO - __main__ -  eval_accuracy:0.8495056755767119 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 23:05:38,182 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 23:05:38,182 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 23:05:38,219 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 23:05:39,787 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8495056755767119}
test:
{'accuracy': 0.8605417276720352}
01/07/2024 23:05:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:05:44 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:05:44 - INFO - __main__ -   Training step = 2900
01/07/2024 23:05:44 - INFO - __main__ -  eval_accuracy:0.9044306114976199 
 30%|██▉       | 2901/9822 [1:18:05<13:26:01,  6.99s/it] 30%|██▉       | 2902/9822 [1:18:06<10:13:48,  5.32s/it] 30%|██▉       | 2903/9822 [1:18:08<7:59:01,  4.15s/it]  30%|██▉       | 2904/9822 [1:18:09<6:24:44,  3.34s/it] 30%|██▉       | 2905/9822 [1:18:10<5:18:57,  2.77s/it] 30%|██▉       | 2906/9822 [1:18:12<4:32:32,  2.36s/it] 30%|██▉       | 2907/9822 [1:18:13<4:00:12,  2.08s/it] 30%|██▉       | 2908/9822 [1:18:15<3:37:40,  1.89s/it] 30%|██▉       | 2909/9822 [1:18:16<3:22:16,  1.76s/it] 30%|██▉       | 2910/9822 [1:18:18<3:10:41,  1.66s/it] 30%|██▉       | 2911/9822 [1:18:19<3:03:20,  1.59s/it] 30%|██▉       | 2912/9822 [1:18:20<2:57:44,  1.54s/it] 30%|██▉       | 2913/9822 [1:18:22<2:54:03,  1.51s/it] 30%|██▉       | 2914/9822 [1:18:23<2:51:25,  1.49s/it] 30%|██▉       | 2915/9822 [1:18:25<2:49:17,  1.47s/it] 30%|██▉       | 2916/9822 [1:18:26<2:48:13,  1.46s/it] 30%|██▉       | 2917/9822 [1:18:28<2:47:06,  1.45s/it] 30%|██▉       | 2918/9822 [1:18:29<2:49:04,  1.47s/it] 30%|██▉       | 2919/9822 [1:18:31<2:47:33,  1.46s/it] 30%|██▉       | 2920/9822 [1:18:32<2:46:46,  1.45s/it] 30%|██▉       | 2921/9822 [1:18:33<2:46:42,  1.45s/it] 30%|██▉       | 2922/9822 [1:18:35<2:46:23,  1.45s/it] 30%|██▉       | 2923/9822 [1:18:36<2:46:11,  1.45s/it] 30%|██▉       | 2924/9822 [1:18:38<2:43:58,  1.43s/it] 30%|██▉       | 2925/9822 [1:18:39<2:44:18,  1.43s/it] 30%|██▉       | 2926/9822 [1:18:41<2:44:12,  1.43s/it] 30%|██▉       | 2927/9822 [1:18:42<2:44:05,  1.43s/it] 30%|██▉       | 2928/9822 [1:18:43<2:44:13,  1.43s/it] 30%|██▉       | 2929/9822 [1:18:45<2:44:46,  1.43s/it] 30%|██▉       | 2930/9822 [1:18:46<2:45:07,  1.44s/it] 30%|██▉       | 2931/9822 [1:18:48<2:45:25,  1.44s/it] 30%|██▉       | 2932/9822 [1:18:49<2:45:22,  1.44s/it] 30%|██▉       | 2933/9822 [1:18:51<2:45:03,  1.44s/it] 30%|██▉       | 2934/9822 [1:18:52<2:44:32,  1.43s/it] 30%|██▉       | 2935/9822 [1:18:54<2:44:42,  1.43s/it] 30%|██▉       | 2936/9822 [1:18:55<2:45:42,  1.44s/it] 30%|██▉       | 2937/9822 [1:18:56<2:45:23,  1.44s/it] 30%|██▉       | 2938/9822 [1:18:58<2:45:26,  1.44s/it] 30%|██▉       | 2939/9822 [1:18:59<2:45:10,  1.44s/it] 30%|██▉       | 2940/9822 [1:19:01<2:44:45,  1.44s/it] 30%|██▉       | 2941/9822 [1:19:02<2:44:20,  1.43s/it] 30%|██▉       | 2942/9822 [1:19:04<2:44:14,  1.43s/it] 30%|██▉       | 2943/9822 [1:19:05<2:43:48,  1.43s/it] 30%|██▉       | 2944/9822 [1:19:06<2:43:43,  1.43s/it] 30%|██▉       | 2945/9822 [1:19:08<2:43:48,  1.43s/it] 30%|██▉       | 2946/9822 [1:19:09<2:43:31,  1.43s/it] 30%|███       | 2947/9822 [1:19:11<2:43:35,  1.43s/it] 30%|███       | 2948/9822 [1:19:12<2:43:53,  1.43s/it] 30%|███       | 2949/9822 [1:19:14<2:43:54,  1.43s/it] 30%|███       | 2950/9822 [1:19:15<2:46:18,  1.45s/it] 30%|███       | 2951/9822 [1:19:16<2:45:21,  1.44s/it] 30%|███       | 2952/9822 [1:19:18<2:44:35,  1.44s/it] 30%|███       | 2953/9822 [1:19:19<2:44:38,  1.44s/it] 30%|███       | 2954/9822 [1:19:21<2:44:31,  1.44s/it] 30%|███       | 2955/9822 [1:19:22<2:43:49,  1.43s/it] 30%|███       | 2956/9822 [1:19:24<2:44:29,  1.44s/it] 30%|███       | 2957/9822 [1:19:25<2:44:15,  1.44s/it] 30%|███       | 2958/9822 [1:19:27<2:44:36,  1.44s/it] 30%|███       | 2959/9822 [1:19:28<2:44:33,  1.44s/it] 30%|███       | 2960/9822 [1:19:29<2:44:55,  1.44s/it] 30%|███       | 2961/9822 [1:19:31<2:44:51,  1.44s/it] 30%|███       | 2962/9822 [1:19:32<2:44:09,  1.44s/it] 30%|███       | 2963/9822 [1:19:34<2:44:01,  1.43s/it] 30%|███       | 2964/9822 [1:19:35<2:44:14,  1.44s/it] 30%|███       | 2965/9822 [1:19:37<2:45:12,  1.45s/it] 30%|███       | 2966/9822 [1:19:38<2:44:46,  1.44s/it] 30%|███       | 2967/9822 [1:19:39<2:44:19,  1.44s/it] 30%|███       | 2968/9822 [1:19:41<2:44:01,  1.44s/it] 30%|███       | 2969/9822 [1:19:42<2:43:25,  1.43s/it] 30%|███       | 2970/9822 [1:19:44<2:43:17,  1.43s/it] 30%|███       | 2971/9822 [1:19:45<2:43:07,  1.43s/it] 30%|███       | 2972/9822 [1:19:47<2:43:00,  1.43s/it] 30%|███       | 2973/9822 [1:19:48<2:42:56,  1.43s/it] 30%|███       | 2974/9822 [1:19:49<2:43:04,  1.43s/it] 30%|███       | 2975/9822 [1:19:51<2:42:47,  1.43s/it] 30%|███       | 2976/9822 [1:19:52<2:42:39,  1.43s/it] 30%|███       | 2977/9822 [1:19:54<2:42:30,  1.42s/it] 30%|███       | 2978/9822 [1:19:55<2:42:49,  1.43s/it] 30%|███       | 2979/9822 [1:19:57<2:42:53,  1.43s/it] 30%|███       | 2980/9822 [1:19:58<2:43:05,  1.43s/it] 30%|███       | 2981/9822 [1:19:59<2:42:57,  1.43s/it] 30%|███       | 2982/9822 [1:20:01<2:46:14,  1.46s/it] 30%|███       | 2983/9822 [1:20:02<2:45:02,  1.45s/it] 30%|███       | 2984/9822 [1:20:04<2:44:03,  1.44s/it] 30%|███       | 2985/9822 [1:20:05<2:44:05,  1.44s/it] 30%|███       | 2986/9822 [1:20:07<2:43:53,  1.44s/it] 30%|███       | 2987/9822 [1:20:08<2:43:28,  1.43s/it] 30%|███       | 2988/9822 [1:20:10<2:43:09,  1.43s/it] 30%|███       | 2989/9822 [1:20:11<2:42:57,  1.43s/it] 30%|███       | 2990/9822 [1:20:12<2:43:15,  1.43s/it] 30%|███       | 2991/9822 [1:20:14<2:43:00,  1.43s/it] 30%|███       | 2992/9822 [1:20:15<2:43:38,  1.44s/it] 30%|███       | 2993/9822 [1:20:17<2:44:59,  1.45s/it] 30%|███       | 2994/9822 [1:20:18<2:44:17,  1.44s/it] 30%|███       | 2995/9822 [1:20:20<2:43:40,  1.44s/it] 31%|███       | 2996/9822 [1:20:21<2:43:42,  1.44s/it] 31%|███       | 2997/9822 [1:20:23<2:43:18,  1.44s/it] 31%|███       | 2998/9822 [1:20:24<2:43:00,  1.43s/it] 31%|███       | 2999/9822 [1:20:25<2:43:06,  1.43s/it] 31%|███       | 3000/9822 [1:20:27<2:43:15,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:08:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:08:14 - INFO - __main__ - ***** test Results*****
01/07/2024 23:08:14 - INFO - __main__ -   Training step = 3000
01/07/2024 23:08:14 - INFO - __main__ -  test_accuracy:0.8499267935578331 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:08:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:08:20 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:08:20 - INFO - __main__ -   Training step = 3000
01/07/2024 23:08:20 - INFO - __main__ -  eval_accuracy:0.8454778469425119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8495056755767119}
test:
{'accuracy': 0.8605417276720352}
01/07/2024 23:08:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:08:24 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:08:24 - INFO - __main__ -   Training step = 3000
01/07/2024 23:08:24 - INFO - __main__ -  eval_accuracy:0.9062614426949835 
 31%|███       | 3001/9822 [1:20:45<12:20:52,  6.52s/it] 31%|███       | 3002/9822 [1:20:47<9:27:31,  4.99s/it]  31%|███       | 3003/9822 [1:20:48<7:26:20,  3.93s/it] 31%|███       | 3004/9822 [1:20:50<6:01:24,  3.18s/it] 31%|███       | 3005/9822 [1:20:51<5:01:42,  2.66s/it] 31%|███       | 3006/9822 [1:20:52<4:20:15,  2.29s/it] 31%|███       | 3007/9822 [1:20:54<3:51:07,  2.03s/it] 31%|███       | 3008/9822 [1:20:55<3:30:55,  1.86s/it] 31%|███       | 3009/9822 [1:20:57<3:19:09,  1.75s/it] 31%|███       | 3010/9822 [1:20:58<3:06:33,  1.64s/it] 31%|███       | 3011/9822 [1:21:00<2:59:47,  1.58s/it] 31%|███       | 3012/9822 [1:21:01<2:54:38,  1.54s/it] 31%|███       | 3013/9822 [1:21:02<2:51:10,  1.51s/it] 31%|███       | 3014/9822 [1:21:04<2:48:26,  1.48s/it] 31%|███       | 3015/9822 [1:21:05<2:46:47,  1.47s/it] 31%|███       | 3016/9822 [1:21:07<2:45:29,  1.46s/it] 31%|███       | 3017/9822 [1:21:08<2:45:06,  1.46s/it] 31%|███       | 3018/9822 [1:21:10<2:45:33,  1.46s/it] 31%|███       | 3019/9822 [1:21:11<2:44:38,  1.45s/it] 31%|███       | 3020/9822 [1:21:13<2:43:49,  1.45s/it] 31%|███       | 3021/9822 [1:21:14<2:43:08,  1.44s/it] 31%|███       | 3022/9822 [1:21:15<2:42:40,  1.44s/it] 31%|███       | 3023/9822 [1:21:17<2:42:35,  1.43s/it] 31%|███       | 3024/9822 [1:21:18<2:42:03,  1.43s/it] 31%|███       | 3025/9822 [1:21:20<2:42:22,  1.43s/it] 31%|███       | 3026/9822 [1:21:21<2:42:29,  1.43s/it] 31%|███       | 3027/9822 [1:21:23<2:42:29,  1.43s/it] 31%|███       | 3028/9822 [1:21:24<2:42:40,  1.44s/it] 31%|███       | 3029/9822 [1:21:25<2:42:18,  1.43s/it] 31%|███       | 3030/9822 [1:21:27<2:42:01,  1.43s/it] 31%|███       | 3031/9822 [1:21:28<2:41:53,  1.43s/it] 31%|███       | 3032/9822 [1:21:30<2:42:11,  1.43s/it] 31%|███       | 3033/9822 [1:21:31<2:42:54,  1.44s/it] 31%|███       | 3034/9822 [1:21:33<2:43:09,  1.44s/it] 31%|███       | 3035/9822 [1:21:34<2:43:27,  1.45s/it] 31%|███       | 3036/9822 [1:21:36<2:43:34,  1.45s/it] 31%|███       | 3037/9822 [1:21:37<2:43:09,  1.44s/it] 31%|███       | 3038/9822 [1:21:38<2:43:12,  1.44s/it] 31%|███       | 3039/9822 [1:21:40<2:46:31,  1.47s/it] 31%|███       | 3040/9822 [1:21:41<2:46:39,  1.47s/it] 31%|███       | 3041/9822 [1:21:43<2:45:51,  1.47s/it] 31%|███       | 3042/9822 [1:21:44<2:44:45,  1.46s/it] 31%|███       | 3043/9822 [1:21:46<2:44:10,  1.45s/it] 31%|███       | 3044/9822 [1:21:47<2:43:24,  1.45s/it] 31%|███       | 3045/9822 [1:21:49<2:43:29,  1.45s/it] 31%|███       | 3046/9822 [1:21:50<2:42:55,  1.44s/it] 31%|███       | 3047/9822 [1:21:52<2:42:38,  1.44s/it] 31%|███       | 3048/9822 [1:21:53<2:42:09,  1.44s/it] 31%|███       | 3049/9822 [1:21:54<2:42:05,  1.44s/it] 31%|███       | 3050/9822 [1:21:56<2:42:20,  1.44s/it] 31%|███       | 3051/9822 [1:21:57<2:42:35,  1.44s/it] 31%|███       | 3052/9822 [1:21:59<2:42:17,  1.44s/it] 31%|███       | 3053/9822 [1:22:00<2:42:32,  1.44s/it] 31%|███       | 3054/9822 [1:22:02<2:42:16,  1.44s/it] 31%|███       | 3055/9822 [1:22:03<2:42:00,  1.44s/it] 31%|███       | 3056/9822 [1:22:04<2:41:50,  1.44s/it] 31%|███       | 3057/9822 [1:22:06<2:41:18,  1.43s/it] 31%|███       | 3058/9822 [1:22:07<2:41:48,  1.44s/it] 31%|███       | 3059/9822 [1:22:09<2:41:47,  1.44s/it] 31%|███       | 3060/9822 [1:22:10<2:41:33,  1.43s/it] 31%|███       | 3061/9822 [1:22:12<2:41:23,  1.43s/it] 31%|███       | 3062/9822 [1:22:13<2:41:34,  1.43s/it] 31%|███       | 3063/9822 [1:22:14<2:41:38,  1.43s/it] 31%|███       | 3064/9822 [1:22:16<2:41:22,  1.43s/it] 31%|███       | 3065/9822 [1:22:17<2:41:16,  1.43s/it] 31%|███       | 3066/9822 [1:22:19<2:41:25,  1.43s/it] 31%|███       | 3067/9822 [1:22:20<2:41:10,  1.43s/it] 31%|███       | 3068/9822 [1:22:22<2:40:56,  1.43s/it] 31%|███       | 3069/9822 [1:22:23<2:40:57,  1.43s/it] 31%|███▏      | 3070/9822 [1:22:24<2:40:37,  1.43s/it] 31%|███▏      | 3071/9822 [1:22:26<2:43:03,  1.45s/it] 31%|███▏      | 3072/9822 [1:22:27<2:42:14,  1.44s/it] 31%|███▏      | 3073/9822 [1:22:29<2:41:44,  1.44s/it] 31%|███▏      | 3074/9822 [1:22:30<2:43:40,  1.46s/it] 31%|███▏      | 3075/9822 [1:22:32<2:44:16,  1.46s/it] 31%|███▏      | 3076/9822 [1:22:33<2:43:33,  1.45s/it] 31%|███▏      | 3077/9822 [1:22:35<2:43:10,  1.45s/it] 31%|███▏      | 3078/9822 [1:22:36<2:42:33,  1.45s/it] 31%|███▏      | 3079/9822 [1:22:38<2:42:48,  1.45s/it] 31%|███▏      | 3080/9822 [1:22:39<2:42:29,  1.45s/it] 31%|███▏      | 3081/9822 [1:22:40<2:42:13,  1.44s/it] 31%|███▏      | 3082/9822 [1:22:42<2:42:01,  1.44s/it] 31%|███▏      | 3083/9822 [1:22:43<2:41:51,  1.44s/it] 31%|███▏      | 3084/9822 [1:22:45<2:42:04,  1.44s/it] 31%|███▏      | 3085/9822 [1:22:46<2:43:12,  1.45s/it] 31%|███▏      | 3086/9822 [1:22:48<2:42:25,  1.45s/it] 31%|███▏      | 3087/9822 [1:22:49<2:41:53,  1.44s/it] 31%|███▏      | 3088/9822 [1:22:51<2:41:45,  1.44s/it] 31%|███▏      | 3089/9822 [1:22:52<2:41:00,  1.43s/it] 31%|███▏      | 3090/9822 [1:22:53<2:40:53,  1.43s/it] 31%|███▏      | 3091/9822 [1:22:55<2:40:51,  1.43s/it] 31%|███▏      | 3092/9822 [1:22:56<2:41:02,  1.44s/it] 31%|███▏      | 3093/9822 [1:22:58<2:40:51,  1.43s/it] 32%|███▏      | 3094/9822 [1:22:59<2:41:02,  1.44s/it] 32%|███▏      | 3095/9822 [1:23:01<2:40:49,  1.43s/it] 32%|███▏      | 3096/9822 [1:23:02<2:38:44,  1.42s/it] 32%|███▏      | 3097/9822 [1:23:03<2:39:58,  1.43s/it] 32%|███▏      | 3098/9822 [1:23:05<2:40:20,  1.43s/it] 32%|███▏      | 3099/9822 [1:23:06<2:40:04,  1.43s/it] 32%|███▏      | 3100/9822 [1:23:08<2:40:02,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:10:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:10:55 - INFO - __main__ - ***** test Results*****
01/07/2024 23:10:55 - INFO - __main__ -   Training step = 3100
01/07/2024 23:10:55 - INFO - __main__ -  test_accuracy:0.8693265007320644 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:11:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:11:01 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:11:01 - INFO - __main__ -   Training step = 3100
01/07/2024 23:11:01 - INFO - __main__ -  eval_accuracy:0.8513365067740755 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 23:11:01,132 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 23:11:01,132 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 23:11:01,168 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 23:11:02,745 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8693265007320644}
01/07/2024 23:11:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:11:07 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:11:07 - INFO - __main__ -   Training step = 3100
01/07/2024 23:11:07 - INFO - __main__ -  eval_accuracy:0.9029659465397291 
 32%|███▏      | 3101/9822 [1:23:28<13:02:37,  6.99s/it] 32%|███▏      | 3102/9822 [1:23:29<9:58:34,  5.34s/it]  32%|███▏      | 3103/9822 [1:23:31<7:47:03,  4.17s/it] 32%|███▏      | 3104/9822 [1:23:32<6:15:14,  3.35s/it] 32%|███▏      | 3105/9822 [1:23:33<5:10:47,  2.78s/it] 32%|███▏      | 3106/9822 [1:23:35<4:25:20,  2.37s/it] 32%|███▏      | 3107/9822 [1:23:36<3:53:29,  2.09s/it] 32%|███▏      | 3108/9822 [1:23:38<3:31:23,  1.89s/it] 32%|███▏      | 3109/9822 [1:23:39<3:16:06,  1.75s/it] 32%|███▏      | 3110/9822 [1:23:41<3:05:01,  1.65s/it] 32%|███▏      | 3111/9822 [1:23:42<2:57:22,  1.59s/it] 32%|███▏      | 3112/9822 [1:23:43<2:52:26,  1.54s/it] 32%|███▏      | 3113/9822 [1:23:45<2:49:02,  1.51s/it] 32%|███▏      | 3114/9822 [1:23:46<2:46:26,  1.49s/it] 32%|███▏      | 3115/9822 [1:23:48<2:44:11,  1.47s/it] 32%|███▏      | 3116/9822 [1:23:49<2:42:52,  1.46s/it] 32%|███▏      | 3117/9822 [1:23:51<2:41:56,  1.45s/it] 32%|███▏      | 3118/9822 [1:23:52<2:41:13,  1.44s/it] 32%|███▏      | 3119/9822 [1:23:53<2:40:36,  1.44s/it] 32%|███▏      | 3120/9822 [1:23:55<2:41:30,  1.45s/it] 32%|███▏      | 3121/9822 [1:23:56<2:40:40,  1.44s/it] 32%|███▏      | 3122/9822 [1:23:58<2:40:08,  1.43s/it] 32%|███▏      | 3123/9822 [1:23:59<2:40:03,  1.43s/it] 32%|███▏      | 3124/9822 [1:24:01<2:39:44,  1.43s/it] 32%|███▏      | 3125/9822 [1:24:02<2:39:42,  1.43s/it] 32%|███▏      | 3126/9822 [1:24:04<2:39:45,  1.43s/it] 32%|███▏      | 3127/9822 [1:24:05<2:39:54,  1.43s/it] 32%|███▏      | 3128/9822 [1:24:06<2:40:13,  1.44s/it] 32%|███▏      | 3129/9822 [1:24:08<2:40:08,  1.44s/it] 32%|███▏      | 3130/9822 [1:24:09<2:39:55,  1.43s/it] 32%|███▏      | 3131/9822 [1:24:11<2:39:33,  1.43s/it] 32%|███▏      | 3132/9822 [1:24:12<2:39:26,  1.43s/it] 32%|███▏      | 3133/9822 [1:24:14<2:42:07,  1.45s/it] 32%|███▏      | 3134/9822 [1:24:15<2:41:29,  1.45s/it] 32%|███▏      | 3135/9822 [1:24:17<2:41:06,  1.45s/it] 32%|███▏      | 3136/9822 [1:24:18<2:40:23,  1.44s/it] 32%|███▏      | 3137/9822 [1:24:19<2:39:47,  1.43s/it] 32%|███▏      | 3138/9822 [1:24:21<2:39:29,  1.43s/it] 32%|███▏      | 3139/9822 [1:24:22<2:39:51,  1.44s/it] 32%|███▏      | 3140/9822 [1:24:24<2:39:48,  1.44s/it] 32%|███▏      | 3141/9822 [1:24:25<2:39:42,  1.43s/it] 32%|███▏      | 3142/9822 [1:24:27<2:39:48,  1.44s/it] 32%|███▏      | 3143/9822 [1:24:28<2:40:08,  1.44s/it] 32%|███▏      | 3144/9822 [1:24:29<2:39:51,  1.44s/it] 32%|███▏      | 3145/9822 [1:24:31<2:39:32,  1.43s/it] 32%|███▏      | 3146/9822 [1:24:32<2:39:30,  1.43s/it] 32%|███▏      | 3147/9822 [1:24:34<2:39:28,  1.43s/it] 32%|███▏      | 3148/9822 [1:24:35<2:39:14,  1.43s/it] 32%|███▏      | 3149/9822 [1:24:37<2:39:26,  1.43s/it] 32%|███▏      | 3150/9822 [1:24:38<2:39:36,  1.44s/it] 32%|███▏      | 3151/9822 [1:24:39<2:41:12,  1.45s/it] 32%|███▏      | 3152/9822 [1:24:41<2:40:40,  1.45s/it] 32%|███▏      | 3153/9822 [1:24:42<2:40:19,  1.44s/it] 32%|███▏      | 3154/9822 [1:24:44<2:39:58,  1.44s/it] 32%|███▏      | 3155/9822 [1:24:45<2:39:34,  1.44s/it] 32%|███▏      | 3156/9822 [1:24:47<2:39:39,  1.44s/it] 32%|███▏      | 3157/9822 [1:24:48<2:39:27,  1.44s/it] 32%|███▏      | 3158/9822 [1:24:50<2:42:22,  1.46s/it] 32%|███▏      | 3159/9822 [1:24:51<2:41:23,  1.45s/it] 32%|███▏      | 3160/9822 [1:24:52<2:40:56,  1.45s/it] 32%|███▏      | 3161/9822 [1:24:54<2:40:20,  1.44s/it] 32%|███▏      | 3162/9822 [1:24:55<2:39:48,  1.44s/it] 32%|███▏      | 3163/9822 [1:24:57<2:39:37,  1.44s/it] 32%|███▏      | 3164/9822 [1:24:58<2:39:20,  1.44s/it] 32%|███▏      | 3165/9822 [1:25:00<2:39:10,  1.43s/it] 32%|███▏      | 3166/9822 [1:25:01<2:38:49,  1.43s/it] 32%|███▏      | 3167/9822 [1:25:02<2:38:41,  1.43s/it] 32%|███▏      | 3168/9822 [1:25:04<2:38:42,  1.43s/it] 32%|███▏      | 3169/9822 [1:25:05<2:38:24,  1.43s/it] 32%|███▏      | 3170/9822 [1:25:07<2:38:16,  1.43s/it] 32%|███▏      | 3171/9822 [1:25:08<2:38:24,  1.43s/it] 32%|███▏      | 3172/9822 [1:25:10<2:38:15,  1.43s/it] 32%|███▏      | 3173/9822 [1:25:11<2:38:23,  1.43s/it] 32%|███▏      | 3174/9822 [1:25:13<2:38:42,  1.43s/it] 32%|███▏      | 3175/9822 [1:25:14<2:38:35,  1.43s/it] 32%|███▏      | 3176/9822 [1:25:15<2:38:06,  1.43s/it] 32%|███▏      | 3177/9822 [1:25:17<2:37:49,  1.43s/it] 32%|███▏      | 3178/9822 [1:25:18<2:37:53,  1.43s/it] 32%|███▏      | 3179/9822 [1:25:20<2:37:59,  1.43s/it] 32%|███▏      | 3180/9822 [1:25:21<2:38:25,  1.43s/it] 32%|███▏      | 3181/9822 [1:25:23<2:38:39,  1.43s/it] 32%|███▏      | 3182/9822 [1:25:24<2:36:51,  1.42s/it] 32%|███▏      | 3183/9822 [1:25:25<2:37:20,  1.42s/it] 32%|███▏      | 3184/9822 [1:25:27<2:37:58,  1.43s/it] 32%|███▏      | 3185/9822 [1:25:28<2:38:43,  1.43s/it] 32%|███▏      | 3186/9822 [1:25:30<2:38:57,  1.44s/it] 32%|███▏      | 3187/9822 [1:25:31<2:38:37,  1.43s/it] 32%|███▏      | 3188/9822 [1:25:33<2:40:02,  1.45s/it] 32%|███▏      | 3189/9822 [1:25:34<2:39:18,  1.44s/it] 32%|███▏      | 3190/9822 [1:25:36<2:41:52,  1.46s/it] 32%|███▏      | 3191/9822 [1:25:37<2:40:32,  1.45s/it] 32%|███▏      | 3192/9822 [1:25:38<2:40:07,  1.45s/it] 33%|███▎      | 3193/9822 [1:25:40<2:39:57,  1.45s/it] 33%|███▎      | 3194/9822 [1:25:41<2:39:46,  1.45s/it] 33%|███▎      | 3195/9822 [1:25:43<2:39:37,  1.45s/it] 33%|███▎      | 3196/9822 [1:25:44<2:39:05,  1.44s/it] 33%|███▎      | 3197/9822 [1:25:46<2:38:54,  1.44s/it] 33%|███▎      | 3198/9822 [1:25:47<2:38:38,  1.44s/it] 33%|███▎      | 3199/9822 [1:25:48<2:38:30,  1.44s/it] 33%|███▎      | 3200/9822 [1:25:50<2:38:33,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:13:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:13:37 - INFO - __main__ - ***** test Results*****
01/07/2024 23:13:37 - INFO - __main__ -   Training step = 3200
01/07/2024 23:13:37 - INFO - __main__ -  test_accuracy:0.8704245973645681 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:13:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:13:43 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:13:43 - INFO - __main__ -   Training step = 3200
01/07/2024 23:13:43 - INFO - __main__ -  eval_accuracy:0.8487733430977664 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8513365067740755}
test:
{'accuracy': 0.8693265007320644}
01/07/2024 23:13:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:13:48 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:13:48 - INFO - __main__ -   Training step = 3200
01/07/2024 23:13:48 - INFO - __main__ -  eval_accuracy:0.9055291102160381 
 33%|███▎      | 3201/9822 [1:26:08<11:58:35,  6.51s/it] 33%|███▎      | 3202/9822 [1:26:10<9:10:22,  4.99s/it]  33%|███▎      | 3203/9822 [1:26:11<7:12:39,  3.92s/it] 33%|███▎      | 3204/9822 [1:26:13<5:49:55,  3.17s/it] 33%|███▎      | 3205/9822 [1:26:14<4:52:11,  2.65s/it] 33%|███▎      | 3206/9822 [1:26:15<4:12:01,  2.29s/it] 33%|███▎      | 3207/9822 [1:26:17<3:43:39,  2.03s/it] 33%|███▎      | 3208/9822 [1:26:18<3:24:27,  1.85s/it] 33%|███▎      | 3209/9822 [1:26:20<3:10:06,  1.72s/it] 33%|███▎      | 3210/9822 [1:26:21<3:00:17,  1.64s/it] 33%|███▎      | 3211/9822 [1:26:23<2:54:10,  1.58s/it] 33%|███▎      | 3212/9822 [1:26:24<2:49:01,  1.53s/it] 33%|███▎      | 3213/9822 [1:26:25<2:45:54,  1.51s/it] 33%|███▎      | 3214/9822 [1:26:27<2:43:45,  1.49s/it] 33%|███▎      | 3215/9822 [1:26:28<2:41:55,  1.47s/it] 33%|███▎      | 3216/9822 [1:26:30<2:40:15,  1.46s/it] 33%|███▎      | 3217/9822 [1:26:31<2:39:31,  1.45s/it] 33%|███▎      | 3218/9822 [1:26:33<2:38:48,  1.44s/it] 33%|███▎      | 3219/9822 [1:26:34<2:38:41,  1.44s/it] 33%|███▎      | 3220/9822 [1:26:35<2:38:03,  1.44s/it] 33%|███▎      | 3221/9822 [1:26:37<2:37:55,  1.44s/it] 33%|███▎      | 3222/9822 [1:26:38<2:37:29,  1.43s/it] 33%|███▎      | 3223/9822 [1:26:40<2:37:14,  1.43s/it] 33%|███▎      | 3224/9822 [1:26:41<2:37:37,  1.43s/it] 33%|███▎      | 3225/9822 [1:26:43<2:37:35,  1.43s/it] 33%|███▎      | 3226/9822 [1:26:44<2:38:17,  1.44s/it] 33%|███▎      | 3227/9822 [1:26:46<2:38:37,  1.44s/it] 33%|███▎      | 3228/9822 [1:26:47<2:40:52,  1.46s/it] 33%|███▎      | 3229/9822 [1:26:48<2:39:38,  1.45s/it] 33%|███▎      | 3230/9822 [1:26:50<2:39:03,  1.45s/it] 33%|███▎      | 3231/9822 [1:26:51<2:38:04,  1.44s/it] 33%|███▎      | 3232/9822 [1:26:53<2:37:36,  1.43s/it] 33%|███▎      | 3233/9822 [1:26:54<2:37:18,  1.43s/it] 33%|███▎      | 3234/9822 [1:26:56<2:37:18,  1.43s/it] 33%|███▎      | 3235/9822 [1:26:57<2:37:00,  1.43s/it] 33%|███▎      | 3236/9822 [1:26:58<2:37:15,  1.43s/it] 33%|███▎      | 3237/9822 [1:27:00<2:37:00,  1.43s/it] 33%|███▎      | 3238/9822 [1:27:01<2:37:01,  1.43s/it] 33%|███▎      | 3239/9822 [1:27:03<2:37:17,  1.43s/it] 33%|███▎      | 3240/9822 [1:27:04<2:37:27,  1.44s/it] 33%|███▎      | 3241/9822 [1:27:06<2:37:18,  1.43s/it] 33%|███▎      | 3242/9822 [1:27:07<2:37:08,  1.43s/it] 33%|███▎      | 3243/9822 [1:27:08<2:37:10,  1.43s/it] 33%|███▎      | 3244/9822 [1:27:10<2:37:04,  1.43s/it] 33%|███▎      | 3245/9822 [1:27:11<2:36:59,  1.43s/it] 33%|███▎      | 3246/9822 [1:27:13<2:37:15,  1.43s/it] 33%|███▎      | 3247/9822 [1:27:14<2:37:03,  1.43s/it] 33%|███▎      | 3248/9822 [1:27:16<2:37:12,  1.43s/it] 33%|███▎      | 3249/9822 [1:27:17<2:37:24,  1.44s/it] 33%|███▎      | 3250/9822 [1:27:19<2:37:35,  1.44s/it] 33%|███▎      | 3251/9822 [1:27:20<2:37:56,  1.44s/it] 33%|███▎      | 3252/9822 [1:27:21<2:37:38,  1.44s/it] 33%|███▎      | 3253/9822 [1:27:23<2:37:21,  1.44s/it] 33%|███▎      | 3254/9822 [1:27:24<2:37:11,  1.44s/it] 33%|███▎      | 3255/9822 [1:27:26<2:37:01,  1.43s/it] 33%|███▎      | 3256/9822 [1:27:27<2:36:38,  1.43s/it] 33%|███▎      | 3257/9822 [1:27:29<2:36:37,  1.43s/it] 33%|███▎      | 3258/9822 [1:27:30<2:36:51,  1.43s/it] 33%|███▎      | 3259/9822 [1:27:31<2:36:35,  1.43s/it] 33%|███▎      | 3260/9822 [1:27:33<2:39:29,  1.46s/it] 33%|███▎      | 3261/9822 [1:27:34<2:38:41,  1.45s/it] 33%|███▎      | 3262/9822 [1:27:36<2:37:47,  1.44s/it] 33%|███▎      | 3263/9822 [1:27:37<2:37:22,  1.44s/it] 33%|███▎      | 3264/9822 [1:27:39<2:36:47,  1.43s/it] 33%|███▎      | 3265/9822 [1:27:40<2:36:59,  1.44s/it] 33%|███▎      | 3266/9822 [1:27:42<2:37:03,  1.44s/it] 33%|███▎      | 3267/9822 [1:27:43<2:36:54,  1.44s/it] 33%|███▎      | 3268/9822 [1:27:44<2:35:09,  1.42s/it] 33%|███▎      | 3269/9822 [1:27:46<2:35:24,  1.42s/it] 33%|███▎      | 3270/9822 [1:27:47<2:35:31,  1.42s/it] 33%|███▎      | 3271/9822 [1:27:49<2:35:59,  1.43s/it] 33%|███▎      | 3272/9822 [1:27:50<2:36:05,  1.43s/it] 33%|███▎      | 3273/9822 [1:27:52<2:36:28,  1.43s/it] 33%|███▎      | 3274/9822 [1:27:52<2:19:23,  1.28s/it] 33%|███▎      | 3275/9822 [1:27:54<2:24:40,  1.33s/it] 33%|███▎      | 3276/9822 [1:27:55<2:28:26,  1.36s/it] 33%|███▎      | 3277/9822 [1:27:57<2:30:42,  1.38s/it] 33%|███▎      | 3278/9822 [1:27:58<2:32:30,  1.40s/it] 33%|███▎      | 3279/9822 [1:28:00<2:33:53,  1.41s/it] 33%|███▎      | 3280/9822 [1:28:01<2:34:37,  1.42s/it] 33%|███▎      | 3281/9822 [1:28:03<2:35:06,  1.42s/it] 33%|███▎      | 3282/9822 [1:28:04<2:35:19,  1.42s/it] 33%|███▎      | 3283/9822 [1:28:05<2:35:44,  1.43s/it] 33%|███▎      | 3284/9822 [1:28:07<2:35:51,  1.43s/it] 33%|███▎      | 3285/9822 [1:28:08<2:38:56,  1.46s/it] 33%|███▎      | 3286/9822 [1:28:10<2:38:04,  1.45s/it] 33%|███▎      | 3287/9822 [1:28:11<2:37:30,  1.45s/it] 33%|███▎      | 3288/9822 [1:28:13<2:37:05,  1.44s/it] 33%|███▎      | 3289/9822 [1:28:14<2:36:14,  1.43s/it] 33%|███▎      | 3290/9822 [1:28:15<2:36:02,  1.43s/it] 34%|███▎      | 3291/9822 [1:28:17<2:35:36,  1.43s/it] 34%|███▎      | 3292/9822 [1:28:18<2:35:17,  1.43s/it] 34%|███▎      | 3293/9822 [1:28:20<2:35:12,  1.43s/it] 34%|███▎      | 3294/9822 [1:28:21<2:35:20,  1.43s/it] 34%|███▎      | 3295/9822 [1:28:23<2:35:18,  1.43s/it] 34%|███▎      | 3296/9822 [1:28:24<2:35:28,  1.43s/it] 34%|███▎      | 3297/9822 [1:28:25<2:35:25,  1.43s/it] 34%|███▎      | 3298/9822 [1:28:27<2:35:26,  1.43s/it] 34%|███▎      | 3299/9822 [1:28:28<2:35:47,  1.43s/it] 34%|███▎      | 3300/9822 [1:28:30<2:35:37,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:16:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:16:17 - INFO - __main__ - ***** test Results*****
01/07/2024 23:16:17 - INFO - __main__ -   Training step = 3300
01/07/2024 23:16:17 - INFO - __main__ -  test_accuracy:0.8627379209370425 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:16:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:16:23 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:16:23 - INFO - __main__ -   Training step = 3300
01/07/2024 23:16:23 - INFO - __main__ -  eval_accuracy:0.8538996704503845 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 23:16:23,202 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 23:16:23,202 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 23:16:23,239 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 23:16:24,826 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8627379209370425}
01/07/2024 23:16:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:16:29 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:16:29 - INFO - __main__ -   Training step = 3300
01/07/2024 23:16:29 - INFO - __main__ -  eval_accuracy:0.9044306114976199 
 34%|███▎      | 3301/9822 [1:28:50<12:40:40,  7.00s/it] 34%|███▎      | 3302/9822 [1:28:51<9:39:19,  5.33s/it]  34%|███▎      | 3303/9822 [1:28:53<7:32:05,  4.16s/it] 34%|███▎      | 3304/9822 [1:28:54<6:03:05,  3.34s/it] 34%|███▎      | 3305/9822 [1:28:56<5:00:53,  2.77s/it] 34%|███▎      | 3306/9822 [1:28:57<4:17:11,  2.37s/it] 34%|███▎      | 3307/9822 [1:28:58<3:46:56,  2.09s/it] 34%|███▎      | 3308/9822 [1:29:00<3:25:31,  1.89s/it] 34%|███▎      | 3309/9822 [1:29:01<3:10:42,  1.76s/it] 34%|███▎      | 3310/9822 [1:29:03<3:00:46,  1.67s/it] 34%|███▎      | 3311/9822 [1:29:04<2:53:09,  1.60s/it] 34%|███▎      | 3312/9822 [1:29:06<2:48:00,  1.55s/it] 34%|███▎      | 3313/9822 [1:29:07<2:43:58,  1.51s/it] 34%|███▎      | 3314/9822 [1:29:08<2:42:17,  1.50s/it] 34%|███▍      | 3315/9822 [1:29:10<2:40:28,  1.48s/it] 34%|███▍      | 3316/9822 [1:29:11<2:39:14,  1.47s/it] 34%|███▍      | 3317/9822 [1:29:13<2:38:19,  1.46s/it] 34%|███▍      | 3318/9822 [1:29:14<2:37:36,  1.45s/it] 34%|███▍      | 3319/9822 [1:29:16<2:36:43,  1.45s/it] 34%|███▍      | 3320/9822 [1:29:17<2:37:03,  1.45s/it] 34%|███▍      | 3321/9822 [1:29:19<2:36:20,  1.44s/it] 34%|███▍      | 3322/9822 [1:29:20<2:36:03,  1.44s/it] 34%|███▍      | 3323/9822 [1:29:21<2:36:09,  1.44s/it] 34%|███▍      | 3324/9822 [1:29:23<2:35:44,  1.44s/it] 34%|███▍      | 3325/9822 [1:29:24<2:35:20,  1.43s/it] 34%|███▍      | 3326/9822 [1:29:26<2:35:02,  1.43s/it] 34%|███▍      | 3327/9822 [1:29:27<2:35:35,  1.44s/it] 34%|███▍      | 3328/9822 [1:29:29<2:35:32,  1.44s/it] 34%|███▍      | 3329/9822 [1:29:30<2:35:00,  1.43s/it] 34%|███▍      | 3330/9822 [1:29:31<2:35:08,  1.43s/it] 34%|███▍      | 3331/9822 [1:29:33<2:34:59,  1.43s/it] 34%|███▍      | 3332/9822 [1:29:34<2:35:17,  1.44s/it] 34%|███▍      | 3333/9822 [1:29:36<2:35:09,  1.43s/it] 34%|███▍      | 3334/9822 [1:29:37<2:35:08,  1.43s/it] 34%|███▍      | 3335/9822 [1:29:39<2:35:03,  1.43s/it] 34%|███▍      | 3336/9822 [1:29:40<2:34:48,  1.43s/it] 34%|███▍      | 3337/9822 [1:29:41<2:34:56,  1.43s/it] 34%|███▍      | 3338/9822 [1:29:43<2:35:10,  1.44s/it] 34%|███▍      | 3339/9822 [1:29:44<2:35:00,  1.43s/it] 34%|███▍      | 3340/9822 [1:29:46<2:34:34,  1.43s/it] 34%|███▍      | 3341/9822 [1:29:47<2:34:44,  1.43s/it] 34%|███▍      | 3342/9822 [1:29:49<2:35:10,  1.44s/it] 34%|███▍      | 3343/9822 [1:29:50<2:35:08,  1.44s/it] 34%|███▍      | 3344/9822 [1:29:52<2:34:44,  1.43s/it] 34%|███▍      | 3345/9822 [1:29:53<2:34:44,  1.43s/it] 34%|███▍      | 3346/9822 [1:29:54<2:34:50,  1.43s/it] 34%|███▍      | 3347/9822 [1:29:56<2:34:28,  1.43s/it] 34%|███▍      | 3348/9822 [1:29:57<2:34:36,  1.43s/it] 34%|███▍      | 3349/9822 [1:29:59<2:34:30,  1.43s/it] 34%|███▍      | 3350/9822 [1:30:00<2:34:18,  1.43s/it] 34%|███▍      | 3351/9822 [1:30:02<2:33:59,  1.43s/it] 34%|███▍      | 3352/9822 [1:30:03<2:34:07,  1.43s/it] 34%|███▍      | 3353/9822 [1:30:04<2:34:14,  1.43s/it] 34%|███▍      | 3354/9822 [1:30:06<2:34:55,  1.44s/it] 34%|███▍      | 3355/9822 [1:30:07<2:35:11,  1.44s/it] 34%|███▍      | 3356/9822 [1:30:09<2:35:02,  1.44s/it] 34%|███▍      | 3357/9822 [1:30:10<2:36:11,  1.45s/it] 34%|███▍      | 3358/9822 [1:30:12<2:35:54,  1.45s/it] 34%|███▍      | 3359/9822 [1:30:13<2:35:57,  1.45s/it] 34%|███▍      | 3360/9822 [1:30:15<2:35:23,  1.44s/it] 34%|███▍      | 3361/9822 [1:30:16<2:35:22,  1.44s/it] 34%|███▍      | 3362/9822 [1:30:17<2:35:00,  1.44s/it] 34%|███▍      | 3363/9822 [1:30:19<2:34:28,  1.44s/it] 34%|███▍      | 3364/9822 [1:30:20<2:34:20,  1.43s/it] 34%|███▍      | 3365/9822 [1:30:22<2:34:15,  1.43s/it] 34%|███▍      | 3366/9822 [1:30:23<2:34:20,  1.43s/it] 34%|███▍      | 3367/9822 [1:30:25<2:34:07,  1.43s/it] 34%|███▍      | 3368/9822 [1:30:26<2:34:05,  1.43s/it] 34%|███▍      | 3369/9822 [1:30:27<2:33:52,  1.43s/it] 34%|███▍      | 3370/9822 [1:30:29<2:33:41,  1.43s/it] 34%|███▍      | 3371/9822 [1:30:30<2:33:43,  1.43s/it] 34%|███▍      | 3372/9822 [1:30:32<2:33:58,  1.43s/it] 34%|███▍      | 3373/9822 [1:30:33<2:33:59,  1.43s/it] 34%|███▍      | 3374/9822 [1:30:35<2:34:54,  1.44s/it] 34%|███▍      | 3375/9822 [1:30:36<2:34:53,  1.44s/it] 34%|███▍      | 3376/9822 [1:30:37<2:34:49,  1.44s/it] 34%|███▍      | 3377/9822 [1:30:39<2:34:09,  1.44s/it] 34%|███▍      | 3378/9822 [1:30:40<2:34:20,  1.44s/it] 34%|███▍      | 3379/9822 [1:30:42<2:34:01,  1.43s/it] 34%|███▍      | 3380/9822 [1:30:43<2:33:53,  1.43s/it] 34%|███▍      | 3381/9822 [1:30:45<2:33:19,  1.43s/it] 34%|███▍      | 3382/9822 [1:30:46<2:33:06,  1.43s/it] 34%|███▍      | 3383/9822 [1:30:48<2:34:09,  1.44s/it] 34%|███▍      | 3384/9822 [1:30:49<2:35:58,  1.45s/it] 34%|███▍      | 3385/9822 [1:30:51<2:39:14,  1.48s/it] 34%|███▍      | 3386/9822 [1:30:52<2:38:02,  1.47s/it] 34%|███▍      | 3387/9822 [1:30:53<2:37:10,  1.47s/it] 34%|███▍      | 3388/9822 [1:30:55<2:36:48,  1.46s/it] 35%|███▍      | 3389/9822 [1:30:56<2:36:49,  1.46s/it] 35%|███▍      | 3390/9822 [1:30:58<2:36:37,  1.46s/it] 35%|███▍      | 3391/9822 [1:30:59<2:35:24,  1.45s/it] 35%|███▍      | 3392/9822 [1:31:01<2:34:47,  1.44s/it] 35%|███▍      | 3393/9822 [1:31:02<2:34:09,  1.44s/it] 35%|███▍      | 3394/9822 [1:31:04<2:34:11,  1.44s/it] 35%|███▍      | 3395/9822 [1:31:05<2:33:53,  1.44s/it] 35%|███▍      | 3396/9822 [1:31:06<2:33:57,  1.44s/it] 35%|███▍      | 3397/9822 [1:31:08<2:34:28,  1.44s/it] 35%|███▍      | 3398/9822 [1:31:09<2:35:45,  1.45s/it] 35%|███▍      | 3399/9822 [1:31:11<2:35:31,  1.45s/it] 35%|███▍      | 3400/9822 [1:31:12<2:35:33,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:18:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:18:59 - INFO - __main__ - ***** test Results*****
01/07/2024 23:18:59 - INFO - __main__ -   Training step = 3400
01/07/2024 23:18:59 - INFO - __main__ -  test_accuracy:0.8587115666178624 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:19:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:19:05 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:19:05 - INFO - __main__ -   Training step = 3400
01/07/2024 23:19:05 - INFO - __main__ -  eval_accuracy:0.8524350054924936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8538996704503845}
test:
{'accuracy': 0.8627379209370425}
01/07/2024 23:19:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:19:10 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:19:10 - INFO - __main__ -   Training step = 3400
01/07/2024 23:19:10 - INFO - __main__ -  eval_accuracy:0.8974734529476383 
 35%|███▍      | 3401/9822 [1:31:31<11:37:11,  6.51s/it] 35%|███▍      | 3402/9822 [1:31:32<8:54:14,  4.99s/it]  35%|███▍      | 3403/9822 [1:31:34<7:01:39,  3.94s/it] 35%|███▍      | 3404/9822 [1:31:35<5:41:24,  3.19s/it] 35%|███▍      | 3405/9822 [1:31:36<4:45:47,  2.67s/it] 35%|███▍      | 3406/9822 [1:31:38<4:05:57,  2.30s/it] 35%|███▍      | 3407/9822 [1:31:39<3:37:49,  2.04s/it] 35%|███▍      | 3408/9822 [1:31:41<3:18:15,  1.85s/it] 35%|███▍      | 3409/9822 [1:31:42<3:04:28,  1.73s/it] 35%|███▍      | 3410/9822 [1:31:44<2:54:48,  1.64s/it] 35%|███▍      | 3411/9822 [1:31:45<2:48:17,  1.58s/it] 35%|███▍      | 3412/9822 [1:31:47<2:46:24,  1.56s/it] 35%|███▍      | 3413/9822 [1:31:48<2:42:06,  1.52s/it] 35%|███▍      | 3414/9822 [1:31:49<2:39:34,  1.49s/it] 35%|███▍      | 3415/9822 [1:31:51<2:37:13,  1.47s/it] 35%|███▍      | 3416/9822 [1:31:52<2:35:46,  1.46s/it] 35%|███▍      | 3417/9822 [1:31:54<2:34:45,  1.45s/it] 35%|███▍      | 3418/9822 [1:31:55<2:34:05,  1.44s/it] 35%|███▍      | 3419/9822 [1:31:57<2:33:30,  1.44s/it] 35%|███▍      | 3420/9822 [1:31:58<2:33:39,  1.44s/it] 35%|███▍      | 3421/9822 [1:31:59<2:33:22,  1.44s/it] 35%|███▍      | 3422/9822 [1:32:01<2:33:13,  1.44s/it] 35%|███▍      | 3423/9822 [1:32:02<2:32:52,  1.43s/it] 35%|███▍      | 3424/9822 [1:32:04<2:32:52,  1.43s/it] 35%|███▍      | 3425/9822 [1:32:05<2:32:52,  1.43s/it] 35%|███▍      | 3426/9822 [1:32:07<2:32:40,  1.43s/it] 35%|███▍      | 3427/9822 [1:32:08<2:33:00,  1.44s/it] 35%|███▍      | 3428/9822 [1:32:09<2:33:01,  1.44s/it] 35%|███▍      | 3429/9822 [1:32:11<2:33:15,  1.44s/it] 35%|███▍      | 3430/9822 [1:32:12<2:32:40,  1.43s/it] 35%|███▍      | 3431/9822 [1:32:14<2:33:20,  1.44s/it] 35%|███▍      | 3432/9822 [1:32:15<2:33:19,  1.44s/it] 35%|███▍      | 3433/9822 [1:32:17<2:32:43,  1.43s/it] 35%|███▍      | 3434/9822 [1:32:18<2:32:50,  1.44s/it] 35%|███▍      | 3435/9822 [1:32:19<2:32:46,  1.44s/it] 35%|███▍      | 3436/9822 [1:32:21<2:32:34,  1.43s/it] 35%|███▍      | 3437/9822 [1:32:22<2:32:48,  1.44s/it] 35%|███▌      | 3438/9822 [1:32:24<2:32:47,  1.44s/it] 35%|███▌      | 3439/9822 [1:32:25<2:33:07,  1.44s/it] 35%|███▌      | 3440/9822 [1:32:27<2:31:18,  1.42s/it] 35%|███▌      | 3441/9822 [1:32:28<2:31:59,  1.43s/it] 35%|███▌      | 3442/9822 [1:32:30<2:34:52,  1.46s/it] 35%|███▌      | 3443/9822 [1:32:31<2:34:08,  1.45s/it] 35%|███▌      | 3444/9822 [1:32:32<2:33:27,  1.44s/it] 35%|███▌      | 3445/9822 [1:32:34<2:32:54,  1.44s/it] 35%|███▌      | 3446/9822 [1:32:35<2:32:22,  1.43s/it] 35%|███▌      | 3447/9822 [1:32:37<2:32:12,  1.43s/it] 35%|███▌      | 3448/9822 [1:32:38<2:32:23,  1.43s/it] 35%|███▌      | 3449/9822 [1:32:40<2:32:13,  1.43s/it] 35%|███▌      | 3450/9822 [1:32:41<2:31:55,  1.43s/it] 35%|███▌      | 3451/9822 [1:32:42<2:32:25,  1.44s/it] 35%|███▌      | 3452/9822 [1:32:44<2:32:15,  1.43s/it] 35%|███▌      | 3453/9822 [1:32:45<2:32:16,  1.43s/it] 35%|███▌      | 3454/9822 [1:32:47<2:32:22,  1.44s/it] 35%|███▌      | 3455/9822 [1:32:48<2:32:02,  1.43s/it] 35%|███▌      | 3456/9822 [1:32:50<2:31:52,  1.43s/it] 35%|███▌      | 3457/9822 [1:32:51<2:31:56,  1.43s/it] 35%|███▌      | 3458/9822 [1:32:52<2:31:50,  1.43s/it] 35%|███▌      | 3459/9822 [1:32:54<2:32:38,  1.44s/it] 35%|███▌      | 3460/9822 [1:32:55<2:32:42,  1.44s/it] 35%|███▌      | 3461/9822 [1:32:57<2:32:51,  1.44s/it] 35%|███▌      | 3462/9822 [1:32:58<2:32:29,  1.44s/it] 35%|███▌      | 3463/9822 [1:33:00<2:32:16,  1.44s/it] 35%|███▌      | 3464/9822 [1:33:01<2:32:06,  1.44s/it] 35%|███▌      | 3465/9822 [1:33:03<2:32:02,  1.44s/it] 35%|███▌      | 3466/9822 [1:33:04<2:31:46,  1.43s/it] 35%|███▌      | 3467/9822 [1:33:05<2:31:54,  1.43s/it] 35%|███▌      | 3468/9822 [1:33:07<2:31:49,  1.43s/it] 35%|███▌      | 3469/9822 [1:33:08<2:31:28,  1.43s/it] 35%|███▌      | 3470/9822 [1:33:10<2:31:36,  1.43s/it] 35%|███▌      | 3471/9822 [1:33:11<2:32:26,  1.44s/it] 35%|███▌      | 3472/9822 [1:33:13<2:32:17,  1.44s/it] 35%|███▌      | 3473/9822 [1:33:14<2:32:01,  1.44s/it] 35%|███▌      | 3474/9822 [1:33:16<2:34:32,  1.46s/it] 35%|███▌      | 3475/9822 [1:33:17<2:34:08,  1.46s/it] 35%|███▌      | 3476/9822 [1:33:18<2:33:50,  1.45s/it] 35%|███▌      | 3477/9822 [1:33:20<2:33:02,  1.45s/it] 35%|███▌      | 3478/9822 [1:33:21<2:32:26,  1.44s/it] 35%|███▌      | 3479/9822 [1:33:23<2:32:18,  1.44s/it] 35%|███▌      | 3480/9822 [1:33:24<2:32:13,  1.44s/it] 35%|███▌      | 3481/9822 [1:33:26<2:31:46,  1.44s/it] 35%|███▌      | 3482/9822 [1:33:27<2:31:26,  1.43s/it] 35%|███▌      | 3483/9822 [1:33:28<2:31:20,  1.43s/it] 35%|███▌      | 3484/9822 [1:33:30<2:31:28,  1.43s/it] 35%|███▌      | 3485/9822 [1:33:31<2:31:27,  1.43s/it] 35%|███▌      | 3486/9822 [1:33:33<2:31:43,  1.44s/it] 36%|███▌      | 3487/9822 [1:33:34<2:31:19,  1.43s/it] 36%|███▌      | 3488/9822 [1:33:36<2:31:02,  1.43s/it] 36%|███▌      | 3489/9822 [1:33:37<2:31:12,  1.43s/it] 36%|███▌      | 3490/9822 [1:33:39<2:31:20,  1.43s/it] 36%|███▌      | 3491/9822 [1:33:40<2:31:49,  1.44s/it] 36%|███▌      | 3492/9822 [1:33:41<2:31:54,  1.44s/it] 36%|███▌      | 3493/9822 [1:33:43<2:31:35,  1.44s/it] 36%|███▌      | 3494/9822 [1:33:44<2:31:12,  1.43s/it] 36%|███▌      | 3495/9822 [1:33:46<2:31:11,  1.43s/it] 36%|███▌      | 3496/9822 [1:33:47<2:31:23,  1.44s/it] 36%|███▌      | 3497/9822 [1:33:49<2:31:27,  1.44s/it] 36%|███▌      | 3498/9822 [1:33:50<2:31:13,  1.43s/it] 36%|███▌      | 3499/9822 [1:33:51<2:31:16,  1.44s/it] 36%|███▌      | 3500/9822 [1:33:53<2:31:18,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:21:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:21:40 - INFO - __main__ - ***** test Results*****
01/07/2024 23:21:40 - INFO - __main__ -   Training step = 3500
01/07/2024 23:21:40 - INFO - __main__ -  test_accuracy:0.8682284040995608 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:21:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:21:46 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:21:46 - INFO - __main__ -   Training step = 3500
01/07/2024 23:21:46 - INFO - __main__ -  eval_accuracy:0.8564628341266936 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 23:21:46,284 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 23:21:46,284 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 23:21:46,320 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 23:21:47,939 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:21:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:21:52 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:21:52 - INFO - __main__ -   Training step = 3500
01/07/2024 23:21:52 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 36%|███▌      | 3501/9822 [1:34:13<12:17:33,  7.00s/it] 36%|███▌      | 3502/9822 [1:34:14<9:22:02,  5.34s/it]  36%|███▌      | 3503/9822 [1:34:16<7:18:33,  4.16s/it] 36%|███▌      | 3504/9822 [1:34:17<5:52:19,  3.35s/it] 36%|███▌      | 3505/9822 [1:34:19<4:54:23,  2.80s/it] 36%|███▌      | 3506/9822 [1:34:20<4:11:17,  2.39s/it] 36%|███▌      | 3507/9822 [1:34:22<3:41:45,  2.11s/it] 36%|███▌      | 3508/9822 [1:34:23<3:22:01,  1.92s/it] 36%|███▌      | 3509/9822 [1:34:25<3:06:57,  1.78s/it] 36%|███▌      | 3510/9822 [1:34:26<2:56:40,  1.68s/it] 36%|███▌      | 3511/9822 [1:34:27<2:49:14,  1.61s/it] 36%|███▌      | 3512/9822 [1:34:29<2:44:01,  1.56s/it] 36%|███▌      | 3513/9822 [1:34:30<2:40:28,  1.53s/it] 36%|███▌      | 3514/9822 [1:34:32<2:37:50,  1.50s/it] 36%|███▌      | 3515/9822 [1:34:33<2:36:56,  1.49s/it] 36%|███▌      | 3516/9822 [1:34:35<2:36:18,  1.49s/it] 36%|███▌      | 3517/9822 [1:34:36<2:35:00,  1.48s/it] 36%|███▌      | 3518/9822 [1:34:38<2:33:43,  1.46s/it] 36%|███▌      | 3519/9822 [1:34:39<2:33:17,  1.46s/it] 36%|███▌      | 3520/9822 [1:34:40<2:32:18,  1.45s/it] 36%|███▌      | 3521/9822 [1:34:42<2:31:32,  1.44s/it] 36%|███▌      | 3522/9822 [1:34:43<2:30:53,  1.44s/it] 36%|███▌      | 3523/9822 [1:34:45<2:30:36,  1.43s/it] 36%|███▌      | 3524/9822 [1:34:46<2:30:21,  1.43s/it] 36%|███▌      | 3525/9822 [1:34:48<2:30:23,  1.43s/it] 36%|███▌      | 3526/9822 [1:34:49<2:29:12,  1.42s/it] 36%|███▌      | 3527/9822 [1:34:50<2:29:49,  1.43s/it] 36%|███▌      | 3528/9822 [1:34:52<2:29:35,  1.43s/it] 36%|███▌      | 3529/9822 [1:34:53<2:29:57,  1.43s/it] 36%|███▌      | 3530/9822 [1:34:55<2:30:18,  1.43s/it] 36%|███▌      | 3531/9822 [1:34:56<2:30:21,  1.43s/it] 36%|███▌      | 3532/9822 [1:34:58<2:31:00,  1.44s/it] 36%|███▌      | 3533/9822 [1:34:59<2:31:04,  1.44s/it] 36%|███▌      | 3534/9822 [1:35:01<2:31:07,  1.44s/it] 36%|███▌      | 3535/9822 [1:35:02<2:31:05,  1.44s/it] 36%|███▌      | 3536/9822 [1:35:03<2:30:37,  1.44s/it] 36%|███▌      | 3537/9822 [1:35:05<2:32:58,  1.46s/it] 36%|███▌      | 3538/9822 [1:35:06<2:31:47,  1.45s/it] 36%|███▌      | 3539/9822 [1:35:08<2:31:26,  1.45s/it] 36%|███▌      | 3540/9822 [1:35:09<2:30:51,  1.44s/it] 36%|███▌      | 3541/9822 [1:35:11<2:30:39,  1.44s/it] 36%|███▌      | 3542/9822 [1:35:12<2:31:00,  1.44s/it] 36%|███▌      | 3543/9822 [1:35:14<2:31:24,  1.45s/it] 36%|███▌      | 3544/9822 [1:35:15<2:30:31,  1.44s/it] 36%|███▌      | 3545/9822 [1:35:16<2:30:02,  1.43s/it] 36%|███▌      | 3546/9822 [1:35:18<2:29:57,  1.43s/it] 36%|███▌      | 3547/9822 [1:35:19<2:29:44,  1.43s/it] 36%|███▌      | 3548/9822 [1:35:21<2:29:32,  1.43s/it] 36%|███▌      | 3549/9822 [1:35:22<2:29:47,  1.43s/it] 36%|███▌      | 3550/9822 [1:35:24<2:29:57,  1.43s/it] 36%|███▌      | 3551/9822 [1:35:25<2:29:54,  1.43s/it] 36%|███▌      | 3552/9822 [1:35:26<2:29:42,  1.43s/it] 36%|███▌      | 3553/9822 [1:35:28<2:29:33,  1.43s/it] 36%|███▌      | 3554/9822 [1:35:29<2:29:25,  1.43s/it] 36%|███▌      | 3555/9822 [1:35:31<2:29:27,  1.43s/it] 36%|███▌      | 3556/9822 [1:35:32<2:29:19,  1.43s/it] 36%|███▌      | 3557/9822 [1:35:34<2:29:02,  1.43s/it] 36%|███▌      | 3558/9822 [1:35:35<2:29:24,  1.43s/it] 36%|███▌      | 3559/9822 [1:35:36<2:29:33,  1.43s/it] 36%|███▌      | 3560/9822 [1:35:38<2:29:32,  1.43s/it] 36%|███▋      | 3561/9822 [1:35:39<2:29:07,  1.43s/it] 36%|███▋      | 3562/9822 [1:35:41<2:30:03,  1.44s/it] 36%|███▋      | 3563/9822 [1:35:42<2:29:44,  1.44s/it] 36%|███▋      | 3564/9822 [1:35:44<2:30:03,  1.44s/it] 36%|███▋      | 3565/9822 [1:35:45<2:29:49,  1.44s/it] 36%|███▋      | 3566/9822 [1:35:46<2:29:37,  1.44s/it] 36%|███▋      | 3567/9822 [1:35:48<2:32:16,  1.46s/it] 36%|███▋      | 3568/9822 [1:35:49<2:31:49,  1.46s/it] 36%|███▋      | 3569/9822 [1:35:51<2:30:50,  1.45s/it] 36%|███▋      | 3570/9822 [1:35:52<2:30:17,  1.44s/it] 36%|███▋      | 3571/9822 [1:35:54<2:29:43,  1.44s/it] 36%|███▋      | 3572/9822 [1:35:55<2:29:26,  1.43s/it] 36%|███▋      | 3573/9822 [1:35:57<2:28:58,  1.43s/it] 36%|███▋      | 3574/9822 [1:35:58<2:28:57,  1.43s/it] 36%|███▋      | 3575/9822 [1:35:59<2:29:08,  1.43s/it] 36%|███▋      | 3576/9822 [1:36:01<2:29:21,  1.43s/it] 36%|███▋      | 3577/9822 [1:36:02<2:29:14,  1.43s/it] 36%|███▋      | 3578/9822 [1:36:04<2:29:03,  1.43s/it] 36%|███▋      | 3579/9822 [1:36:05<2:28:56,  1.43s/it] 36%|███▋      | 3580/9822 [1:36:07<2:29:18,  1.44s/it] 36%|███▋      | 3581/9822 [1:36:08<2:29:10,  1.43s/it] 36%|███▋      | 3582/9822 [1:36:09<2:29:03,  1.43s/it] 36%|███▋      | 3583/9822 [1:36:11<2:29:12,  1.43s/it] 36%|███▋      | 3584/9822 [1:36:12<2:29:19,  1.44s/it] 36%|███▋      | 3585/9822 [1:36:14<2:29:02,  1.43s/it] 37%|███▋      | 3586/9822 [1:36:15<2:28:59,  1.43s/it] 37%|███▋      | 3587/9822 [1:36:17<2:29:18,  1.44s/it] 37%|███▋      | 3588/9822 [1:36:18<2:29:12,  1.44s/it] 37%|███▋      | 3589/9822 [1:36:20<2:29:26,  1.44s/it] 37%|███▋      | 3590/9822 [1:36:21<2:29:44,  1.44s/it] 37%|███▋      | 3591/9822 [1:36:22<2:29:16,  1.44s/it] 37%|███▋      | 3592/9822 [1:36:24<2:29:07,  1.44s/it] 37%|███▋      | 3593/9822 [1:36:25<2:28:48,  1.43s/it] 37%|███▋      | 3594/9822 [1:36:27<2:28:51,  1.43s/it] 37%|███▋      | 3595/9822 [1:36:28<2:28:32,  1.43s/it] 37%|███▋      | 3596/9822 [1:36:30<2:28:53,  1.43s/it] 37%|███▋      | 3597/9822 [1:36:31<2:28:52,  1.43s/it] 37%|███▋      | 3598/9822 [1:36:32<2:29:01,  1.44s/it] 37%|███▋      | 3599/9822 [1:36:34<2:31:17,  1.46s/it] 37%|███▋      | 3600/9822 [1:36:35<2:30:25,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:24:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:24:22 - INFO - __main__ - ***** test Results*****
01/07/2024 23:24:22 - INFO - __main__ -   Training step = 3600
01/07/2024 23:24:22 - INFO - __main__ -  test_accuracy:0.8685944363103953 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:24:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:24:28 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:24:28 - INFO - __main__ -   Training step = 3600
01/07/2024 23:24:28 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:24:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:24:33 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:24:33 - INFO - __main__ -   Training step = 3600
01/07/2024 23:24:33 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 37%|███▋      | 3601/9822 [1:36:54<11:16:17,  6.52s/it] 37%|███▋      | 3602/9822 [1:36:55<8:37:52,  5.00s/it]  37%|███▋      | 3603/9822 [1:36:57<6:46:50,  3.93s/it] 37%|███▋      | 3604/9822 [1:36:58<5:29:19,  3.18s/it] 37%|███▋      | 3605/9822 [1:36:59<4:35:13,  2.66s/it] 37%|███▋      | 3606/9822 [1:37:01<3:57:10,  2.29s/it] 37%|███▋      | 3607/9822 [1:37:02<3:30:37,  2.03s/it] 37%|███▋      | 3608/9822 [1:37:04<3:11:55,  1.85s/it] 37%|███▋      | 3609/9822 [1:37:05<2:59:00,  1.73s/it] 37%|███▋      | 3610/9822 [1:37:07<2:49:53,  1.64s/it] 37%|███▋      | 3611/9822 [1:37:08<2:43:25,  1.58s/it] 37%|███▋      | 3612/9822 [1:37:09<2:37:07,  1.52s/it] 37%|███▋      | 3613/9822 [1:37:11<2:34:24,  1.49s/it] 37%|███▋      | 3614/9822 [1:37:12<2:32:27,  1.47s/it] 37%|███▋      | 3615/9822 [1:37:14<2:30:49,  1.46s/it] 37%|███▋      | 3616/9822 [1:37:15<2:30:27,  1.45s/it] 37%|███▋      | 3617/9822 [1:37:17<2:29:42,  1.45s/it] 37%|███▋      | 3618/9822 [1:37:18<2:29:16,  1.44s/it] 37%|███▋      | 3619/9822 [1:37:19<2:28:41,  1.44s/it] 37%|███▋      | 3620/9822 [1:37:21<2:29:30,  1.45s/it] 37%|███▋      | 3621/9822 [1:37:22<2:29:10,  1.44s/it] 37%|███▋      | 3622/9822 [1:37:24<2:29:17,  1.44s/it] 37%|███▋      | 3623/9822 [1:37:25<2:30:34,  1.46s/it] 37%|███▋      | 3624/9822 [1:37:27<2:29:52,  1.45s/it] 37%|███▋      | 3625/9822 [1:37:28<2:29:21,  1.45s/it] 37%|███▋      | 3626/9822 [1:37:30<2:28:56,  1.44s/it] 37%|███▋      | 3627/9822 [1:37:31<2:28:42,  1.44s/it] 37%|███▋      | 3628/9822 [1:37:32<2:28:14,  1.44s/it] 37%|███▋      | 3629/9822 [1:37:34<2:28:07,  1.44s/it] 37%|███▋      | 3630/9822 [1:37:35<2:28:06,  1.44s/it] 37%|███▋      | 3631/9822 [1:37:37<2:28:09,  1.44s/it] 37%|███▋      | 3632/9822 [1:37:38<2:28:05,  1.44s/it] 37%|███▋      | 3633/9822 [1:37:40<2:28:21,  1.44s/it] 37%|███▋      | 3634/9822 [1:37:41<2:28:07,  1.44s/it] 37%|███▋      | 3635/9822 [1:37:43<2:30:07,  1.46s/it] 37%|███▋      | 3636/9822 [1:37:44<2:29:30,  1.45s/it] 37%|███▋      | 3637/9822 [1:37:45<2:29:13,  1.45s/it] 37%|███▋      | 3638/9822 [1:37:47<2:28:37,  1.44s/it] 37%|███▋      | 3639/9822 [1:37:48<2:28:04,  1.44s/it] 37%|███▋      | 3640/9822 [1:37:50<2:28:19,  1.44s/it] 37%|███▋      | 3641/9822 [1:37:51<2:27:58,  1.44s/it] 37%|███▋      | 3642/9822 [1:37:53<2:27:42,  1.43s/it] 37%|███▋      | 3643/9822 [1:37:54<2:27:50,  1.44s/it] 37%|███▋      | 3644/9822 [1:37:56<2:27:44,  1.43s/it] 37%|███▋      | 3645/9822 [1:37:57<2:27:38,  1.43s/it] 37%|███▋      | 3646/9822 [1:37:58<2:27:38,  1.43s/it] 37%|███▋      | 3647/9822 [1:38:00<2:27:22,  1.43s/it] 37%|███▋      | 3648/9822 [1:38:01<2:27:19,  1.43s/it] 37%|███▋      | 3649/9822 [1:38:03<2:27:15,  1.43s/it] 37%|███▋      | 3650/9822 [1:38:04<2:27:16,  1.43s/it] 37%|███▋      | 3651/9822 [1:38:06<2:27:11,  1.43s/it] 37%|███▋      | 3652/9822 [1:38:07<2:26:53,  1.43s/it] 37%|███▋      | 3653/9822 [1:38:08<2:26:49,  1.43s/it] 37%|███▋      | 3654/9822 [1:38:10<2:26:31,  1.43s/it] 37%|███▋      | 3655/9822 [1:38:11<2:26:58,  1.43s/it] 37%|███▋      | 3656/9822 [1:38:13<2:26:59,  1.43s/it] 37%|███▋      | 3657/9822 [1:38:14<2:27:07,  1.43s/it] 37%|███▋      | 3658/9822 [1:38:16<2:27:16,  1.43s/it] 37%|███▋      | 3659/9822 [1:38:17<2:27:10,  1.43s/it] 37%|███▋      | 3660/9822 [1:38:18<2:26:49,  1.43s/it] 37%|███▋      | 3661/9822 [1:38:20<2:26:33,  1.43s/it] 37%|███▋      | 3662/9822 [1:38:21<2:26:51,  1.43s/it] 37%|███▋      | 3663/9822 [1:38:23<2:27:17,  1.43s/it] 37%|███▋      | 3664/9822 [1:38:24<2:27:15,  1.43s/it] 37%|███▋      | 3665/9822 [1:38:26<2:27:02,  1.43s/it] 37%|███▋      | 3666/9822 [1:38:27<2:29:09,  1.45s/it] 37%|███▋      | 3667/9822 [1:38:28<2:28:10,  1.44s/it] 37%|███▋      | 3668/9822 [1:38:30<2:27:27,  1.44s/it] 37%|███▋      | 3669/9822 [1:38:31<2:26:59,  1.43s/it] 37%|███▋      | 3670/9822 [1:38:33<2:26:33,  1.43s/it] 37%|███▋      | 3671/9822 [1:38:34<2:26:45,  1.43s/it] 37%|███▋      | 3672/9822 [1:38:36<2:26:45,  1.43s/it] 37%|███▋      | 3673/9822 [1:38:37<2:26:45,  1.43s/it] 37%|███▋      | 3674/9822 [1:38:39<2:27:17,  1.44s/it] 37%|███▋      | 3675/9822 [1:38:40<2:27:54,  1.44s/it] 37%|███▋      | 3676/9822 [1:38:41<2:29:02,  1.46s/it] 37%|███▋      | 3677/9822 [1:38:43<2:28:36,  1.45s/it] 37%|███▋      | 3678/9822 [1:38:44<2:28:11,  1.45s/it] 37%|███▋      | 3679/9822 [1:38:46<2:27:50,  1.44s/it] 37%|███▋      | 3680/9822 [1:38:47<2:27:36,  1.44s/it] 37%|███▋      | 3681/9822 [1:38:49<2:27:28,  1.44s/it] 37%|███▋      | 3682/9822 [1:38:50<2:27:56,  1.45s/it] 37%|███▋      | 3683/9822 [1:38:52<2:28:45,  1.45s/it] 38%|███▊      | 3684/9822 [1:38:53<2:28:31,  1.45s/it] 38%|███▊      | 3685/9822 [1:38:54<2:28:17,  1.45s/it] 38%|███▊      | 3686/9822 [1:38:56<2:27:38,  1.44s/it] 38%|███▊      | 3687/9822 [1:38:57<2:27:10,  1.44s/it] 38%|███▊      | 3688/9822 [1:38:59<2:27:19,  1.44s/it] 38%|███▊      | 3689/9822 [1:39:00<2:27:25,  1.44s/it] 38%|███▊      | 3690/9822 [1:39:02<2:27:16,  1.44s/it] 38%|███▊      | 3691/9822 [1:39:03<2:29:49,  1.47s/it] 38%|███▊      | 3692/9822 [1:39:05<2:28:27,  1.45s/it] 38%|███▊      | 3693/9822 [1:39:06<2:27:53,  1.45s/it] 38%|███▊      | 3694/9822 [1:39:07<2:27:13,  1.44s/it] 38%|███▊      | 3695/9822 [1:39:09<2:27:02,  1.44s/it] 38%|███▊      | 3696/9822 [1:39:10<2:27:34,  1.45s/it] 38%|███▊      | 3697/9822 [1:39:12<2:28:29,  1.45s/it] 38%|███▊      | 3698/9822 [1:39:13<2:26:04,  1.43s/it] 38%|███▊      | 3699/9822 [1:39:15<2:25:47,  1.43s/it] 38%|███▊      | 3700/9822 [1:39:16<2:25:35,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:27:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:27:03 - INFO - __main__ - ***** test Results*****
01/07/2024 23:27:03 - INFO - __main__ -   Training step = 3700
01/07/2024 23:27:03 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:27:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:27:09 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:27:09 - INFO - __main__ -   Training step = 3700
01/07/2024 23:27:09 - INFO - __main__ -  eval_accuracy:0.8528011717319663 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:27:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:27:14 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:27:14 - INFO - __main__ -   Training step = 3700
01/07/2024 23:27:14 - INFO - __main__ -  eval_accuracy:0.9044306114976199 
 38%|███▊      | 3701/9822 [1:39:34<11:02:53,  6.50s/it] 38%|███▊      | 3702/9822 [1:39:36<8:27:53,  4.98s/it]  38%|███▊      | 3703/9822 [1:39:37<6:39:47,  3.92s/it] 38%|███▊      | 3704/9822 [1:39:39<5:23:37,  3.17s/it] 38%|███▊      | 3705/9822 [1:39:40<4:30:10,  2.65s/it] 38%|███▊      | 3706/9822 [1:39:42<3:52:45,  2.28s/it] 38%|███▊      | 3707/9822 [1:39:43<3:26:27,  2.03s/it] 38%|███▊      | 3708/9822 [1:39:44<3:08:08,  1.85s/it] 38%|███▊      | 3709/9822 [1:39:46<2:55:59,  1.73s/it] 38%|███▊      | 3710/9822 [1:39:47<2:46:44,  1.64s/it] 38%|███▊      | 3711/9822 [1:39:49<2:40:21,  1.57s/it] 38%|███▊      | 3712/9822 [1:39:50<2:35:53,  1.53s/it] 38%|███▊      | 3713/9822 [1:39:52<2:32:34,  1.50s/it] 38%|███▊      | 3714/9822 [1:39:53<2:30:46,  1.48s/it] 38%|███▊      | 3715/9822 [1:39:54<2:29:09,  1.47s/it] 38%|███▊      | 3716/9822 [1:39:56<2:28:16,  1.46s/it] 38%|███▊      | 3717/9822 [1:39:57<2:29:52,  1.47s/it] 38%|███▊      | 3718/9822 [1:39:59<2:28:58,  1.46s/it] 38%|███▊      | 3719/9822 [1:40:00<2:27:48,  1.45s/it] 38%|███▊      | 3720/9822 [1:40:02<2:27:34,  1.45s/it] 38%|███▊      | 3721/9822 [1:40:03<2:27:18,  1.45s/it] 38%|███▊      | 3722/9822 [1:40:05<2:26:45,  1.44s/it] 38%|███▊      | 3723/9822 [1:40:06<2:26:40,  1.44s/it] 38%|███▊      | 3724/9822 [1:40:07<2:26:47,  1.44s/it] 38%|███▊      | 3725/9822 [1:40:09<2:26:25,  1.44s/it] 38%|███▊      | 3726/9822 [1:40:10<2:26:25,  1.44s/it] 38%|███▊      | 3727/9822 [1:40:12<2:26:04,  1.44s/it] 38%|███▊      | 3728/9822 [1:40:13<2:26:26,  1.44s/it] 38%|███▊      | 3729/9822 [1:40:15<2:26:34,  1.44s/it] 38%|███▊      | 3730/9822 [1:40:16<2:26:49,  1.45s/it] 38%|███▊      | 3731/9822 [1:40:18<2:26:53,  1.45s/it] 38%|███▊      | 3732/9822 [1:40:19<2:26:23,  1.44s/it] 38%|███▊      | 3733/9822 [1:40:20<2:25:59,  1.44s/it] 38%|███▊      | 3734/9822 [1:40:22<2:25:26,  1.43s/it] 38%|███▊      | 3735/9822 [1:40:23<2:26:28,  1.44s/it] 38%|███▊      | 3736/9822 [1:40:25<2:25:59,  1.44s/it] 38%|███▊      | 3737/9822 [1:40:26<2:26:13,  1.44s/it] 38%|███▊      | 3738/9822 [1:40:28<2:25:55,  1.44s/it] 38%|███▊      | 3739/9822 [1:40:29<2:25:43,  1.44s/it] 38%|███▊      | 3740/9822 [1:40:30<2:25:43,  1.44s/it] 38%|███▊      | 3741/9822 [1:40:32<2:25:36,  1.44s/it] 38%|███▊      | 3742/9822 [1:40:33<2:25:14,  1.43s/it] 38%|███▊      | 3743/9822 [1:40:35<2:25:15,  1.43s/it] 38%|███▊      | 3744/9822 [1:40:36<2:25:28,  1.44s/it] 38%|███▊      | 3745/9822 [1:40:38<2:25:40,  1.44s/it] 38%|███▊      | 3746/9822 [1:40:39<2:25:44,  1.44s/it] 38%|███▊      | 3747/9822 [1:40:41<2:25:47,  1.44s/it] 38%|███▊      | 3748/9822 [1:40:42<2:26:50,  1.45s/it] 38%|███▊      | 3749/9822 [1:40:44<2:28:36,  1.47s/it] 38%|███▊      | 3750/9822 [1:40:45<2:27:31,  1.46s/it] 38%|███▊      | 3751/9822 [1:40:46<2:28:07,  1.46s/it] 38%|███▊      | 3752/9822 [1:40:48<2:29:06,  1.47s/it] 38%|███▊      | 3753/9822 [1:40:49<2:28:39,  1.47s/it] 38%|███▊      | 3754/9822 [1:40:51<2:27:57,  1.46s/it] 38%|███▊      | 3755/9822 [1:40:52<2:27:26,  1.46s/it] 38%|███▊      | 3756/9822 [1:40:54<2:26:46,  1.45s/it] 38%|███▊      | 3757/9822 [1:40:55<2:26:11,  1.45s/it] 38%|███▊      | 3758/9822 [1:40:57<2:26:01,  1.44s/it] 38%|███▊      | 3759/9822 [1:40:58<2:25:45,  1.44s/it] 38%|███▊      | 3760/9822 [1:41:00<2:26:05,  1.45s/it] 38%|███▊      | 3761/9822 [1:41:01<2:25:16,  1.44s/it] 38%|███▊      | 3762/9822 [1:41:02<2:24:53,  1.43s/it] 38%|███▊      | 3763/9822 [1:41:04<2:25:03,  1.44s/it] 38%|███▊      | 3764/9822 [1:41:05<2:24:57,  1.44s/it] 38%|███▊      | 3765/9822 [1:41:07<2:24:34,  1.43s/it] 38%|███▊      | 3766/9822 [1:41:08<2:24:28,  1.43s/it] 38%|███▊      | 3767/9822 [1:41:10<2:24:13,  1.43s/it] 38%|███▊      | 3768/9822 [1:41:11<2:24:07,  1.43s/it] 38%|███▊      | 3769/9822 [1:41:12<2:24:12,  1.43s/it] 38%|███▊      | 3770/9822 [1:41:14<2:24:31,  1.43s/it] 38%|███▊      | 3771/9822 [1:41:15<2:24:51,  1.44s/it] 38%|███▊      | 3772/9822 [1:41:17<2:24:54,  1.44s/it] 38%|███▊      | 3773/9822 [1:41:18<2:24:44,  1.44s/it] 38%|███▊      | 3774/9822 [1:41:20<2:25:01,  1.44s/it] 38%|███▊      | 3775/9822 [1:41:21<2:24:52,  1.44s/it] 38%|███▊      | 3776/9822 [1:41:22<2:25:02,  1.44s/it] 38%|███▊      | 3777/9822 [1:41:24<2:24:39,  1.44s/it] 38%|███▊      | 3778/9822 [1:41:25<2:24:51,  1.44s/it] 38%|███▊      | 3779/9822 [1:41:27<2:24:25,  1.43s/it] 38%|███▊      | 3780/9822 [1:41:28<2:24:19,  1.43s/it] 38%|███▊      | 3781/9822 [1:41:30<2:26:19,  1.45s/it] 39%|███▊      | 3782/9822 [1:41:31<2:25:21,  1.44s/it] 39%|███▊      | 3783/9822 [1:41:33<2:24:34,  1.44s/it] 39%|███▊      | 3784/9822 [1:41:34<2:22:47,  1.42s/it] 39%|███▊      | 3785/9822 [1:41:35<2:22:50,  1.42s/it] 39%|███▊      | 3786/9822 [1:41:37<2:23:02,  1.42s/it] 39%|███▊      | 3787/9822 [1:41:38<2:23:48,  1.43s/it] 39%|███▊      | 3788/9822 [1:41:40<2:23:41,  1.43s/it] 39%|███▊      | 3789/9822 [1:41:41<2:23:54,  1.43s/it] 39%|███▊      | 3790/9822 [1:41:42<2:23:57,  1.43s/it] 39%|███▊      | 3791/9822 [1:41:44<2:24:03,  1.43s/it] 39%|███▊      | 3792/9822 [1:41:45<2:24:06,  1.43s/it] 39%|███▊      | 3793/9822 [1:41:47<2:24:02,  1.43s/it] 39%|███▊      | 3794/9822 [1:41:48<2:24:11,  1.44s/it] 39%|███▊      | 3795/9822 [1:41:50<2:24:12,  1.44s/it] 39%|███▊      | 3796/9822 [1:41:51<2:24:18,  1.44s/it] 39%|███▊      | 3797/9822 [1:41:53<2:25:51,  1.45s/it] 39%|███▊      | 3798/9822 [1:41:54<2:25:14,  1.45s/it] 39%|███▊      | 3799/9822 [1:41:55<2:24:51,  1.44s/it] 39%|███▊      | 3800/9822 [1:41:57<2:24:17,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:29:44 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:29:44 - INFO - __main__ - ***** test Results*****
01/07/2024 23:29:44 - INFO - __main__ -   Training step = 3800
01/07/2024 23:29:44 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:29:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:29:50 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:29:50 - INFO - __main__ -   Training step = 3800
01/07/2024 23:29:50 - INFO - __main__ -  eval_accuracy:0.85463200292933 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:29:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:29:55 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:29:55 - INFO - __main__ -   Training step = 3800
01/07/2024 23:29:55 - INFO - __main__ -  eval_accuracy:0.9055291102160381 
 39%|███▊      | 3801/9822 [1:42:15<10:52:48,  6.51s/it] 39%|███▊      | 3802/9822 [1:42:17<8:20:08,  4.98s/it]  39%|███▊      | 3803/9822 [1:42:18<6:32:56,  3.92s/it] 39%|███▊      | 3804/9822 [1:42:20<5:18:29,  3.18s/it] 39%|███▊      | 3805/9822 [1:42:21<4:26:01,  2.65s/it] 39%|███▊      | 3806/9822 [1:42:22<3:49:09,  2.29s/it] 39%|███▉      | 3807/9822 [1:42:24<3:23:36,  2.03s/it] 39%|███▉      | 3808/9822 [1:42:25<3:08:01,  1.88s/it] 39%|███▉      | 3809/9822 [1:42:27<2:54:34,  1.74s/it] 39%|███▉      | 3810/9822 [1:42:28<2:45:49,  1.65s/it] 39%|███▉      | 3811/9822 [1:42:30<2:40:24,  1.60s/it] 39%|███▉      | 3812/9822 [1:42:31<2:35:14,  1.55s/it] 39%|███▉      | 3813/9822 [1:42:33<2:31:27,  1.51s/it] 39%|███▉      | 3814/9822 [1:42:34<2:28:57,  1.49s/it] 39%|███▉      | 3815/9822 [1:42:35<2:27:01,  1.47s/it] 39%|███▉      | 3816/9822 [1:42:37<2:25:46,  1.46s/it] 39%|███▉      | 3817/9822 [1:42:38<2:25:12,  1.45s/it] 39%|███▉      | 3818/9822 [1:42:40<2:24:30,  1.44s/it] 39%|███▉      | 3819/9822 [1:42:41<2:24:33,  1.44s/it] 39%|███▉      | 3820/9822 [1:42:43<2:24:13,  1.44s/it] 39%|███▉      | 3821/9822 [1:42:44<2:24:02,  1.44s/it] 39%|███▉      | 3822/9822 [1:42:45<2:23:45,  1.44s/it] 39%|███▉      | 3823/9822 [1:42:47<2:23:51,  1.44s/it] 39%|███▉      | 3824/9822 [1:42:48<2:23:49,  1.44s/it] 39%|███▉      | 3825/9822 [1:42:50<2:23:37,  1.44s/it] 39%|███▉      | 3826/9822 [1:42:51<2:23:45,  1.44s/it] 39%|███▉      | 3827/9822 [1:42:53<2:23:48,  1.44s/it] 39%|███▉      | 3828/9822 [1:42:54<2:23:20,  1.43s/it] 39%|███▉      | 3829/9822 [1:42:55<2:23:09,  1.43s/it] 39%|███▉      | 3830/9822 [1:42:57<2:22:52,  1.43s/it] 39%|███▉      | 3831/9822 [1:42:58<2:22:52,  1.43s/it] 39%|███▉      | 3832/9822 [1:43:00<2:22:56,  1.43s/it] 39%|███▉      | 3833/9822 [1:43:01<2:23:01,  1.43s/it] 39%|███▉      | 3834/9822 [1:43:03<2:23:03,  1.43s/it] 39%|███▉      | 3835/9822 [1:43:04<2:23:23,  1.44s/it] 39%|███▉      | 3836/9822 [1:43:06<2:24:02,  1.44s/it] 39%|███▉      | 3837/9822 [1:43:07<2:23:28,  1.44s/it] 39%|███▉      | 3838/9822 [1:43:08<2:25:19,  1.46s/it] 39%|███▉      | 3839/9822 [1:43:10<2:24:51,  1.45s/it] 39%|███▉      | 3840/9822 [1:43:11<2:24:31,  1.45s/it] 39%|███▉      | 3841/9822 [1:43:13<2:24:19,  1.45s/it] 39%|███▉      | 3842/9822 [1:43:14<2:24:11,  1.45s/it] 39%|███▉      | 3843/9822 [1:43:16<2:23:45,  1.44s/it] 39%|███▉      | 3844/9822 [1:43:17<2:23:24,  1.44s/it] 39%|███▉      | 3845/9822 [1:43:19<2:23:30,  1.44s/it] 39%|███▉      | 3846/9822 [1:43:20<2:23:28,  1.44s/it] 39%|███▉      | 3847/9822 [1:43:21<2:23:12,  1.44s/it] 39%|███▉      | 3848/9822 [1:43:23<2:22:58,  1.44s/it] 39%|███▉      | 3849/9822 [1:43:24<2:23:15,  1.44s/it] 39%|███▉      | 3850/9822 [1:43:26<2:22:40,  1.43s/it] 39%|███▉      | 3851/9822 [1:43:27<2:22:36,  1.43s/it] 39%|███▉      | 3852/9822 [1:43:29<2:22:30,  1.43s/it] 39%|███▉      | 3853/9822 [1:43:30<2:22:41,  1.43s/it] 39%|███▉      | 3854/9822 [1:43:31<2:22:30,  1.43s/it] 39%|███▉      | 3855/9822 [1:43:33<2:22:32,  1.43s/it] 39%|███▉      | 3856/9822 [1:43:34<2:22:32,  1.43s/it] 39%|███▉      | 3857/9822 [1:43:36<2:24:15,  1.45s/it] 39%|███▉      | 3858/9822 [1:43:37<2:25:16,  1.46s/it] 39%|███▉      | 3859/9822 [1:43:39<2:25:01,  1.46s/it] 39%|███▉      | 3860/9822 [1:43:40<2:25:13,  1.46s/it] 39%|███▉      | 3861/9822 [1:43:42<2:24:25,  1.45s/it] 39%|███▉      | 3862/9822 [1:43:43<2:23:56,  1.45s/it] 39%|███▉      | 3863/9822 [1:43:45<2:23:42,  1.45s/it] 39%|███▉      | 3864/9822 [1:43:46<2:23:26,  1.44s/it] 39%|███▉      | 3865/9822 [1:43:47<2:23:26,  1.44s/it] 39%|███▉      | 3866/9822 [1:43:49<2:23:05,  1.44s/it] 39%|███▉      | 3867/9822 [1:43:50<2:22:30,  1.44s/it] 39%|███▉      | 3868/9822 [1:43:52<2:23:21,  1.44s/it] 39%|███▉      | 3869/9822 [1:43:53<2:23:05,  1.44s/it] 39%|███▉      | 3870/9822 [1:43:55<2:23:40,  1.45s/it] 39%|███▉      | 3871/9822 [1:43:56<2:23:21,  1.45s/it] 39%|███▉      | 3872/9822 [1:43:58<2:22:39,  1.44s/it] 39%|███▉      | 3873/9822 [1:43:59<2:22:43,  1.44s/it] 39%|███▉      | 3874/9822 [1:44:00<2:22:29,  1.44s/it] 39%|███▉      | 3875/9822 [1:44:02<2:22:22,  1.44s/it] 39%|███▉      | 3876/9822 [1:44:03<2:22:22,  1.44s/it] 39%|███▉      | 3877/9822 [1:44:05<2:22:02,  1.43s/it] 39%|███▉      | 3878/9822 [1:44:06<2:21:59,  1.43s/it] 39%|███▉      | 3879/9822 [1:44:08<2:21:52,  1.43s/it] 40%|███▉      | 3880/9822 [1:44:09<2:21:56,  1.43s/it] 40%|███▉      | 3881/9822 [1:44:10<2:21:38,  1.43s/it] 40%|███▉      | 3882/9822 [1:44:12<2:22:10,  1.44s/it] 40%|███▉      | 3883/9822 [1:44:13<2:21:54,  1.43s/it] 40%|███▉      | 3884/9822 [1:44:15<2:21:50,  1.43s/it] 40%|███▉      | 3885/9822 [1:44:16<2:21:48,  1.43s/it] 40%|███▉      | 3886/9822 [1:44:18<2:21:20,  1.43s/it] 40%|███▉      | 3887/9822 [1:44:19<2:21:31,  1.43s/it] 40%|███▉      | 3888/9822 [1:44:20<2:21:51,  1.43s/it] 40%|███▉      | 3889/9822 [1:44:22<2:22:15,  1.44s/it] 40%|███▉      | 3890/9822 [1:44:23<2:22:03,  1.44s/it] 40%|███▉      | 3891/9822 [1:44:25<2:21:39,  1.43s/it] 40%|███▉      | 3892/9822 [1:44:26<2:21:19,  1.43s/it] 40%|███▉      | 3893/9822 [1:44:28<2:21:09,  1.43s/it] 40%|███▉      | 3894/9822 [1:44:29<2:21:12,  1.43s/it] 40%|███▉      | 3895/9822 [1:44:30<2:21:28,  1.43s/it] 40%|███▉      | 3896/9822 [1:44:32<2:21:17,  1.43s/it] 40%|███▉      | 3897/9822 [1:44:33<2:21:18,  1.43s/it] 40%|███▉      | 3898/9822 [1:44:35<2:21:10,  1.43s/it] 40%|███▉      | 3899/9822 [1:44:36<2:21:21,  1.43s/it] 40%|███▉      | 3900/9822 [1:44:38<2:21:17,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1254, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0279, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:32:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:32:24 - INFO - __main__ - ***** test Results*****
01/07/2024 23:32:24 - INFO - __main__ -   Training step = 3900
01/07/2024 23:32:24 - INFO - __main__ -  test_accuracy:0.8535871156661786 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:32:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:32:31 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:32:31 - INFO - __main__ -   Training step = 3900
01/07/2024 23:32:31 - INFO - __main__ -  eval_accuracy:0.844013181984621 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:32:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:32:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:32:35 - INFO - __main__ -   Training step = 3900
01/07/2024 23:32:35 - INFO - __main__ -  eval_accuracy:0.9036982790186745 
 40%|███▉      | 3901/9822 [1:44:56<10:42:05,  6.51s/it] 40%|███▉      | 3902/9822 [1:44:57<8:13:54,  5.01s/it]  40%|███▉      | 3903/9822 [1:44:59<6:27:52,  3.93s/it] 40%|███▉      | 3904/9822 [1:45:00<5:14:00,  3.18s/it] 40%|███▉      | 3905/9822 [1:45:02<4:22:36,  2.66s/it] 40%|███▉      | 3906/9822 [1:45:03<3:46:29,  2.30s/it] 40%|███▉      | 3907/9822 [1:45:05<3:20:48,  2.04s/it] 40%|███▉      | 3908/9822 [1:45:06<3:03:04,  1.86s/it] 40%|███▉      | 3909/9822 [1:45:08<2:50:43,  1.73s/it] 40%|███▉      | 3910/9822 [1:45:09<2:41:47,  1.64s/it] 40%|███▉      | 3911/9822 [1:45:10<2:35:33,  1.58s/it] 40%|███▉      | 3912/9822 [1:45:12<2:31:34,  1.54s/it] 40%|███▉      | 3913/9822 [1:45:13<2:28:30,  1.51s/it] 40%|███▉      | 3914/9822 [1:45:15<2:26:11,  1.48s/it] 40%|███▉      | 3915/9822 [1:45:16<2:24:49,  1.47s/it] 40%|███▉      | 3916/9822 [1:45:18<2:23:40,  1.46s/it] 40%|███▉      | 3917/9822 [1:45:19<2:22:48,  1.45s/it] 40%|███▉      | 3918/9822 [1:45:20<2:22:16,  1.45s/it] 40%|███▉      | 3919/9822 [1:45:22<2:21:36,  1.44s/it] 40%|███▉      | 3920/9822 [1:45:23<2:21:18,  1.44s/it] 40%|███▉      | 3921/9822 [1:45:25<2:21:20,  1.44s/it] 40%|███▉      | 3922/9822 [1:45:26<2:21:06,  1.44s/it] 40%|███▉      | 3923/9822 [1:45:28<2:21:07,  1.44s/it] 40%|███▉      | 3924/9822 [1:45:29<2:20:52,  1.43s/it] 40%|███▉      | 3925/9822 [1:45:31<2:21:43,  1.44s/it] 40%|███▉      | 3926/9822 [1:45:32<2:21:24,  1.44s/it] 40%|███▉      | 3927/9822 [1:45:33<2:21:01,  1.44s/it] 40%|███▉      | 3928/9822 [1:45:35<2:21:05,  1.44s/it] 40%|████      | 3929/9822 [1:45:36<2:21:01,  1.44s/it] 40%|████      | 3930/9822 [1:45:38<2:20:52,  1.43s/it] 40%|████      | 3931/9822 [1:45:39<2:20:46,  1.43s/it] 40%|████      | 3932/9822 [1:45:41<2:20:42,  1.43s/it] 40%|████      | 3933/9822 [1:45:42<2:20:24,  1.43s/it] 40%|████      | 3934/9822 [1:45:43<2:22:53,  1.46s/it] 40%|████      | 3935/9822 [1:45:45<2:22:44,  1.45s/it] 40%|████      | 3936/9822 [1:45:46<2:22:08,  1.45s/it] 40%|████      | 3937/9822 [1:45:48<2:21:18,  1.44s/it] 40%|████      | 3938/9822 [1:45:49<2:20:57,  1.44s/it] 40%|████      | 3939/9822 [1:45:51<2:20:58,  1.44s/it] 40%|████      | 3940/9822 [1:45:52<2:20:48,  1.44s/it] 40%|████      | 3941/9822 [1:45:54<2:20:32,  1.43s/it] 40%|████      | 3942/9822 [1:45:55<2:20:50,  1.44s/it] 40%|████      | 3943/9822 [1:45:56<2:20:49,  1.44s/it] 40%|████      | 3944/9822 [1:45:58<2:20:48,  1.44s/it] 40%|████      | 3945/9822 [1:45:59<2:20:27,  1.43s/it] 40%|████      | 3946/9822 [1:46:01<2:20:08,  1.43s/it] 40%|████      | 3947/9822 [1:46:02<2:20:13,  1.43s/it] 40%|████      | 3948/9822 [1:46:04<2:20:17,  1.43s/it] 40%|████      | 3949/9822 [1:46:05<2:20:09,  1.43s/it] 40%|████      | 3950/9822 [1:46:06<2:20:13,  1.43s/it] 40%|████      | 3951/9822 [1:46:08<2:20:19,  1.43s/it] 40%|████      | 3952/9822 [1:46:09<2:20:09,  1.43s/it] 40%|████      | 3953/9822 [1:46:11<2:20:27,  1.44s/it] 40%|████      | 3954/9822 [1:46:12<2:20:45,  1.44s/it] 40%|████      | 3955/9822 [1:46:14<2:20:50,  1.44s/it] 40%|████      | 3956/9822 [1:46:15<2:19:03,  1.42s/it] 40%|████      | 3957/9822 [1:46:16<2:19:43,  1.43s/it] 40%|████      | 3958/9822 [1:46:18<2:19:35,  1.43s/it] 40%|████      | 3959/9822 [1:46:19<2:21:53,  1.45s/it] 40%|████      | 3960/9822 [1:46:21<2:21:38,  1.45s/it] 40%|████      | 3961/9822 [1:46:22<2:20:48,  1.44s/it] 40%|████      | 3962/9822 [1:46:24<2:20:23,  1.44s/it] 40%|████      | 3963/9822 [1:46:25<2:20:02,  1.43s/it] 40%|████      | 3964/9822 [1:46:27<2:20:03,  1.43s/it] 40%|████      | 3965/9822 [1:46:28<2:20:37,  1.44s/it] 40%|████      | 3966/9822 [1:46:29<2:20:19,  1.44s/it] 40%|████      | 3967/9822 [1:46:31<2:20:30,  1.44s/it] 40%|████      | 3968/9822 [1:46:32<2:20:34,  1.44s/it] 40%|████      | 3969/9822 [1:46:34<2:20:01,  1.44s/it] 40%|████      | 3970/9822 [1:46:35<2:19:48,  1.43s/it] 40%|████      | 3971/9822 [1:46:37<2:19:47,  1.43s/it] 40%|████      | 3972/9822 [1:46:38<2:20:01,  1.44s/it] 40%|████      | 3973/9822 [1:46:39<2:20:20,  1.44s/it] 40%|████      | 3974/9822 [1:46:41<2:20:08,  1.44s/it] 40%|████      | 3975/9822 [1:46:42<2:20:27,  1.44s/it] 40%|████      | 3976/9822 [1:46:44<2:20:08,  1.44s/it] 40%|████      | 3977/9822 [1:46:45<2:20:01,  1.44s/it] 41%|████      | 3978/9822 [1:46:47<2:19:44,  1.43s/it] 41%|████      | 3979/9822 [1:46:48<2:19:50,  1.44s/it] 41%|████      | 3980/9822 [1:46:50<2:19:32,  1.43s/it] 41%|████      | 3981/9822 [1:46:51<2:19:38,  1.43s/it] 41%|████      | 3982/9822 [1:46:52<2:19:44,  1.44s/it] 41%|████      | 3983/9822 [1:46:54<2:20:16,  1.44s/it] 41%|████      | 3984/9822 [1:46:55<2:20:07,  1.44s/it] 41%|████      | 3985/9822 [1:46:57<2:19:49,  1.44s/it] 41%|████      | 3986/9822 [1:46:58<2:19:31,  1.43s/it] 41%|████      | 3987/9822 [1:47:00<2:19:22,  1.43s/it] 41%|████      | 3988/9822 [1:47:01<2:19:25,  1.43s/it] 41%|████      | 3989/9822 [1:47:02<2:19:32,  1.44s/it] 41%|████      | 3990/9822 [1:47:04<2:19:23,  1.43s/it] 41%|████      | 3991/9822 [1:47:05<2:22:00,  1.46s/it] 41%|████      | 3992/9822 [1:47:07<2:21:08,  1.45s/it] 41%|████      | 3993/9822 [1:47:08<2:20:26,  1.45s/it] 41%|████      | 3994/9822 [1:47:10<2:20:07,  1.44s/it] 41%|████      | 3995/9822 [1:47:11<2:19:50,  1.44s/it] 41%|████      | 3996/9822 [1:47:13<2:19:58,  1.44s/it] 41%|████      | 3997/9822 [1:47:14<2:19:37,  1.44s/it] 41%|████      | 3998/9822 [1:47:15<2:19:25,  1.44s/it] 41%|████      | 3999/9822 [1:47:17<2:19:11,  1.43s/it] 41%|████      | 4000/9822 [1:47:18<2:19:08,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:35:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:35:05 - INFO - __main__ - ***** test Results*****
01/07/2024 23:35:05 - INFO - __main__ -   Training step = 4000
01/07/2024 23:35:05 - INFO - __main__ -  test_accuracy:0.8557833089311859 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:35:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:35:11 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:35:11 - INFO - __main__ -   Training step = 4000
01/07/2024 23:35:11 - INFO - __main__ -  eval_accuracy:0.8454778469425119 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:35:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:35:16 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:35:16 - INFO - __main__ -   Training step = 4000
01/07/2024 23:35:16 - INFO - __main__ -  eval_accuracy:0.9073599414134017 
 41%|████      | 4001/9822 [1:47:37<10:31:52,  6.51s/it] 41%|████      | 4002/9822 [1:47:38<8:04:04,  4.99s/it]  41%|████      | 4003/9822 [1:47:40<6:21:15,  3.93s/it] 41%|████      | 4004/9822 [1:47:41<5:08:46,  3.18s/it] 41%|████      | 4005/9822 [1:47:42<4:18:13,  2.66s/it] 41%|████      | 4006/9822 [1:47:44<3:42:55,  2.30s/it] 41%|████      | 4007/9822 [1:47:45<3:18:18,  2.05s/it] 41%|████      | 4008/9822 [1:47:47<3:02:19,  1.88s/it] 41%|████      | 4009/9822 [1:47:48<2:50:58,  1.76s/it] 41%|████      | 4010/9822 [1:47:50<2:42:54,  1.68s/it] 41%|████      | 4011/9822 [1:47:51<2:35:38,  1.61s/it] 41%|████      | 4012/9822 [1:47:53<2:30:45,  1.56s/it] 41%|████      | 4013/9822 [1:47:54<2:27:12,  1.52s/it] 41%|████      | 4014/9822 [1:47:56<2:24:58,  1.50s/it] 41%|████      | 4015/9822 [1:47:57<2:23:02,  1.48s/it] 41%|████      | 4016/9822 [1:47:58<2:21:57,  1.47s/it] 41%|████      | 4017/9822 [1:48:00<2:21:04,  1.46s/it] 41%|████      | 4018/9822 [1:48:01<2:20:42,  1.45s/it] 41%|████      | 4019/9822 [1:48:03<2:20:01,  1.45s/it] 41%|████      | 4020/9822 [1:48:04<2:19:39,  1.44s/it] 41%|████      | 4021/9822 [1:48:06<2:19:28,  1.44s/it] 41%|████      | 4022/9822 [1:48:07<2:19:15,  1.44s/it] 41%|████      | 4023/9822 [1:48:09<2:18:29,  1.43s/it] 41%|████      | 4024/9822 [1:48:10<2:18:44,  1.44s/it] 41%|████      | 4025/9822 [1:48:11<2:19:09,  1.44s/it] 41%|████      | 4026/9822 [1:48:13<2:18:37,  1.43s/it] 41%|████      | 4027/9822 [1:48:14<2:18:11,  1.43s/it] 41%|████      | 4028/9822 [1:48:16<2:18:01,  1.43s/it] 41%|████      | 4029/9822 [1:48:17<2:20:09,  1.45s/it] 41%|████      | 4030/9822 [1:48:19<2:19:34,  1.45s/it] 41%|████      | 4031/9822 [1:48:20<2:19:21,  1.44s/it] 41%|████      | 4032/9822 [1:48:21<2:18:55,  1.44s/it] 41%|████      | 4033/9822 [1:48:23<2:18:49,  1.44s/it] 41%|████      | 4034/9822 [1:48:24<2:18:54,  1.44s/it] 41%|████      | 4035/9822 [1:48:26<2:18:37,  1.44s/it] 41%|████      | 4036/9822 [1:48:27<2:19:16,  1.44s/it] 41%|████      | 4037/9822 [1:48:29<2:20:35,  1.46s/it] 41%|████      | 4038/9822 [1:48:30<2:20:08,  1.45s/it] 41%|████      | 4039/9822 [1:48:32<2:20:02,  1.45s/it] 41%|████      | 4040/9822 [1:48:33<2:19:52,  1.45s/it] 41%|████      | 4041/9822 [1:48:35<2:19:07,  1.44s/it] 41%|████      | 4042/9822 [1:48:36<2:17:05,  1.42s/it] 41%|████      | 4043/9822 [1:48:37<2:18:12,  1.43s/it] 41%|████      | 4044/9822 [1:48:39<2:19:48,  1.45s/it] 41%|████      | 4045/9822 [1:48:40<2:19:48,  1.45s/it] 41%|████      | 4046/9822 [1:48:42<2:19:26,  1.45s/it] 41%|████      | 4047/9822 [1:48:43<2:18:52,  1.44s/it] 41%|████      | 4048/9822 [1:48:45<2:18:30,  1.44s/it] 41%|████      | 4049/9822 [1:48:46<2:18:22,  1.44s/it] 41%|████      | 4050/9822 [1:48:47<2:18:39,  1.44s/it] 41%|████      | 4051/9822 [1:48:49<2:18:21,  1.44s/it] 41%|████▏     | 4052/9822 [1:48:50<2:18:05,  1.44s/it] 41%|████▏     | 4053/9822 [1:48:52<2:17:52,  1.43s/it] 41%|████▏     | 4054/9822 [1:48:53<2:18:00,  1.44s/it] 41%|████▏     | 4055/9822 [1:48:55<2:17:40,  1.43s/it] 41%|████▏     | 4056/9822 [1:48:56<2:17:57,  1.44s/it] 41%|████▏     | 4057/9822 [1:48:58<2:18:02,  1.44s/it] 41%|████▏     | 4058/9822 [1:48:59<2:17:51,  1.44s/it] 41%|████▏     | 4059/9822 [1:49:00<2:17:46,  1.43s/it] 41%|████▏     | 4060/9822 [1:49:02<2:17:42,  1.43s/it] 41%|████▏     | 4061/9822 [1:49:03<2:19:56,  1.46s/it] 41%|████▏     | 4062/9822 [1:49:05<2:19:35,  1.45s/it] 41%|████▏     | 4063/9822 [1:49:06<2:18:52,  1.45s/it] 41%|████▏     | 4064/9822 [1:49:08<2:18:42,  1.45s/it] 41%|████▏     | 4065/9822 [1:49:09<2:18:06,  1.44s/it] 41%|████▏     | 4066/9822 [1:49:10<2:17:36,  1.43s/it] 41%|████▏     | 4067/9822 [1:49:12<2:17:05,  1.43s/it] 41%|████▏     | 4068/9822 [1:49:13<2:16:39,  1.43s/it] 41%|████▏     | 4069/9822 [1:49:15<2:16:28,  1.42s/it] 41%|████▏     | 4070/9822 [1:49:16<2:16:19,  1.42s/it] 41%|████▏     | 4071/9822 [1:49:18<2:16:47,  1.43s/it] 41%|████▏     | 4072/9822 [1:49:19<2:16:39,  1.43s/it] 41%|████▏     | 4073/9822 [1:49:20<2:17:07,  1.43s/it] 41%|████▏     | 4074/9822 [1:49:22<2:17:02,  1.43s/it] 41%|████▏     | 4075/9822 [1:49:23<2:17:12,  1.43s/it] 41%|████▏     | 4076/9822 [1:49:25<2:17:57,  1.44s/it] 42%|████▏     | 4077/9822 [1:49:26<2:17:43,  1.44s/it] 42%|████▏     | 4078/9822 [1:49:28<2:17:29,  1.44s/it] 42%|████▏     | 4079/9822 [1:49:29<2:17:22,  1.44s/it] 42%|████▏     | 4080/9822 [1:49:31<2:17:18,  1.43s/it] 42%|████▏     | 4081/9822 [1:49:32<2:16:50,  1.43s/it] 42%|████▏     | 4082/9822 [1:49:33<2:16:50,  1.43s/it] 42%|████▏     | 4083/9822 [1:49:35<2:16:49,  1.43s/it] 42%|████▏     | 4084/9822 [1:49:36<2:17:19,  1.44s/it] 42%|████▏     | 4085/9822 [1:49:38<2:16:59,  1.43s/it] 42%|████▏     | 4086/9822 [1:49:39<2:19:13,  1.46s/it] 42%|████▏     | 4087/9822 [1:49:41<2:18:24,  1.45s/it] 42%|████▏     | 4088/9822 [1:49:42<2:17:59,  1.44s/it] 42%|████▏     | 4089/9822 [1:49:43<2:17:57,  1.44s/it] 42%|████▏     | 4090/9822 [1:49:45<2:17:49,  1.44s/it] 42%|████▏     | 4091/9822 [1:49:46<2:17:15,  1.44s/it] 42%|████▏     | 4092/9822 [1:49:48<2:16:59,  1.43s/it] 42%|████▏     | 4093/9822 [1:49:49<2:17:11,  1.44s/it] 42%|████▏     | 4094/9822 [1:49:51<2:17:43,  1.44s/it] 42%|████▏     | 4095/9822 [1:49:52<2:18:09,  1.45s/it] 42%|████▏     | 4096/9822 [1:49:54<2:17:42,  1.44s/it] 42%|████▏     | 4097/9822 [1:49:55<2:17:16,  1.44s/it] 42%|████▏     | 4098/9822 [1:49:56<2:17:04,  1.44s/it] 42%|████▏     | 4099/9822 [1:49:58<2:16:49,  1.43s/it] 42%|████▏     | 4100/9822 [1:49:59<2:16:38,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:37:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:37:46 - INFO - __main__ - ***** test Results*****
01/07/2024 23:37:46 - INFO - __main__ -   Training step = 4100
01/07/2024 23:37:46 - INFO - __main__ -  test_accuracy:0.8744509516837482 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:37:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:37:52 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:37:52 - INFO - __main__ -   Training step = 4100
01/07/2024 23:37:52 - INFO - __main__ -  eval_accuracy:0.8528011717319663 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:37:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:37:57 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:37:57 - INFO - __main__ -   Training step = 4100
01/07/2024 23:37:57 - INFO - __main__ -  eval_accuracy:0.9077261076528744 
 42%|████▏     | 4101/9822 [1:50:18<10:20:53,  6.51s/it] 42%|████▏     | 4102/9822 [1:50:19<7:55:42,  4.99s/it]  42%|████▏     | 4103/9822 [1:50:21<6:13:57,  3.92s/it] 42%|████▏     | 4104/9822 [1:50:22<5:02:37,  3.18s/it] 42%|████▏     | 4105/9822 [1:50:23<4:12:59,  2.66s/it] 42%|████▏     | 4106/9822 [1:50:25<3:37:50,  2.29s/it] 42%|████▏     | 4107/9822 [1:50:26<3:13:30,  2.03s/it] 42%|████▏     | 4108/9822 [1:50:28<2:56:51,  1.86s/it] 42%|████▏     | 4109/9822 [1:50:29<2:44:39,  1.73s/it] 42%|████▏     | 4110/9822 [1:50:31<2:36:15,  1.64s/it] 42%|████▏     | 4111/9822 [1:50:32<2:30:22,  1.58s/it] 42%|████▏     | 4112/9822 [1:50:33<2:25:44,  1.53s/it] 42%|████▏     | 4113/9822 [1:50:35<2:22:54,  1.50s/it] 42%|████▏     | 4114/9822 [1:50:36<2:21:27,  1.49s/it] 42%|████▏     | 4115/9822 [1:50:38<2:22:30,  1.50s/it] 42%|████▏     | 4116/9822 [1:50:39<2:20:22,  1.48s/it] 42%|████▏     | 4117/9822 [1:50:41<2:18:55,  1.46s/it] 42%|████▏     | 4118/9822 [1:50:42<2:18:07,  1.45s/it] 42%|████▏     | 4119/9822 [1:50:44<2:17:07,  1.44s/it] 42%|████▏     | 4120/9822 [1:50:45<2:16:49,  1.44s/it] 42%|████▏     | 4121/9822 [1:50:46<2:16:32,  1.44s/it] 42%|████▏     | 4122/9822 [1:50:48<2:16:45,  1.44s/it] 42%|████▏     | 4123/9822 [1:50:49<2:17:33,  1.45s/it] 42%|████▏     | 4124/9822 [1:50:51<2:17:12,  1.44s/it] 42%|████▏     | 4125/9822 [1:50:52<2:16:49,  1.44s/it] 42%|████▏     | 4126/9822 [1:50:54<2:16:29,  1.44s/it] 42%|████▏     | 4127/9822 [1:50:55<2:16:52,  1.44s/it] 42%|████▏     | 4128/9822 [1:50:56<2:15:16,  1.43s/it] 42%|████▏     | 4129/9822 [1:50:58<2:15:27,  1.43s/it] 42%|████▏     | 4130/9822 [1:50:59<2:15:40,  1.43s/it] 42%|████▏     | 4131/9822 [1:51:01<2:15:28,  1.43s/it] 42%|████▏     | 4132/9822 [1:51:02<2:15:42,  1.43s/it] 42%|████▏     | 4133/9822 [1:51:04<2:15:36,  1.43s/it] 42%|████▏     | 4134/9822 [1:51:05<2:15:46,  1.43s/it] 42%|████▏     | 4135/9822 [1:51:06<2:15:38,  1.43s/it] 42%|████▏     | 4136/9822 [1:51:08<2:15:47,  1.43s/it] 42%|████▏     | 4137/9822 [1:51:09<2:15:44,  1.43s/it] 42%|████▏     | 4138/9822 [1:51:11<2:15:53,  1.43s/it] 42%|████▏     | 4139/9822 [1:51:12<2:15:53,  1.43s/it] 42%|████▏     | 4140/9822 [1:51:14<2:15:42,  1.43s/it] 42%|████▏     | 4141/9822 [1:51:15<2:15:24,  1.43s/it] 42%|████▏     | 4142/9822 [1:51:17<2:15:27,  1.43s/it] 42%|████▏     | 4143/9822 [1:51:18<2:15:36,  1.43s/it] 42%|████▏     | 4144/9822 [1:51:19<2:15:54,  1.44s/it] 42%|████▏     | 4145/9822 [1:51:21<2:15:31,  1.43s/it] 42%|████▏     | 4146/9822 [1:51:22<2:15:33,  1.43s/it] 42%|████▏     | 4147/9822 [1:51:24<2:17:36,  1.45s/it] 42%|████▏     | 4148/9822 [1:51:25<2:16:58,  1.45s/it] 42%|████▏     | 4149/9822 [1:51:27<2:16:31,  1.44s/it] 42%|████▏     | 4150/9822 [1:51:28<2:15:56,  1.44s/it] 42%|████▏     | 4151/9822 [1:51:29<2:15:46,  1.44s/it] 42%|████▏     | 4152/9822 [1:51:31<2:15:53,  1.44s/it] 42%|████▏     | 4153/9822 [1:51:32<2:16:01,  1.44s/it] 42%|████▏     | 4154/9822 [1:51:34<2:16:37,  1.45s/it] 42%|████▏     | 4155/9822 [1:51:35<2:16:31,  1.45s/it] 42%|████▏     | 4156/9822 [1:51:37<2:16:14,  1.44s/it] 42%|████▏     | 4157/9822 [1:51:38<2:16:02,  1.44s/it] 42%|████▏     | 4158/9822 [1:51:40<2:16:30,  1.45s/it] 42%|████▏     | 4159/9822 [1:51:41<2:16:29,  1.45s/it] 42%|████▏     | 4160/9822 [1:51:42<2:16:03,  1.44s/it] 42%|████▏     | 4161/9822 [1:51:44<2:15:30,  1.44s/it] 42%|████▏     | 4162/9822 [1:51:45<2:15:34,  1.44s/it] 42%|████▏     | 4163/9822 [1:51:47<2:15:48,  1.44s/it] 42%|████▏     | 4164/9822 [1:51:48<2:15:38,  1.44s/it] 42%|████▏     | 4165/9822 [1:51:50<2:17:13,  1.46s/it] 42%|████▏     | 4166/9822 [1:51:51<2:18:11,  1.47s/it] 42%|████▏     | 4167/9822 [1:51:53<2:17:44,  1.46s/it] 42%|████▏     | 4168/9822 [1:51:54<2:17:17,  1.46s/it] 42%|████▏     | 4169/9822 [1:51:56<2:17:32,  1.46s/it] 42%|████▏     | 4170/9822 [1:51:57<2:18:27,  1.47s/it] 42%|████▏     | 4171/9822 [1:51:59<2:19:04,  1.48s/it] 42%|████▏     | 4172/9822 [1:52:00<2:17:49,  1.46s/it] 42%|████▏     | 4173/9822 [1:52:01<2:17:16,  1.46s/it] 42%|████▏     | 4174/9822 [1:52:03<2:16:39,  1.45s/it] 43%|████▎     | 4175/9822 [1:52:04<2:16:29,  1.45s/it] 43%|████▎     | 4176/9822 [1:52:06<2:16:12,  1.45s/it] 43%|████▎     | 4177/9822 [1:52:07<2:16:08,  1.45s/it] 43%|████▎     | 4178/9822 [1:52:09<2:16:29,  1.45s/it] 43%|████▎     | 4179/9822 [1:52:10<2:18:40,  1.47s/it] 43%|████▎     | 4180/9822 [1:52:12<2:17:50,  1.47s/it] 43%|████▎     | 4181/9822 [1:52:13<2:16:44,  1.45s/it] 43%|████▎     | 4182/9822 [1:52:15<2:15:44,  1.44s/it] 43%|████▎     | 4183/9822 [1:52:16<2:15:15,  1.44s/it] 43%|████▎     | 4184/9822 [1:52:17<2:15:00,  1.44s/it] 43%|████▎     | 4185/9822 [1:52:19<2:14:39,  1.43s/it] 43%|████▎     | 4186/9822 [1:52:20<2:15:06,  1.44s/it] 43%|████▎     | 4187/9822 [1:52:22<2:16:32,  1.45s/it] 43%|████▎     | 4188/9822 [1:52:23<2:16:30,  1.45s/it] 43%|████▎     | 4189/9822 [1:52:25<2:16:46,  1.46s/it] 43%|████▎     | 4190/9822 [1:52:26<2:16:06,  1.45s/it] 43%|████▎     | 4191/9822 [1:52:28<2:15:21,  1.44s/it] 43%|████▎     | 4192/9822 [1:52:29<2:15:15,  1.44s/it] 43%|████▎     | 4193/9822 [1:52:30<2:15:18,  1.44s/it] 43%|████▎     | 4194/9822 [1:52:32<2:14:50,  1.44s/it] 43%|████▎     | 4195/9822 [1:52:33<2:14:23,  1.43s/it] 43%|████▎     | 4196/9822 [1:52:35<2:14:48,  1.44s/it] 43%|████▎     | 4197/9822 [1:52:36<2:16:13,  1.45s/it] 43%|████▎     | 4198/9822 [1:52:38<2:17:08,  1.46s/it] 43%|████▎     | 4199/9822 [1:52:39<2:16:34,  1.46s/it] 43%|████▎     | 4200/9822 [1:52:41<2:16:27,  1.46s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:40:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:40:27 - INFO - __main__ - ***** test Results*****
01/07/2024 23:40:27 - INFO - __main__ -   Training step = 4200
01/07/2024 23:40:27 - INFO - __main__ -  test_accuracy:0.8674963396778916 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:40:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:40:33 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:40:33 - INFO - __main__ -   Training step = 4200
01/07/2024 23:40:33 - INFO - __main__ -  eval_accuracy:0.8557305016477481 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:40:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:40:38 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:40:38 - INFO - __main__ -   Training step = 4200
01/07/2024 23:40:38 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 43%|████▎     | 4201/9822 [1:52:59<10:10:53,  6.52s/it] 43%|████▎     | 4202/9822 [1:53:00<7:48:07,  5.00s/it]  43%|████▎     | 4203/9822 [1:53:02<6:08:03,  3.93s/it] 43%|████▎     | 4204/9822 [1:53:03<4:57:59,  3.18s/it] 43%|████▎     | 4205/9822 [1:53:05<4:11:04,  2.68s/it] 43%|████▎     | 4206/9822 [1:53:06<3:37:19,  2.32s/it] 43%|████▎     | 4207/9822 [1:53:08<3:13:07,  2.06s/it] 43%|████▎     | 4208/9822 [1:53:09<2:55:33,  1.88s/it] 43%|████▎     | 4209/9822 [1:53:11<2:43:12,  1.74s/it] 43%|████▎     | 4210/9822 [1:53:12<2:34:34,  1.65s/it] 43%|████▎     | 4211/9822 [1:53:13<2:28:18,  1.59s/it] 43%|████▎     | 4212/9822 [1:53:15<2:24:17,  1.54s/it] 43%|████▎     | 4213/9822 [1:53:16<2:21:36,  1.51s/it] 43%|████▎     | 4214/9822 [1:53:18<2:18:50,  1.49s/it] 43%|████▎     | 4215/9822 [1:53:19<2:17:24,  1.47s/it] 43%|████▎     | 4216/9822 [1:53:21<2:17:17,  1.47s/it] 43%|████▎     | 4217/9822 [1:53:22<2:16:22,  1.46s/it] 43%|████▎     | 4218/9822 [1:53:24<2:15:53,  1.45s/it] 43%|████▎     | 4219/9822 [1:53:25<2:15:03,  1.45s/it] 43%|████▎     | 4220/9822 [1:53:26<2:14:56,  1.45s/it] 43%|████▎     | 4221/9822 [1:53:28<2:14:34,  1.44s/it] 43%|████▎     | 4222/9822 [1:53:29<2:14:51,  1.44s/it] 43%|████▎     | 4223/9822 [1:53:31<2:14:53,  1.45s/it] 43%|████▎     | 4224/9822 [1:53:32<2:14:28,  1.44s/it] 43%|████▎     | 4225/9822 [1:53:34<2:14:12,  1.44s/it] 43%|████▎     | 4226/9822 [1:53:35<2:14:09,  1.44s/it] 43%|████▎     | 4227/9822 [1:53:36<2:13:54,  1.44s/it] 43%|████▎     | 4228/9822 [1:53:38<2:13:37,  1.43s/it] 43%|████▎     | 4229/9822 [1:53:39<2:13:58,  1.44s/it] 43%|████▎     | 4230/9822 [1:53:41<2:13:45,  1.44s/it] 43%|████▎     | 4231/9822 [1:53:42<2:13:45,  1.44s/it] 43%|████▎     | 4232/9822 [1:53:44<2:13:45,  1.44s/it] 43%|████▎     | 4233/9822 [1:53:45<2:13:35,  1.43s/it] 43%|████▎     | 4234/9822 [1:53:46<2:13:43,  1.44s/it] 43%|████▎     | 4235/9822 [1:53:48<2:13:24,  1.43s/it] 43%|████▎     | 4236/9822 [1:53:49<2:13:18,  1.43s/it] 43%|████▎     | 4237/9822 [1:53:51<2:15:50,  1.46s/it] 43%|████▎     | 4238/9822 [1:53:52<2:15:20,  1.45s/it] 43%|████▎     | 4239/9822 [1:53:54<2:14:33,  1.45s/it] 43%|████▎     | 4240/9822 [1:53:55<2:14:05,  1.44s/it] 43%|████▎     | 4241/9822 [1:53:57<2:13:51,  1.44s/it] 43%|████▎     | 4242/9822 [1:53:58<2:13:39,  1.44s/it] 43%|████▎     | 4243/9822 [1:53:59<2:13:29,  1.44s/it] 43%|████▎     | 4244/9822 [1:54:01<2:13:42,  1.44s/it] 43%|████▎     | 4245/9822 [1:54:02<2:13:29,  1.44s/it] 43%|████▎     | 4246/9822 [1:54:04<2:13:41,  1.44s/it] 43%|████▎     | 4247/9822 [1:54:05<2:15:22,  1.46s/it] 43%|████▎     | 4248/9822 [1:54:07<2:14:38,  1.45s/it] 43%|████▎     | 4249/9822 [1:54:08<2:13:52,  1.44s/it] 43%|████▎     | 4250/9822 [1:54:10<2:13:30,  1.44s/it] 43%|████▎     | 4251/9822 [1:54:11<2:13:29,  1.44s/it] 43%|████▎     | 4252/9822 [1:54:12<2:13:27,  1.44s/it] 43%|████▎     | 4253/9822 [1:54:14<2:13:24,  1.44s/it] 43%|████▎     | 4254/9822 [1:54:15<2:13:17,  1.44s/it] 43%|████▎     | 4255/9822 [1:54:17<2:13:10,  1.44s/it] 43%|████▎     | 4256/9822 [1:54:18<2:14:02,  1.44s/it] 43%|████▎     | 4257/9822 [1:54:20<2:13:45,  1.44s/it] 43%|████▎     | 4258/9822 [1:54:21<2:13:17,  1.44s/it] 43%|████▎     | 4259/9822 [1:54:23<2:12:53,  1.43s/it] 43%|████▎     | 4260/9822 [1:54:24<2:12:57,  1.43s/it] 43%|████▎     | 4261/9822 [1:54:25<2:12:37,  1.43s/it] 43%|████▎     | 4262/9822 [1:54:27<2:14:52,  1.46s/it] 43%|████▎     | 4263/9822 [1:54:28<2:14:04,  1.45s/it] 43%|████▎     | 4264/9822 [1:54:30<2:13:33,  1.44s/it] 43%|████▎     | 4265/9822 [1:54:31<2:13:21,  1.44s/it] 43%|████▎     | 4266/9822 [1:54:33<2:13:06,  1.44s/it] 43%|████▎     | 4267/9822 [1:54:34<2:12:58,  1.44s/it] 43%|████▎     | 4268/9822 [1:54:35<2:12:51,  1.44s/it] 43%|████▎     | 4269/9822 [1:54:37<2:12:24,  1.43s/it] 43%|████▎     | 4270/9822 [1:54:38<2:12:33,  1.43s/it] 43%|████▎     | 4271/9822 [1:54:40<2:12:44,  1.43s/it] 43%|████▎     | 4272/9822 [1:54:41<2:12:42,  1.43s/it] 44%|████▎     | 4273/9822 [1:54:43<2:13:05,  1.44s/it] 44%|████▎     | 4274/9822 [1:54:44<2:12:58,  1.44s/it] 44%|████▎     | 4275/9822 [1:54:46<2:13:09,  1.44s/it] 44%|████▎     | 4276/9822 [1:54:47<2:13:07,  1.44s/it] 44%|████▎     | 4277/9822 [1:54:48<2:12:51,  1.44s/it] 44%|████▎     | 4278/9822 [1:54:50<2:12:33,  1.43s/it] 44%|████▎     | 4279/9822 [1:54:51<2:13:15,  1.44s/it] 44%|████▎     | 4280/9822 [1:54:53<2:12:54,  1.44s/it] 44%|████▎     | 4281/9822 [1:54:54<2:12:33,  1.44s/it] 44%|████▎     | 4282/9822 [1:54:56<2:12:30,  1.44s/it] 44%|████▎     | 4283/9822 [1:54:57<2:12:30,  1.44s/it] 44%|████▎     | 4284/9822 [1:54:58<2:12:31,  1.44s/it] 44%|████▎     | 4285/9822 [1:55:00<2:12:15,  1.43s/it] 44%|████▎     | 4286/9822 [1:55:01<2:12:04,  1.43s/it] 44%|████▎     | 4287/9822 [1:55:03<2:12:10,  1.43s/it] 44%|████▎     | 4288/9822 [1:55:04<2:11:55,  1.43s/it] 44%|████▎     | 4289/9822 [1:55:06<2:12:15,  1.43s/it] 44%|████▎     | 4290/9822 [1:55:07<2:12:16,  1.43s/it] 44%|████▎     | 4291/9822 [1:55:08<2:12:03,  1.43s/it] 44%|████▎     | 4292/9822 [1:55:10<2:11:56,  1.43s/it] 44%|████▎     | 4293/9822 [1:55:11<2:12:06,  1.43s/it] 44%|████▎     | 4294/9822 [1:55:13<2:14:18,  1.46s/it] 44%|████▎     | 4295/9822 [1:55:14<2:13:15,  1.45s/it] 44%|████▎     | 4296/9822 [1:55:16<2:12:53,  1.44s/it] 44%|████▎     | 4297/9822 [1:55:17<2:12:36,  1.44s/it] 44%|████▍     | 4298/9822 [1:55:19<2:12:26,  1.44s/it] 44%|████▍     | 4299/9822 [1:55:20<2:12:08,  1.44s/it] 44%|████▍     | 4300/9822 [1:55:21<2:10:58,  1.42s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1226, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:43:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:43:08 - INFO - __main__ - ***** test Results*****
01/07/2024 23:43:08 - INFO - __main__ -   Training step = 4300
01/07/2024 23:43:08 - INFO - __main__ -  test_accuracy:0.8671303074670571 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:43:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:43:14 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:43:14 - INFO - __main__ -   Training step = 4300
01/07/2024 23:43:14 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:43:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:43:19 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:43:19 - INFO - __main__ -   Training step = 4300
01/07/2024 23:43:19 - INFO - __main__ -  eval_accuracy:0.9084584401318199 
 44%|████▍     | 4301/9822 [1:55:40<9:57:23,  6.49s/it] 44%|████▍     | 4302/9822 [1:55:41<7:37:14,  4.97s/it] 44%|████▍     | 4303/9822 [1:55:43<5:59:25,  3.91s/it] 44%|████▍     | 4304/9822 [1:55:44<4:51:48,  3.17s/it] 44%|████▍     | 4305/9822 [1:55:45<4:04:01,  2.65s/it] 44%|████▍     | 4306/9822 [1:55:47<3:30:30,  2.29s/it] 44%|████▍     | 4307/9822 [1:55:48<3:07:04,  2.04s/it] 44%|████▍     | 4308/9822 [1:55:50<2:50:28,  1.85s/it] 44%|████▍     | 4309/9822 [1:55:51<2:38:59,  1.73s/it] 44%|████▍     | 4310/9822 [1:55:53<2:30:41,  1.64s/it] 44%|████▍     | 4311/9822 [1:55:54<2:25:01,  1.58s/it] 44%|████▍     | 4312/9822 [1:55:56<2:21:03,  1.54s/it] 44%|████▍     | 4313/9822 [1:55:57<2:18:06,  1.50s/it] 44%|████▍     | 4314/9822 [1:55:58<2:15:55,  1.48s/it] 44%|████▍     | 4315/9822 [1:56:00<2:14:46,  1.47s/it] 44%|████▍     | 4316/9822 [1:56:01<2:13:58,  1.46s/it] 44%|████▍     | 4317/9822 [1:56:03<2:13:21,  1.45s/it] 44%|████▍     | 4318/9822 [1:56:04<2:12:53,  1.45s/it] 44%|████▍     | 4319/9822 [1:56:06<2:12:30,  1.44s/it] 44%|████▍     | 4320/9822 [1:56:07<2:12:45,  1.45s/it] 44%|████▍     | 4321/9822 [1:56:08<2:12:18,  1.44s/it] 44%|████▍     | 4322/9822 [1:56:10<2:12:05,  1.44s/it] 44%|████▍     | 4323/9822 [1:56:11<2:12:20,  1.44s/it] 44%|████▍     | 4324/9822 [1:56:13<2:12:34,  1.45s/it] 44%|████▍     | 4325/9822 [1:56:14<2:12:22,  1.44s/it] 44%|████▍     | 4326/9822 [1:56:16<2:11:58,  1.44s/it] 44%|████▍     | 4327/9822 [1:56:17<2:11:56,  1.44s/it] 44%|████▍     | 4328/9822 [1:56:19<2:13:01,  1.45s/it] 44%|████▍     | 4329/9822 [1:56:20<2:12:20,  1.45s/it] 44%|████▍     | 4330/9822 [1:56:21<2:12:22,  1.45s/it] 44%|████▍     | 4331/9822 [1:56:23<2:12:07,  1.44s/it] 44%|████▍     | 4332/9822 [1:56:24<2:12:45,  1.45s/it] 44%|████▍     | 4333/9822 [1:56:26<2:14:26,  1.47s/it] 44%|████▍     | 4334/9822 [1:56:27<2:13:20,  1.46s/it] 44%|████▍     | 4335/9822 [1:56:29<2:13:21,  1.46s/it] 44%|████▍     | 4336/9822 [1:56:30<2:13:06,  1.46s/it] 44%|████▍     | 4337/9822 [1:56:32<2:12:34,  1.45s/it] 44%|████▍     | 4338/9822 [1:56:33<2:11:57,  1.44s/it] 44%|████▍     | 4339/9822 [1:56:35<2:12:14,  1.45s/it] 44%|████▍     | 4340/9822 [1:56:36<2:13:05,  1.46s/it] 44%|████▍     | 4341/9822 [1:56:37<2:12:15,  1.45s/it] 44%|████▍     | 4342/9822 [1:56:39<2:11:38,  1.44s/it] 44%|████▍     | 4343/9822 [1:56:40<2:11:40,  1.44s/it] 44%|████▍     | 4344/9822 [1:56:42<2:12:10,  1.45s/it] 44%|████▍     | 4345/9822 [1:56:43<2:12:21,  1.45s/it] 44%|████▍     | 4346/9822 [1:56:45<2:12:08,  1.45s/it] 44%|████▍     | 4347/9822 [1:56:46<2:11:59,  1.45s/it] 44%|████▍     | 4348/9822 [1:56:48<2:11:48,  1.44s/it] 44%|████▍     | 4349/9822 [1:56:49<2:11:24,  1.44s/it] 44%|████▍     | 4350/9822 [1:56:50<2:11:04,  1.44s/it] 44%|████▍     | 4351/9822 [1:56:52<2:11:04,  1.44s/it] 44%|████▍     | 4352/9822 [1:56:53<2:11:17,  1.44s/it] 44%|████▍     | 4353/9822 [1:56:55<2:10:48,  1.44s/it] 44%|████▍     | 4354/9822 [1:56:56<2:10:37,  1.43s/it] 44%|████▍     | 4355/9822 [1:56:58<2:10:18,  1.43s/it] 44%|████▍     | 4356/9822 [1:56:59<2:10:11,  1.43s/it] 44%|████▍     | 4357/9822 [1:57:00<2:09:59,  1.43s/it] 44%|████▍     | 4358/9822 [1:57:02<2:09:54,  1.43s/it] 44%|████▍     | 4359/9822 [1:57:03<2:09:50,  1.43s/it] 44%|████▍     | 4360/9822 [1:57:05<2:09:48,  1.43s/it] 44%|████▍     | 4361/9822 [1:57:06<2:09:57,  1.43s/it] 44%|████▍     | 4362/9822 [1:57:08<2:09:57,  1.43s/it] 44%|████▍     | 4363/9822 [1:57:09<2:10:00,  1.43s/it] 44%|████▍     | 4364/9822 [1:57:11<2:11:58,  1.45s/it] 44%|████▍     | 4365/9822 [1:57:12<2:11:54,  1.45s/it] 44%|████▍     | 4366/9822 [1:57:13<2:12:03,  1.45s/it] 44%|████▍     | 4367/9822 [1:57:15<2:11:22,  1.44s/it] 44%|████▍     | 4368/9822 [1:57:16<2:10:51,  1.44s/it] 44%|████▍     | 4369/9822 [1:57:18<2:10:54,  1.44s/it] 44%|████▍     | 4370/9822 [1:57:19<2:10:50,  1.44s/it] 45%|████▍     | 4371/9822 [1:57:21<2:10:40,  1.44s/it] 45%|████▍     | 4372/9822 [1:57:22<2:10:42,  1.44s/it] 45%|████▍     | 4373/9822 [1:57:23<2:10:29,  1.44s/it] 45%|████▍     | 4374/9822 [1:57:25<2:10:26,  1.44s/it] 45%|████▍     | 4375/9822 [1:57:26<2:10:18,  1.44s/it] 45%|████▍     | 4376/9822 [1:57:28<2:10:38,  1.44s/it] 45%|████▍     | 4377/9822 [1:57:29<2:11:13,  1.45s/it] 45%|████▍     | 4378/9822 [1:57:31<2:10:41,  1.44s/it] 45%|████▍     | 4379/9822 [1:57:32<2:10:24,  1.44s/it] 45%|████▍     | 4380/9822 [1:57:34<2:10:33,  1.44s/it] 45%|████▍     | 4381/9822 [1:57:35<2:10:25,  1.44s/it] 45%|████▍     | 4382/9822 [1:57:36<2:10:28,  1.44s/it] 45%|████▍     | 4383/9822 [1:57:38<2:10:21,  1.44s/it] 45%|████▍     | 4384/9822 [1:57:39<2:11:04,  1.45s/it] 45%|████▍     | 4385/9822 [1:57:41<2:10:42,  1.44s/it] 45%|████▍     | 4386/9822 [1:57:42<2:09:08,  1.43s/it] 45%|████▍     | 4387/9822 [1:57:44<2:09:35,  1.43s/it] 45%|████▍     | 4388/9822 [1:57:45<2:09:50,  1.43s/it] 45%|████▍     | 4389/9822 [1:57:47<2:11:54,  1.46s/it] 45%|████▍     | 4390/9822 [1:57:48<2:11:22,  1.45s/it] 45%|████▍     | 4391/9822 [1:57:49<2:11:41,  1.45s/it] 45%|████▍     | 4392/9822 [1:57:51<2:11:07,  1.45s/it] 45%|████▍     | 4393/9822 [1:57:52<2:10:44,  1.44s/it] 45%|████▍     | 4394/9822 [1:57:54<2:10:11,  1.44s/it] 45%|████▍     | 4395/9822 [1:57:55<2:11:04,  1.45s/it] 45%|████▍     | 4396/9822 [1:57:57<2:10:11,  1.44s/it] 45%|████▍     | 4397/9822 [1:57:58<2:09:40,  1.43s/it] 45%|████▍     | 4398/9822 [1:57:59<2:09:32,  1.43s/it] 45%|████▍     | 4399/9822 [1:58:01<2:11:05,  1.45s/it] 45%|████▍     | 4400/9822 [1:58:02<2:11:43,  1.46s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:45:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:45:49 - INFO - __main__ - ***** test Results*****
01/07/2024 23:45:49 - INFO - __main__ -   Training step = 4400
01/07/2024 23:45:49 - INFO - __main__ -  test_accuracy:0.8667642752562226 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:45:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:45:55 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:45:55 - INFO - __main__ -   Training step = 4400
01/07/2024 23:45:55 - INFO - __main__ -  eval_accuracy:0.8502380080556573 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:46:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:46:00 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:46:00 - INFO - __main__ -   Training step = 4400
01/07/2024 23:46:00 - INFO - __main__ -  eval_accuracy:0.9047967777370927 
 45%|████▍     | 4401/9822 [1:58:21<9:48:51,  6.52s/it] 45%|████▍     | 4402/9822 [1:58:22<7:30:49,  4.99s/it] 45%|████▍     | 4403/9822 [1:58:24<5:54:33,  3.93s/it] 45%|████▍     | 4404/9822 [1:58:25<4:47:29,  3.18s/it] 45%|████▍     | 4405/9822 [1:58:27<3:59:56,  2.66s/it] 45%|████▍     | 4406/9822 [1:58:28<3:26:45,  2.29s/it] 45%|████▍     | 4407/9822 [1:58:29<3:03:14,  2.03s/it] 45%|████▍     | 4408/9822 [1:58:31<2:47:02,  1.85s/it] 45%|████▍     | 4409/9822 [1:58:32<2:35:47,  1.73s/it] 45%|████▍     | 4410/9822 [1:58:34<2:27:36,  1.64s/it] 45%|████▍     | 4411/9822 [1:58:35<2:22:04,  1.58s/it] 45%|████▍     | 4412/9822 [1:58:37<2:18:02,  1.53s/it] 45%|████▍     | 4413/9822 [1:58:38<2:15:12,  1.50s/it] 45%|████▍     | 4414/9822 [1:58:39<2:13:05,  1.48s/it] 45%|████▍     | 4415/9822 [1:58:41<2:11:59,  1.46s/it] 45%|████▍     | 4416/9822 [1:58:42<2:13:05,  1.48s/it] 45%|████▍     | 4417/9822 [1:58:44<2:12:02,  1.47s/it] 45%|████▍     | 4418/9822 [1:58:45<2:10:58,  1.45s/it] 45%|████▍     | 4419/9822 [1:58:47<2:10:33,  1.45s/it] 45%|████▌     | 4420/9822 [1:58:48<2:10:01,  1.44s/it] 45%|████▌     | 4421/9822 [1:58:49<2:09:26,  1.44s/it] 45%|████▌     | 4422/9822 [1:58:51<2:09:13,  1.44s/it] 45%|████▌     | 4423/9822 [1:58:52<2:09:22,  1.44s/it] 45%|████▌     | 4424/9822 [1:58:54<2:08:55,  1.43s/it] 45%|████▌     | 4425/9822 [1:58:55<2:08:48,  1.43s/it] 45%|████▌     | 4426/9822 [1:58:57<2:08:46,  1.43s/it] 45%|████▌     | 4427/9822 [1:58:58<2:08:34,  1.43s/it] 45%|████▌     | 4428/9822 [1:58:59<2:08:44,  1.43s/it] 45%|████▌     | 4429/9822 [1:59:01<2:08:48,  1.43s/it] 45%|████▌     | 4430/9822 [1:59:02<2:09:11,  1.44s/it] 45%|████▌     | 4431/9822 [1:59:04<2:09:00,  1.44s/it] 45%|████▌     | 4432/9822 [1:59:05<2:08:54,  1.44s/it] 45%|████▌     | 4433/9822 [1:59:07<2:08:35,  1.43s/it] 45%|████▌     | 4434/9822 [1:59:08<2:09:24,  1.44s/it] 45%|████▌     | 4435/9822 [1:59:10<2:09:14,  1.44s/it] 45%|████▌     | 4436/9822 [1:59:11<2:08:58,  1.44s/it] 45%|████▌     | 4437/9822 [1:59:12<2:08:45,  1.43s/it] 45%|████▌     | 4438/9822 [1:59:14<2:08:33,  1.43s/it] 45%|████▌     | 4439/9822 [1:59:15<2:08:13,  1.43s/it] 45%|████▌     | 4440/9822 [1:59:17<2:08:43,  1.44s/it] 45%|████▌     | 4441/9822 [1:59:18<2:08:59,  1.44s/it] 45%|████▌     | 4442/9822 [1:59:20<2:09:07,  1.44s/it] 45%|████▌     | 4443/9822 [1:59:21<2:08:42,  1.44s/it] 45%|████▌     | 4444/9822 [1:59:22<2:08:38,  1.44s/it] 45%|████▌     | 4445/9822 [1:59:24<2:08:33,  1.43s/it] 45%|████▌     | 4446/9822 [1:59:25<2:08:13,  1.43s/it] 45%|████▌     | 4447/9822 [1:59:27<2:08:28,  1.43s/it] 45%|████▌     | 4448/9822 [1:59:28<2:11:18,  1.47s/it] 45%|████▌     | 4449/9822 [1:59:30<2:10:24,  1.46s/it] 45%|████▌     | 4450/9822 [1:59:31<2:10:08,  1.45s/it] 45%|████▌     | 4451/9822 [1:59:33<2:09:38,  1.45s/it] 45%|████▌     | 4452/9822 [1:59:34<2:09:29,  1.45s/it] 45%|████▌     | 4453/9822 [1:59:36<2:09:26,  1.45s/it] 45%|████▌     | 4454/9822 [1:59:37<2:08:54,  1.44s/it] 45%|████▌     | 4455/9822 [1:59:38<2:08:56,  1.44s/it] 45%|████▌     | 4456/9822 [1:59:40<2:09:01,  1.44s/it] 45%|████▌     | 4457/9822 [1:59:41<2:08:27,  1.44s/it] 45%|████▌     | 4458/9822 [1:59:43<2:08:12,  1.43s/it] 45%|████▌     | 4459/9822 [1:59:44<2:07:49,  1.43s/it] 45%|████▌     | 4460/9822 [1:59:46<2:07:46,  1.43s/it] 45%|████▌     | 4461/9822 [1:59:47<2:07:42,  1.43s/it] 45%|████▌     | 4462/9822 [1:59:48<2:07:37,  1.43s/it] 45%|████▌     | 4463/9822 [1:59:50<2:07:38,  1.43s/it] 45%|████▌     | 4464/9822 [1:59:51<2:08:16,  1.44s/it] 45%|████▌     | 4465/9822 [1:59:53<2:08:14,  1.44s/it] 45%|████▌     | 4466/9822 [1:59:54<2:08:03,  1.43s/it] 45%|████▌     | 4467/9822 [1:59:56<2:08:02,  1.43s/it] 45%|████▌     | 4468/9822 [1:59:57<2:07:40,  1.43s/it] 45%|████▌     | 4469/9822 [1:59:58<2:07:19,  1.43s/it] 46%|████▌     | 4470/9822 [2:00:00<2:07:14,  1.43s/it] 46%|████▌     | 4471/9822 [2:00:01<2:07:21,  1.43s/it] 46%|████▌     | 4472/9822 [2:00:03<2:07:06,  1.43s/it] 46%|████▌     | 4473/9822 [2:00:04<2:07:18,  1.43s/it] 46%|████▌     | 4474/9822 [2:00:06<2:07:36,  1.43s/it] 46%|████▌     | 4475/9822 [2:00:07<2:07:46,  1.43s/it] 46%|████▌     | 4476/9822 [2:00:08<2:07:33,  1.43s/it] 46%|████▌     | 4477/9822 [2:00:10<2:07:17,  1.43s/it] 46%|████▌     | 4478/9822 [2:00:11<2:09:38,  1.46s/it] 46%|████▌     | 4479/9822 [2:00:13<2:09:02,  1.45s/it] 46%|████▌     | 4480/9822 [2:00:14<2:08:46,  1.45s/it] 46%|████▌     | 4481/9822 [2:00:16<2:08:16,  1.44s/it] 46%|████▌     | 4482/9822 [2:00:17<2:08:08,  1.44s/it] 46%|████▌     | 4483/9822 [2:00:19<2:08:24,  1.44s/it] 46%|████▌     | 4484/9822 [2:00:20<2:07:57,  1.44s/it] 46%|████▌     | 4485/9822 [2:00:21<2:08:15,  1.44s/it] 46%|████▌     | 4486/9822 [2:00:23<2:07:52,  1.44s/it] 46%|████▌     | 4487/9822 [2:00:24<2:07:39,  1.44s/it] 46%|████▌     | 4488/9822 [2:00:26<2:07:39,  1.44s/it] 46%|████▌     | 4489/9822 [2:00:27<2:07:39,  1.44s/it] 46%|████▌     | 4490/9822 [2:00:29<2:07:52,  1.44s/it] 46%|████▌     | 4491/9822 [2:00:30<2:07:28,  1.43s/it] 46%|████▌     | 4492/9822 [2:00:31<2:07:21,  1.43s/it] 46%|████▌     | 4493/9822 [2:00:33<2:07:09,  1.43s/it] 46%|████▌     | 4494/9822 [2:00:34<2:07:38,  1.44s/it] 46%|████▌     | 4495/9822 [2:00:36<2:07:25,  1.44s/it] 46%|████▌     | 4496/9822 [2:00:37<2:08:24,  1.45s/it] 46%|████▌     | 4497/9822 [2:00:39<2:08:19,  1.45s/it] 46%|████▌     | 4498/9822 [2:00:40<2:07:49,  1.44s/it] 46%|████▌     | 4499/9822 [2:00:42<2:07:58,  1.44s/it] 46%|████▌     | 4500/9822 [2:00:43<2:07:29,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:48:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:48:30 - INFO - __main__ - ***** test Results*****
01/07/2024 23:48:30 - INFO - __main__ -   Training step = 4500
01/07/2024 23:48:30 - INFO - __main__ -  test_accuracy:0.8704245973645681 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:48:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:48:36 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:48:36 - INFO - __main__ -   Training step = 4500
01/07/2024 23:48:36 - INFO - __main__ -  eval_accuracy:0.8535335042109118 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:48:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:48:41 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:48:41 - INFO - __main__ -   Training step = 4500
01/07/2024 23:48:41 - INFO - __main__ -  eval_accuracy:0.9066276089344563 
 46%|████▌     | 4501/9822 [2:01:01<9:37:52,  6.52s/it] 46%|████▌     | 4502/9822 [2:01:03<7:22:55,  5.00s/it] 46%|████▌     | 4503/9822 [2:01:04<5:48:01,  3.93s/it] 46%|████▌     | 4504/9822 [2:01:06<4:41:50,  3.18s/it] 46%|████▌     | 4505/9822 [2:01:07<3:55:11,  2.65s/it] 46%|████▌     | 4506/9822 [2:01:09<3:22:30,  2.29s/it] 46%|████▌     | 4507/9822 [2:01:10<2:59:49,  2.03s/it] 46%|████▌     | 4508/9822 [2:01:11<2:45:58,  1.87s/it] 46%|████▌     | 4509/9822 [2:01:13<2:34:20,  1.74s/it] 46%|████▌     | 4510/9822 [2:01:14<2:26:18,  1.65s/it] 46%|████▌     | 4511/9822 [2:01:16<2:20:25,  1.59s/it] 46%|████▌     | 4512/9822 [2:01:17<2:16:05,  1.54s/it] 46%|████▌     | 4513/9822 [2:01:19<2:13:20,  1.51s/it] 46%|████▌     | 4514/9822 [2:01:20<2:11:00,  1.48s/it] 46%|████▌     | 4515/9822 [2:01:22<2:11:07,  1.48s/it] 46%|████▌     | 4516/9822 [2:01:23<2:11:13,  1.48s/it] 46%|████▌     | 4517/9822 [2:01:24<2:10:08,  1.47s/it] 46%|████▌     | 4518/9822 [2:01:26<2:09:01,  1.46s/it] 46%|████▌     | 4519/9822 [2:01:27<2:08:44,  1.46s/it] 46%|████▌     | 4520/9822 [2:01:29<2:08:05,  1.45s/it] 46%|████▌     | 4521/9822 [2:01:30<2:07:25,  1.44s/it] 46%|████▌     | 4522/9822 [2:01:32<2:07:07,  1.44s/it] 46%|████▌     | 4523/9822 [2:01:33<2:07:03,  1.44s/it] 46%|████▌     | 4524/9822 [2:01:35<2:07:35,  1.45s/it] 46%|████▌     | 4525/9822 [2:01:36<2:08:57,  1.46s/it] 46%|████▌     | 4526/9822 [2:01:38<2:09:44,  1.47s/it] 46%|████▌     | 4527/9822 [2:01:39<2:09:05,  1.46s/it] 46%|████▌     | 4528/9822 [2:01:40<2:08:57,  1.46s/it] 46%|████▌     | 4529/9822 [2:01:42<2:09:31,  1.47s/it] 46%|████▌     | 4530/9822 [2:01:43<2:08:59,  1.46s/it] 46%|████▌     | 4531/9822 [2:01:45<2:08:38,  1.46s/it] 46%|████▌     | 4532/9822 [2:01:46<2:08:25,  1.46s/it] 46%|████▌     | 4533/9822 [2:01:48<2:08:11,  1.45s/it] 46%|████▌     | 4534/9822 [2:01:49<2:08:05,  1.45s/it] 46%|████▌     | 4535/9822 [2:01:51<2:07:55,  1.45s/it] 46%|████▌     | 4536/9822 [2:01:52<2:07:35,  1.45s/it] 46%|████▌     | 4537/9822 [2:01:54<2:07:23,  1.45s/it] 46%|████▌     | 4538/9822 [2:01:55<2:07:06,  1.44s/it] 46%|████▌     | 4539/9822 [2:01:56<2:07:19,  1.45s/it] 46%|████▌     | 4540/9822 [2:01:58<2:09:09,  1.47s/it] 46%|████▌     | 4541/9822 [2:01:59<2:08:03,  1.45s/it] 46%|████▌     | 4542/9822 [2:02:01<2:07:47,  1.45s/it] 46%|████▋     | 4543/9822 [2:02:02<2:07:45,  1.45s/it] 46%|████▋     | 4544/9822 [2:02:04<2:07:29,  1.45s/it] 46%|████▋     | 4545/9822 [2:02:05<2:07:10,  1.45s/it] 46%|████▋     | 4546/9822 [2:02:07<2:06:58,  1.44s/it] 46%|████▋     | 4547/9822 [2:02:08<2:06:25,  1.44s/it] 46%|████▋     | 4548/9822 [2:02:09<2:06:40,  1.44s/it] 46%|████▋     | 4549/9822 [2:02:11<2:06:56,  1.44s/it] 46%|████▋     | 4550/9822 [2:02:12<2:06:57,  1.44s/it] 46%|████▋     | 4551/9822 [2:02:14<2:06:50,  1.44s/it] 46%|████▋     | 4552/9822 [2:02:15<2:06:36,  1.44s/it] 46%|████▋     | 4553/9822 [2:02:17<2:06:31,  1.44s/it] 46%|████▋     | 4554/9822 [2:02:18<2:06:31,  1.44s/it] 46%|████▋     | 4555/9822 [2:02:20<2:06:34,  1.44s/it] 46%|████▋     | 4556/9822 [2:02:21<2:06:48,  1.44s/it] 46%|████▋     | 4557/9822 [2:02:22<2:06:25,  1.44s/it] 46%|████▋     | 4558/9822 [2:02:24<2:05:13,  1.43s/it] 46%|████▋     | 4559/9822 [2:02:25<2:05:32,  1.43s/it] 46%|████▋     | 4560/9822 [2:02:27<2:05:14,  1.43s/it] 46%|████▋     | 4561/9822 [2:02:28<2:05:16,  1.43s/it] 46%|████▋     | 4562/9822 [2:02:30<2:05:21,  1.43s/it] 46%|████▋     | 4563/9822 [2:02:31<2:05:12,  1.43s/it] 46%|████▋     | 4564/9822 [2:02:32<2:05:29,  1.43s/it] 46%|████▋     | 4565/9822 [2:02:34<2:05:32,  1.43s/it] 46%|████▋     | 4566/9822 [2:02:35<2:06:04,  1.44s/it] 46%|████▋     | 4567/9822 [2:02:37<2:05:54,  1.44s/it] 47%|████▋     | 4568/9822 [2:02:38<2:06:04,  1.44s/it] 47%|████▋     | 4569/9822 [2:02:40<2:06:07,  1.44s/it] 47%|████▋     | 4570/9822 [2:02:41<2:07:56,  1.46s/it] 47%|████▋     | 4571/9822 [2:02:43<2:07:19,  1.45s/it] 47%|████▋     | 4572/9822 [2:02:44<2:06:30,  1.45s/it] 47%|████▋     | 4573/9822 [2:02:45<2:06:06,  1.44s/it] 47%|████▋     | 4574/9822 [2:02:47<2:05:49,  1.44s/it] 47%|████▋     | 4575/9822 [2:02:48<2:05:30,  1.44s/it] 47%|████▋     | 4576/9822 [2:02:50<2:05:25,  1.43s/it] 47%|████▋     | 4577/9822 [2:02:51<2:05:07,  1.43s/it] 47%|████▋     | 4578/9822 [2:02:53<2:05:12,  1.43s/it] 47%|████▋     | 4579/9822 [2:02:54<2:05:14,  1.43s/it] 47%|████▋     | 4580/9822 [2:02:55<2:05:17,  1.43s/it] 47%|████▋     | 4581/9822 [2:02:57<2:05:00,  1.43s/it] 47%|████▋     | 4582/9822 [2:02:58<2:04:55,  1.43s/it] 47%|████▋     | 4583/9822 [2:03:00<2:04:53,  1.43s/it] 47%|████▋     | 4584/9822 [2:03:01<2:04:37,  1.43s/it] 47%|████▋     | 4585/9822 [2:03:03<2:04:38,  1.43s/it] 47%|████▋     | 4586/9822 [2:03:04<2:04:47,  1.43s/it] 47%|████▋     | 4587/9822 [2:03:05<2:04:35,  1.43s/it] 47%|████▋     | 4588/9822 [2:03:07<2:04:33,  1.43s/it] 47%|████▋     | 4589/9822 [2:03:08<2:04:52,  1.43s/it] 47%|████▋     | 4590/9822 [2:03:10<2:04:48,  1.43s/it] 47%|████▋     | 4591/9822 [2:03:11<2:04:39,  1.43s/it] 47%|████▋     | 4592/9822 [2:03:13<2:04:40,  1.43s/it] 47%|████▋     | 4593/9822 [2:03:14<2:04:50,  1.43s/it] 47%|████▋     | 4594/9822 [2:03:15<2:05:02,  1.44s/it] 47%|████▋     | 4595/9822 [2:03:17<2:04:57,  1.43s/it] 47%|████▋     | 4596/9822 [2:03:18<2:04:47,  1.43s/it] 47%|████▋     | 4597/9822 [2:03:20<2:04:43,  1.43s/it] 47%|████▋     | 4598/9822 [2:03:21<2:04:35,  1.43s/it] 47%|████▋     | 4599/9822 [2:03:23<2:04:29,  1.43s/it] 47%|████▋     | 4600/9822 [2:03:24<2:04:24,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:51:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:51:11 - INFO - __main__ - ***** test Results*****
01/07/2024 23:51:11 - INFO - __main__ -   Training step = 4600
01/07/2024 23:51:11 - INFO - __main__ -  test_accuracy:0.8583455344070278 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:51:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:51:17 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:51:17 - INFO - __main__ -   Training step = 4600
01/07/2024 23:51:17 - INFO - __main__ -  eval_accuracy:0.8491395093372391 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:51:22 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:51:22 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:51:22 - INFO - __main__ -   Training step = 4600
01/07/2024 23:51:22 - INFO - __main__ -  eval_accuracy:0.9007689491028927 
 47%|████▋     | 4601/9822 [2:03:42<9:25:50,  6.50s/it] 47%|████▋     | 4602/9822 [2:03:44<7:15:29,  5.01s/it] 47%|████▋     | 4603/9822 [2:03:45<5:42:25,  3.94s/it] 47%|████▋     | 4604/9822 [2:03:47<4:37:14,  3.19s/it] 47%|████▋     | 4605/9822 [2:03:48<3:52:14,  2.67s/it] 47%|████▋     | 4606/9822 [2:03:50<3:19:56,  2.30s/it] 47%|████▋     | 4607/9822 [2:03:51<2:57:21,  2.04s/it] 47%|████▋     | 4608/9822 [2:03:53<2:42:24,  1.87s/it] 47%|████▋     | 4609/9822 [2:03:54<2:30:58,  1.74s/it] 47%|████▋     | 4610/9822 [2:03:55<2:22:59,  1.65s/it] 47%|████▋     | 4611/9822 [2:03:57<2:17:16,  1.58s/it] 47%|████▋     | 4612/9822 [2:03:58<2:14:35,  1.55s/it] 47%|████▋     | 4613/9822 [2:04:00<2:12:54,  1.53s/it] 47%|████▋     | 4614/9822 [2:04:01<2:10:28,  1.50s/it] 47%|████▋     | 4615/9822 [2:04:03<2:09:12,  1.49s/it] 47%|████▋     | 4616/9822 [2:04:04<2:07:47,  1.47s/it] 47%|████▋     | 4617/9822 [2:04:06<2:06:43,  1.46s/it] 47%|████▋     | 4618/9822 [2:04:07<2:06:05,  1.45s/it] 47%|████▋     | 4619/9822 [2:04:08<2:05:31,  1.45s/it] 47%|████▋     | 4620/9822 [2:04:10<2:05:09,  1.44s/it] 47%|████▋     | 4621/9822 [2:04:11<2:04:50,  1.44s/it] 47%|████▋     | 4622/9822 [2:04:13<2:04:35,  1.44s/it] 47%|████▋     | 4623/9822 [2:04:14<2:04:20,  1.43s/it] 47%|████▋     | 4624/9822 [2:04:16<2:04:08,  1.43s/it] 47%|████▋     | 4625/9822 [2:04:17<2:04:09,  1.43s/it] 47%|████▋     | 4626/9822 [2:04:18<2:04:10,  1.43s/it] 47%|████▋     | 4627/9822 [2:04:20<2:04:19,  1.44s/it] 47%|████▋     | 4628/9822 [2:04:21<2:04:12,  1.43s/it] 47%|████▋     | 4629/9822 [2:04:23<2:04:02,  1.43s/it] 47%|████▋     | 4630/9822 [2:04:24<2:03:57,  1.43s/it] 47%|████▋     | 4631/9822 [2:04:26<2:03:52,  1.43s/it] 47%|████▋     | 4632/9822 [2:04:27<2:03:41,  1.43s/it] 47%|████▋     | 4633/9822 [2:04:29<2:03:39,  1.43s/it] 47%|████▋     | 4634/9822 [2:04:30<2:05:53,  1.46s/it] 47%|████▋     | 4635/9822 [2:04:31<2:05:06,  1.45s/it] 47%|████▋     | 4636/9822 [2:04:33<2:04:40,  1.44s/it] 47%|████▋     | 4637/9822 [2:04:34<2:04:32,  1.44s/it] 47%|████▋     | 4638/9822 [2:04:36<2:03:59,  1.44s/it] 47%|████▋     | 4639/9822 [2:04:37<2:03:42,  1.43s/it] 47%|████▋     | 4640/9822 [2:04:39<2:03:34,  1.43s/it] 47%|████▋     | 4641/9822 [2:04:40<2:03:39,  1.43s/it] 47%|████▋     | 4642/9822 [2:04:41<2:03:58,  1.44s/it] 47%|████▋     | 4643/9822 [2:04:43<2:04:07,  1.44s/it] 47%|████▋     | 4644/9822 [2:04:44<2:02:27,  1.42s/it] 47%|████▋     | 4645/9822 [2:04:46<2:02:50,  1.42s/it] 47%|████▋     | 4646/9822 [2:04:47<2:03:21,  1.43s/it] 47%|████▋     | 4647/9822 [2:04:49<2:03:24,  1.43s/it] 47%|████▋     | 4648/9822 [2:04:50<2:03:35,  1.43s/it] 47%|████▋     | 4649/9822 [2:04:51<2:03:13,  1.43s/it] 47%|████▋     | 4650/9822 [2:04:53<2:03:12,  1.43s/it] 47%|████▋     | 4651/9822 [2:04:54<2:03:12,  1.43s/it] 47%|████▋     | 4652/9822 [2:04:56<2:03:28,  1.43s/it] 47%|████▋     | 4653/9822 [2:04:57<2:03:26,  1.43s/it] 47%|████▋     | 4654/9822 [2:04:59<2:03:27,  1.43s/it] 47%|████▋     | 4655/9822 [2:05:00<2:03:42,  1.44s/it] 47%|████▋     | 4656/9822 [2:05:02<2:03:34,  1.44s/it] 47%|████▋     | 4657/9822 [2:05:03<2:03:18,  1.43s/it] 47%|████▋     | 4658/9822 [2:05:04<2:03:26,  1.43s/it] 47%|████▋     | 4659/9822 [2:05:06<2:03:38,  1.44s/it] 47%|████▋     | 4660/9822 [2:05:07<2:03:29,  1.44s/it] 47%|████▋     | 4661/9822 [2:05:09<2:03:34,  1.44s/it] 47%|████▋     | 4662/9822 [2:05:10<2:03:58,  1.44s/it] 47%|████▋     | 4663/9822 [2:05:12<2:04:01,  1.44s/it] 47%|████▋     | 4664/9822 [2:05:13<2:04:09,  1.44s/it] 47%|████▋     | 4665/9822 [2:05:14<2:04:09,  1.44s/it] 48%|████▊     | 4666/9822 [2:05:16<2:06:18,  1.47s/it] 48%|████▊     | 4667/9822 [2:05:17<2:05:41,  1.46s/it] 48%|████▊     | 4668/9822 [2:05:19<2:04:29,  1.45s/it] 48%|████▊     | 4669/9822 [2:05:20<2:04:18,  1.45s/it] 48%|████▊     | 4670/9822 [2:05:22<2:03:55,  1.44s/it] 48%|████▊     | 4671/9822 [2:05:23<2:03:25,  1.44s/it] 48%|████▊     | 4672/9822 [2:05:25<2:02:54,  1.43s/it] 48%|████▊     | 4673/9822 [2:05:26<2:02:35,  1.43s/it] 48%|████▊     | 4674/9822 [2:05:27<2:02:29,  1.43s/it] 48%|████▊     | 4675/9822 [2:05:29<2:02:27,  1.43s/it] 48%|████▊     | 4676/9822 [2:05:30<2:02:51,  1.43s/it] 48%|████▊     | 4677/9822 [2:05:32<2:03:08,  1.44s/it] 48%|████▊     | 4678/9822 [2:05:33<2:02:54,  1.43s/it] 48%|████▊     | 4679/9822 [2:05:35<2:03:07,  1.44s/it] 48%|████▊     | 4680/9822 [2:05:36<2:03:17,  1.44s/it] 48%|████▊     | 4681/9822 [2:05:38<2:03:32,  1.44s/it] 48%|████▊     | 4682/9822 [2:05:39<2:03:40,  1.44s/it] 48%|████▊     | 4683/9822 [2:05:40<2:03:31,  1.44s/it] 48%|████▊     | 4684/9822 [2:05:42<2:03:18,  1.44s/it] 48%|████▊     | 4685/9822 [2:05:43<2:04:15,  1.45s/it] 48%|████▊     | 4686/9822 [2:05:45<2:03:46,  1.45s/it] 48%|████▊     | 4687/9822 [2:05:46<2:04:36,  1.46s/it] 48%|████▊     | 4688/9822 [2:05:48<2:05:28,  1.47s/it] 48%|████▊     | 4689/9822 [2:05:49<2:04:55,  1.46s/it] 48%|████▊     | 4690/9822 [2:05:51<2:04:26,  1.45s/it] 48%|████▊     | 4691/9822 [2:05:52<2:06:06,  1.47s/it] 48%|████▊     | 4692/9822 [2:05:54<2:05:09,  1.46s/it] 48%|████▊     | 4693/9822 [2:05:55<2:04:17,  1.45s/it] 48%|████▊     | 4694/9822 [2:05:56<2:03:31,  1.45s/it] 48%|████▊     | 4695/9822 [2:05:58<2:03:11,  1.44s/it] 48%|████▊     | 4696/9822 [2:05:59<2:02:53,  1.44s/it] 48%|████▊     | 4697/9822 [2:06:01<2:02:25,  1.43s/it] 48%|████▊     | 4698/9822 [2:06:02<2:02:29,  1.43s/it] 48%|████▊     | 4699/9822 [2:06:04<2:02:21,  1.43s/it] 48%|████▊     | 4700/9822 [2:06:05<2:02:09,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:53:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:53:52 - INFO - __main__ - ***** test Results*****
01/07/2024 23:53:52 - INFO - __main__ -   Training step = 4700
01/07/2024 23:53:52 - INFO - __main__ -  test_accuracy:0.8660322108345534 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:53:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:53:58 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:53:58 - INFO - __main__ -   Training step = 4700
01/07/2024 23:53:58 - INFO - __main__ -  eval_accuracy:0.8520688392530209 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8564628341266936}
test:
{'accuracy': 0.8682284040995608}
01/07/2024 23:54:03 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:54:03 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:54:03 - INFO - __main__ -   Training step = 4700
01/07/2024 23:54:03 - INFO - __main__ -  eval_accuracy:0.9062614426949835 
 48%|████▊     | 4701/9822 [2:06:23<9:15:20,  6.51s/it] 48%|████▊     | 4702/9822 [2:06:25<7:05:16,  4.98s/it] 48%|████▊     | 4703/9822 [2:06:26<5:34:49,  3.92s/it] 48%|████▊     | 4704/9822 [2:06:28<4:32:22,  3.19s/it] 48%|████▊     | 4705/9822 [2:06:29<3:47:17,  2.67s/it] 48%|████▊     | 4706/9822 [2:06:31<3:16:02,  2.30s/it] 48%|████▊     | 4707/9822 [2:06:32<2:53:51,  2.04s/it] 48%|████▊     | 4708/9822 [2:06:33<2:38:39,  1.86s/it] 48%|████▊     | 4709/9822 [2:06:35<2:27:48,  1.73s/it] 48%|████▊     | 4710/9822 [2:06:36<2:20:12,  1.65s/it] 48%|████▊     | 4711/9822 [2:06:38<2:14:59,  1.58s/it] 48%|████▊     | 4712/9822 [2:06:39<2:11:13,  1.54s/it] 48%|████▊     | 4713/9822 [2:06:41<2:08:17,  1.51s/it] 48%|████▊     | 4714/9822 [2:06:42<2:06:31,  1.49s/it] 48%|████▊     | 4715/9822 [2:06:44<2:04:58,  1.47s/it] 48%|████▊     | 4716/9822 [2:06:45<2:03:51,  1.46s/it] 48%|████▊     | 4717/9822 [2:06:46<2:05:19,  1.47s/it] 48%|████▊     | 4718/9822 [2:06:48<2:04:22,  1.46s/it] 48%|████▊     | 4719/9822 [2:06:49<2:03:59,  1.46s/it] 48%|████▊     | 4720/9822 [2:06:51<2:03:29,  1.45s/it] 48%|████▊     | 4721/9822 [2:06:52<2:02:55,  1.45s/it] 48%|████▊     | 4722/9822 [2:06:54<2:02:18,  1.44s/it] 48%|████▊     | 4723/9822 [2:06:55<2:01:58,  1.44s/it] 48%|████▊     | 4724/9822 [2:06:57<2:01:51,  1.43s/it] 48%|████▊     | 4725/9822 [2:06:58<2:02:06,  1.44s/it] 48%|████▊     | 4726/9822 [2:06:59<2:02:03,  1.44s/it] 48%|████▊     | 4727/9822 [2:07:01<2:02:03,  1.44s/it] 48%|████▊     | 4728/9822 [2:07:02<2:01:55,  1.44s/it] 48%|████▊     | 4729/9822 [2:07:04<2:01:38,  1.43s/it] 48%|████▊     | 4730/9822 [2:07:05<2:00:03,  1.41s/it] 48%|████▊     | 4731/9822 [2:07:06<2:00:23,  1.42s/it] 48%|████▊     | 4732/9822 [2:07:08<2:01:04,  1.43s/it] 48%|████▊     | 4733/9822 [2:07:09<2:01:01,  1.43s/it] 48%|████▊     | 4734/9822 [2:07:11<2:01:18,  1.43s/it] 48%|████▊     | 4735/9822 [2:07:12<2:01:29,  1.43s/it] 48%|████▊     | 4736/9822 [2:07:14<2:01:30,  1.43s/it] 48%|████▊     | 4737/9822 [2:07:15<2:01:19,  1.43s/it] 48%|████▊     | 4738/9822 [2:07:17<2:01:27,  1.43s/it] 48%|████▊     | 4739/9822 [2:07:18<2:01:11,  1.43s/it] 48%|████▊     | 4740/9822 [2:07:19<2:01:15,  1.43s/it] 48%|████▊     | 4741/9822 [2:07:21<2:01:01,  1.43s/it] 48%|████▊     | 4742/9822 [2:07:22<2:01:04,  1.43s/it] 48%|████▊     | 4743/9822 [2:07:24<2:00:46,  1.43s/it] 48%|████▊     | 4744/9822 [2:07:25<2:00:35,  1.42s/it] 48%|████▊     | 4745/9822 [2:07:27<2:00:48,  1.43s/it] 48%|████▊     | 4746/9822 [2:07:28<2:01:00,  1.43s/it] 48%|████▊     | 4747/9822 [2:07:29<2:01:25,  1.44s/it] 48%|████▊     | 4748/9822 [2:07:31<2:01:10,  1.43s/it] 48%|████▊     | 4749/9822 [2:07:32<2:03:14,  1.46s/it] 48%|████▊     | 4750/9822 [2:07:34<2:02:13,  1.45s/it] 48%|████▊     | 4751/9822 [2:07:35<2:01:39,  1.44s/it] 48%|████▊     | 4752/9822 [2:07:37<2:01:25,  1.44s/it] 48%|████▊     | 4753/9822 [2:07:38<2:01:42,  1.44s/it] 48%|████▊     | 4754/9822 [2:07:40<2:01:26,  1.44s/it] 48%|████▊     | 4755/9822 [2:07:41<2:01:45,  1.44s/it] 48%|████▊     | 4756/9822 [2:07:42<2:01:50,  1.44s/it] 48%|████▊     | 4757/9822 [2:07:44<2:01:35,  1.44s/it] 48%|████▊     | 4758/9822 [2:07:45<2:01:18,  1.44s/it] 48%|████▊     | 4759/9822 [2:07:47<2:00:52,  1.43s/it] 48%|████▊     | 4760/9822 [2:07:48<2:00:57,  1.43s/it] 48%|████▊     | 4761/9822 [2:07:50<2:01:00,  1.43s/it] 48%|████▊     | 4762/9822 [2:07:51<2:01:14,  1.44s/it] 48%|████▊     | 4763/9822 [2:07:52<2:01:04,  1.44s/it] 49%|████▊     | 4764/9822 [2:07:54<2:00:53,  1.43s/it] 49%|████▊     | 4765/9822 [2:07:55<2:01:04,  1.44s/it] 49%|████▊     | 4766/9822 [2:07:57<2:00:56,  1.44s/it] 49%|████▊     | 4767/9822 [2:07:58<2:00:47,  1.43s/it] 49%|████▊     | 4768/9822 [2:08:00<2:00:40,  1.43s/it] 49%|████▊     | 4769/9822 [2:08:01<2:00:42,  1.43s/it] 49%|████▊     | 4770/9822 [2:08:02<2:00:30,  1.43s/it] 49%|████▊     | 4771/9822 [2:08:04<2:00:22,  1.43s/it] 49%|████▊     | 4772/9822 [2:08:05<2:00:27,  1.43s/it] 49%|████▊     | 4773/9822 [2:08:07<2:00:38,  1.43s/it] 49%|████▊     | 4774/9822 [2:08:08<2:00:30,  1.43s/it] 49%|████▊     | 4775/9822 [2:08:10<2:00:26,  1.43s/it] 49%|████▊     | 4776/9822 [2:08:11<2:00:27,  1.43s/it] 49%|████▊     | 4777/9822 [2:08:12<2:00:05,  1.43s/it] 49%|████▊     | 4778/9822 [2:08:14<2:00:08,  1.43s/it] 49%|████▊     | 4779/9822 [2:08:15<2:00:03,  1.43s/it] 49%|████▊     | 4780/9822 [2:08:17<2:00:00,  1.43s/it] 49%|████▊     | 4781/9822 [2:08:18<2:02:10,  1.45s/it] 49%|████▊     | 4782/9822 [2:08:20<2:01:38,  1.45s/it] 49%|████▊     | 4783/9822 [2:08:21<2:01:12,  1.44s/it] 49%|████▊     | 4784/9822 [2:08:23<2:00:47,  1.44s/it] 49%|████▊     | 4785/9822 [2:08:24<2:01:08,  1.44s/it] 49%|████▊     | 4786/9822 [2:08:25<2:01:21,  1.45s/it] 49%|████▊     | 4787/9822 [2:08:27<2:01:11,  1.44s/it] 49%|████▊     | 4788/9822 [2:08:28<2:00:40,  1.44s/it] 49%|████▉     | 4789/9822 [2:08:30<2:00:28,  1.44s/it] 49%|████▉     | 4790/9822 [2:08:31<2:00:03,  1.43s/it] 49%|████▉     | 4791/9822 [2:08:33<2:00:00,  1.43s/it] 49%|████▉     | 4792/9822 [2:08:34<1:59:51,  1.43s/it] 49%|████▉     | 4793/9822 [2:08:36<2:00:20,  1.44s/it] 49%|████▉     | 4794/9822 [2:08:37<2:00:10,  1.43s/it] 49%|████▉     | 4795/9822 [2:08:38<2:00:07,  1.43s/it] 49%|████▉     | 4796/9822 [2:08:40<2:00:27,  1.44s/it] 49%|████▉     | 4797/9822 [2:08:41<2:00:04,  1.43s/it] 49%|████▉     | 4798/9822 [2:08:43<1:59:53,  1.43s/it] 49%|████▉     | 4799/9822 [2:08:44<1:59:44,  1.43s/it] 49%|████▉     | 4800/9822 [2:08:46<1:59:40,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:56:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:56:32 - INFO - __main__ - ***** test Results*****
01/07/2024 23:56:32 - INFO - __main__ -   Training step = 4800
01/07/2024 23:56:32 - INFO - __main__ -  test_accuracy:0.8726207906295754 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:56:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:56:38 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:56:38 - INFO - __main__ -   Training step = 4800
01/07/2024 23:56:38 - INFO - __main__ -  eval_accuracy:0.8575613328451117 
[INFO|tokenization_utils_base.py:2094] 2024-01-07 23:56:38,934 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-07 23:56:38,934 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-07 23:56:38,971 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-07 23:56:40,569 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/07/2024 23:56:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:56:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:56:45 - INFO - __main__ -   Training step = 4800
01/07/2024 23:56:45 - INFO - __main__ -  eval_accuracy:0.9062614426949835 
 49%|████▉     | 4801/9822 [2:09:05<9:44:54,  6.99s/it] 49%|████▉     | 4802/9822 [2:09:07<7:25:13,  5.32s/it] 49%|████▉     | 4803/9822 [2:09:08<5:47:29,  4.15s/it] 49%|████▉     | 4804/9822 [2:09:10<4:39:10,  3.34s/it] 49%|████▉     | 4805/9822 [2:09:11<3:51:30,  2.77s/it] 49%|████▉     | 4806/9822 [2:09:13<3:17:55,  2.37s/it] 49%|████▉     | 4807/9822 [2:09:14<2:54:42,  2.09s/it] 49%|████▉     | 4808/9822 [2:09:16<2:38:07,  1.89s/it] 49%|████▉     | 4809/9822 [2:09:17<2:28:39,  1.78s/it] 49%|████▉     | 4810/9822 [2:09:18<2:19:56,  1.68s/it] 49%|████▉     | 4811/9822 [2:09:20<2:13:49,  1.60s/it] 49%|████▉     | 4812/9822 [2:09:21<2:09:28,  1.55s/it] 49%|████▉     | 4813/9822 [2:09:23<2:06:31,  1.52s/it] 49%|████▉     | 4814/9822 [2:09:24<2:04:28,  1.49s/it] 49%|████▉     | 4815/9822 [2:09:26<2:02:58,  1.47s/it] 49%|████▉     | 4816/9822 [2:09:27<2:00:48,  1.45s/it] 49%|████▉     | 4817/9822 [2:09:28<2:00:47,  1.45s/it] 49%|████▉     | 4818/9822 [2:09:30<2:00:21,  1.44s/it] 49%|████▉     | 4819/9822 [2:09:31<2:00:14,  1.44s/it] 49%|████▉     | 4820/9822 [2:09:33<2:00:01,  1.44s/it] 49%|████▉     | 4821/9822 [2:09:34<1:59:49,  1.44s/it] 49%|████▉     | 4822/9822 [2:09:36<1:59:53,  1.44s/it] 49%|████▉     | 4823/9822 [2:09:37<1:59:28,  1.43s/it] 49%|████▉     | 4824/9822 [2:09:39<1:59:45,  1.44s/it] 49%|████▉     | 4825/9822 [2:09:40<1:59:19,  1.43s/it] 49%|████▉     | 4826/9822 [2:09:41<1:59:20,  1.43s/it] 49%|████▉     | 4827/9822 [2:09:43<1:59:38,  1.44s/it] 49%|████▉     | 4828/9822 [2:09:44<1:59:36,  1.44s/it] 49%|████▉     | 4829/9822 [2:09:46<1:59:32,  1.44s/it] 49%|████▉     | 4830/9822 [2:09:47<1:59:25,  1.44s/it] 49%|████▉     | 4831/9822 [2:09:49<2:00:18,  1.45s/it] 49%|████▉     | 4832/9822 [2:09:50<1:59:55,  1.44s/it] 49%|████▉     | 4833/9822 [2:09:51<1:59:53,  1.44s/it] 49%|████▉     | 4834/9822 [2:09:53<1:59:29,  1.44s/it] 49%|████▉     | 4835/9822 [2:09:54<1:59:09,  1.43s/it] 49%|████▉     | 4836/9822 [2:09:56<1:59:28,  1.44s/it] 49%|████▉     | 4837/9822 [2:09:57<1:59:49,  1.44s/it] 49%|████▉     | 4838/9822 [2:09:59<1:59:58,  1.44s/it] 49%|████▉     | 4839/9822 [2:10:00<2:00:38,  1.45s/it] 49%|████▉     | 4840/9822 [2:10:02<2:02:08,  1.47s/it] 49%|████▉     | 4841/9822 [2:10:03<2:01:05,  1.46s/it] 49%|████▉     | 4842/9822 [2:10:05<2:00:44,  1.45s/it] 49%|████▉     | 4843/9822 [2:10:06<1:59:58,  1.45s/it] 49%|████▉     | 4844/9822 [2:10:07<2:00:48,  1.46s/it] 49%|████▉     | 4845/9822 [2:10:09<2:00:40,  1.45s/it] 49%|████▉     | 4846/9822 [2:10:10<2:00:30,  1.45s/it] 49%|████▉     | 4847/9822 [2:10:12<1:59:52,  1.45s/it] 49%|████▉     | 4848/9822 [2:10:13<1:59:40,  1.44s/it] 49%|████▉     | 4849/9822 [2:10:15<1:59:16,  1.44s/it] 49%|████▉     | 4850/9822 [2:10:16<1:59:01,  1.44s/it] 49%|████▉     | 4851/9822 [2:10:17<1:58:32,  1.43s/it] 49%|████▉     | 4852/9822 [2:10:19<1:58:13,  1.43s/it] 49%|████▉     | 4853/9822 [2:10:20<1:58:33,  1.43s/it] 49%|████▉     | 4854/9822 [2:10:22<1:58:53,  1.44s/it] 49%|████▉     | 4855/9822 [2:10:23<1:58:56,  1.44s/it] 49%|████▉     | 4856/9822 [2:10:25<1:58:51,  1.44s/it] 49%|████▉     | 4857/9822 [2:10:26<1:59:09,  1.44s/it] 49%|████▉     | 4858/9822 [2:10:28<2:00:32,  1.46s/it] 49%|████▉     | 4859/9822 [2:10:29<2:01:26,  1.47s/it] 49%|████▉     | 4860/9822 [2:10:31<2:01:42,  1.47s/it] 49%|████▉     | 4861/9822 [2:10:32<2:01:13,  1.47s/it] 50%|████▉     | 4862/9822 [2:10:33<2:00:36,  1.46s/it] 50%|████▉     | 4863/9822 [2:10:35<2:00:12,  1.45s/it] 50%|████▉     | 4864/9822 [2:10:36<2:00:18,  1.46s/it] 50%|████▉     | 4865/9822 [2:10:38<2:02:44,  1.49s/it] 50%|████▉     | 4866/9822 [2:10:39<2:01:36,  1.47s/it] 50%|████▉     | 4867/9822 [2:10:41<2:00:53,  1.46s/it] 50%|████▉     | 4868/9822 [2:10:42<2:00:21,  1.46s/it] 50%|████▉     | 4869/9822 [2:10:44<2:00:07,  1.46s/it] 50%|████▉     | 4870/9822 [2:10:45<2:00:03,  1.45s/it] 50%|████▉     | 4871/9822 [2:10:47<1:59:28,  1.45s/it] 50%|████▉     | 4872/9822 [2:10:48<1:59:35,  1.45s/it] 50%|████▉     | 4873/9822 [2:10:50<1:59:45,  1.45s/it] 50%|████▉     | 4874/9822 [2:10:51<1:59:13,  1.45s/it] 50%|████▉     | 4875/9822 [2:10:52<1:59:18,  1.45s/it] 50%|████▉     | 4876/9822 [2:10:54<1:59:06,  1.44s/it] 50%|████▉     | 4877/9822 [2:10:55<1:58:33,  1.44s/it] 50%|████▉     | 4878/9822 [2:10:57<1:58:27,  1.44s/it] 50%|████▉     | 4879/9822 [2:10:58<1:58:04,  1.43s/it] 50%|████▉     | 4880/9822 [2:11:00<1:58:02,  1.43s/it] 50%|████▉     | 4881/9822 [2:11:01<1:58:01,  1.43s/it] 50%|████▉     | 4882/9822 [2:11:02<1:58:01,  1.43s/it] 50%|████▉     | 4883/9822 [2:11:04<1:58:15,  1.44s/it] 50%|████▉     | 4884/9822 [2:11:05<1:58:19,  1.44s/it] 50%|████▉     | 4885/9822 [2:11:07<1:58:16,  1.44s/it] 50%|████▉     | 4886/9822 [2:11:08<1:57:54,  1.43s/it] 50%|████▉     | 4887/9822 [2:11:10<1:58:19,  1.44s/it] 50%|████▉     | 4888/9822 [2:11:11<1:58:13,  1.44s/it] 50%|████▉     | 4889/9822 [2:11:12<1:58:38,  1.44s/it] 50%|████▉     | 4890/9822 [2:11:14<1:58:08,  1.44s/it] 50%|████▉     | 4891/9822 [2:11:15<1:57:56,  1.44s/it] 50%|████▉     | 4892/9822 [2:11:17<1:58:42,  1.44s/it] 50%|████▉     | 4893/9822 [2:11:18<1:58:24,  1.44s/it] 50%|████▉     | 4894/9822 [2:11:20<1:58:12,  1.44s/it] 50%|████▉     | 4895/9822 [2:11:21<1:57:47,  1.43s/it] 50%|████▉     | 4896/9822 [2:11:23<1:57:37,  1.43s/it] 50%|████▉     | 4897/9822 [2:11:24<1:59:45,  1.46s/it] 50%|████▉     | 4898/9822 [2:11:26<2:00:35,  1.47s/it] 50%|████▉     | 4899/9822 [2:11:27<1:59:25,  1.46s/it] 50%|████▉     | 4900/9822 [2:11:28<1:58:50,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
01/07/2024 23:59:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:59:15 - INFO - __main__ - ***** test Results*****
01/07/2024 23:59:15 - INFO - __main__ -   Training step = 4900
01/07/2024 23:59:15 - INFO - __main__ -  test_accuracy:0.8583455344070278 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:59:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:59:21 - INFO - __main__ - ***** Evaluation Results*****
01/07/2024 23:59:21 - INFO - __main__ -   Training step = 4900
01/07/2024 23:59:21 - INFO - __main__ -  eval_accuracy:0.8502380080556573 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/07/2024 23:59:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/07/2024 23:59:26 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/07/2024 23:59:26 - INFO - __main__ -   Training step = 4900
01/07/2024 23:59:26 - INFO - __main__ -  eval_accuracy:0.9018674478213109 
 50%|████▉     | 4901/9822 [2:11:47<8:54:34,  6.52s/it] 50%|████▉     | 4902/9822 [2:11:48<6:48:11,  4.98s/it] 50%|████▉     | 4903/9822 [2:11:50<5:21:01,  3.92s/it] 50%|████▉     | 4904/9822 [2:11:51<4:19:54,  3.17s/it] 50%|████▉     | 4905/9822 [2:11:52<3:37:21,  2.65s/it] 50%|████▉     | 4906/9822 [2:11:54<3:07:23,  2.29s/it] 50%|████▉     | 4907/9822 [2:11:55<2:46:05,  2.03s/it] 50%|████▉     | 4908/9822 [2:11:57<2:31:15,  1.85s/it] 50%|████▉     | 4909/9822 [2:11:58<2:20:57,  1.72s/it] 50%|████▉     | 4910/9822 [2:12:00<2:13:37,  1.63s/it] 50%|█████     | 4911/9822 [2:12:01<2:08:52,  1.57s/it] 50%|█████     | 4912/9822 [2:12:02<2:05:47,  1.54s/it] 50%|█████     | 4913/9822 [2:12:04<2:03:44,  1.51s/it] 50%|█████     | 4914/9822 [2:12:05<2:01:38,  1.49s/it] 50%|█████     | 4915/9822 [2:12:07<2:00:09,  1.47s/it] 50%|█████     | 4916/9822 [2:12:08<1:59:20,  1.46s/it] 50%|█████     | 4917/9822 [2:12:10<1:58:37,  1.45s/it] 50%|█████     | 4918/9822 [2:12:11<1:58:13,  1.45s/it] 50%|█████     | 4919/9822 [2:12:13<1:57:44,  1.44s/it] 50%|█████     | 4920/9822 [2:12:14<1:57:19,  1.44s/it] 50%|█████     | 4921/9822 [2:12:15<1:57:19,  1.44s/it] 50%|█████     | 4922/9822 [2:12:17<1:57:22,  1.44s/it] 50%|█████     | 4923/9822 [2:12:18<1:57:20,  1.44s/it] 50%|█████     | 4924/9822 [2:12:20<1:57:10,  1.44s/it] 50%|█████     | 4925/9822 [2:12:21<1:56:50,  1.43s/it] 50%|█████     | 4926/9822 [2:12:23<1:56:43,  1.43s/it] 50%|█████     | 4927/9822 [2:12:24<1:56:29,  1.43s/it] 50%|█████     | 4928/9822 [2:12:25<1:56:38,  1.43s/it] 50%|█████     | 4929/9822 [2:12:27<1:56:39,  1.43s/it] 50%|█████     | 4930/9822 [2:12:28<1:56:35,  1.43s/it] 50%|█████     | 4931/9822 [2:12:30<1:56:41,  1.43s/it] 50%|█████     | 4932/9822 [2:12:31<1:56:40,  1.43s/it] 50%|█████     | 4933/9822 [2:12:33<1:56:52,  1.43s/it] 50%|█████     | 4934/9822 [2:12:34<1:56:51,  1.43s/it] 50%|█████     | 4935/9822 [2:12:35<1:56:38,  1.43s/it] 50%|█████     | 4936/9822 [2:12:37<1:59:00,  1.46s/it] 50%|█████     | 4937/9822 [2:12:38<1:57:58,  1.45s/it] 50%|█████     | 4938/9822 [2:12:40<1:57:33,  1.44s/it] 50%|█████     | 4939/9822 [2:12:41<1:57:32,  1.44s/it] 50%|█████     | 4940/9822 [2:12:43<1:57:17,  1.44s/it] 50%|█████     | 4941/9822 [2:12:44<1:57:11,  1.44s/it] 50%|█████     | 4942/9822 [2:12:46<1:56:46,  1.44s/it] 50%|█████     | 4943/9822 [2:12:47<1:56:50,  1.44s/it] 50%|█████     | 4944/9822 [2:12:48<1:56:34,  1.43s/it] 50%|█████     | 4945/9822 [2:12:50<1:56:45,  1.44s/it] 50%|█████     | 4946/9822 [2:12:51<1:56:25,  1.43s/it] 50%|█████     | 4947/9822 [2:12:53<1:56:44,  1.44s/it] 50%|█████     | 4948/9822 [2:12:54<1:57:23,  1.45s/it] 50%|█████     | 4949/9822 [2:12:56<1:57:23,  1.45s/it] 50%|█████     | 4950/9822 [2:12:57<1:57:00,  1.44s/it] 50%|█████     | 4951/9822 [2:12:59<1:56:52,  1.44s/it] 50%|█████     | 4952/9822 [2:13:00<1:56:44,  1.44s/it] 50%|█████     | 4953/9822 [2:13:01<1:56:57,  1.44s/it] 50%|█████     | 4954/9822 [2:13:03<1:56:45,  1.44s/it] 50%|█████     | 4955/9822 [2:13:04<1:56:44,  1.44s/it] 50%|█████     | 4956/9822 [2:13:06<1:56:25,  1.44s/it] 50%|█████     | 4957/9822 [2:13:07<1:56:15,  1.43s/it] 50%|█████     | 4958/9822 [2:13:09<1:56:11,  1.43s/it] 50%|█████     | 4959/9822 [2:13:10<1:56:07,  1.43s/it] 50%|█████     | 4960/9822 [2:13:11<1:56:13,  1.43s/it] 51%|█████     | 4961/9822 [2:13:13<1:56:30,  1.44s/it] 51%|█████     | 4962/9822 [2:13:14<1:56:32,  1.44s/it] 51%|█████     | 4963/9822 [2:13:16<1:56:17,  1.44s/it] 51%|█████     | 4964/9822 [2:13:17<1:56:16,  1.44s/it] 51%|█████     | 4965/9822 [2:13:19<1:56:15,  1.44s/it] 51%|█████     | 4966/9822 [2:13:20<1:56:07,  1.43s/it] 51%|█████     | 4967/9822 [2:13:22<1:58:05,  1.46s/it] 51%|█████     | 4968/9822 [2:13:23<1:57:33,  1.45s/it] 51%|█████     | 4969/9822 [2:13:24<1:57:13,  1.45s/it] 51%|█████     | 4970/9822 [2:13:26<1:56:59,  1.45s/it] 51%|█████     | 4971/9822 [2:13:27<1:56:33,  1.44s/it] 51%|█████     | 4972/9822 [2:13:29<1:56:22,  1.44s/it] 51%|█████     | 4973/9822 [2:13:30<1:56:12,  1.44s/it] 51%|█████     | 4974/9822 [2:13:32<1:56:19,  1.44s/it] 51%|█████     | 4975/9822 [2:13:33<1:56:19,  1.44s/it] 51%|█████     | 4976/9822 [2:13:34<1:56:11,  1.44s/it] 51%|█████     | 4977/9822 [2:13:36<1:55:57,  1.44s/it] 51%|█████     | 4978/9822 [2:13:37<1:55:34,  1.43s/it] 51%|█████     | 4979/9822 [2:13:39<1:55:29,  1.43s/it] 51%|█████     | 4980/9822 [2:13:40<1:55:27,  1.43s/it] 51%|█████     | 4981/9822 [2:13:42<1:55:33,  1.43s/it] 51%|█████     | 4982/9822 [2:13:43<1:55:21,  1.43s/it] 51%|█████     | 4983/9822 [2:13:44<1:55:16,  1.43s/it] 51%|█████     | 4984/9822 [2:13:46<1:55:21,  1.43s/it] 51%|█████     | 4985/9822 [2:13:47<1:55:13,  1.43s/it] 51%|█████     | 4986/9822 [2:13:49<1:55:19,  1.43s/it] 51%|█████     | 4987/9822 [2:13:50<1:55:37,  1.43s/it] 51%|█████     | 4988/9822 [2:13:52<1:54:26,  1.42s/it] 51%|█████     | 4989/9822 [2:13:53<1:54:45,  1.42s/it] 51%|█████     | 4990/9822 [2:13:54<1:55:03,  1.43s/it] 51%|█████     | 4991/9822 [2:13:56<1:55:34,  1.44s/it] 51%|█████     | 4992/9822 [2:13:57<1:57:19,  1.46s/it] 51%|█████     | 4993/9822 [2:13:59<1:56:35,  1.45s/it] 51%|█████     | 4994/9822 [2:14:00<1:56:06,  1.44s/it] 51%|█████     | 4995/9822 [2:14:02<1:55:55,  1.44s/it] 51%|█████     | 4996/9822 [2:14:03<1:55:36,  1.44s/it] 51%|█████     | 4997/9822 [2:14:05<1:55:21,  1.43s/it] 51%|█████     | 4998/9822 [2:14:06<1:55:04,  1.43s/it] 51%|█████     | 4999/9822 [2:14:07<1:55:07,  1.43s/it] 51%|█████     | 5000/9822 [2:14:09<1:55:14,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:01:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:01:56 - INFO - __main__ - ***** test Results*****
01/08/2024 00:01:56 - INFO - __main__ -   Training step = 5000
01/08/2024 00:01:56 - INFO - __main__ -  test_accuracy:0.8759150805270863 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:02:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:02:02 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:02:02 - INFO - __main__ -   Training step = 5000
01/08/2024 00:02:02 - INFO - __main__ -  eval_accuracy:0.8564628341266936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:02:07 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:02:07 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:02:07 - INFO - __main__ -   Training step = 5000
01/08/2024 00:02:07 - INFO - __main__ -  eval_accuracy:0.9055291102160381 
 51%|█████     | 5001/9822 [2:14:27<8:43:05,  6.51s/it] 51%|█████     | 5002/9822 [2:14:29<6:40:55,  4.99s/it] 51%|█████     | 5003/9822 [2:14:30<5:15:17,  3.93s/it] 51%|█████     | 5004/9822 [2:14:32<4:15:20,  3.18s/it] 51%|█████     | 5005/9822 [2:14:33<3:33:01,  2.65s/it] 51%|█████     | 5006/9822 [2:14:34<3:03:40,  2.29s/it] 51%|█████     | 5007/9822 [2:14:36<2:42:49,  2.03s/it] 51%|█████     | 5008/9822 [2:14:37<2:28:29,  1.85s/it] 51%|█████     | 5009/9822 [2:14:39<2:18:32,  1.73s/it] 51%|█████     | 5010/9822 [2:14:40<2:11:15,  1.64s/it] 51%|█████     | 5011/9822 [2:14:42<2:06:30,  1.58s/it] 51%|█████     | 5012/9822 [2:14:43<2:02:49,  1.53s/it] 51%|█████     | 5013/9822 [2:14:44<2:00:39,  1.51s/it] 51%|█████     | 5014/9822 [2:14:46<1:59:04,  1.49s/it] 51%|█████     | 5015/9822 [2:14:47<1:57:47,  1.47s/it] 51%|█████     | 5016/9822 [2:14:49<1:57:12,  1.46s/it] 51%|█████     | 5017/9822 [2:14:50<1:56:23,  1.45s/it] 51%|█████     | 5018/9822 [2:14:52<1:57:48,  1.47s/it] 51%|█████     | 5019/9822 [2:14:53<1:56:51,  1.46s/it] 51%|█████     | 5020/9822 [2:14:55<1:56:18,  1.45s/it] 51%|█████     | 5021/9822 [2:14:56<1:55:52,  1.45s/it] 51%|█████     | 5022/9822 [2:14:57<1:55:39,  1.45s/it] 51%|█████     | 5023/9822 [2:14:59<1:55:34,  1.44s/it] 51%|█████     | 5024/9822 [2:15:00<1:55:37,  1.45s/it] 51%|█████     | 5025/9822 [2:15:02<1:55:42,  1.45s/it] 51%|█████     | 5026/9822 [2:15:03<1:55:39,  1.45s/it] 51%|█████     | 5027/9822 [2:15:05<1:55:04,  1.44s/it] 51%|█████     | 5028/9822 [2:15:06<1:55:04,  1.44s/it] 51%|█████     | 5029/9822 [2:15:08<1:54:48,  1.44s/it] 51%|█████     | 5030/9822 [2:15:09<1:55:03,  1.44s/it] 51%|█████     | 5031/9822 [2:15:10<1:55:01,  1.44s/it] 51%|█████     | 5032/9822 [2:15:12<1:54:54,  1.44s/it] 51%|█████     | 5033/9822 [2:15:13<1:54:54,  1.44s/it] 51%|█████▏    | 5034/9822 [2:15:15<1:54:59,  1.44s/it] 51%|█████▏    | 5035/9822 [2:15:16<1:55:12,  1.44s/it] 51%|█████▏    | 5036/9822 [2:15:18<1:54:51,  1.44s/it] 51%|█████▏    | 5037/9822 [2:15:19<1:54:32,  1.44s/it] 51%|█████▏    | 5038/9822 [2:15:21<1:54:07,  1.43s/it] 51%|█████▏    | 5039/9822 [2:15:22<1:54:31,  1.44s/it] 51%|█████▏    | 5040/9822 [2:15:23<1:54:34,  1.44s/it] 51%|█████▏    | 5041/9822 [2:15:25<1:54:51,  1.44s/it] 51%|█████▏    | 5042/9822 [2:15:26<1:55:19,  1.45s/it] 51%|█████▏    | 5043/9822 [2:15:28<1:54:52,  1.44s/it] 51%|█████▏    | 5044/9822 [2:15:29<1:54:51,  1.44s/it] 51%|█████▏    | 5045/9822 [2:15:31<1:54:41,  1.44s/it] 51%|█████▏    | 5046/9822 [2:15:32<1:54:46,  1.44s/it] 51%|█████▏    | 5047/9822 [2:15:34<1:54:46,  1.44s/it] 51%|█████▏    | 5048/9822 [2:15:35<1:54:38,  1.44s/it] 51%|█████▏    | 5049/9822 [2:15:36<1:54:38,  1.44s/it] 51%|█████▏    | 5050/9822 [2:15:38<1:56:43,  1.47s/it] 51%|█████▏    | 5051/9822 [2:15:39<1:56:02,  1.46s/it] 51%|█████▏    | 5052/9822 [2:15:41<1:55:33,  1.45s/it] 51%|█████▏    | 5053/9822 [2:15:42<1:55:13,  1.45s/it] 51%|█████▏    | 5054/9822 [2:15:44<1:54:56,  1.45s/it] 51%|█████▏    | 5055/9822 [2:15:45<1:54:52,  1.45s/it] 51%|█████▏    | 5056/9822 [2:15:47<1:54:41,  1.44s/it] 51%|█████▏    | 5057/9822 [2:15:48<1:54:18,  1.44s/it] 51%|█████▏    | 5058/9822 [2:15:49<1:54:11,  1.44s/it] 52%|█████▏    | 5059/9822 [2:15:51<1:53:54,  1.43s/it] 52%|█████▏    | 5060/9822 [2:15:52<1:53:46,  1.43s/it] 52%|█████▏    | 5061/9822 [2:15:54<1:53:29,  1.43s/it] 52%|█████▏    | 5062/9822 [2:15:55<1:53:42,  1.43s/it] 52%|█████▏    | 5063/9822 [2:15:57<1:54:11,  1.44s/it] 52%|█████▏    | 5064/9822 [2:15:58<1:54:06,  1.44s/it] 52%|█████▏    | 5065/9822 [2:15:59<1:54:13,  1.44s/it] 52%|█████▏    | 5066/9822 [2:16:01<1:54:20,  1.44s/it] 52%|█████▏    | 5067/9822 [2:16:02<1:54:27,  1.44s/it] 52%|█████▏    | 5068/9822 [2:16:04<1:54:22,  1.44s/it] 52%|█████▏    | 5069/9822 [2:16:05<1:54:29,  1.45s/it] 52%|█████▏    | 5070/9822 [2:16:07<1:53:55,  1.44s/it] 52%|█████▏    | 5071/9822 [2:16:08<1:53:32,  1.43s/it] 52%|█████▏    | 5072/9822 [2:16:10<1:53:09,  1.43s/it] 52%|█████▏    | 5073/9822 [2:16:11<1:53:08,  1.43s/it] 52%|█████▏    | 5074/9822 [2:16:12<1:52:23,  1.42s/it] 52%|█████▏    | 5075/9822 [2:16:14<1:52:56,  1.43s/it] 52%|█████▏    | 5076/9822 [2:16:15<1:53:43,  1.44s/it] 52%|█████▏    | 5077/9822 [2:16:17<1:53:35,  1.44s/it] 52%|█████▏    | 5078/9822 [2:16:18<1:53:38,  1.44s/it] 52%|█████▏    | 5079/9822 [2:16:20<1:53:26,  1.43s/it] 52%|█████▏    | 5080/9822 [2:16:21<1:53:26,  1.44s/it] 52%|█████▏    | 5081/9822 [2:16:22<1:53:35,  1.44s/it] 52%|█████▏    | 5082/9822 [2:16:24<1:55:38,  1.46s/it] 52%|█████▏    | 5083/9822 [2:16:25<1:54:51,  1.45s/it] 52%|█████▏    | 5084/9822 [2:16:27<1:54:10,  1.45s/it] 52%|█████▏    | 5085/9822 [2:16:28<1:53:50,  1.44s/it] 52%|█████▏    | 5086/9822 [2:16:30<1:53:34,  1.44s/it] 52%|█████▏    | 5087/9822 [2:16:31<1:53:58,  1.44s/it] 52%|█████▏    | 5088/9822 [2:16:33<1:53:50,  1.44s/it] 52%|█████▏    | 5089/9822 [2:16:34<1:54:02,  1.45s/it] 52%|█████▏    | 5090/9822 [2:16:35<1:54:14,  1.45s/it] 52%|█████▏    | 5091/9822 [2:16:37<1:53:54,  1.44s/it] 52%|█████▏    | 5092/9822 [2:16:38<1:54:05,  1.45s/it] 52%|█████▏    | 5093/9822 [2:16:40<1:53:50,  1.44s/it] 52%|█████▏    | 5094/9822 [2:16:41<1:53:30,  1.44s/it] 52%|█████▏    | 5095/9822 [2:16:43<1:53:05,  1.44s/it] 52%|█████▏    | 5096/9822 [2:16:44<1:52:51,  1.43s/it] 52%|█████▏    | 5097/9822 [2:16:46<1:52:52,  1.43s/it] 52%|█████▏    | 5098/9822 [2:16:47<1:52:28,  1.43s/it] 52%|█████▏    | 5099/9822 [2:16:48<1:52:35,  1.43s/it] 52%|█████▏    | 5100/9822 [2:16:50<1:52:42,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:04:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:04:37 - INFO - __main__ - ***** test Results*****
01/08/2024 00:04:37 - INFO - __main__ -   Training step = 5100
01/08/2024 00:04:37 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:04:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:04:43 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:04:43 - INFO - __main__ -   Training step = 5100
01/08/2024 00:04:43 - INFO - __main__ -  eval_accuracy:0.8564628341266936 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:04:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:04:48 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:04:48 - INFO - __main__ -   Training step = 5100
01/08/2024 00:04:48 - INFO - __main__ -  eval_accuracy:0.9047967777370927 
 52%|█████▏    | 5101/9822 [2:17:08<8:32:25,  6.51s/it] 52%|█████▏    | 5102/9822 [2:17:10<6:32:25,  4.99s/it] 52%|█████▏    | 5103/9822 [2:17:11<5:08:22,  3.92s/it] 52%|█████▏    | 5104/9822 [2:17:12<4:09:33,  3.17s/it] 52%|█████▏    | 5105/9822 [2:17:14<3:28:31,  2.65s/it] 52%|█████▏    | 5106/9822 [2:17:15<2:59:49,  2.29s/it] 52%|█████▏    | 5107/9822 [2:17:17<2:39:45,  2.03s/it] 52%|█████▏    | 5108/9822 [2:17:18<2:26:14,  1.86s/it] 52%|█████▏    | 5109/9822 [2:17:20<2:18:19,  1.76s/it] 52%|█████▏    | 5110/9822 [2:17:21<2:10:53,  1.67s/it] 52%|█████▏    | 5111/9822 [2:17:23<2:05:27,  1.60s/it] 52%|█████▏    | 5112/9822 [2:17:24<2:01:29,  1.55s/it] 52%|█████▏    | 5113/9822 [2:17:26<1:59:09,  1.52s/it] 52%|█████▏    | 5114/9822 [2:17:27<1:56:55,  1.49s/it] 52%|█████▏    | 5115/9822 [2:17:28<1:55:35,  1.47s/it] 52%|█████▏    | 5116/9822 [2:17:30<1:54:36,  1.46s/it] 52%|█████▏    | 5117/9822 [2:17:31<1:53:58,  1.45s/it] 52%|█████▏    | 5118/9822 [2:17:33<1:53:38,  1.45s/it] 52%|█████▏    | 5119/9822 [2:17:34<1:53:15,  1.45s/it] 52%|█████▏    | 5120/9822 [2:17:36<1:53:03,  1.44s/it] 52%|█████▏    | 5121/9822 [2:17:37<1:53:23,  1.45s/it] 52%|█████▏    | 5122/9822 [2:17:39<1:54:07,  1.46s/it] 52%|█████▏    | 5123/9822 [2:17:40<1:53:36,  1.45s/it] 52%|█████▏    | 5124/9822 [2:17:41<1:53:27,  1.45s/it] 52%|█████▏    | 5125/9822 [2:17:43<1:53:40,  1.45s/it] 52%|█████▏    | 5126/9822 [2:17:44<1:53:11,  1.45s/it] 52%|█████▏    | 5127/9822 [2:17:46<1:52:56,  1.44s/it] 52%|█████▏    | 5128/9822 [2:17:47<1:53:10,  1.45s/it] 52%|█████▏    | 5129/9822 [2:17:49<1:52:46,  1.44s/it] 52%|█████▏    | 5130/9822 [2:17:50<1:52:35,  1.44s/it] 52%|█████▏    | 5131/9822 [2:17:51<1:52:33,  1.44s/it] 52%|█████▏    | 5132/9822 [2:17:53<1:52:20,  1.44s/it] 52%|█████▏    | 5133/9822 [2:17:54<1:52:15,  1.44s/it] 52%|█████▏    | 5134/9822 [2:17:56<1:52:06,  1.43s/it] 52%|█████▏    | 5135/9822 [2:17:57<1:52:13,  1.44s/it] 52%|█████▏    | 5136/9822 [2:17:59<1:52:16,  1.44s/it] 52%|█████▏    | 5137/9822 [2:18:00<1:52:18,  1.44s/it] 52%|█████▏    | 5138/9822 [2:18:02<1:52:07,  1.44s/it] 52%|█████▏    | 5139/9822 [2:18:03<1:54:07,  1.46s/it] 52%|█████▏    | 5140/9822 [2:18:05<1:53:20,  1.45s/it] 52%|█████▏    | 5141/9822 [2:18:06<1:53:02,  1.45s/it] 52%|█████▏    | 5142/9822 [2:18:07<1:52:32,  1.44s/it] 52%|█████▏    | 5143/9822 [2:18:09<1:52:13,  1.44s/it] 52%|█████▏    | 5144/9822 [2:18:10<1:52:38,  1.44s/it] 52%|█████▏    | 5145/9822 [2:18:12<1:52:24,  1.44s/it] 52%|█████▏    | 5146/9822 [2:18:13<1:51:56,  1.44s/it] 52%|█████▏    | 5147/9822 [2:18:15<1:51:41,  1.43s/it] 52%|█████▏    | 5148/9822 [2:18:16<1:51:38,  1.43s/it] 52%|█████▏    | 5149/9822 [2:18:17<1:51:47,  1.44s/it] 52%|█████▏    | 5150/9822 [2:18:19<1:51:51,  1.44s/it] 52%|█████▏    | 5151/9822 [2:18:20<1:52:00,  1.44s/it] 52%|█████▏    | 5152/9822 [2:18:22<1:51:54,  1.44s/it] 52%|█████▏    | 5153/9822 [2:18:23<1:51:56,  1.44s/it] 52%|█████▏    | 5154/9822 [2:18:25<1:51:48,  1.44s/it] 52%|█████▏    | 5155/9822 [2:18:26<1:51:48,  1.44s/it] 52%|█████▏    | 5156/9822 [2:18:27<1:51:53,  1.44s/it] 53%|█████▎    | 5157/9822 [2:18:29<1:51:33,  1.43s/it] 53%|█████▎    | 5158/9822 [2:18:30<1:51:31,  1.43s/it] 53%|█████▎    | 5159/9822 [2:18:32<1:51:33,  1.44s/it] 53%|█████▎    | 5160/9822 [2:18:33<1:50:32,  1.42s/it] 53%|█████▎    | 5161/9822 [2:18:35<1:50:50,  1.43s/it] 53%|█████▎    | 5162/9822 [2:18:36<1:51:08,  1.43s/it] 53%|█████▎    | 5163/9822 [2:18:37<1:51:23,  1.43s/it] 53%|█████▎    | 5164/9822 [2:18:39<1:51:26,  1.44s/it] 53%|█████▎    | 5165/9822 [2:18:40<1:51:19,  1.43s/it] 53%|█████▎    | 5166/9822 [2:18:42<1:51:13,  1.43s/it] 53%|█████▎    | 5167/9822 [2:18:43<1:51:38,  1.44s/it] 53%|█████▎    | 5168/9822 [2:18:45<1:51:27,  1.44s/it] 53%|█████▎    | 5169/9822 [2:18:46<1:51:17,  1.43s/it] 53%|█████▎    | 5170/9822 [2:18:48<1:51:03,  1.43s/it] 53%|█████▎    | 5171/9822 [2:18:49<1:52:58,  1.46s/it] 53%|█████▎    | 5172/9822 [2:18:50<1:52:18,  1.45s/it] 53%|█████▎    | 5173/9822 [2:18:52<1:51:58,  1.45s/it] 53%|█████▎    | 5174/9822 [2:18:53<1:51:30,  1.44s/it] 53%|█████▎    | 5175/9822 [2:18:55<1:51:34,  1.44s/it] 53%|█████▎    | 5176/9822 [2:18:56<1:51:29,  1.44s/it] 53%|█████▎    | 5177/9822 [2:18:58<1:51:14,  1.44s/it] 53%|█████▎    | 5178/9822 [2:18:59<1:50:59,  1.43s/it] 53%|█████▎    | 5179/9822 [2:19:01<1:51:01,  1.43s/it] 53%|█████▎    | 5180/9822 [2:19:02<1:51:14,  1.44s/it] 53%|█████▎    | 5181/9822 [2:19:03<1:51:01,  1.44s/it] 53%|█████▎    | 5182/9822 [2:19:05<1:50:58,  1.44s/it] 53%|█████▎    | 5183/9822 [2:19:06<1:51:20,  1.44s/it] 53%|█████▎    | 5184/9822 [2:19:08<1:51:11,  1.44s/it] 53%|█████▎    | 5185/9822 [2:19:09<1:51:29,  1.44s/it] 53%|█████▎    | 5186/9822 [2:19:11<1:51:39,  1.45s/it] 53%|█████▎    | 5187/9822 [2:19:12<1:51:40,  1.45s/it] 53%|█████▎    | 5188/9822 [2:19:14<1:51:42,  1.45s/it] 53%|█████▎    | 5189/9822 [2:19:15<1:51:28,  1.44s/it] 53%|█████▎    | 5190/9822 [2:19:16<1:51:19,  1.44s/it] 53%|█████▎    | 5191/9822 [2:19:18<1:51:10,  1.44s/it] 53%|█████▎    | 5192/9822 [2:19:19<1:50:55,  1.44s/it] 53%|█████▎    | 5193/9822 [2:19:21<1:50:41,  1.43s/it] 53%|█████▎    | 5194/9822 [2:19:22<1:50:41,  1.44s/it] 53%|█████▎    | 5195/9822 [2:19:24<1:50:19,  1.43s/it] 53%|█████▎    | 5196/9822 [2:19:25<1:50:40,  1.44s/it] 53%|█████▎    | 5197/9822 [2:19:26<1:50:33,  1.43s/it] 53%|█████▎    | 5198/9822 [2:19:28<1:50:15,  1.43s/it] 53%|█████▎    | 5199/9822 [2:19:29<1:50:02,  1.43s/it] 53%|█████▎    | 5200/9822 [2:19:31<1:49:55,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:07:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:07:18 - INFO - __main__ - ***** test Results*****
01/08/2024 00:07:18 - INFO - __main__ -   Training step = 5200
01/08/2024 00:07:18 - INFO - __main__ -  test_accuracy:0.8678623718887262 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:07:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:07:24 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:07:24 - INFO - __main__ -   Training step = 5200
01/08/2024 00:07:24 - INFO - __main__ -  eval_accuracy:0.8575613328451117 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:07:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:07:28 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:07:28 - INFO - __main__ -   Training step = 5200
01/08/2024 00:07:28 - INFO - __main__ -  eval_accuracy:0.9058952764555108 
 53%|█████▎    | 5201/9822 [2:19:49<8:21:01,  6.51s/it] 53%|█████▎    | 5202/9822 [2:19:50<6:24:11,  4.99s/it] 53%|█████▎    | 5203/9822 [2:19:52<5:03:59,  3.95s/it] 53%|█████▎    | 5204/9822 [2:19:53<4:05:55,  3.20s/it] 53%|█████▎    | 5205/9822 [2:19:55<3:25:20,  2.67s/it] 53%|█████▎    | 5206/9822 [2:19:56<2:57:05,  2.30s/it] 53%|█████▎    | 5207/9822 [2:19:58<2:37:17,  2.05s/it] 53%|█████▎    | 5208/9822 [2:19:59<2:23:33,  1.87s/it] 53%|█████▎    | 5209/9822 [2:20:01<2:13:46,  1.74s/it] 53%|█████▎    | 5210/9822 [2:20:02<2:06:32,  1.65s/it] 53%|█████▎    | 5211/9822 [2:20:04<2:01:25,  1.58s/it] 53%|█████▎    | 5212/9822 [2:20:05<1:58:00,  1.54s/it] 53%|█████▎    | 5213/9822 [2:20:06<1:55:30,  1.50s/it] 53%|█████▎    | 5214/9822 [2:20:08<1:53:41,  1.48s/it] 53%|█████▎    | 5215/9822 [2:20:09<1:52:22,  1.46s/it] 53%|█████▎    | 5216/9822 [2:20:11<1:51:46,  1.46s/it] 53%|█████▎    | 5217/9822 [2:20:12<1:51:36,  1.45s/it] 53%|█████▎    | 5218/9822 [2:20:14<1:51:15,  1.45s/it] 53%|█████▎    | 5219/9822 [2:20:15<1:50:55,  1.45s/it] 53%|█████▎    | 5220/9822 [2:20:16<1:50:40,  1.44s/it] 53%|█████▎    | 5221/9822 [2:20:18<1:50:27,  1.44s/it] 53%|█████▎    | 5222/9822 [2:20:19<1:50:14,  1.44s/it] 53%|█████▎    | 5223/9822 [2:20:21<1:50:26,  1.44s/it] 53%|█████▎    | 5224/9822 [2:20:22<1:50:10,  1.44s/it] 53%|█████▎    | 5225/9822 [2:20:24<1:49:56,  1.44s/it] 53%|█████▎    | 5226/9822 [2:20:25<1:50:12,  1.44s/it] 53%|█████▎    | 5227/9822 [2:20:26<1:49:59,  1.44s/it] 53%|█████▎    | 5228/9822 [2:20:28<1:49:42,  1.43s/it] 53%|█████▎    | 5229/9822 [2:20:29<1:49:38,  1.43s/it] 53%|█████▎    | 5230/9822 [2:20:31<1:49:36,  1.43s/it] 53%|█████▎    | 5231/9822 [2:20:32<1:49:51,  1.44s/it] 53%|█████▎    | 5232/9822 [2:20:34<1:49:46,  1.43s/it] 53%|█████▎    | 5233/9822 [2:20:35<1:50:02,  1.44s/it] 53%|█████▎    | 5234/9822 [2:20:37<1:49:38,  1.43s/it] 53%|█████▎    | 5235/9822 [2:20:38<1:51:25,  1.46s/it] 53%|█████▎    | 5236/9822 [2:20:40<1:51:20,  1.46s/it] 53%|█████▎    | 5237/9822 [2:20:41<1:50:40,  1.45s/it] 53%|█████▎    | 5238/9822 [2:20:42<1:50:18,  1.44s/it] 53%|█████▎    | 5239/9822 [2:20:44<1:49:58,  1.44s/it] 53%|█████▎    | 5240/9822 [2:20:45<1:49:45,  1.44s/it] 53%|█████▎    | 5241/9822 [2:20:47<1:49:41,  1.44s/it] 53%|█████▎    | 5242/9822 [2:20:48<1:49:28,  1.43s/it] 53%|█████▎    | 5243/9822 [2:20:50<1:49:38,  1.44s/it] 53%|█████▎    | 5244/9822 [2:20:51<1:49:40,  1.44s/it] 53%|█████▎    | 5245/9822 [2:20:52<1:49:23,  1.43s/it] 53%|█████▎    | 5246/9822 [2:20:54<1:48:09,  1.42s/it] 53%|█████▎    | 5247/9822 [2:20:55<1:48:27,  1.42s/it] 53%|█████▎    | 5248/9822 [2:20:57<1:48:42,  1.43s/it] 53%|█████▎    | 5249/9822 [2:20:58<1:48:52,  1.43s/it] 53%|█████▎    | 5250/9822 [2:21:00<1:49:12,  1.43s/it] 53%|█████▎    | 5251/9822 [2:21:01<1:48:55,  1.43s/it] 53%|█████▎    | 5252/9822 [2:21:02<1:48:59,  1.43s/it] 53%|█████▎    | 5253/9822 [2:21:04<1:48:49,  1.43s/it] 53%|█████▎    | 5254/9822 [2:21:05<1:48:47,  1.43s/it] 54%|█████▎    | 5255/9822 [2:21:07<1:48:51,  1.43s/it] 54%|█████▎    | 5256/9822 [2:21:08<1:48:37,  1.43s/it] 54%|█████▎    | 5257/9822 [2:21:10<1:48:35,  1.43s/it] 54%|█████▎    | 5258/9822 [2:21:11<1:48:35,  1.43s/it] 54%|█████▎    | 5259/9822 [2:21:12<1:49:00,  1.43s/it] 54%|█████▎    | 5260/9822 [2:21:14<1:50:41,  1.46s/it] 54%|█████▎    | 5261/9822 [2:21:15<1:50:14,  1.45s/it] 54%|█████▎    | 5262/9822 [2:21:17<1:49:40,  1.44s/it] 54%|█████▎    | 5263/9822 [2:21:18<1:49:32,  1.44s/it] 54%|█████▎    | 5264/9822 [2:21:20<1:49:23,  1.44s/it] 54%|█████▎    | 5265/9822 [2:21:21<1:49:09,  1.44s/it] 54%|█████▎    | 5266/9822 [2:21:22<1:48:54,  1.43s/it] 54%|█████▎    | 5267/9822 [2:21:24<1:48:46,  1.43s/it] 54%|█████▎    | 5268/9822 [2:21:25<1:48:40,  1.43s/it] 54%|█████▎    | 5269/9822 [2:21:27<1:49:07,  1.44s/it] 54%|█████▎    | 5270/9822 [2:21:28<1:49:12,  1.44s/it] 54%|█████▎    | 5271/9822 [2:21:30<1:48:53,  1.44s/it] 54%|█████▎    | 5272/9822 [2:21:31<1:48:45,  1.43s/it] 54%|█████▎    | 5273/9822 [2:21:33<1:48:50,  1.44s/it] 54%|█████▎    | 5274/9822 [2:21:34<1:48:38,  1.43s/it] 54%|█████▎    | 5275/9822 [2:21:35<1:48:53,  1.44s/it] 54%|█████▎    | 5276/9822 [2:21:37<1:48:52,  1.44s/it] 54%|█████▎    | 5277/9822 [2:21:38<1:48:40,  1.43s/it] 54%|█████▎    | 5278/9822 [2:21:40<1:48:25,  1.43s/it] 54%|█████▎    | 5279/9822 [2:21:41<1:48:34,  1.43s/it] 54%|█████▍    | 5280/9822 [2:21:43<1:48:31,  1.43s/it] 54%|█████▍    | 5281/9822 [2:21:44<1:48:55,  1.44s/it] 54%|█████▍    | 5282/9822 [2:21:45<1:48:54,  1.44s/it] 54%|█████▍    | 5283/9822 [2:21:47<1:48:43,  1.44s/it] 54%|█████▍    | 5284/9822 [2:21:48<1:48:45,  1.44s/it] 54%|█████▍    | 5285/9822 [2:21:50<1:48:36,  1.44s/it] 54%|█████▍    | 5286/9822 [2:21:51<1:48:46,  1.44s/it] 54%|█████▍    | 5287/9822 [2:21:53<1:48:59,  1.44s/it] 54%|█████▍    | 5288/9822 [2:21:54<1:48:44,  1.44s/it] 54%|█████▍    | 5289/9822 [2:21:56<1:48:38,  1.44s/it] 54%|█████▍    | 5290/9822 [2:21:57<1:48:30,  1.44s/it] 54%|█████▍    | 5291/9822 [2:21:58<1:48:48,  1.44s/it] 54%|█████▍    | 5292/9822 [2:22:00<1:50:30,  1.46s/it] 54%|█████▍    | 5293/9822 [2:22:01<1:49:55,  1.46s/it] 54%|█████▍    | 5294/9822 [2:22:03<1:49:27,  1.45s/it] 54%|█████▍    | 5295/9822 [2:22:04<1:49:04,  1.45s/it] 54%|█████▍    | 5296/9822 [2:22:06<1:48:45,  1.44s/it] 54%|█████▍    | 5297/9822 [2:22:07<1:48:33,  1.44s/it] 54%|█████▍    | 5298/9822 [2:22:09<1:48:25,  1.44s/it] 54%|█████▍    | 5299/9822 [2:22:10<1:48:22,  1.44s/it] 54%|█████▍    | 5300/9822 [2:22:11<1:48:19,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:09:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:09:58 - INFO - __main__ - ***** test Results*****
01/08/2024 00:09:58 - INFO - __main__ -   Training step = 5300
01/08/2024 00:09:58 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:10:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:10:04 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:10:04 - INFO - __main__ -   Training step = 5300
01/08/2024 00:10:04 - INFO - __main__ -  eval_accuracy:0.8575613328451117 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:10:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:10:09 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:10:09 - INFO - __main__ -   Training step = 5300
01/08/2024 00:10:09 - INFO - __main__ -  eval_accuracy:0.9055291102160381 
 54%|█████▍    | 5301/9822 [2:22:30<8:10:49,  6.51s/it] 54%|█████▍    | 5302/9822 [2:22:31<6:15:51,  4.99s/it] 54%|█████▍    | 5303/9822 [2:22:33<4:55:08,  3.92s/it] 54%|█████▍    | 5304/9822 [2:22:34<3:59:05,  3.18s/it] 54%|█████▍    | 5305/9822 [2:22:36<3:19:43,  2.65s/it] 54%|█████▍    | 5306/9822 [2:22:37<2:52:00,  2.29s/it] 54%|█████▍    | 5307/9822 [2:22:38<2:32:39,  2.03s/it] 54%|█████▍    | 5308/9822 [2:22:40<2:19:35,  1.86s/it] 54%|█████▍    | 5309/9822 [2:22:41<2:10:14,  1.73s/it] 54%|█████▍    | 5310/9822 [2:22:43<2:03:26,  1.64s/it] 54%|█████▍    | 5311/9822 [2:22:44<1:58:37,  1.58s/it] 54%|█████▍    | 5312/9822 [2:22:46<1:55:23,  1.54s/it] 54%|█████▍    | 5313/9822 [2:22:47<1:53:00,  1.50s/it] 54%|█████▍    | 5314/9822 [2:22:48<1:51:30,  1.48s/it] 54%|█████▍    | 5315/9822 [2:22:50<1:50:13,  1.47s/it] 54%|█████▍    | 5316/9822 [2:22:51<1:49:16,  1.45s/it] 54%|█████▍    | 5317/9822 [2:22:53<1:48:46,  1.45s/it] 54%|█████▍    | 5318/9822 [2:22:54<1:48:27,  1.44s/it] 54%|█████▍    | 5319/9822 [2:22:56<1:47:59,  1.44s/it] 54%|█████▍    | 5320/9822 [2:22:57<1:47:56,  1.44s/it] 54%|█████▍    | 5321/9822 [2:22:58<1:47:58,  1.44s/it] 54%|█████▍    | 5322/9822 [2:23:00<1:47:57,  1.44s/it] 54%|█████▍    | 5323/9822 [2:23:01<1:47:52,  1.44s/it] 54%|█████▍    | 5324/9822 [2:23:03<1:47:54,  1.44s/it] 54%|█████▍    | 5325/9822 [2:23:04<1:48:14,  1.44s/it] 54%|█████▍    | 5326/9822 [2:23:06<1:48:12,  1.44s/it] 54%|█████▍    | 5327/9822 [2:23:07<1:48:10,  1.44s/it] 54%|█████▍    | 5328/9822 [2:23:09<1:48:52,  1.45s/it] 54%|█████▍    | 5329/9822 [2:23:10<1:48:14,  1.45s/it] 54%|█████▍    | 5330/9822 [2:23:12<1:49:52,  1.47s/it] 54%|█████▍    | 5331/9822 [2:23:13<1:49:20,  1.46s/it] 54%|█████▍    | 5332/9822 [2:23:14<1:47:46,  1.44s/it] 54%|█████▍    | 5333/9822 [2:23:16<1:47:51,  1.44s/it] 54%|█████▍    | 5334/9822 [2:23:17<1:48:04,  1.44s/it] 54%|█████▍    | 5335/9822 [2:23:19<1:47:30,  1.44s/it] 54%|█████▍    | 5336/9822 [2:23:20<1:47:35,  1.44s/it] 54%|█████▍    | 5337/9822 [2:23:22<1:47:25,  1.44s/it] 54%|█████▍    | 5338/9822 [2:23:23<1:47:20,  1.44s/it] 54%|█████▍    | 5339/9822 [2:23:24<1:47:46,  1.44s/it] 54%|█████▍    | 5340/9822 [2:23:26<1:48:01,  1.45s/it] 54%|█████▍    | 5341/9822 [2:23:27<1:47:42,  1.44s/it] 54%|█████▍    | 5342/9822 [2:23:29<1:47:31,  1.44s/it] 54%|█████▍    | 5343/9822 [2:23:30<1:47:29,  1.44s/it] 54%|█████▍    | 5344/9822 [2:23:32<1:47:05,  1.43s/it] 54%|█████▍    | 5345/9822 [2:23:33<1:46:52,  1.43s/it] 54%|█████▍    | 5346/9822 [2:23:35<1:46:49,  1.43s/it] 54%|█████▍    | 5347/9822 [2:23:36<1:46:57,  1.43s/it] 54%|█████▍    | 5348/9822 [2:23:37<1:46:49,  1.43s/it] 54%|█████▍    | 5349/9822 [2:23:39<1:46:35,  1.43s/it] 54%|█████▍    | 5350/9822 [2:23:40<1:46:34,  1.43s/it] 54%|█████▍    | 5351/9822 [2:23:42<1:46:46,  1.43s/it] 54%|█████▍    | 5352/9822 [2:23:43<1:46:37,  1.43s/it] 55%|█████▍    | 5353/9822 [2:23:45<1:46:53,  1.44s/it] 55%|█████▍    | 5354/9822 [2:23:46<1:46:48,  1.43s/it] 55%|█████▍    | 5355/9822 [2:23:47<1:46:39,  1.43s/it] 55%|█████▍    | 5356/9822 [2:23:49<1:46:38,  1.43s/it] 55%|█████▍    | 5357/9822 [2:23:50<1:46:28,  1.43s/it] 55%|█████▍    | 5358/9822 [2:23:52<1:46:36,  1.43s/it] 55%|█████▍    | 5359/9822 [2:23:53<1:47:25,  1.44s/it] 55%|█████▍    | 5360/9822 [2:23:55<1:47:17,  1.44s/it] 55%|█████▍    | 5361/9822 [2:23:56<1:47:00,  1.44s/it] 55%|█████▍    | 5362/9822 [2:23:58<1:48:58,  1.47s/it] 55%|█████▍    | 5363/9822 [2:23:59<1:47:54,  1.45s/it] 55%|█████▍    | 5364/9822 [2:24:00<1:47:44,  1.45s/it] 55%|█████▍    | 5365/9822 [2:24:02<1:47:15,  1.44s/it] 55%|█████▍    | 5366/9822 [2:24:03<1:46:40,  1.44s/it] 55%|█████▍    | 5367/9822 [2:24:05<1:46:25,  1.43s/it] 55%|█████▍    | 5368/9822 [2:24:06<1:46:18,  1.43s/it] 55%|█████▍    | 5369/9822 [2:24:08<1:46:32,  1.44s/it] 55%|█████▍    | 5370/9822 [2:24:09<1:46:27,  1.43s/it] 55%|█████▍    | 5371/9822 [2:24:10<1:46:23,  1.43s/it] 55%|█████▍    | 5372/9822 [2:24:12<1:46:19,  1.43s/it] 55%|█████▍    | 5373/9822 [2:24:13<1:46:22,  1.43s/it] 55%|█████▍    | 5374/9822 [2:24:15<1:46:15,  1.43s/it] 55%|█████▍    | 5375/9822 [2:24:16<1:46:04,  1.43s/it] 55%|█████▍    | 5376/9822 [2:24:18<1:45:59,  1.43s/it] 55%|█████▍    | 5377/9822 [2:24:19<1:46:02,  1.43s/it] 55%|█████▍    | 5378/9822 [2:24:20<1:46:13,  1.43s/it] 55%|█████▍    | 5379/9822 [2:24:22<1:45:58,  1.43s/it] 55%|█████▍    | 5380/9822 [2:24:23<1:45:48,  1.43s/it] 55%|█████▍    | 5381/9822 [2:24:25<1:45:46,  1.43s/it] 55%|█████▍    | 5382/9822 [2:24:26<1:45:59,  1.43s/it] 55%|█████▍    | 5383/9822 [2:24:28<1:46:01,  1.43s/it] 55%|█████▍    | 5384/9822 [2:24:29<1:46:21,  1.44s/it] 55%|█████▍    | 5385/9822 [2:24:31<1:46:07,  1.44s/it] 55%|█████▍    | 5386/9822 [2:24:32<1:46:00,  1.43s/it] 55%|█████▍    | 5387/9822 [2:24:33<1:47:54,  1.46s/it] 55%|█████▍    | 5388/9822 [2:24:35<1:47:19,  1.45s/it] 55%|█████▍    | 5389/9822 [2:24:36<1:46:52,  1.45s/it] 55%|█████▍    | 5390/9822 [2:24:38<1:46:29,  1.44s/it] 55%|█████▍    | 5391/9822 [2:24:39<1:46:28,  1.44s/it] 55%|█████▍    | 5392/9822 [2:24:41<1:46:34,  1.44s/it] 55%|█████▍    | 5393/9822 [2:24:42<1:46:48,  1.45s/it] 55%|█████▍    | 5394/9822 [2:24:44<1:46:39,  1.45s/it] 55%|█████▍    | 5395/9822 [2:24:45<1:46:13,  1.44s/it] 55%|█████▍    | 5396/9822 [2:24:46<1:46:03,  1.44s/it] 55%|█████▍    | 5397/9822 [2:24:48<1:45:55,  1.44s/it] 55%|█████▍    | 5398/9822 [2:24:49<1:46:22,  1.44s/it] 55%|█████▍    | 5399/9822 [2:24:51<1:46:11,  1.44s/it] 55%|█████▍    | 5400/9822 [2:24:52<1:46:00,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1290, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:12:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:12:39 - INFO - __main__ - ***** test Results*****
01/08/2024 00:12:39 - INFO - __main__ -   Training step = 5400
01/08/2024 00:12:39 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:12:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:12:45 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:12:45 - INFO - __main__ -   Training step = 5400
01/08/2024 00:12:45 - INFO - __main__ -  eval_accuracy:0.8568290003661663 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:12:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:12:50 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:12:50 - INFO - __main__ -   Training step = 5400
01/08/2024 00:12:50 - INFO - __main__ -  eval_accuracy:0.9088246063712926 
 55%|█████▍    | 5401/9822 [2:25:11<7:59:59,  6.51s/it] 55%|█████▍    | 5402/9822 [2:25:12<6:07:39,  4.99s/it] 55%|█████▌    | 5403/9822 [2:25:13<4:49:02,  3.92s/it] 55%|█████▌    | 5404/9822 [2:25:15<3:53:49,  3.18s/it] 55%|█████▌    | 5405/9822 [2:25:16<3:15:17,  2.65s/it] 55%|█████▌    | 5406/9822 [2:25:18<2:48:40,  2.29s/it] 55%|█████▌    | 5407/9822 [2:25:19<2:29:31,  2.03s/it] 55%|█████▌    | 5408/9822 [2:25:21<2:16:26,  1.85s/it] 55%|█████▌    | 5409/9822 [2:25:22<2:07:01,  1.73s/it] 55%|█████▌    | 5410/9822 [2:25:23<2:00:31,  1.64s/it] 55%|█████▌    | 5411/9822 [2:25:25<1:55:58,  1.58s/it] 55%|█████▌    | 5412/9822 [2:25:26<1:52:37,  1.53s/it] 55%|█████▌    | 5413/9822 [2:25:28<1:50:28,  1.50s/it] 55%|█████▌    | 5414/9822 [2:25:29<1:48:50,  1.48s/it] 55%|█████▌    | 5415/9822 [2:25:31<1:47:36,  1.47s/it] 55%|█████▌    | 5416/9822 [2:25:32<1:48:41,  1.48s/it] 55%|█████▌    | 5417/9822 [2:25:34<1:47:23,  1.46s/it] 55%|█████▌    | 5418/9822 [2:25:35<1:45:34,  1.44s/it] 55%|█████▌    | 5419/9822 [2:25:36<1:45:31,  1.44s/it] 55%|█████▌    | 5420/9822 [2:25:38<1:45:12,  1.43s/it] 55%|█████▌    | 5421/9822 [2:25:39<1:44:55,  1.43s/it] 55%|█████▌    | 5422/9822 [2:25:41<1:45:01,  1.43s/it] 55%|█████▌    | 5423/9822 [2:25:42<1:44:53,  1.43s/it] 55%|█████▌    | 5424/9822 [2:25:43<1:45:10,  1.43s/it] 55%|█████▌    | 5425/9822 [2:25:45<1:45:10,  1.44s/it] 55%|█████▌    | 5426/9822 [2:25:46<1:45:01,  1.43s/it] 55%|█████▌    | 5427/9822 [2:25:48<1:45:02,  1.43s/it] 55%|█████▌    | 5428/9822 [2:25:49<1:45:00,  1.43s/it] 55%|█████▌    | 5429/9822 [2:25:51<1:45:00,  1.43s/it] 55%|█████▌    | 5430/9822 [2:25:52<1:44:48,  1.43s/it] 55%|█████▌    | 5431/9822 [2:25:54<1:44:43,  1.43s/it] 55%|█████▌    | 5432/9822 [2:25:55<1:44:36,  1.43s/it] 55%|█████▌    | 5433/9822 [2:25:56<1:45:05,  1.44s/it] 55%|█████▌    | 5434/9822 [2:25:58<1:45:14,  1.44s/it] 55%|█████▌    | 5435/9822 [2:25:59<1:45:05,  1.44s/it] 55%|█████▌    | 5436/9822 [2:26:01<1:45:00,  1.44s/it] 55%|█████▌    | 5437/9822 [2:26:02<1:44:59,  1.44s/it] 55%|█████▌    | 5438/9822 [2:26:04<1:44:54,  1.44s/it] 55%|█████▌    | 5439/9822 [2:26:05<1:45:16,  1.44s/it] 55%|█████▌    | 5440/9822 [2:26:07<1:45:50,  1.45s/it] 55%|█████▌    | 5441/9822 [2:26:08<1:45:41,  1.45s/it] 55%|█████▌    | 5442/9822 [2:26:09<1:45:26,  1.44s/it] 55%|█████▌    | 5443/9822 [2:26:11<1:45:13,  1.44s/it] 55%|█████▌    | 5444/9822 [2:26:12<1:45:09,  1.44s/it] 55%|█████▌    | 5445/9822 [2:26:14<1:45:07,  1.44s/it] 55%|█████▌    | 5446/9822 [2:26:15<1:44:53,  1.44s/it] 55%|█████▌    | 5447/9822 [2:26:17<1:44:40,  1.44s/it] 55%|█████▌    | 5448/9822 [2:26:18<1:46:25,  1.46s/it] 55%|█████▌    | 5449/9822 [2:26:20<1:45:53,  1.45s/it] 55%|█████▌    | 5450/9822 [2:26:21<1:45:32,  1.45s/it] 55%|█████▌    | 5451/9822 [2:26:22<1:45:28,  1.45s/it] 56%|█████▌    | 5452/9822 [2:26:24<1:45:11,  1.44s/it] 56%|█████▌    | 5453/9822 [2:26:25<1:44:56,  1.44s/it] 56%|█████▌    | 5454/9822 [2:26:27<1:44:50,  1.44s/it] 56%|█████▌    | 5455/9822 [2:26:28<1:44:32,  1.44s/it] 56%|█████▌    | 5456/9822 [2:26:30<1:44:30,  1.44s/it] 56%|█████▌    | 5457/9822 [2:26:31<1:44:29,  1.44s/it] 56%|█████▌    | 5458/9822 [2:26:32<1:44:16,  1.43s/it] 56%|█████▌    | 5459/9822 [2:26:34<1:44:02,  1.43s/it] 56%|█████▌    | 5460/9822 [2:26:35<1:44:16,  1.43s/it] 56%|█████▌    | 5461/9822 [2:26:37<1:44:36,  1.44s/it] 56%|█████▌    | 5462/9822 [2:26:38<1:44:08,  1.43s/it] 56%|█████▌    | 5463/9822 [2:26:40<1:44:25,  1.44s/it] 56%|█████▌    | 5464/9822 [2:26:41<1:44:44,  1.44s/it] 56%|█████▌    | 5465/9822 [2:26:42<1:44:16,  1.44s/it] 56%|█████▌    | 5466/9822 [2:26:44<1:44:12,  1.44s/it] 56%|█████▌    | 5467/9822 [2:26:45<1:43:48,  1.43s/it] 56%|█████▌    | 5468/9822 [2:26:47<1:43:46,  1.43s/it] 56%|█████▌    | 5469/9822 [2:26:48<1:43:43,  1.43s/it] 56%|█████▌    | 5470/9822 [2:26:50<1:43:44,  1.43s/it] 56%|█████▌    | 5471/9822 [2:26:51<1:43:44,  1.43s/it] 56%|█████▌    | 5472/9822 [2:26:52<1:43:48,  1.43s/it] 56%|█████▌    | 5473/9822 [2:26:54<1:43:34,  1.43s/it] 56%|█████▌    | 5474/9822 [2:26:55<1:43:41,  1.43s/it] 56%|█████▌    | 5475/9822 [2:26:57<1:43:44,  1.43s/it] 56%|█████▌    | 5476/9822 [2:26:58<1:43:55,  1.43s/it] 56%|█████▌    | 5477/9822 [2:27:00<1:43:55,  1.43s/it] 56%|█████▌    | 5478/9822 [2:27:01<1:43:48,  1.43s/it] 56%|█████▌    | 5479/9822 [2:27:03<1:44:09,  1.44s/it] 56%|█████▌    | 5480/9822 [2:27:04<1:45:57,  1.46s/it] 56%|█████▌    | 5481/9822 [2:27:06<1:45:23,  1.46s/it] 56%|█████▌    | 5482/9822 [2:27:07<1:44:46,  1.45s/it] 56%|█████▌    | 5483/9822 [2:27:08<1:44:23,  1.44s/it] 56%|█████▌    | 5484/9822 [2:27:10<1:44:19,  1.44s/it] 56%|█████▌    | 5485/9822 [2:27:11<1:44:20,  1.44s/it] 56%|█████▌    | 5486/9822 [2:27:13<1:44:00,  1.44s/it] 56%|█████▌    | 5487/9822 [2:27:14<1:43:44,  1.44s/it] 56%|█████▌    | 5488/9822 [2:27:16<1:43:46,  1.44s/it] 56%|█████▌    | 5489/9822 [2:27:17<1:43:43,  1.44s/it] 56%|█████▌    | 5490/9822 [2:27:18<1:43:38,  1.44s/it] 56%|█████▌    | 5491/9822 [2:27:20<1:43:44,  1.44s/it] 56%|█████▌    | 5492/9822 [2:27:21<1:43:31,  1.43s/it] 56%|█████▌    | 5493/9822 [2:27:23<1:43:25,  1.43s/it] 56%|█████▌    | 5494/9822 [2:27:24<1:43:15,  1.43s/it] 56%|█████▌    | 5495/9822 [2:27:26<1:43:21,  1.43s/it] 56%|█████▌    | 5496/9822 [2:27:27<1:43:38,  1.44s/it] 56%|█████▌    | 5497/9822 [2:27:28<1:43:23,  1.43s/it] 56%|█████▌    | 5498/9822 [2:27:30<1:43:34,  1.44s/it] 56%|█████▌    | 5499/9822 [2:27:31<1:43:34,  1.44s/it] 56%|█████▌    | 5500/9822 [2:27:33<1:43:10,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0906, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:15:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:15:20 - INFO - __main__ - ***** test Results*****
01/08/2024 00:15:20 - INFO - __main__ -   Training step = 5500
01/08/2024 00:15:20 - INFO - __main__ -  test_accuracy:0.8792093704245973 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:15:26 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:15:26 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:15:26 - INFO - __main__ -   Training step = 5500
01/08/2024 00:15:26 - INFO - __main__ -  eval_accuracy:0.8553643354082754 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:15:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:15:30 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:15:30 - INFO - __main__ -   Training step = 5500
01/08/2024 00:15:30 - INFO - __main__ -  eval_accuracy:0.9110216038081289 
 56%|█████▌    | 5501/9822 [2:27:51<7:48:44,  6.51s/it] 56%|█████▌    | 5502/9822 [2:27:53<5:59:01,  4.99s/it] 56%|█████▌    | 5503/9822 [2:27:54<4:42:11,  3.92s/it] 56%|█████▌    | 5504/9822 [2:27:55<3:47:26,  3.16s/it] 56%|█████▌    | 5505/9822 [2:27:57<3:10:07,  2.64s/it] 56%|█████▌    | 5506/9822 [2:27:58<2:44:02,  2.28s/it] 56%|█████▌    | 5507/9822 [2:28:00<2:27:22,  2.05s/it] 56%|█████▌    | 5508/9822 [2:28:01<2:14:20,  1.87s/it] 56%|█████▌    | 5509/9822 [2:28:03<2:04:42,  1.73s/it] 56%|█████▌    | 5510/9822 [2:28:04<1:58:00,  1.64s/it] 56%|█████▌    | 5511/9822 [2:28:06<1:54:04,  1.59s/it] 56%|█████▌    | 5512/9822 [2:28:07<1:50:55,  1.54s/it] 56%|█████▌    | 5513/9822 [2:28:08<1:48:21,  1.51s/it] 56%|█████▌    | 5514/9822 [2:28:10<1:46:44,  1.49s/it] 56%|█████▌    | 5515/9822 [2:28:11<1:45:36,  1.47s/it] 56%|█████▌    | 5516/9822 [2:28:13<1:44:48,  1.46s/it] 56%|█████▌    | 5517/9822 [2:28:14<1:44:01,  1.45s/it] 56%|█████▌    | 5518/9822 [2:28:16<1:43:36,  1.44s/it] 56%|█████▌    | 5519/9822 [2:28:17<1:43:16,  1.44s/it] 56%|█████▌    | 5520/9822 [2:28:18<1:43:06,  1.44s/it] 56%|█████▌    | 5521/9822 [2:28:20<1:43:18,  1.44s/it] 56%|█████▌    | 5522/9822 [2:28:21<1:43:02,  1.44s/it] 56%|█████▌    | 5523/9822 [2:28:23<1:42:55,  1.44s/it] 56%|█████▌    | 5524/9822 [2:28:24<1:42:39,  1.43s/it] 56%|█████▋    | 5525/9822 [2:28:26<1:42:36,  1.43s/it] 56%|█████▋    | 5526/9822 [2:28:27<1:42:38,  1.43s/it] 56%|█████▋    | 5527/9822 [2:28:28<1:42:30,  1.43s/it] 56%|█████▋    | 5528/9822 [2:28:30<1:42:29,  1.43s/it] 56%|█████▋    | 5529/9822 [2:28:31<1:42:48,  1.44s/it] 56%|█████▋    | 5530/9822 [2:28:33<1:42:47,  1.44s/it] 56%|█████▋    | 5531/9822 [2:28:34<1:42:27,  1.43s/it] 56%|█████▋    | 5532/9822 [2:28:36<1:42:24,  1.43s/it] 56%|█████▋    | 5533/9822 [2:28:37<1:42:39,  1.44s/it] 56%|█████▋    | 5534/9822 [2:28:38<1:42:34,  1.44s/it] 56%|█████▋    | 5535/9822 [2:28:40<1:42:54,  1.44s/it] 56%|█████▋    | 5536/9822 [2:28:41<1:42:53,  1.44s/it] 56%|█████▋    | 5537/9822 [2:28:43<1:44:45,  1.47s/it] 56%|█████▋    | 5538/9822 [2:28:44<1:44:11,  1.46s/it] 56%|█████▋    | 5539/9822 [2:28:46<1:43:54,  1.46s/it] 56%|█████▋    | 5540/9822 [2:28:47<1:43:53,  1.46s/it] 56%|█████▋    | 5541/9822 [2:28:49<1:43:20,  1.45s/it] 56%|█████▋    | 5542/9822 [2:28:50<1:43:08,  1.45s/it] 56%|█████▋    | 5543/9822 [2:28:52<1:44:07,  1.46s/it] 56%|█████▋    | 5544/9822 [2:28:53<1:44:50,  1.47s/it] 56%|█████▋    | 5545/9822 [2:28:55<1:44:22,  1.46s/it] 56%|█████▋    | 5546/9822 [2:28:56<1:44:00,  1.46s/it] 56%|█████▋    | 5547/9822 [2:28:57<1:43:47,  1.46s/it] 56%|█████▋    | 5548/9822 [2:28:59<1:43:25,  1.45s/it] 56%|█████▋    | 5549/9822 [2:29:00<1:43:02,  1.45s/it] 57%|█████▋    | 5550/9822 [2:29:02<1:42:32,  1.44s/it] 57%|█████▋    | 5551/9822 [2:29:03<1:42:27,  1.44s/it] 57%|█████▋    | 5552/9822 [2:29:05<1:42:20,  1.44s/it] 57%|█████▋    | 5553/9822 [2:29:06<1:42:11,  1.44s/it] 57%|█████▋    | 5554/9822 [2:29:07<1:42:01,  1.43s/it] 57%|█████▋    | 5555/9822 [2:29:09<1:42:25,  1.44s/it] 57%|█████▋    | 5556/9822 [2:29:10<1:42:25,  1.44s/it] 57%|█████▋    | 5557/9822 [2:29:12<1:42:04,  1.44s/it] 57%|█████▋    | 5558/9822 [2:29:13<1:42:02,  1.44s/it] 57%|█████▋    | 5559/9822 [2:29:15<1:42:03,  1.44s/it] 57%|█████▋    | 5560/9822 [2:29:16<1:42:15,  1.44s/it] 57%|█████▋    | 5561/9822 [2:29:18<1:41:58,  1.44s/it] 57%|█████▋    | 5562/9822 [2:29:19<1:41:55,  1.44s/it] 57%|█████▋    | 5563/9822 [2:29:20<1:41:49,  1.43s/it] 57%|█████▋    | 5564/9822 [2:29:22<1:41:38,  1.43s/it] 57%|█████▋    | 5565/9822 [2:29:23<1:41:42,  1.43s/it] 57%|█████▋    | 5566/9822 [2:29:25<1:41:22,  1.43s/it] 57%|█████▋    | 5567/9822 [2:29:26<1:41:20,  1.43s/it] 57%|█████▋    | 5568/9822 [2:29:28<1:41:23,  1.43s/it] 57%|█████▋    | 5569/9822 [2:29:29<1:43:02,  1.45s/it] 57%|█████▋    | 5570/9822 [2:29:31<1:42:21,  1.44s/it] 57%|█████▋    | 5571/9822 [2:29:32<1:42:00,  1.44s/it] 57%|█████▋    | 5572/9822 [2:29:33<1:41:57,  1.44s/it] 57%|█████▋    | 5573/9822 [2:29:35<1:41:44,  1.44s/it] 57%|█████▋    | 5574/9822 [2:29:36<1:41:28,  1.43s/it] 57%|█████▋    | 5575/9822 [2:29:38<1:41:34,  1.43s/it] 57%|█████▋    | 5576/9822 [2:29:39<1:41:42,  1.44s/it] 57%|█████▋    | 5577/9822 [2:29:41<1:41:47,  1.44s/it] 57%|█████▋    | 5578/9822 [2:29:42<1:41:24,  1.43s/it] 57%|█████▋    | 5579/9822 [2:29:43<1:41:07,  1.43s/it] 57%|█████▋    | 5580/9822 [2:29:45<1:41:09,  1.43s/it] 57%|█████▋    | 5581/9822 [2:29:46<1:41:12,  1.43s/it] 57%|█████▋    | 5582/9822 [2:29:48<1:41:13,  1.43s/it] 57%|█████▋    | 5583/9822 [2:29:49<1:41:25,  1.44s/it] 57%|█████▋    | 5584/9822 [2:29:51<1:41:26,  1.44s/it] 57%|█████▋    | 5585/9822 [2:29:52<1:41:33,  1.44s/it] 57%|█████▋    | 5586/9822 [2:29:53<1:41:26,  1.44s/it] 57%|█████▋    | 5587/9822 [2:29:55<1:41:17,  1.44s/it] 57%|█████▋    | 5588/9822 [2:29:56<1:41:12,  1.43s/it] 57%|█████▋    | 5589/9822 [2:29:58<1:41:27,  1.44s/it] 57%|█████▋    | 5590/9822 [2:29:59<1:40:02,  1.42s/it] 57%|█████▋    | 5591/9822 [2:30:01<1:40:30,  1.43s/it] 57%|█████▋    | 5592/9822 [2:30:02<1:40:41,  1.43s/it] 57%|█████▋    | 5593/9822 [2:30:03<1:40:39,  1.43s/it] 57%|█████▋    | 5594/9822 [2:30:05<1:40:39,  1.43s/it] 57%|█████▋    | 5595/9822 [2:30:06<1:40:37,  1.43s/it] 57%|█████▋    | 5596/9822 [2:30:08<1:40:38,  1.43s/it] 57%|█████▋    | 5597/9822 [2:30:09<1:40:31,  1.43s/it] 57%|█████▋    | 5598/9822 [2:30:11<1:40:43,  1.43s/it] 57%|█████▋    | 5599/9822 [2:30:12<1:41:24,  1.44s/it] 57%|█████▋    | 5600/9822 [2:30:13<1:41:14,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:18:00 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:18:00 - INFO - __main__ - ***** test Results*****
01/08/2024 00:18:00 - INFO - __main__ -   Training step = 5600
01/08/2024 00:18:00 - INFO - __main__ -  test_accuracy:0.876281112737921 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:18:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:18:06 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:18:06 - INFO - __main__ -   Training step = 5600
01/08/2024 00:18:06 - INFO - __main__ -  eval_accuracy:0.8575613328451117 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8575613328451117}
test:
{'accuracy': 0.8726207906295754}
01/08/2024 00:18:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:18:11 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:18:11 - INFO - __main__ -   Training step = 5600
01/08/2024 00:18:11 - INFO - __main__ -  eval_accuracy:0.9091907726107653 
 57%|█████▋    | 5601/9822 [2:30:32<7:38:31,  6.52s/it] 57%|█████▋    | 5602/9822 [2:30:33<5:53:07,  5.02s/it] 57%|█████▋    | 5603/9822 [2:30:35<4:37:28,  3.95s/it] 57%|█████▋    | 5604/9822 [2:30:36<3:44:24,  3.19s/it] 57%|█████▋    | 5605/9822 [2:30:38<3:07:35,  2.67s/it] 57%|█████▋    | 5606/9822 [2:30:39<2:41:35,  2.30s/it] 57%|█████▋    | 5607/9822 [2:30:41<2:23:20,  2.04s/it] 57%|█████▋    | 5608/9822 [2:30:42<2:10:36,  1.86s/it] 57%|█████▋    | 5609/9822 [2:30:43<2:01:34,  1.73s/it] 57%|█████▋    | 5610/9822 [2:30:45<1:55:26,  1.64s/it] 57%|█████▋    | 5611/9822 [2:30:46<1:51:00,  1.58s/it] 57%|█████▋    | 5612/9822 [2:30:48<1:47:44,  1.54s/it] 57%|█████▋    | 5613/9822 [2:30:49<1:45:40,  1.51s/it] 57%|█████▋    | 5614/9822 [2:30:51<1:44:10,  1.49s/it] 57%|█████▋    | 5615/9822 [2:30:52<1:43:13,  1.47s/it] 57%|█████▋    | 5616/9822 [2:30:53<1:42:14,  1.46s/it] 57%|█████▋    | 5617/9822 [2:30:55<1:41:40,  1.45s/it] 57%|█████▋    | 5618/9822 [2:30:56<1:41:15,  1.45s/it] 57%|█████▋    | 5619/9822 [2:30:58<1:41:00,  1.44s/it] 57%|█████▋    | 5620/9822 [2:30:59<1:40:58,  1.44s/it] 57%|█████▋    | 5621/9822 [2:31:01<1:40:54,  1.44s/it] 57%|█████▋    | 5622/9822 [2:31:02<1:40:50,  1.44s/it] 57%|█████▋    | 5623/9822 [2:31:04<1:41:10,  1.45s/it] 57%|█████▋    | 5624/9822 [2:31:05<1:40:52,  1.44s/it] 57%|█████▋    | 5625/9822 [2:31:06<1:40:44,  1.44s/it] 57%|█████▋    | 5626/9822 [2:31:08<1:40:29,  1.44s/it] 57%|█████▋    | 5627/9822 [2:31:09<1:40:40,  1.44s/it] 57%|█████▋    | 5628/9822 [2:31:11<1:40:17,  1.43s/it] 57%|█████▋    | 5629/9822 [2:31:12<1:40:12,  1.43s/it] 57%|█████▋    | 5630/9822 [2:31:14<1:40:37,  1.44s/it] 57%|█████▋    | 5631/9822 [2:31:15<1:40:21,  1.44s/it] 57%|█████▋    | 5632/9822 [2:31:16<1:40:14,  1.44s/it] 57%|█████▋    | 5633/9822 [2:31:18<1:42:02,  1.46s/it] 57%|█████▋    | 5634/9822 [2:31:19<1:41:26,  1.45s/it] 57%|█████▋    | 5635/9822 [2:31:21<1:41:02,  1.45s/it] 57%|█████▋    | 5636/9822 [2:31:22<1:40:47,  1.44s/it] 57%|█████▋    | 5637/9822 [2:31:24<1:40:59,  1.45s/it] 57%|█████▋    | 5638/9822 [2:31:25<1:41:49,  1.46s/it] 57%|█████▋    | 5639/9822 [2:31:27<1:41:29,  1.46s/it] 57%|█████▋    | 5640/9822 [2:31:28<1:40:53,  1.45s/it] 57%|█████▋    | 5641/9822 [2:31:30<1:40:30,  1.44s/it] 57%|█████▋    | 5642/9822 [2:31:31<1:40:49,  1.45s/it] 57%|█████▋    | 5643/9822 [2:31:32<1:40:38,  1.45s/it] 57%|█████▋    | 5644/9822 [2:31:34<1:40:19,  1.44s/it] 57%|█████▋    | 5645/9822 [2:31:35<1:40:03,  1.44s/it] 57%|█████▋    | 5646/9822 [2:31:37<1:40:05,  1.44s/it] 57%|█████▋    | 5647/9822 [2:31:38<1:40:14,  1.44s/it] 58%|█████▊    | 5648/9822 [2:31:40<1:40:00,  1.44s/it] 58%|█████▊    | 5649/9822 [2:31:41<1:40:00,  1.44s/it] 58%|█████▊    | 5650/9822 [2:31:43<1:39:49,  1.44s/it] 58%|█████▊    | 5651/9822 [2:31:44<1:39:40,  1.43s/it] 58%|█████▊    | 5652/9822 [2:31:45<1:39:35,  1.43s/it] 58%|█████▊    | 5653/9822 [2:31:47<1:39:28,  1.43s/it] 58%|█████▊    | 5654/9822 [2:31:48<1:39:33,  1.43s/it] 58%|█████▊    | 5655/9822 [2:31:50<1:39:36,  1.43s/it] 58%|█████▊    | 5656/9822 [2:31:51<1:39:55,  1.44s/it] 58%|█████▊    | 5657/9822 [2:31:53<1:39:54,  1.44s/it] 58%|█████▊    | 5658/9822 [2:31:54<1:41:28,  1.46s/it] 58%|█████▊    | 5659/9822 [2:31:56<1:40:58,  1.46s/it] 58%|█████▊    | 5660/9822 [2:31:57<1:40:36,  1.45s/it] 58%|█████▊    | 5661/9822 [2:31:58<1:40:07,  1.44s/it] 58%|█████▊    | 5662/9822 [2:32:00<1:40:07,  1.44s/it] 58%|█████▊    | 5663/9822 [2:32:01<1:40:00,  1.44s/it] 58%|█████▊    | 5664/9822 [2:32:03<1:39:48,  1.44s/it] 58%|█████▊    | 5665/9822 [2:32:04<1:39:44,  1.44s/it] 58%|█████▊    | 5666/9822 [2:32:06<1:39:20,  1.43s/it] 58%|█████▊    | 5667/9822 [2:32:07<1:39:31,  1.44s/it] 58%|█████▊    | 5668/9822 [2:32:08<1:39:22,  1.44s/it] 58%|█████▊    | 5669/9822 [2:32:10<1:39:32,  1.44s/it] 58%|█████▊    | 5670/9822 [2:32:11<1:40:45,  1.46s/it] 58%|█████▊    | 5671/9822 [2:32:13<1:41:38,  1.47s/it] 58%|█████▊    | 5672/9822 [2:32:14<1:41:55,  1.47s/it] 58%|█████▊    | 5673/9822 [2:32:16<1:42:03,  1.48s/it] 58%|█████▊    | 5674/9822 [2:32:17<1:41:33,  1.47s/it] 58%|█████▊    | 5675/9822 [2:32:19<1:41:07,  1.46s/it] 58%|█████▊    | 5676/9822 [2:32:20<1:40:33,  1.46s/it] 58%|█████▊    | 5677/9822 [2:32:22<1:40:19,  1.45s/it] 58%|█████▊    | 5678/9822 [2:32:23<1:40:05,  1.45s/it] 58%|█████▊    | 5679/9822 [2:32:25<1:39:56,  1.45s/it] 58%|█████▊    | 5680/9822 [2:32:26<1:39:53,  1.45s/it] 58%|█████▊    | 5681/9822 [2:32:27<1:39:51,  1.45s/it] 58%|█████▊    | 5682/9822 [2:32:29<1:39:46,  1.45s/it] 58%|█████▊    | 5683/9822 [2:32:30<1:39:57,  1.45s/it] 58%|█████▊    | 5684/9822 [2:32:32<1:40:12,  1.45s/it] 58%|█████▊    | 5685/9822 [2:32:33<1:39:50,  1.45s/it] 58%|█████▊    | 5686/9822 [2:32:35<1:39:29,  1.44s/it] 58%|█████▊    | 5687/9822 [2:32:36<1:39:28,  1.44s/it] 58%|█████▊    | 5688/9822 [2:32:38<1:39:12,  1.44s/it] 58%|█████▊    | 5689/9822 [2:32:39<1:39:08,  1.44s/it] 58%|█████▊    | 5690/9822 [2:32:40<1:40:36,  1.46s/it] 58%|█████▊    | 5691/9822 [2:32:42<1:40:00,  1.45s/it] 58%|█████▊    | 5692/9822 [2:32:43<1:39:46,  1.45s/it] 58%|█████▊    | 5693/9822 [2:32:45<1:39:15,  1.44s/it] 58%|█████▊    | 5694/9822 [2:32:46<1:38:57,  1.44s/it] 58%|█████▊    | 5695/9822 [2:32:48<1:38:48,  1.44s/it] 58%|█████▊    | 5696/9822 [2:32:49<1:38:51,  1.44s/it] 58%|█████▊    | 5697/9822 [2:32:51<1:38:50,  1.44s/it] 58%|█████▊    | 5698/9822 [2:32:52<1:38:51,  1.44s/it] 58%|█████▊    | 5699/9822 [2:32:53<1:38:52,  1.44s/it] 58%|█████▊    | 5700/9822 [2:32:55<1:38:51,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1222, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:20:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:20:42 - INFO - __main__ - ***** test Results*****
01/08/2024 00:20:42 - INFO - __main__ -   Training step = 5700
01/08/2024 00:20:42 - INFO - __main__ -  test_accuracy:0.8733528550512445 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:20:48 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:20:48 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:20:48 - INFO - __main__ -   Training step = 5700
01/08/2024 00:20:48 - INFO - __main__ -  eval_accuracy:0.8586598315635299 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 00:20:48,233 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 00:20:48,233 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 00:20:48,270 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 00:20:49,867 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8586598315635299}
test:
{'accuracy': 0.8733528550512445}
01/08/2024 00:20:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:20:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:20:54 - INFO - __main__ -   Training step = 5700
01/08/2024 00:20:54 - INFO - __main__ -  eval_accuracy:0.9077261076528744 
 58%|█████▊    | 5701/9822 [2:33:15<8:00:26,  6.99s/it] 58%|█████▊    | 5702/9822 [2:33:16<6:05:39,  5.33s/it] 58%|█████▊    | 5703/9822 [2:33:18<4:45:16,  4.16s/it] 58%|█████▊    | 5704/9822 [2:33:19<3:49:44,  3.35s/it] 58%|█████▊    | 5705/9822 [2:33:21<3:10:14,  2.77s/it] 58%|█████▊    | 5706/9822 [2:33:22<2:42:41,  2.37s/it] 58%|█████▊    | 5707/9822 [2:33:23<2:23:54,  2.10s/it] 58%|█████▊    | 5708/9822 [2:33:25<2:09:57,  1.90s/it] 58%|█████▊    | 5709/9822 [2:33:26<2:00:23,  1.76s/it] 58%|█████▊    | 5710/9822 [2:33:28<1:53:52,  1.66s/it] 58%|█████▊    | 5711/9822 [2:33:29<1:48:55,  1.59s/it] 58%|█████▊    | 5712/9822 [2:33:31<1:45:39,  1.54s/it] 58%|█████▊    | 5713/9822 [2:33:32<1:43:42,  1.51s/it] 58%|█████▊    | 5714/9822 [2:33:33<1:42:18,  1.49s/it] 58%|█████▊    | 5715/9822 [2:33:35<1:41:11,  1.48s/it] 58%|█████▊    | 5716/9822 [2:33:36<1:40:22,  1.47s/it] 58%|█████▊    | 5717/9822 [2:33:38<1:41:29,  1.48s/it] 58%|█████▊    | 5718/9822 [2:33:39<1:40:45,  1.47s/it] 58%|█████▊    | 5719/9822 [2:33:41<1:39:52,  1.46s/it] 58%|█████▊    | 5720/9822 [2:33:42<1:39:15,  1.45s/it] 58%|█████▊    | 5721/9822 [2:33:44<1:38:59,  1.45s/it] 58%|█████▊    | 5722/9822 [2:33:45<1:38:34,  1.44s/it] 58%|█████▊    | 5723/9822 [2:33:46<1:38:12,  1.44s/it] 58%|█████▊    | 5724/9822 [2:33:48<1:38:05,  1.44s/it] 58%|█████▊    | 5725/9822 [2:33:49<1:38:01,  1.44s/it] 58%|█████▊    | 5726/9822 [2:33:51<1:37:47,  1.43s/it] 58%|█████▊    | 5727/9822 [2:33:52<1:37:48,  1.43s/it] 58%|█████▊    | 5728/9822 [2:33:54<1:37:54,  1.43s/it] 58%|█████▊    | 5729/9822 [2:33:55<1:37:45,  1.43s/it] 58%|█████▊    | 5730/9822 [2:33:57<1:37:56,  1.44s/it] 58%|█████▊    | 5731/9822 [2:33:58<1:37:49,  1.43s/it] 58%|█████▊    | 5732/9822 [2:33:59<1:37:41,  1.43s/it] 58%|█████▊    | 5733/9822 [2:34:01<1:38:00,  1.44s/it] 58%|█████▊    | 5734/9822 [2:34:02<1:38:01,  1.44s/it] 58%|█████▊    | 5735/9822 [2:34:04<1:37:45,  1.44s/it] 58%|█████▊    | 5736/9822 [2:34:05<1:37:48,  1.44s/it] 58%|█████▊    | 5737/9822 [2:34:07<1:37:49,  1.44s/it] 58%|█████▊    | 5738/9822 [2:34:08<1:37:52,  1.44s/it] 58%|█████▊    | 5739/9822 [2:34:09<1:37:48,  1.44s/it] 58%|█████▊    | 5740/9822 [2:34:11<1:37:41,  1.44s/it] 58%|█████▊    | 5741/9822 [2:34:12<1:37:38,  1.44s/it] 58%|█████▊    | 5742/9822 [2:34:14<1:37:47,  1.44s/it] 58%|█████▊    | 5743/9822 [2:34:15<1:37:38,  1.44s/it] 58%|█████▊    | 5744/9822 [2:34:17<1:37:35,  1.44s/it] 58%|█████▊    | 5745/9822 [2:34:18<1:37:30,  1.44s/it] 59%|█████▊    | 5746/9822 [2:34:19<1:37:21,  1.43s/it] 59%|█████▊    | 5747/9822 [2:34:21<1:37:15,  1.43s/it] 59%|█████▊    | 5748/9822 [2:34:22<1:37:06,  1.43s/it] 59%|█████▊    | 5749/9822 [2:34:24<1:39:01,  1.46s/it] 59%|█████▊    | 5750/9822 [2:34:25<1:38:51,  1.46s/it] 59%|█████▊    | 5751/9822 [2:34:27<1:38:28,  1.45s/it] 59%|█████▊    | 5752/9822 [2:34:28<1:38:08,  1.45s/it] 59%|█████▊    | 5753/9822 [2:34:30<1:37:46,  1.44s/it] 59%|█████▊    | 5754/9822 [2:34:31<1:37:40,  1.44s/it] 59%|█████▊    | 5755/9822 [2:34:33<1:37:30,  1.44s/it] 59%|█████▊    | 5756/9822 [2:34:34<1:37:23,  1.44s/it] 59%|█████▊    | 5757/9822 [2:34:35<1:37:18,  1.44s/it] 59%|█████▊    | 5758/9822 [2:34:37<1:37:01,  1.43s/it] 59%|█████▊    | 5759/9822 [2:34:38<1:37:06,  1.43s/it] 59%|█████▊    | 5760/9822 [2:34:40<1:36:52,  1.43s/it] 59%|█████▊    | 5761/9822 [2:34:41<1:37:03,  1.43s/it] 59%|█████▊    | 5762/9822 [2:34:42<1:36:03,  1.42s/it] 59%|█████▊    | 5763/9822 [2:34:44<1:36:10,  1.42s/it] 59%|█████▊    | 5764/9822 [2:34:45<1:36:21,  1.42s/it] 59%|█████▊    | 5765/9822 [2:34:47<1:36:31,  1.43s/it] 59%|█████▊    | 5766/9822 [2:34:48<1:36:30,  1.43s/it] 59%|█████▊    | 5767/9822 [2:34:50<1:36:21,  1.43s/it] 59%|█████▊    | 5768/9822 [2:34:51<1:36:35,  1.43s/it] 59%|█████▊    | 5769/9822 [2:34:52<1:36:37,  1.43s/it] 59%|█████▊    | 5770/9822 [2:34:54<1:36:34,  1.43s/it] 59%|█████▉    | 5771/9822 [2:34:55<1:36:29,  1.43s/it] 59%|█████▉    | 5772/9822 [2:34:57<1:36:29,  1.43s/it] 59%|█████▉    | 5773/9822 [2:34:58<1:36:24,  1.43s/it] 59%|█████▉    | 5774/9822 [2:35:00<1:36:12,  1.43s/it] 59%|█████▉    | 5775/9822 [2:35:01<1:36:13,  1.43s/it] 59%|█████▉    | 5776/9822 [2:35:02<1:36:22,  1.43s/it] 59%|█████▉    | 5777/9822 [2:35:04<1:36:22,  1.43s/it] 59%|█████▉    | 5778/9822 [2:35:05<1:36:23,  1.43s/it] 59%|█████▉    | 5779/9822 [2:35:07<1:36:39,  1.43s/it] 59%|█████▉    | 5780/9822 [2:35:08<1:36:38,  1.43s/it] 59%|█████▉    | 5781/9822 [2:35:10<1:38:16,  1.46s/it] 59%|█████▉    | 5782/9822 [2:35:11<1:37:51,  1.45s/it] 59%|█████▉    | 5783/9822 [2:35:13<1:37:28,  1.45s/it] 59%|█████▉    | 5784/9822 [2:35:14<1:37:24,  1.45s/it] 59%|█████▉    | 5785/9822 [2:35:16<1:37:23,  1.45s/it] 59%|█████▉    | 5786/9822 [2:35:17<1:37:44,  1.45s/it] 59%|█████▉    | 5787/9822 [2:35:18<1:37:11,  1.45s/it] 59%|█████▉    | 5788/9822 [2:35:20<1:37:00,  1.44s/it] 59%|█████▉    | 5789/9822 [2:35:21<1:36:55,  1.44s/it] 59%|█████▉    | 5790/9822 [2:35:23<1:36:42,  1.44s/it] 59%|█████▉    | 5791/9822 [2:35:24<1:36:26,  1.44s/it] 59%|█████▉    | 5792/9822 [2:35:26<1:36:26,  1.44s/it] 59%|█████▉    | 5793/9822 [2:35:27<1:36:15,  1.43s/it] 59%|█████▉    | 5794/9822 [2:35:28<1:36:08,  1.43s/it] 59%|█████▉    | 5795/9822 [2:35:30<1:36:04,  1.43s/it] 59%|█████▉    | 5796/9822 [2:35:31<1:35:57,  1.43s/it] 59%|█████▉    | 5797/9822 [2:35:33<1:35:49,  1.43s/it] 59%|█████▉    | 5798/9822 [2:35:34<1:36:18,  1.44s/it] 59%|█████▉    | 5799/9822 [2:35:36<1:36:12,  1.43s/it] 59%|█████▉    | 5800/9822 [2:35:37<1:36:07,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:23:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:23:24 - INFO - __main__ - ***** test Results*****
01/08/2024 00:23:24 - INFO - __main__ -   Training step = 5800
01/08/2024 00:23:24 - INFO - __main__ -  test_accuracy:0.8715226939970717 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:23:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:23:30 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:23:30 - INFO - __main__ -   Training step = 5800
01/08/2024 00:23:30 - INFO - __main__ -  eval_accuracy:0.8557305016477481 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8586598315635299}
test:
{'accuracy': 0.8733528550512445}
01/08/2024 00:23:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:23:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:23:35 - INFO - __main__ -   Training step = 5800
01/08/2024 00:23:35 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 59%|█████▉    | 5801/9822 [2:35:55<7:16:23,  6.51s/it] 59%|█████▉    | 5802/9822 [2:35:57<5:34:13,  4.99s/it] 59%|█████▉    | 5803/9822 [2:35:58<4:22:36,  3.92s/it] 59%|█████▉    | 5804/9822 [2:36:00<3:32:32,  3.17s/it] 59%|█████▉    | 5805/9822 [2:36:01<2:57:43,  2.65s/it] 59%|█████▉    | 5806/9822 [2:36:03<2:33:07,  2.29s/it] 59%|█████▉    | 5807/9822 [2:36:04<2:16:12,  2.04s/it] 59%|█████▉    | 5808/9822 [2:36:06<2:05:43,  1.88s/it] 59%|█████▉    | 5809/9822 [2:36:07<1:56:56,  1.75s/it] 59%|█████▉    | 5810/9822 [2:36:08<1:50:40,  1.66s/it] 59%|█████▉    | 5811/9822 [2:36:10<1:46:26,  1.59s/it] 59%|█████▉    | 5812/9822 [2:36:11<1:43:24,  1.55s/it] 59%|█████▉    | 5813/9822 [2:36:13<1:41:01,  1.51s/it] 59%|█████▉    | 5814/9822 [2:36:14<1:39:41,  1.49s/it] 59%|█████▉    | 5815/9822 [2:36:16<1:38:23,  1.47s/it] 59%|█████▉    | 5816/9822 [2:36:17<1:37:33,  1.46s/it] 59%|█████▉    | 5817/9822 [2:36:18<1:36:53,  1.45s/it] 59%|█████▉    | 5818/9822 [2:36:20<1:36:23,  1.44s/it] 59%|█████▉    | 5819/9822 [2:36:21<1:36:08,  1.44s/it] 59%|█████▉    | 5820/9822 [2:36:23<1:35:47,  1.44s/it] 59%|█████▉    | 5821/9822 [2:36:24<1:35:41,  1.43s/it] 59%|█████▉    | 5822/9822 [2:36:26<1:35:45,  1.44s/it] 59%|█████▉    | 5823/9822 [2:36:27<1:35:43,  1.44s/it] 59%|█████▉    | 5824/9822 [2:36:29<1:35:51,  1.44s/it] 59%|█████▉    | 5825/9822 [2:36:30<1:35:58,  1.44s/it] 59%|█████▉    | 5826/9822 [2:36:31<1:35:48,  1.44s/it] 59%|█████▉    | 5827/9822 [2:36:33<1:35:31,  1.43s/it] 59%|█████▉    | 5828/9822 [2:36:34<1:35:28,  1.43s/it] 59%|█████▉    | 5829/9822 [2:36:36<1:35:21,  1.43s/it] 59%|█████▉    | 5830/9822 [2:36:37<1:35:21,  1.43s/it] 59%|█████▉    | 5831/9822 [2:36:39<1:35:04,  1.43s/it] 59%|█████▉    | 5832/9822 [2:36:40<1:35:13,  1.43s/it] 59%|█████▉    | 5833/9822 [2:36:41<1:35:07,  1.43s/it] 59%|█████▉    | 5834/9822 [2:36:43<1:34:59,  1.43s/it] 59%|█████▉    | 5835/9822 [2:36:44<1:34:54,  1.43s/it] 59%|█████▉    | 5836/9822 [2:36:46<1:34:59,  1.43s/it] 59%|█████▉    | 5837/9822 [2:36:47<1:35:21,  1.44s/it] 59%|█████▉    | 5838/9822 [2:36:49<1:37:00,  1.46s/it] 59%|█████▉    | 5839/9822 [2:36:50<1:36:45,  1.46s/it] 59%|█████▉    | 5840/9822 [2:36:52<1:36:28,  1.45s/it] 59%|█████▉    | 5841/9822 [2:36:53<1:36:01,  1.45s/it] 59%|█████▉    | 5842/9822 [2:36:54<1:35:33,  1.44s/it] 59%|█████▉    | 5843/9822 [2:36:56<1:35:33,  1.44s/it] 59%|█████▉    | 5844/9822 [2:36:57<1:35:13,  1.44s/it] 60%|█████▉    | 5845/9822 [2:36:59<1:35:08,  1.44s/it] 60%|█████▉    | 5846/9822 [2:37:00<1:35:43,  1.44s/it] 60%|█████▉    | 5847/9822 [2:37:02<1:36:18,  1.45s/it] 60%|█████▉    | 5848/9822 [2:37:03<1:35:05,  1.44s/it] 60%|█████▉    | 5849/9822 [2:37:04<1:35:09,  1.44s/it] 60%|█████▉    | 5850/9822 [2:37:06<1:35:13,  1.44s/it] 60%|█████▉    | 5851/9822 [2:37:07<1:35:17,  1.44s/it] 60%|█████▉    | 5852/9822 [2:37:09<1:35:02,  1.44s/it] 60%|█████▉    | 5853/9822 [2:37:10<1:34:57,  1.44s/it] 60%|█████▉    | 5854/9822 [2:37:12<1:35:23,  1.44s/it] 60%|█████▉    | 5855/9822 [2:37:13<1:35:12,  1.44s/it] 60%|█████▉    | 5856/9822 [2:37:15<1:34:58,  1.44s/it] 60%|█████▉    | 5857/9822 [2:37:16<1:35:13,  1.44s/it] 60%|█████▉    | 5858/9822 [2:37:17<1:35:18,  1.44s/it] 60%|█████▉    | 5859/9822 [2:37:19<1:35:12,  1.44s/it] 60%|█████▉    | 5860/9822 [2:37:20<1:35:12,  1.44s/it] 60%|█████▉    | 5861/9822 [2:37:22<1:35:11,  1.44s/it] 60%|█████▉    | 5862/9822 [2:37:23<1:34:59,  1.44s/it] 60%|█████▉    | 5863/9822 [2:37:25<1:34:42,  1.44s/it] 60%|█████▉    | 5864/9822 [2:37:26<1:34:40,  1.44s/it] 60%|█████▉    | 5865/9822 [2:37:27<1:34:38,  1.44s/it] 60%|█████▉    | 5866/9822 [2:37:29<1:34:17,  1.43s/it] 60%|█████▉    | 5867/9822 [2:37:30<1:34:24,  1.43s/it] 60%|█████▉    | 5868/9822 [2:37:32<1:34:30,  1.43s/it] 60%|█████▉    | 5869/9822 [2:37:33<1:35:05,  1.44s/it] 60%|█████▉    | 5870/9822 [2:37:35<1:36:16,  1.46s/it] 60%|█████▉    | 5871/9822 [2:37:36<1:36:46,  1.47s/it] 60%|█████▉    | 5872/9822 [2:37:38<1:36:08,  1.46s/it] 60%|█████▉    | 5873/9822 [2:37:39<1:36:05,  1.46s/it] 60%|█████▉    | 5874/9822 [2:37:41<1:35:36,  1.45s/it] 60%|█████▉    | 5875/9822 [2:37:42<1:35:30,  1.45s/it] 60%|█████▉    | 5876/9822 [2:37:43<1:35:13,  1.45s/it] 60%|█████▉    | 5877/9822 [2:37:45<1:35:06,  1.45s/it] 60%|█████▉    | 5878/9822 [2:37:46<1:34:59,  1.45s/it] 60%|█████▉    | 5879/9822 [2:37:48<1:34:42,  1.44s/it] 60%|█████▉    | 5880/9822 [2:37:49<1:34:34,  1.44s/it] 60%|█████▉    | 5881/9822 [2:37:51<1:34:19,  1.44s/it] 60%|█████▉    | 5882/9822 [2:37:52<1:34:13,  1.43s/it] 60%|█████▉    | 5883/9822 [2:37:54<1:34:09,  1.43s/it] 60%|█████▉    | 5884/9822 [2:37:55<1:34:17,  1.44s/it] 60%|█████▉    | 5885/9822 [2:37:56<1:34:37,  1.44s/it] 60%|█████▉    | 5886/9822 [2:37:58<1:34:27,  1.44s/it] 60%|█████▉    | 5887/9822 [2:37:59<1:34:24,  1.44s/it] 60%|█████▉    | 5888/9822 [2:38:01<1:34:05,  1.44s/it] 60%|█████▉    | 5889/9822 [2:38:02<1:33:46,  1.43s/it] 60%|█████▉    | 5890/9822 [2:38:04<1:33:54,  1.43s/it] 60%|█████▉    | 5891/9822 [2:38:05<1:33:55,  1.43s/it] 60%|█████▉    | 5892/9822 [2:38:06<1:33:55,  1.43s/it] 60%|█████▉    | 5893/9822 [2:38:08<1:33:42,  1.43s/it] 60%|██████    | 5894/9822 [2:38:09<1:33:53,  1.43s/it] 60%|██████    | 5895/9822 [2:38:11<1:33:42,  1.43s/it] 60%|██████    | 5896/9822 [2:38:12<1:33:40,  1.43s/it] 60%|██████    | 5897/9822 [2:38:14<1:33:39,  1.43s/it] 60%|██████    | 5898/9822 [2:38:15<1:33:40,  1.43s/it] 60%|██████    | 5899/9822 [2:38:16<1:33:39,  1.43s/it] 60%|██████    | 5900/9822 [2:38:18<1:33:36,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:26:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:26:05 - INFO - __main__ - ***** test Results*****
01/08/2024 00:26:05 - INFO - __main__ -   Training step = 5900
01/08/2024 00:26:05 - INFO - __main__ -  test_accuracy:0.8773792093704246 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:26:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:26:11 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:26:11 - INFO - __main__ -   Training step = 5900
01/08/2024 00:26:11 - INFO - __main__ -  eval_accuracy:0.8590259978030026 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 00:26:11,319 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 00:26:11,320 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 00:26:11,356 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 00:26:12,984 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8590259978030026}
test:
{'accuracy': 0.8773792093704246}
01/08/2024 00:26:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:26:17 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:26:17 - INFO - __main__ -   Training step = 5900
01/08/2024 00:26:17 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 60%|██████    | 5901/9822 [2:38:38<7:39:36,  7.03s/it] 60%|██████    | 5902/9822 [2:38:39<5:49:54,  5.36s/it] 60%|██████    | 5903/9822 [2:38:41<4:32:42,  4.18s/it] 60%|██████    | 5904/9822 [2:38:42<3:39:03,  3.35s/it] 60%|██████    | 5905/9822 [2:38:44<3:01:15,  2.78s/it] 60%|██████    | 5906/9822 [2:38:45<2:35:01,  2.38s/it] 60%|██████    | 5907/9822 [2:38:47<2:16:32,  2.09s/it] 60%|██████    | 5908/9822 [2:38:48<2:03:32,  1.89s/it] 60%|██████    | 5909/9822 [2:38:49<1:54:21,  1.75s/it] 60%|██████    | 5910/9822 [2:38:51<1:48:03,  1.66s/it] 60%|██████    | 5911/9822 [2:38:52<1:43:28,  1.59s/it] 60%|██████    | 5912/9822 [2:38:54<1:40:16,  1.54s/it] 60%|██████    | 5913/9822 [2:38:55<1:38:13,  1.51s/it] 60%|██████    | 5914/9822 [2:38:57<1:36:45,  1.49s/it] 60%|██████    | 5915/9822 [2:38:58<1:35:44,  1.47s/it] 60%|██████    | 5916/9822 [2:38:59<1:34:53,  1.46s/it] 60%|██████    | 5917/9822 [2:39:01<1:34:27,  1.45s/it] 60%|██████    | 5918/9822 [2:39:02<1:33:54,  1.44s/it] 60%|██████    | 5919/9822 [2:39:04<1:33:26,  1.44s/it] 60%|██████    | 5920/9822 [2:39:05<1:33:17,  1.43s/it] 60%|██████    | 5921/9822 [2:39:07<1:33:25,  1.44s/it] 60%|██████    | 5922/9822 [2:39:08<1:33:28,  1.44s/it] 60%|██████    | 5923/9822 [2:39:09<1:33:26,  1.44s/it] 60%|██████    | 5924/9822 [2:39:11<1:33:22,  1.44s/it] 60%|██████    | 5925/9822 [2:39:12<1:33:18,  1.44s/it] 60%|██████    | 5926/9822 [2:39:14<1:33:14,  1.44s/it] 60%|██████    | 5927/9822 [2:39:15<1:33:06,  1.43s/it] 60%|██████    | 5928/9822 [2:39:17<1:32:52,  1.43s/it] 60%|██████    | 5929/9822 [2:39:18<1:32:50,  1.43s/it] 60%|██████    | 5930/9822 [2:39:20<1:32:44,  1.43s/it] 60%|██████    | 5931/9822 [2:39:21<1:32:52,  1.43s/it] 60%|██████    | 5932/9822 [2:39:22<1:34:44,  1.46s/it] 60%|██████    | 5933/9822 [2:39:24<1:34:01,  1.45s/it] 60%|██████    | 5934/9822 [2:39:25<1:32:36,  1.43s/it] 60%|██████    | 5935/9822 [2:39:27<1:32:37,  1.43s/it] 60%|██████    | 5936/9822 [2:39:28<1:32:29,  1.43s/it] 60%|██████    | 5937/9822 [2:39:30<1:32:37,  1.43s/it] 60%|██████    | 5938/9822 [2:39:31<1:32:38,  1.43s/it] 60%|██████    | 5939/9822 [2:39:32<1:32:40,  1.43s/it] 60%|██████    | 5940/9822 [2:39:34<1:32:38,  1.43s/it] 60%|██████    | 5941/9822 [2:39:35<1:32:26,  1.43s/it] 60%|██████    | 5942/9822 [2:39:37<1:32:17,  1.43s/it] 61%|██████    | 5943/9822 [2:39:38<1:32:26,  1.43s/it] 61%|██████    | 5944/9822 [2:39:40<1:32:35,  1.43s/it] 61%|██████    | 5945/9822 [2:39:41<1:32:25,  1.43s/it] 61%|██████    | 5946/9822 [2:39:42<1:32:22,  1.43s/it] 61%|██████    | 5947/9822 [2:39:44<1:32:09,  1.43s/it] 61%|██████    | 5948/9822 [2:39:45<1:32:09,  1.43s/it] 61%|██████    | 5949/9822 [2:39:47<1:32:08,  1.43s/it] 61%|██████    | 5950/9822 [2:39:48<1:32:00,  1.43s/it] 61%|██████    | 5951/9822 [2:39:50<1:32:01,  1.43s/it] 61%|██████    | 5952/9822 [2:39:51<1:32:32,  1.43s/it] 61%|██████    | 5953/9822 [2:39:52<1:32:19,  1.43s/it] 61%|██████    | 5954/9822 [2:39:54<1:32:19,  1.43s/it] 61%|██████    | 5955/9822 [2:39:55<1:32:28,  1.43s/it] 61%|██████    | 5956/9822 [2:39:57<1:32:08,  1.43s/it] 61%|██████    | 5957/9822 [2:39:58<1:33:42,  1.45s/it] 61%|██████    | 5958/9822 [2:40:00<1:33:12,  1.45s/it] 61%|██████    | 5959/9822 [2:40:01<1:32:51,  1.44s/it] 61%|██████    | 5960/9822 [2:40:03<1:32:39,  1.44s/it] 61%|██████    | 5961/9822 [2:40:04<1:32:34,  1.44s/it] 61%|██████    | 5962/9822 [2:40:05<1:33:01,  1.45s/it] 61%|██████    | 5963/9822 [2:40:07<1:32:55,  1.44s/it] 61%|██████    | 5964/9822 [2:40:08<1:32:44,  1.44s/it] 61%|██████    | 5965/9822 [2:40:10<1:32:25,  1.44s/it] 61%|██████    | 5966/9822 [2:40:11<1:32:21,  1.44s/it] 61%|██████    | 5967/9822 [2:40:13<1:32:06,  1.43s/it] 61%|██████    | 5968/9822 [2:40:14<1:32:03,  1.43s/it] 61%|██████    | 5969/9822 [2:40:15<1:31:52,  1.43s/it] 61%|██████    | 5970/9822 [2:40:17<1:31:50,  1.43s/it] 61%|██████    | 5971/9822 [2:40:18<1:32:04,  1.43s/it] 61%|██████    | 5972/9822 [2:40:20<1:32:05,  1.44s/it] 61%|██████    | 5973/9822 [2:40:21<1:31:49,  1.43s/it] 61%|██████    | 5974/9822 [2:40:23<1:31:44,  1.43s/it] 61%|██████    | 5975/9822 [2:40:24<1:31:32,  1.43s/it] 61%|██████    | 5976/9822 [2:40:25<1:31:21,  1.43s/it] 61%|██████    | 5977/9822 [2:40:27<1:31:33,  1.43s/it] 61%|██████    | 5978/9822 [2:40:28<1:31:36,  1.43s/it] 61%|██████    | 5979/9822 [2:40:30<1:31:46,  1.43s/it] 61%|██████    | 5980/9822 [2:40:31<1:31:53,  1.44s/it] 61%|██████    | 5981/9822 [2:40:33<1:32:00,  1.44s/it] 61%|██████    | 5982/9822 [2:40:34<1:31:58,  1.44s/it] 61%|██████    | 5983/9822 [2:40:36<1:31:51,  1.44s/it] 61%|██████    | 5984/9822 [2:40:37<1:31:45,  1.43s/it] 61%|██████    | 5985/9822 [2:40:38<1:32:19,  1.44s/it] 61%|██████    | 5986/9822 [2:40:40<1:32:02,  1.44s/it] 61%|██████    | 5987/9822 [2:40:41<1:32:20,  1.44s/it] 61%|██████    | 5988/9822 [2:40:43<1:32:15,  1.44s/it] 61%|██████    | 5989/9822 [2:40:44<1:33:31,  1.46s/it] 61%|██████    | 5990/9822 [2:40:46<1:32:47,  1.45s/it] 61%|██████    | 5991/9822 [2:40:47<1:32:25,  1.45s/it] 61%|██████    | 5992/9822 [2:40:49<1:32:29,  1.45s/it] 61%|██████    | 5993/9822 [2:40:50<1:32:08,  1.44s/it] 61%|██████    | 5994/9822 [2:40:51<1:32:02,  1.44s/it] 61%|██████    | 5995/9822 [2:40:53<1:31:43,  1.44s/it] 61%|██████    | 5996/9822 [2:40:54<1:31:39,  1.44s/it] 61%|██████    | 5997/9822 [2:40:56<1:31:23,  1.43s/it] 61%|██████    | 5998/9822 [2:40:57<1:31:32,  1.44s/it] 61%|██████    | 5999/9822 [2:40:59<1:31:24,  1.43s/it] 61%|██████    | 6000/9822 [2:41:00<1:31:22,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1168, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:28:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:28:47 - INFO - __main__ - ***** test Results*****
01/08/2024 00:28:47 - INFO - __main__ -   Training step = 6000
01/08/2024 00:28:47 - INFO - __main__ -  test_accuracy:0.8777452415812591 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:28:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:28:53 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:28:53 - INFO - __main__ -   Training step = 6000
01/08/2024 00:28:53 - INFO - __main__ -  eval_accuracy:0.8608568290003662 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 00:28:53,484 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 00:28:53,484 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 00:28:53,520 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 00:28:55,163 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8608568290003662}
test:
{'accuracy': 0.8777452415812591}
01/08/2024 00:28:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:28:59 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:28:59 - INFO - __main__ -   Training step = 6000
01/08/2024 00:28:59 - INFO - __main__ -  eval_accuracy:0.9099231050897107 
 61%|██████    | 6001/9822 [2:41:20<7:26:58,  7.02s/it] 61%|██████    | 6002/9822 [2:41:22<5:40:04,  5.34s/it] 61%|██████    | 6003/9822 [2:41:23<4:25:34,  4.17s/it] 61%|██████    | 6004/9822 [2:41:24<3:33:21,  3.35s/it] 61%|██████    | 6005/9822 [2:41:26<2:56:48,  2.78s/it] 61%|██████    | 6006/9822 [2:41:27<2:31:01,  2.37s/it] 61%|██████    | 6007/9822 [2:41:29<2:13:03,  2.09s/it] 61%|██████    | 6008/9822 [2:41:30<2:00:27,  1.89s/it] 61%|██████    | 6009/9822 [2:41:32<1:51:21,  1.75s/it] 61%|██████    | 6010/9822 [2:41:33<1:45:19,  1.66s/it] 61%|██████    | 6011/9822 [2:41:34<1:41:01,  1.59s/it] 61%|██████    | 6012/9822 [2:41:36<1:38:04,  1.54s/it] 61%|██████    | 6013/9822 [2:41:37<1:35:58,  1.51s/it] 61%|██████    | 6014/9822 [2:41:39<1:34:25,  1.49s/it] 61%|██████    | 6015/9822 [2:41:40<1:33:20,  1.47s/it] 61%|██████▏   | 6016/9822 [2:41:42<1:32:48,  1.46s/it] 61%|██████▏   | 6017/9822 [2:41:43<1:33:52,  1.48s/it] 61%|██████▏   | 6018/9822 [2:41:45<1:33:01,  1.47s/it] 61%|██████▏   | 6019/9822 [2:41:46<1:32:08,  1.45s/it] 61%|██████▏   | 6020/9822 [2:41:47<1:30:46,  1.43s/it] 61%|██████▏   | 6021/9822 [2:41:49<1:30:54,  1.43s/it] 61%|██████▏   | 6022/9822 [2:41:50<1:30:59,  1.44s/it] 61%|██████▏   | 6023/9822 [2:41:52<1:30:57,  1.44s/it] 61%|██████▏   | 6024/9822 [2:41:53<1:30:56,  1.44s/it] 61%|██████▏   | 6025/9822 [2:41:55<1:30:43,  1.43s/it] 61%|██████▏   | 6026/9822 [2:41:56<1:30:35,  1.43s/it] 61%|██████▏   | 6027/9822 [2:41:57<1:30:34,  1.43s/it] 61%|██████▏   | 6028/9822 [2:41:59<1:30:37,  1.43s/it] 61%|██████▏   | 6029/9822 [2:42:00<1:30:35,  1.43s/it] 61%|██████▏   | 6030/9822 [2:42:02<1:30:27,  1.43s/it] 61%|██████▏   | 6031/9822 [2:42:03<1:30:20,  1.43s/it] 61%|██████▏   | 6032/9822 [2:42:05<1:30:35,  1.43s/it] 61%|██████▏   | 6033/9822 [2:42:06<1:30:32,  1.43s/it] 61%|██████▏   | 6034/9822 [2:42:07<1:30:29,  1.43s/it] 61%|██████▏   | 6035/9822 [2:42:09<1:30:23,  1.43s/it] 61%|██████▏   | 6036/9822 [2:42:10<1:30:22,  1.43s/it] 61%|██████▏   | 6037/9822 [2:42:12<1:30:18,  1.43s/it] 61%|██████▏   | 6038/9822 [2:42:13<1:30:13,  1.43s/it] 61%|██████▏   | 6039/9822 [2:42:15<1:30:23,  1.43s/it] 61%|██████▏   | 6040/9822 [2:42:16<1:30:29,  1.44s/it] 62%|██████▏   | 6041/9822 [2:42:17<1:30:21,  1.43s/it] 62%|██████▏   | 6042/9822 [2:42:19<1:30:12,  1.43s/it] 62%|██████▏   | 6043/9822 [2:42:20<1:30:19,  1.43s/it] 62%|██████▏   | 6044/9822 [2:42:22<1:30:36,  1.44s/it] 62%|██████▏   | 6045/9822 [2:42:23<1:30:48,  1.44s/it] 62%|██████▏   | 6046/9822 [2:42:25<1:30:42,  1.44s/it] 62%|██████▏   | 6047/9822 [2:42:26<1:31:03,  1.45s/it] 62%|██████▏   | 6048/9822 [2:42:28<1:31:58,  1.46s/it] 62%|██████▏   | 6049/9822 [2:42:29<1:33:32,  1.49s/it] 62%|██████▏   | 6050/9822 [2:42:31<1:32:53,  1.48s/it] 62%|██████▏   | 6051/9822 [2:42:32<1:32:22,  1.47s/it] 62%|██████▏   | 6052/9822 [2:42:34<1:31:53,  1.46s/it] 62%|██████▏   | 6053/9822 [2:42:35<1:31:37,  1.46s/it] 62%|██████▏   | 6054/9822 [2:42:36<1:31:22,  1.46s/it] 62%|██████▏   | 6055/9822 [2:42:38<1:31:12,  1.45s/it] 62%|██████▏   | 6056/9822 [2:42:39<1:31:05,  1.45s/it] 62%|██████▏   | 6057/9822 [2:42:41<1:30:44,  1.45s/it] 62%|██████▏   | 6058/9822 [2:42:42<1:30:42,  1.45s/it] 62%|██████▏   | 6059/9822 [2:42:44<1:30:26,  1.44s/it] 62%|██████▏   | 6060/9822 [2:42:45<1:30:17,  1.44s/it] 62%|██████▏   | 6061/9822 [2:42:47<1:30:07,  1.44s/it] 62%|██████▏   | 6062/9822 [2:42:48<1:30:09,  1.44s/it] 62%|██████▏   | 6063/9822 [2:42:49<1:30:06,  1.44s/it] 62%|██████▏   | 6064/9822 [2:42:51<1:30:05,  1.44s/it] 62%|██████▏   | 6065/9822 [2:42:52<1:30:10,  1.44s/it] 62%|██████▏   | 6066/9822 [2:42:54<1:29:56,  1.44s/it] 62%|██████▏   | 6067/9822 [2:42:55<1:30:00,  1.44s/it] 62%|██████▏   | 6068/9822 [2:42:57<1:29:49,  1.44s/it] 62%|██████▏   | 6069/9822 [2:42:58<1:29:57,  1.44s/it] 62%|██████▏   | 6070/9822 [2:42:59<1:30:04,  1.44s/it] 62%|██████▏   | 6071/9822 [2:43:01<1:30:09,  1.44s/it] 62%|██████▏   | 6072/9822 [2:43:02<1:29:51,  1.44s/it] 62%|██████▏   | 6073/9822 [2:43:04<1:29:38,  1.43s/it] 62%|██████▏   | 6074/9822 [2:43:05<1:29:31,  1.43s/it] 62%|██████▏   | 6075/9822 [2:43:07<1:29:35,  1.43s/it] 62%|██████▏   | 6076/9822 [2:43:08<1:29:25,  1.43s/it] 62%|██████▏   | 6077/9822 [2:43:10<1:29:28,  1.43s/it] 62%|██████▏   | 6078/9822 [2:43:11<1:30:04,  1.44s/it] 62%|██████▏   | 6079/9822 [2:43:13<1:31:26,  1.47s/it] 62%|██████▏   | 6080/9822 [2:43:14<1:30:58,  1.46s/it] 62%|██████▏   | 6081/9822 [2:43:15<1:30:38,  1.45s/it] 62%|██████▏   | 6082/9822 [2:43:17<1:30:07,  1.45s/it] 62%|██████▏   | 6083/9822 [2:43:18<1:29:41,  1.44s/it] 62%|██████▏   | 6084/9822 [2:43:20<1:29:32,  1.44s/it] 62%|██████▏   | 6085/9822 [2:43:21<1:29:14,  1.43s/it] 62%|██████▏   | 6086/9822 [2:43:23<1:29:06,  1.43s/it] 62%|██████▏   | 6087/9822 [2:43:24<1:29:07,  1.43s/it] 62%|██████▏   | 6088/9822 [2:43:25<1:29:25,  1.44s/it] 62%|██████▏   | 6089/9822 [2:43:27<1:29:14,  1.43s/it] 62%|██████▏   | 6090/9822 [2:43:28<1:29:05,  1.43s/it] 62%|██████▏   | 6091/9822 [2:43:30<1:29:13,  1.43s/it] 62%|██████▏   | 6092/9822 [2:43:31<1:29:09,  1.43s/it] 62%|██████▏   | 6093/9822 [2:43:33<1:29:16,  1.44s/it] 62%|██████▏   | 6094/9822 [2:43:34<1:29:04,  1.43s/it] 62%|██████▏   | 6095/9822 [2:43:35<1:28:50,  1.43s/it] 62%|██████▏   | 6096/9822 [2:43:37<1:28:59,  1.43s/it] 62%|██████▏   | 6097/9822 [2:43:38<1:29:19,  1.44s/it] 62%|██████▏   | 6098/9822 [2:43:40<1:29:03,  1.43s/it] 62%|██████▏   | 6099/9822 [2:43:41<1:29:03,  1.44s/it] 62%|██████▏   | 6100/9822 [2:43:43<1:29:07,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:31:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:31:29 - INFO - __main__ - ***** test Results*****
01/08/2024 00:31:29 - INFO - __main__ -   Training step = 6100
01/08/2024 00:31:29 - INFO - __main__ -  test_accuracy:0.8718887262079063 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:31:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:31:36 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:31:36 - INFO - __main__ -   Training step = 6100
01/08/2024 00:31:36 - INFO - __main__ -  eval_accuracy:0.8586598315635299 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8608568290003662}
test:
{'accuracy': 0.8777452415812591}
01/08/2024 00:31:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:31:40 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:31:40 - INFO - __main__ -   Training step = 6100
01/08/2024 00:31:40 - INFO - __main__ -  eval_accuracy:0.9062614426949835 
 62%|██████▏   | 6101/9822 [2:44:01<6:43:54,  6.51s/it] 62%|██████▏   | 6102/9822 [2:44:02<5:09:22,  4.99s/it] 62%|██████▏   | 6103/9822 [2:44:04<4:03:14,  3.92s/it] 62%|██████▏   | 6104/9822 [2:44:05<3:16:49,  3.18s/it] 62%|██████▏   | 6105/9822 [2:44:07<2:44:24,  2.65s/it] 62%|██████▏   | 6106/9822 [2:44:08<2:20:48,  2.27s/it] 62%|██████▏   | 6107/9822 [2:44:10<2:05:24,  2.03s/it] 62%|██████▏   | 6108/9822 [2:44:11<1:55:54,  1.87s/it] 62%|██████▏   | 6109/9822 [2:44:13<1:47:55,  1.74s/it] 62%|██████▏   | 6110/9822 [2:44:14<1:42:19,  1.65s/it] 62%|██████▏   | 6111/9822 [2:44:15<1:38:10,  1.59s/it] 62%|██████▏   | 6112/9822 [2:44:17<1:35:11,  1.54s/it] 62%|██████▏   | 6113/9822 [2:44:18<1:33:22,  1.51s/it] 62%|██████▏   | 6114/9822 [2:44:20<1:31:51,  1.49s/it] 62%|██████▏   | 6115/9822 [2:44:21<1:30:45,  1.47s/it] 62%|██████▏   | 6116/9822 [2:44:23<1:30:03,  1.46s/it] 62%|██████▏   | 6117/9822 [2:44:24<1:29:30,  1.45s/it] 62%|██████▏   | 6118/9822 [2:44:25<1:29:08,  1.44s/it] 62%|██████▏   | 6119/9822 [2:44:27<1:28:49,  1.44s/it] 62%|██████▏   | 6120/9822 [2:44:28<1:28:39,  1.44s/it] 62%|██████▏   | 6121/9822 [2:44:30<1:28:28,  1.43s/it] 62%|██████▏   | 6122/9822 [2:44:31<1:28:15,  1.43s/it] 62%|██████▏   | 6123/9822 [2:44:33<1:28:12,  1.43s/it] 62%|██████▏   | 6124/9822 [2:44:34<1:28:12,  1.43s/it] 62%|██████▏   | 6125/9822 [2:44:35<1:28:26,  1.44s/it] 62%|██████▏   | 6126/9822 [2:44:37<1:28:16,  1.43s/it] 62%|██████▏   | 6127/9822 [2:44:38<1:28:12,  1.43s/it] 62%|██████▏   | 6128/9822 [2:44:40<1:28:12,  1.43s/it] 62%|██████▏   | 6129/9822 [2:44:41<1:28:17,  1.43s/it] 62%|██████▏   | 6130/9822 [2:44:43<1:28:36,  1.44s/it] 62%|██████▏   | 6131/9822 [2:44:44<1:28:22,  1.44s/it] 62%|██████▏   | 6132/9822 [2:44:45<1:28:11,  1.43s/it] 62%|██████▏   | 6133/9822 [2:44:47<1:27:59,  1.43s/it] 62%|██████▏   | 6134/9822 [2:44:48<1:28:02,  1.43s/it] 62%|██████▏   | 6135/9822 [2:44:50<1:27:54,  1.43s/it] 62%|██████▏   | 6136/9822 [2:44:51<1:27:55,  1.43s/it] 62%|██████▏   | 6137/9822 [2:44:53<1:27:57,  1.43s/it] 62%|██████▏   | 6138/9822 [2:44:54<1:28:01,  1.43s/it] 63%|██████▎   | 6139/9822 [2:44:55<1:28:01,  1.43s/it] 63%|██████▎   | 6140/9822 [2:44:57<1:29:35,  1.46s/it] 63%|██████▎   | 6141/9822 [2:44:58<1:29:06,  1.45s/it] 63%|██████▎   | 6142/9822 [2:45:00<1:28:48,  1.45s/it] 63%|██████▎   | 6143/9822 [2:45:01<1:28:34,  1.44s/it] 63%|██████▎   | 6144/9822 [2:45:03<1:28:33,  1.44s/it] 63%|██████▎   | 6145/9822 [2:45:04<1:28:22,  1.44s/it] 63%|██████▎   | 6146/9822 [2:45:06<1:28:06,  1.44s/it] 63%|██████▎   | 6147/9822 [2:45:07<1:28:02,  1.44s/it] 63%|██████▎   | 6148/9822 [2:45:09<1:28:10,  1.44s/it] 63%|██████▎   | 6149/9822 [2:45:10<1:27:50,  1.43s/it] 63%|██████▎   | 6150/9822 [2:45:11<1:27:55,  1.44s/it] 63%|██████▎   | 6151/9822 [2:45:13<1:27:50,  1.44s/it] 63%|██████▎   | 6152/9822 [2:45:14<1:27:50,  1.44s/it] 63%|██████▎   | 6153/9822 [2:45:16<1:28:07,  1.44s/it] 63%|██████▎   | 6154/9822 [2:45:17<1:28:42,  1.45s/it] 63%|██████▎   | 6155/9822 [2:45:19<1:29:10,  1.46s/it] 63%|██████▎   | 6156/9822 [2:45:20<1:28:41,  1.45s/it] 63%|██████▎   | 6157/9822 [2:45:22<1:28:30,  1.45s/it] 63%|██████▎   | 6158/9822 [2:45:23<1:28:23,  1.45s/it] 63%|██████▎   | 6159/9822 [2:45:24<1:28:12,  1.44s/it] 63%|██████▎   | 6160/9822 [2:45:26<1:27:59,  1.44s/it] 63%|██████▎   | 6161/9822 [2:45:27<1:27:41,  1.44s/it] 63%|██████▎   | 6162/9822 [2:45:29<1:27:35,  1.44s/it] 63%|██████▎   | 6163/9822 [2:45:30<1:27:21,  1.43s/it] 63%|██████▎   | 6164/9822 [2:45:32<1:27:27,  1.43s/it] 63%|██████▎   | 6165/9822 [2:45:33<1:27:19,  1.43s/it] 63%|██████▎   | 6166/9822 [2:45:34<1:27:15,  1.43s/it] 63%|██████▎   | 6167/9822 [2:45:36<1:27:19,  1.43s/it] 63%|██████▎   | 6168/9822 [2:45:37<1:27:13,  1.43s/it] 63%|██████▎   | 6169/9822 [2:45:39<1:27:09,  1.43s/it] 63%|██████▎   | 6170/9822 [2:45:40<1:27:04,  1.43s/it] 63%|██████▎   | 6171/9822 [2:45:42<1:27:00,  1.43s/it] 63%|██████▎   | 6172/9822 [2:45:43<1:28:25,  1.45s/it] 63%|██████▎   | 6173/9822 [2:45:45<1:28:09,  1.45s/it] 63%|██████▎   | 6174/9822 [2:45:46<1:27:53,  1.45s/it] 63%|██████▎   | 6175/9822 [2:45:47<1:27:31,  1.44s/it] 63%|██████▎   | 6176/9822 [2:45:49<1:27:43,  1.44s/it] 63%|██████▎   | 6177/9822 [2:45:50<1:27:30,  1.44s/it] 63%|██████▎   | 6178/9822 [2:45:52<1:27:17,  1.44s/it] 63%|██████▎   | 6179/9822 [2:45:53<1:27:16,  1.44s/it] 63%|██████▎   | 6180/9822 [2:45:55<1:27:06,  1.44s/it] 63%|██████▎   | 6181/9822 [2:45:56<1:26:53,  1.43s/it] 63%|██████▎   | 6182/9822 [2:45:57<1:26:52,  1.43s/it] 63%|██████▎   | 6183/9822 [2:45:59<1:26:52,  1.43s/it] 63%|██████▎   | 6184/9822 [2:46:00<1:27:08,  1.44s/it] 63%|██████▎   | 6185/9822 [2:46:02<1:27:09,  1.44s/it] 63%|██████▎   | 6186/9822 [2:46:03<1:27:13,  1.44s/it] 63%|██████▎   | 6187/9822 [2:46:05<1:27:23,  1.44s/it] 63%|██████▎   | 6188/9822 [2:46:06<1:27:15,  1.44s/it] 63%|██████▎   | 6189/9822 [2:46:08<1:27:06,  1.44s/it] 63%|██████▎   | 6190/9822 [2:46:09<1:26:39,  1.43s/it] 63%|██████▎   | 6191/9822 [2:46:10<1:26:35,  1.43s/it] 63%|██████▎   | 6192/9822 [2:46:12<1:25:27,  1.41s/it] 63%|██████▎   | 6193/9822 [2:46:13<1:25:34,  1.41s/it] 63%|██████▎   | 6194/9822 [2:46:15<1:25:52,  1.42s/it] 63%|██████▎   | 6195/9822 [2:46:16<1:26:09,  1.43s/it] 63%|██████▎   | 6196/9822 [2:46:17<1:26:14,  1.43s/it] 63%|██████▎   | 6197/9822 [2:46:19<1:28:04,  1.46s/it] 63%|██████▎   | 6198/9822 [2:46:20<1:27:33,  1.45s/it] 63%|██████▎   | 6199/9822 [2:46:22<1:27:37,  1.45s/it] 63%|██████▎   | 6200/9822 [2:46:23<1:27:35,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0967, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:34:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:34:10 - INFO - __main__ - ***** test Results*****
01/08/2024 00:34:10 - INFO - __main__ -   Training step = 6200
01/08/2024 00:34:10 - INFO - __main__ -  test_accuracy:0.8795754026354319 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:34:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:34:16 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:34:16 - INFO - __main__ -   Training step = 6200
01/08/2024 00:34:16 - INFO - __main__ -  eval_accuracy:0.8590259978030026 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8608568290003662}
test:
{'accuracy': 0.8777452415812591}
01/08/2024 00:34:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:34:21 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:34:21 - INFO - __main__ -   Training step = 6200
01/08/2024 00:34:21 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 63%|██████▎   | 6201/9822 [2:46:42<6:33:38,  6.52s/it] 63%|██████▎   | 6202/9822 [2:46:43<5:02:04,  5.01s/it] 63%|██████▎   | 6203/9822 [2:46:45<3:57:26,  3.94s/it] 63%|██████▎   | 6204/9822 [2:46:46<3:12:31,  3.19s/it] 63%|██████▎   | 6205/9822 [2:46:47<2:41:12,  2.67s/it] 63%|██████▎   | 6206/9822 [2:46:49<2:19:17,  2.31s/it] 63%|██████▎   | 6207/9822 [2:46:50<2:03:27,  2.05s/it] 63%|██████▎   | 6208/9822 [2:46:52<1:52:25,  1.87s/it] 63%|██████▎   | 6209/9822 [2:46:53<1:44:44,  1.74s/it] 63%|██████▎   | 6210/9822 [2:46:55<1:39:25,  1.65s/it] 63%|██████▎   | 6211/9822 [2:46:56<1:35:41,  1.59s/it] 63%|██████▎   | 6212/9822 [2:46:58<1:33:07,  1.55s/it] 63%|██████▎   | 6213/9822 [2:46:59<1:31:14,  1.52s/it] 63%|██████▎   | 6214/9822 [2:47:01<1:29:54,  1.50s/it] 63%|██████▎   | 6215/9822 [2:47:02<1:28:43,  1.48s/it] 63%|██████▎   | 6216/9822 [2:47:03<1:27:47,  1.46s/it] 63%|██████▎   | 6217/9822 [2:47:05<1:27:30,  1.46s/it] 63%|██████▎   | 6218/9822 [2:47:06<1:26:55,  1.45s/it] 63%|██████▎   | 6219/9822 [2:47:08<1:26:45,  1.44s/it] 63%|██████▎   | 6220/9822 [2:47:09<1:26:38,  1.44s/it] 63%|██████▎   | 6221/9822 [2:47:11<1:26:25,  1.44s/it] 63%|██████▎   | 6222/9822 [2:47:12<1:26:32,  1.44s/it] 63%|██████▎   | 6223/9822 [2:47:14<1:27:45,  1.46s/it] 63%|██████▎   | 6224/9822 [2:47:15<1:27:07,  1.45s/it] 63%|██████▎   | 6225/9822 [2:47:16<1:26:59,  1.45s/it] 63%|██████▎   | 6226/9822 [2:47:18<1:26:44,  1.45s/it] 63%|██████▎   | 6227/9822 [2:47:19<1:26:24,  1.44s/it] 63%|██████▎   | 6228/9822 [2:47:21<1:26:05,  1.44s/it] 63%|██████▎   | 6229/9822 [2:47:22<1:25:55,  1.43s/it] 63%|██████▎   | 6230/9822 [2:47:24<1:26:12,  1.44s/it] 63%|██████▎   | 6231/9822 [2:47:25<1:26:04,  1.44s/it] 63%|██████▎   | 6232/9822 [2:47:26<1:26:02,  1.44s/it] 63%|██████▎   | 6233/9822 [2:47:28<1:25:53,  1.44s/it] 63%|██████▎   | 6234/9822 [2:47:29<1:25:56,  1.44s/it] 63%|██████▎   | 6235/9822 [2:47:31<1:25:45,  1.43s/it] 63%|██████▎   | 6236/9822 [2:47:32<1:25:34,  1.43s/it] 64%|██████▎   | 6237/9822 [2:47:34<1:26:01,  1.44s/it] 64%|██████▎   | 6238/9822 [2:47:35<1:26:39,  1.45s/it] 64%|██████▎   | 6239/9822 [2:47:37<1:26:27,  1.45s/it] 64%|██████▎   | 6240/9822 [2:47:38<1:26:13,  1.44s/it] 64%|██████▎   | 6241/9822 [2:47:39<1:26:29,  1.45s/it] 64%|██████▎   | 6242/9822 [2:47:41<1:26:22,  1.45s/it] 64%|██████▎   | 6243/9822 [2:47:42<1:26:09,  1.44s/it] 64%|██████▎   | 6244/9822 [2:47:44<1:26:01,  1.44s/it] 64%|██████▎   | 6245/9822 [2:47:45<1:26:03,  1.44s/it] 64%|██████▎   | 6246/9822 [2:47:47<1:26:00,  1.44s/it] 64%|██████▎   | 6247/9822 [2:47:48<1:25:49,  1.44s/it] 64%|██████▎   | 6248/9822 [2:47:50<1:25:36,  1.44s/it] 64%|██████▎   | 6249/9822 [2:47:51<1:25:35,  1.44s/it] 64%|██████▎   | 6250/9822 [2:47:52<1:25:30,  1.44s/it] 64%|██████▎   | 6251/9822 [2:47:54<1:25:17,  1.43s/it] 64%|██████▎   | 6252/9822 [2:47:55<1:25:14,  1.43s/it] 64%|██████▎   | 6253/9822 [2:47:57<1:25:22,  1.44s/it] 64%|██████▎   | 6254/9822 [2:47:58<1:25:26,  1.44s/it] 64%|██████▎   | 6255/9822 [2:48:00<1:26:48,  1.46s/it] 64%|██████▎   | 6256/9822 [2:48:01<1:26:37,  1.46s/it] 64%|██████▎   | 6257/9822 [2:48:03<1:26:07,  1.45s/it] 64%|██████▎   | 6258/9822 [2:48:04<1:25:54,  1.45s/it] 64%|██████▎   | 6259/9822 [2:48:05<1:25:45,  1.44s/it] 64%|██████▎   | 6260/9822 [2:48:07<1:25:41,  1.44s/it] 64%|██████▎   | 6261/9822 [2:48:08<1:25:37,  1.44s/it] 64%|██████▍   | 6262/9822 [2:48:10<1:25:30,  1.44s/it] 64%|██████▍   | 6263/9822 [2:48:11<1:25:43,  1.45s/it] 64%|██████▍   | 6264/9822 [2:48:13<1:25:42,  1.45s/it] 64%|██████▍   | 6265/9822 [2:48:14<1:25:22,  1.44s/it] 64%|██████▍   | 6266/9822 [2:48:15<1:25:36,  1.44s/it] 64%|██████▍   | 6267/9822 [2:48:17<1:25:16,  1.44s/it] 64%|██████▍   | 6268/9822 [2:48:18<1:25:13,  1.44s/it] 64%|██████▍   | 6269/9822 [2:48:20<1:25:01,  1.44s/it] 64%|██████▍   | 6270/9822 [2:48:21<1:24:52,  1.43s/it] 64%|██████▍   | 6271/9822 [2:48:23<1:24:41,  1.43s/it] 64%|██████▍   | 6272/9822 [2:48:24<1:24:45,  1.43s/it] 64%|██████▍   | 6273/9822 [2:48:26<1:25:04,  1.44s/it] 64%|██████▍   | 6274/9822 [2:48:27<1:24:53,  1.44s/it] 64%|██████▍   | 6275/9822 [2:48:28<1:24:52,  1.44s/it] 64%|██████▍   | 6276/9822 [2:48:30<1:24:46,  1.43s/it] 64%|██████▍   | 6277/9822 [2:48:31<1:24:35,  1.43s/it] 64%|██████▍   | 6278/9822 [2:48:33<1:23:47,  1.42s/it] 64%|██████▍   | 6279/9822 [2:48:34<1:24:06,  1.42s/it] 64%|██████▍   | 6280/9822 [2:48:36<1:24:27,  1.43s/it] 64%|██████▍   | 6281/9822 [2:48:37<1:24:31,  1.43s/it] 64%|██████▍   | 6282/9822 [2:48:38<1:24:38,  1.43s/it] 64%|██████▍   | 6283/9822 [2:48:40<1:24:37,  1.43s/it] 64%|██████▍   | 6284/9822 [2:48:41<1:24:41,  1.44s/it] 64%|██████▍   | 6285/9822 [2:48:43<1:24:46,  1.44s/it] 64%|██████▍   | 6286/9822 [2:48:44<1:24:45,  1.44s/it] 64%|██████▍   | 6287/9822 [2:48:46<1:26:08,  1.46s/it] 64%|██████▍   | 6288/9822 [2:48:47<1:25:38,  1.45s/it] 64%|██████▍   | 6289/9822 [2:48:49<1:25:10,  1.45s/it] 64%|██████▍   | 6290/9822 [2:48:50<1:25:00,  1.44s/it] 64%|██████▍   | 6291/9822 [2:48:51<1:24:54,  1.44s/it] 64%|██████▍   | 6292/9822 [2:48:53<1:25:27,  1.45s/it] 64%|██████▍   | 6293/9822 [2:48:54<1:25:06,  1.45s/it] 64%|██████▍   | 6294/9822 [2:48:56<1:24:57,  1.44s/it] 64%|██████▍   | 6295/9822 [2:48:57<1:24:48,  1.44s/it] 64%|██████▍   | 6296/9822 [2:48:59<1:25:38,  1.46s/it] 64%|██████▍   | 6297/9822 [2:49:00<1:25:28,  1.45s/it] 64%|██████▍   | 6298/9822 [2:49:02<1:25:09,  1.45s/it] 64%|██████▍   | 6299/9822 [2:49:03<1:24:56,  1.45s/it] 64%|██████▍   | 6300/9822 [2:49:04<1:24:43,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:36:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:36:51 - INFO - __main__ - ***** test Results*****
01/08/2024 00:36:51 - INFO - __main__ -   Training step = 6300
01/08/2024 00:36:51 - INFO - __main__ -  test_accuracy:0.8814055636896047 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:36:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:36:57 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:36:57 - INFO - __main__ -   Training step = 6300
01/08/2024 00:36:57 - INFO - __main__ -  eval_accuracy:0.8608568290003662 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8608568290003662}
test:
{'accuracy': 0.8777452415812591}
01/08/2024 00:37:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:37:02 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:37:02 - INFO - __main__ -   Training step = 6300
01/08/2024 00:37:02 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 64%|██████▍   | 6301/9822 [2:49:23<6:22:30,  6.52s/it] 64%|██████▍   | 6302/9822 [2:49:24<4:53:03,  5.00s/it] 64%|██████▍   | 6303/9822 [2:49:26<3:50:12,  3.93s/it] 64%|██████▍   | 6304/9822 [2:49:27<3:06:37,  3.18s/it] 64%|██████▍   | 6305/9822 [2:49:29<2:36:18,  2.67s/it] 64%|██████▍   | 6306/9822 [2:49:30<2:14:48,  2.30s/it] 64%|██████▍   | 6307/9822 [2:49:31<1:59:46,  2.04s/it] 64%|██████▍   | 6308/9822 [2:49:33<1:49:02,  1.86s/it] 64%|██████▍   | 6309/9822 [2:49:34<1:41:37,  1.74s/it] 64%|██████▍   | 6310/9822 [2:49:36<1:36:28,  1.65s/it] 64%|██████▍   | 6311/9822 [2:49:37<1:32:36,  1.58s/it] 64%|██████▍   | 6312/9822 [2:49:39<1:30:05,  1.54s/it] 64%|██████▍   | 6313/9822 [2:49:40<1:28:01,  1.51s/it] 64%|██████▍   | 6314/9822 [2:49:42<1:26:36,  1.48s/it] 64%|██████▍   | 6315/9822 [2:49:43<1:25:43,  1.47s/it] 64%|██████▍   | 6316/9822 [2:49:44<1:26:28,  1.48s/it] 64%|██████▍   | 6317/9822 [2:49:46<1:25:34,  1.46s/it] 64%|██████▍   | 6318/9822 [2:49:47<1:24:45,  1.45s/it] 64%|██████▍   | 6319/9822 [2:49:49<1:24:14,  1.44s/it] 64%|██████▍   | 6320/9822 [2:49:50<1:23:53,  1.44s/it] 64%|██████▍   | 6321/9822 [2:49:52<1:23:42,  1.43s/it] 64%|██████▍   | 6322/9822 [2:49:53<1:23:23,  1.43s/it] 64%|██████▍   | 6323/9822 [2:49:54<1:23:14,  1.43s/it] 64%|██████▍   | 6324/9822 [2:49:56<1:23:20,  1.43s/it] 64%|██████▍   | 6325/9822 [2:49:57<1:23:34,  1.43s/it] 64%|██████▍   | 6326/9822 [2:49:59<1:23:17,  1.43s/it] 64%|██████▍   | 6327/9822 [2:50:00<1:23:07,  1.43s/it] 64%|██████▍   | 6328/9822 [2:50:02<1:23:15,  1.43s/it] 64%|██████▍   | 6329/9822 [2:50:03<1:23:11,  1.43s/it] 64%|██████▍   | 6330/9822 [2:50:04<1:23:06,  1.43s/it] 64%|██████▍   | 6331/9822 [2:50:06<1:23:03,  1.43s/it] 64%|██████▍   | 6332/9822 [2:50:07<1:23:38,  1.44s/it] 64%|██████▍   | 6333/9822 [2:50:09<1:24:42,  1.46s/it] 64%|██████▍   | 6334/9822 [2:50:10<1:25:14,  1.47s/it] 64%|██████▍   | 6335/9822 [2:50:12<1:24:58,  1.46s/it] 65%|██████▍   | 6336/9822 [2:50:13<1:25:17,  1.47s/it] 65%|██████▍   | 6337/9822 [2:50:15<1:25:00,  1.46s/it] 65%|██████▍   | 6338/9822 [2:50:16<1:25:25,  1.47s/it] 65%|██████▍   | 6339/9822 [2:50:18<1:25:01,  1.46s/it] 65%|██████▍   | 6340/9822 [2:50:19<1:24:56,  1.46s/it] 65%|██████▍   | 6341/9822 [2:50:21<1:25:09,  1.47s/it] 65%|██████▍   | 6342/9822 [2:50:22<1:24:34,  1.46s/it] 65%|██████▍   | 6343/9822 [2:50:23<1:24:10,  1.45s/it] 65%|██████▍   | 6344/9822 [2:50:25<1:23:52,  1.45s/it] 65%|██████▍   | 6345/9822 [2:50:26<1:23:53,  1.45s/it] 65%|██████▍   | 6346/9822 [2:50:28<1:23:39,  1.44s/it] 65%|██████▍   | 6347/9822 [2:50:29<1:23:27,  1.44s/it] 65%|██████▍   | 6348/9822 [2:50:31<1:24:47,  1.46s/it] 65%|██████▍   | 6349/9822 [2:50:32<1:24:23,  1.46s/it] 65%|██████▍   | 6350/9822 [2:50:34<1:23:57,  1.45s/it] 65%|██████▍   | 6351/9822 [2:50:35<1:23:30,  1.44s/it] 65%|██████▍   | 6352/9822 [2:50:36<1:23:29,  1.44s/it] 65%|██████▍   | 6353/9822 [2:50:38<1:23:11,  1.44s/it] 65%|██████▍   | 6354/9822 [2:50:39<1:22:55,  1.43s/it] 65%|██████▍   | 6355/9822 [2:50:41<1:22:42,  1.43s/it] 65%|██████▍   | 6356/9822 [2:50:42<1:22:30,  1.43s/it] 65%|██████▍   | 6357/9822 [2:50:44<1:22:36,  1.43s/it] 65%|██████▍   | 6358/9822 [2:50:45<1:22:39,  1.43s/it] 65%|██████▍   | 6359/9822 [2:50:46<1:22:48,  1.43s/it] 65%|██████▍   | 6360/9822 [2:50:48<1:22:50,  1.44s/it] 65%|██████▍   | 6361/9822 [2:50:49<1:22:53,  1.44s/it] 65%|██████▍   | 6362/9822 [2:50:51<1:22:53,  1.44s/it] 65%|██████▍   | 6363/9822 [2:50:52<1:22:40,  1.43s/it] 65%|██████▍   | 6364/9822 [2:50:54<1:21:54,  1.42s/it] 65%|██████▍   | 6365/9822 [2:50:55<1:22:05,  1.42s/it] 65%|██████▍   | 6366/9822 [2:50:56<1:22:03,  1.42s/it] 65%|██████▍   | 6367/9822 [2:50:58<1:22:19,  1.43s/it] 65%|██████▍   | 6368/9822 [2:50:59<1:22:26,  1.43s/it] 65%|██████▍   | 6369/9822 [2:51:01<1:22:28,  1.43s/it] 65%|██████▍   | 6370/9822 [2:51:02<1:22:20,  1.43s/it] 65%|██████▍   | 6371/9822 [2:51:04<1:22:33,  1.44s/it] 65%|██████▍   | 6372/9822 [2:51:05<1:22:36,  1.44s/it] 65%|██████▍   | 6373/9822 [2:51:07<1:23:57,  1.46s/it] 65%|██████▍   | 6374/9822 [2:51:08<1:23:22,  1.45s/it] 65%|██████▍   | 6375/9822 [2:51:09<1:23:05,  1.45s/it] 65%|██████▍   | 6376/9822 [2:51:11<1:22:45,  1.44s/it] 65%|██████▍   | 6377/9822 [2:51:12<1:22:38,  1.44s/it] 65%|██████▍   | 6378/9822 [2:51:14<1:22:39,  1.44s/it] 65%|██████▍   | 6379/9822 [2:51:15<1:22:35,  1.44s/it] 65%|██████▍   | 6380/9822 [2:51:17<1:22:48,  1.44s/it] 65%|██████▍   | 6381/9822 [2:51:18<1:22:41,  1.44s/it] 65%|██████▍   | 6382/9822 [2:51:20<1:22:42,  1.44s/it] 65%|██████▍   | 6383/9822 [2:51:21<1:22:44,  1.44s/it] 65%|██████▍   | 6384/9822 [2:51:22<1:22:46,  1.44s/it] 65%|██████▌   | 6385/9822 [2:51:24<1:22:49,  1.45s/it] 65%|██████▌   | 6386/9822 [2:51:25<1:22:41,  1.44s/it] 65%|██████▌   | 6387/9822 [2:51:27<1:22:35,  1.44s/it] 65%|██████▌   | 6388/9822 [2:51:28<1:22:15,  1.44s/it] 65%|██████▌   | 6389/9822 [2:51:30<1:22:15,  1.44s/it] 65%|██████▌   | 6390/9822 [2:51:31<1:22:20,  1.44s/it] 65%|██████▌   | 6391/9822 [2:51:33<1:22:18,  1.44s/it] 65%|██████▌   | 6392/9822 [2:51:34<1:22:11,  1.44s/it] 65%|██████▌   | 6393/9822 [2:51:35<1:22:09,  1.44s/it] 65%|██████▌   | 6394/9822 [2:51:37<1:21:57,  1.43s/it] 65%|██████▌   | 6395/9822 [2:51:38<1:21:53,  1.43s/it] 65%|██████▌   | 6396/9822 [2:51:40<1:21:48,  1.43s/it] 65%|██████▌   | 6397/9822 [2:51:41<1:21:56,  1.44s/it] 65%|██████▌   | 6398/9822 [2:51:43<1:21:58,  1.44s/it] 65%|██████▌   | 6399/9822 [2:51:44<1:21:58,  1.44s/it] 65%|██████▌   | 6400/9822 [2:51:45<1:21:54,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1208, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:39:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:39:32 - INFO - __main__ - ***** test Results*****
01/08/2024 00:39:32 - INFO - __main__ -   Training step = 6400
01/08/2024 00:39:32 - INFO - __main__ -  test_accuracy:0.8821376281112738 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:39:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:39:38 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:39:38 - INFO - __main__ -   Training step = 6400
01/08/2024 00:39:38 - INFO - __main__ -  eval_accuracy:0.8612229952398389 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 00:39:38,879 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 00:39:38,879 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 00:39:38,916 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 00:39:40,491 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8612229952398389}
test:
{'accuracy': 0.8821376281112738}
01/08/2024 00:39:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:39:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:39:45 - INFO - __main__ -   Training step = 6400
01/08/2024 00:39:45 - INFO - __main__ -  eval_accuracy:0.9088246063712926 
 65%|██████▌   | 6401/9822 [2:52:05<6:38:48,  6.99s/it] 65%|██████▌   | 6402/9822 [2:52:07<5:03:38,  5.33s/it] 65%|██████▌   | 6403/9822 [2:52:08<3:58:28,  4.18s/it] 65%|██████▌   | 6404/9822 [2:52:10<3:11:36,  3.36s/it] 65%|██████▌   | 6405/9822 [2:52:11<2:38:36,  2.79s/it] 65%|██████▌   | 6406/9822 [2:52:13<2:15:28,  2.38s/it] 65%|██████▌   | 6407/9822 [2:52:14<1:59:19,  2.10s/it] 65%|██████▌   | 6408/9822 [2:52:16<1:47:57,  1.90s/it] 65%|██████▌   | 6409/9822 [2:52:17<1:39:59,  1.76s/it] 65%|██████▌   | 6410/9822 [2:52:18<1:34:21,  1.66s/it] 65%|██████▌   | 6411/9822 [2:52:20<1:30:34,  1.59s/it] 65%|██████▌   | 6412/9822 [2:52:21<1:27:48,  1.55s/it] 65%|██████▌   | 6413/9822 [2:52:23<1:25:58,  1.51s/it] 65%|██████▌   | 6414/9822 [2:52:24<1:24:58,  1.50s/it] 65%|██████▌   | 6415/9822 [2:52:26<1:23:52,  1.48s/it] 65%|██████▌   | 6416/9822 [2:52:27<1:23:01,  1.46s/it] 65%|██████▌   | 6417/9822 [2:52:28<1:22:39,  1.46s/it] 65%|██████▌   | 6418/9822 [2:52:30<1:22:45,  1.46s/it] 65%|██████▌   | 6419/9822 [2:52:31<1:22:35,  1.46s/it] 65%|██████▌   | 6420/9822 [2:52:33<1:22:00,  1.45s/it] 65%|██████▌   | 6421/9822 [2:52:34<1:21:41,  1.44s/it] 65%|██████▌   | 6422/9822 [2:52:36<1:21:50,  1.44s/it] 65%|██████▌   | 6423/9822 [2:52:37<1:21:40,  1.44s/it] 65%|██████▌   | 6424/9822 [2:52:39<1:21:32,  1.44s/it] 65%|██████▌   | 6425/9822 [2:52:40<1:21:33,  1.44s/it] 65%|██████▌   | 6426/9822 [2:52:41<1:21:17,  1.44s/it] 65%|██████▌   | 6427/9822 [2:52:43<1:21:13,  1.44s/it] 65%|██████▌   | 6428/9822 [2:52:44<1:21:02,  1.43s/it] 65%|██████▌   | 6429/9822 [2:52:46<1:21:09,  1.44s/it] 65%|██████▌   | 6430/9822 [2:52:47<1:21:05,  1.43s/it] 65%|██████▌   | 6431/9822 [2:52:49<1:20:57,  1.43s/it] 65%|██████▌   | 6432/9822 [2:52:50<1:20:54,  1.43s/it] 65%|██████▌   | 6433/9822 [2:52:52<1:21:28,  1.44s/it] 66%|██████▌   | 6434/9822 [2:52:53<1:21:06,  1.44s/it] 66%|██████▌   | 6435/9822 [2:52:54<1:22:59,  1.47s/it] 66%|██████▌   | 6436/9822 [2:52:56<1:22:31,  1.46s/it] 66%|██████▌   | 6437/9822 [2:52:57<1:21:59,  1.45s/it] 66%|██████▌   | 6438/9822 [2:52:59<1:21:47,  1.45s/it] 66%|██████▌   | 6439/9822 [2:53:00<1:21:39,  1.45s/it] 66%|██████▌   | 6440/9822 [2:53:02<1:21:34,  1.45s/it] 66%|██████▌   | 6441/9822 [2:53:03<1:21:18,  1.44s/it] 66%|██████▌   | 6442/9822 [2:53:05<1:21:03,  1.44s/it] 66%|██████▌   | 6443/9822 [2:53:06<1:21:12,  1.44s/it] 66%|██████▌   | 6444/9822 [2:53:07<1:21:10,  1.44s/it] 66%|██████▌   | 6445/9822 [2:53:09<1:21:03,  1.44s/it] 66%|██████▌   | 6446/9822 [2:53:10<1:20:57,  1.44s/it] 66%|██████▌   | 6447/9822 [2:53:12<1:21:04,  1.44s/it] 66%|██████▌   | 6448/9822 [2:53:13<1:20:53,  1.44s/it] 66%|██████▌   | 6449/9822 [2:53:15<1:20:54,  1.44s/it] 66%|██████▌   | 6450/9822 [2:53:16<1:19:58,  1.42s/it] 66%|██████▌   | 6451/9822 [2:53:17<1:20:07,  1.43s/it] 66%|██████▌   | 6452/9822 [2:53:19<1:20:06,  1.43s/it] 66%|██████▌   | 6453/9822 [2:53:20<1:20:05,  1.43s/it] 66%|██████▌   | 6454/9822 [2:53:22<1:20:07,  1.43s/it] 66%|██████▌   | 6455/9822 [2:53:23<1:20:16,  1.43s/it] 66%|██████▌   | 6456/9822 [2:53:25<1:20:17,  1.43s/it] 66%|██████▌   | 6457/9822 [2:53:26<1:20:25,  1.43s/it] 66%|██████▌   | 6458/9822 [2:53:27<1:20:22,  1.43s/it] 66%|██████▌   | 6459/9822 [2:53:29<1:20:06,  1.43s/it] 66%|██████▌   | 6460/9822 [2:53:30<1:20:05,  1.43s/it] 66%|██████▌   | 6461/9822 [2:53:32<1:20:21,  1.43s/it] 66%|██████▌   | 6462/9822 [2:53:33<1:20:19,  1.43s/it] 66%|██████▌   | 6463/9822 [2:53:35<1:20:05,  1.43s/it] 66%|██████▌   | 6464/9822 [2:53:36<1:20:08,  1.43s/it] 66%|██████▌   | 6465/9822 [2:53:37<1:19:59,  1.43s/it] 66%|██████▌   | 6466/9822 [2:53:39<1:19:47,  1.43s/it] 66%|██████▌   | 6467/9822 [2:53:40<1:21:13,  1.45s/it] 66%|██████▌   | 6468/9822 [2:53:42<1:20:51,  1.45s/it] 66%|██████▌   | 6469/9822 [2:53:43<1:20:26,  1.44s/it] 66%|██████▌   | 6470/9822 [2:53:45<1:20:15,  1.44s/it] 66%|██████▌   | 6471/9822 [2:53:46<1:20:02,  1.43s/it] 66%|██████▌   | 6472/9822 [2:53:48<1:20:00,  1.43s/it] 66%|██████▌   | 6473/9822 [2:53:49<1:19:54,  1.43s/it] 66%|██████▌   | 6474/9822 [2:53:50<1:19:58,  1.43s/it] 66%|██████▌   | 6475/9822 [2:53:52<1:20:02,  1.43s/it] 66%|██████▌   | 6476/9822 [2:53:53<1:20:10,  1.44s/it] 66%|██████▌   | 6477/9822 [2:53:55<1:20:07,  1.44s/it] 66%|██████▌   | 6478/9822 [2:53:56<1:19:52,  1.43s/it] 66%|██████▌   | 6479/9822 [2:53:58<1:19:44,  1.43s/it] 66%|██████▌   | 6480/9822 [2:53:59<1:19:41,  1.43s/it] 66%|██████▌   | 6481/9822 [2:54:00<1:19:40,  1.43s/it] 66%|██████▌   | 6482/9822 [2:54:02<1:19:42,  1.43s/it] 66%|██████▌   | 6483/9822 [2:54:03<1:19:53,  1.44s/it] 66%|██████▌   | 6484/9822 [2:54:05<1:19:46,  1.43s/it] 66%|██████▌   | 6485/9822 [2:54:06<1:19:41,  1.43s/it] 66%|██████▌   | 6486/9822 [2:54:08<1:19:43,  1.43s/it] 66%|██████▌   | 6487/9822 [2:54:09<1:19:39,  1.43s/it] 66%|██████▌   | 6488/9822 [2:54:11<1:19:40,  1.43s/it] 66%|██████▌   | 6489/9822 [2:54:12<1:19:40,  1.43s/it] 66%|██████▌   | 6490/9822 [2:54:13<1:19:36,  1.43s/it] 66%|██████▌   | 6491/9822 [2:54:15<1:19:30,  1.43s/it] 66%|██████▌   | 6492/9822 [2:54:16<1:21:03,  1.46s/it] 66%|██████▌   | 6493/9822 [2:54:18<1:20:34,  1.45s/it] 66%|██████▌   | 6494/9822 [2:54:19<1:20:15,  1.45s/it] 66%|██████▌   | 6495/9822 [2:54:21<1:19:52,  1.44s/it] 66%|██████▌   | 6496/9822 [2:54:22<1:19:38,  1.44s/it] 66%|██████▌   | 6497/9822 [2:54:23<1:19:40,  1.44s/it] 66%|██████▌   | 6498/9822 [2:54:25<1:19:41,  1.44s/it] 66%|██████▌   | 6499/9822 [2:54:26<1:19:36,  1.44s/it] 66%|██████▌   | 6500/9822 [2:54:28<1:19:41,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:42:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:42:15 - INFO - __main__ - ***** test Results*****
01/08/2024 00:42:15 - INFO - __main__ -   Training step = 6500
01/08/2024 00:42:15 - INFO - __main__ -  test_accuracy:0.8792093704245973 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:42:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:42:21 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:42:21 - INFO - __main__ -   Training step = 6500
01/08/2024 00:42:21 - INFO - __main__ -  eval_accuracy:0.8604906627608935 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8612229952398389}
test:
{'accuracy': 0.8821376281112738}
01/08/2024 00:42:25 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:42:25 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:42:25 - INFO - __main__ -   Training step = 6500
01/08/2024 00:42:25 - INFO - __main__ -  eval_accuracy:0.9088246063712926 
 66%|██████▌   | 6501/9822 [2:54:46<6:00:39,  6.52s/it] 66%|██████▌   | 6502/9822 [2:54:48<4:36:18,  4.99s/it] 66%|██████▌   | 6503/9822 [2:54:49<3:37:45,  3.94s/it] 66%|██████▌   | 6504/9822 [2:54:51<2:56:02,  3.18s/it] 66%|██████▌   | 6505/9822 [2:54:52<2:26:58,  2.66s/it] 66%|██████▌   | 6506/9822 [2:54:53<2:06:37,  2.29s/it] 66%|██████▌   | 6507/9822 [2:54:55<1:52:19,  2.03s/it] 66%|██████▋   | 6508/9822 [2:54:56<1:42:18,  1.85s/it] 66%|██████▋   | 6509/9822 [2:54:58<1:35:14,  1.72s/it] 66%|██████▋   | 6510/9822 [2:54:59<1:30:41,  1.64s/it] 66%|██████▋   | 6511/9822 [2:55:01<1:27:30,  1.59s/it] 66%|██████▋   | 6512/9822 [2:55:02<1:25:57,  1.56s/it] 66%|██████▋   | 6513/9822 [2:55:03<1:23:51,  1.52s/it] 66%|██████▋   | 6514/9822 [2:55:05<1:22:56,  1.50s/it] 66%|██████▋   | 6515/9822 [2:55:06<1:22:36,  1.50s/it] 66%|██████▋   | 6516/9822 [2:55:08<1:22:09,  1.49s/it] 66%|██████▋   | 6517/9822 [2:55:09<1:21:07,  1.47s/it] 66%|██████▋   | 6518/9822 [2:55:11<1:21:49,  1.49s/it] 66%|██████▋   | 6519/9822 [2:55:12<1:21:06,  1.47s/it] 66%|██████▋   | 6520/9822 [2:55:14<1:20:36,  1.46s/it] 66%|██████▋   | 6521/9822 [2:55:15<1:19:59,  1.45s/it] 66%|██████▋   | 6522/9822 [2:55:17<1:19:49,  1.45s/it] 66%|██████▋   | 6523/9822 [2:55:18<1:19:34,  1.45s/it] 66%|██████▋   | 6524/9822 [2:55:19<1:19:21,  1.44s/it] 66%|██████▋   | 6525/9822 [2:55:21<1:19:04,  1.44s/it] 66%|██████▋   | 6526/9822 [2:55:22<1:18:50,  1.44s/it] 66%|██████▋   | 6527/9822 [2:55:24<1:18:54,  1.44s/it] 66%|██████▋   | 6528/9822 [2:55:25<1:18:45,  1.43s/it] 66%|██████▋   | 6529/9822 [2:55:27<1:18:33,  1.43s/it] 66%|██████▋   | 6530/9822 [2:55:28<1:18:22,  1.43s/it] 66%|██████▋   | 6531/9822 [2:55:30<1:18:26,  1.43s/it] 67%|██████▋   | 6532/9822 [2:55:31<1:18:37,  1.43s/it] 67%|██████▋   | 6533/9822 [2:55:32<1:18:35,  1.43s/it] 67%|██████▋   | 6534/9822 [2:55:34<1:18:40,  1.44s/it] 67%|██████▋   | 6535/9822 [2:55:35<1:18:48,  1.44s/it] 67%|██████▋   | 6536/9822 [2:55:37<1:17:55,  1.42s/it] 67%|██████▋   | 6537/9822 [2:55:38<1:18:19,  1.43s/it] 67%|██████▋   | 6538/9822 [2:55:40<1:18:29,  1.43s/it] 67%|██████▋   | 6539/9822 [2:55:41<1:18:25,  1.43s/it] 67%|██████▋   | 6540/9822 [2:55:42<1:18:28,  1.43s/it] 67%|██████▋   | 6541/9822 [2:55:44<1:18:45,  1.44s/it] 67%|██████▋   | 6542/9822 [2:55:45<1:18:36,  1.44s/it] 67%|██████▋   | 6543/9822 [2:55:47<1:18:32,  1.44s/it] 67%|██████▋   | 6544/9822 [2:55:48<1:18:40,  1.44s/it] 67%|██████▋   | 6545/9822 [2:55:50<1:18:28,  1.44s/it] 67%|██████▋   | 6546/9822 [2:55:51<1:18:27,  1.44s/it] 67%|██████▋   | 6547/9822 [2:55:52<1:18:18,  1.43s/it] 67%|██████▋   | 6548/9822 [2:55:53<1:09:40,  1.28s/it] 67%|██████▋   | 6549/9822 [2:55:55<1:12:32,  1.33s/it] 67%|██████▋   | 6550/9822 [2:55:56<1:15:49,  1.39s/it] 67%|██████▋   | 6551/9822 [2:55:58<1:16:42,  1.41s/it] 67%|██████▋   | 6552/9822 [2:55:59<1:17:15,  1.42s/it] 67%|██████▋   | 6553/9822 [2:56:01<1:17:31,  1.42s/it] 67%|██████▋   | 6554/9822 [2:56:02<1:17:27,  1.42s/it] 67%|██████▋   | 6555/9822 [2:56:04<1:17:37,  1.43s/it] 67%|██████▋   | 6556/9822 [2:56:05<1:17:37,  1.43s/it] 67%|██████▋   | 6557/9822 [2:56:06<1:17:40,  1.43s/it] 67%|██████▋   | 6558/9822 [2:56:08<1:17:58,  1.43s/it] 67%|██████▋   | 6559/9822 [2:56:09<1:18:00,  1.43s/it] 67%|██████▋   | 6560/9822 [2:56:11<1:17:58,  1.43s/it] 67%|██████▋   | 6561/9822 [2:56:12<1:17:52,  1.43s/it] 67%|██████▋   | 6562/9822 [2:56:14<1:17:45,  1.43s/it] 67%|██████▋   | 6563/9822 [2:56:15<1:17:52,  1.43s/it] 67%|██████▋   | 6564/9822 [2:56:16<1:17:50,  1.43s/it] 67%|██████▋   | 6565/9822 [2:56:18<1:17:51,  1.43s/it] 67%|██████▋   | 6566/9822 [2:56:19<1:17:41,  1.43s/it] 67%|██████▋   | 6567/9822 [2:56:21<1:17:44,  1.43s/it] 67%|██████▋   | 6568/9822 [2:56:22<1:17:53,  1.44s/it] 67%|██████▋   | 6569/9822 [2:56:24<1:17:48,  1.43s/it] 67%|██████▋   | 6570/9822 [2:56:25<1:17:53,  1.44s/it] 67%|██████▋   | 6571/9822 [2:56:27<1:18:01,  1.44s/it] 67%|██████▋   | 6572/9822 [2:56:28<1:17:59,  1.44s/it] 67%|██████▋   | 6573/9822 [2:56:29<1:17:45,  1.44s/it] 67%|██████▋   | 6574/9822 [2:56:31<1:17:33,  1.43s/it] 67%|██████▋   | 6575/9822 [2:56:32<1:17:45,  1.44s/it] 67%|██████▋   | 6576/9822 [2:56:34<1:17:42,  1.44s/it] 67%|██████▋   | 6577/9822 [2:56:35<1:17:33,  1.43s/it] 67%|██████▋   | 6578/9822 [2:56:37<1:17:43,  1.44s/it] 67%|██████▋   | 6579/9822 [2:56:38<1:17:40,  1.44s/it] 67%|██████▋   | 6580/9822 [2:56:39<1:17:42,  1.44s/it] 67%|██████▋   | 6581/9822 [2:56:41<1:17:37,  1.44s/it] 67%|██████▋   | 6582/9822 [2:56:42<1:18:48,  1.46s/it] 67%|██████▋   | 6583/9822 [2:56:44<1:18:23,  1.45s/it] 67%|██████▋   | 6584/9822 [2:56:45<1:18:06,  1.45s/it] 67%|██████▋   | 6585/9822 [2:56:47<1:17:42,  1.44s/it] 67%|██████▋   | 6586/9822 [2:56:48<1:17:29,  1.44s/it] 67%|██████▋   | 6587/9822 [2:56:50<1:17:19,  1.43s/it] 67%|██████▋   | 6588/9822 [2:56:51<1:17:29,  1.44s/it] 67%|██████▋   | 6589/9822 [2:56:52<1:17:58,  1.45s/it] 67%|██████▋   | 6590/9822 [2:56:54<1:17:56,  1.45s/it] 67%|██████▋   | 6591/9822 [2:56:55<1:17:41,  1.44s/it] 67%|██████▋   | 6592/9822 [2:56:57<1:17:24,  1.44s/it] 67%|██████▋   | 6593/9822 [2:56:58<1:17:18,  1.44s/it] 67%|██████▋   | 6594/9822 [2:57:00<1:17:10,  1.43s/it] 67%|██████▋   | 6595/9822 [2:57:01<1:16:59,  1.43s/it] 67%|██████▋   | 6596/9822 [2:57:02<1:16:57,  1.43s/it] 67%|██████▋   | 6597/9822 [2:57:04<1:16:50,  1.43s/it] 67%|██████▋   | 6598/9822 [2:57:05<1:16:42,  1.43s/it] 67%|██████▋   | 6599/9822 [2:57:07<1:16:56,  1.43s/it] 67%|██████▋   | 6600/9822 [2:57:08<1:16:48,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1266, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:44:55 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:44:55 - INFO - __main__ - ***** test Results*****
01/08/2024 00:44:55 - INFO - __main__ -   Training step = 6600
01/08/2024 00:44:55 - INFO - __main__ -  test_accuracy:0.8784773060029283 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:45:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:45:01 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:45:01 - INFO - __main__ -   Training step = 6600
01/08/2024 00:45:01 - INFO - __main__ -  eval_accuracy:0.8608568290003662 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8612229952398389}
test:
{'accuracy': 0.8821376281112738}
01/08/2024 00:45:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:45:06 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:45:06 - INFO - __main__ -   Training step = 6600
01/08/2024 00:45:06 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 67%|██████▋   | 6601/9822 [2:57:27<5:49:14,  6.51s/it] 67%|██████▋   | 6602/9822 [2:57:28<4:27:19,  4.98s/it] 67%|██████▋   | 6603/9822 [2:57:29<3:30:10,  3.92s/it] 67%|██████▋   | 6604/9822 [2:57:31<2:50:09,  3.17s/it] 67%|██████▋   | 6605/9822 [2:57:32<2:22:09,  2.65s/it] 67%|██████▋   | 6606/9822 [2:57:34<2:02:33,  2.29s/it] 67%|██████▋   | 6607/9822 [2:57:35<1:48:58,  2.03s/it] 67%|██████▋   | 6608/9822 [2:57:37<1:39:30,  1.86s/it] 67%|██████▋   | 6609/9822 [2:57:38<1:33:53,  1.75s/it] 67%|██████▋   | 6610/9822 [2:57:40<1:28:41,  1.66s/it] 67%|██████▋   | 6611/9822 [2:57:41<1:25:10,  1.59s/it] 67%|██████▋   | 6612/9822 [2:57:42<1:22:37,  1.54s/it] 67%|██████▋   | 6613/9822 [2:57:44<1:20:52,  1.51s/it] 67%|██████▋   | 6614/9822 [2:57:45<1:20:17,  1.50s/it] 67%|██████▋   | 6615/9822 [2:57:47<1:19:16,  1.48s/it] 67%|██████▋   | 6616/9822 [2:57:48<1:18:33,  1.47s/it] 67%|██████▋   | 6617/9822 [2:57:50<1:18:00,  1.46s/it] 67%|██████▋   | 6618/9822 [2:57:51<1:17:32,  1.45s/it] 67%|██████▋   | 6619/9822 [2:57:53<1:17:19,  1.45s/it] 67%|██████▋   | 6620/9822 [2:57:54<1:16:59,  1.44s/it] 67%|██████▋   | 6621/9822 [2:57:55<1:17:23,  1.45s/it] 67%|██████▋   | 6622/9822 [2:57:57<1:16:06,  1.43s/it] 67%|██████▋   | 6623/9822 [2:57:58<1:16:09,  1.43s/it] 67%|██████▋   | 6624/9822 [2:58:00<1:16:20,  1.43s/it] 67%|██████▋   | 6625/9822 [2:58:01<1:16:39,  1.44s/it] 67%|██████▋   | 6626/9822 [2:58:03<1:16:34,  1.44s/it] 67%|██████▋   | 6627/9822 [2:58:04<1:17:10,  1.45s/it] 67%|██████▋   | 6628/9822 [2:58:05<1:17:09,  1.45s/it] 67%|██████▋   | 6629/9822 [2:58:07<1:17:00,  1.45s/it] 68%|██████▊   | 6630/9822 [2:58:08<1:17:35,  1.46s/it] 68%|██████▊   | 6631/9822 [2:58:10<1:17:04,  1.45s/it] 68%|██████▊   | 6632/9822 [2:58:11<1:16:36,  1.44s/it] 68%|██████▊   | 6633/9822 [2:58:13<1:16:30,  1.44s/it] 68%|██████▊   | 6634/9822 [2:58:14<1:16:27,  1.44s/it] 68%|██████▊   | 6635/9822 [2:58:16<1:16:23,  1.44s/it] 68%|██████▊   | 6636/9822 [2:58:17<1:16:09,  1.43s/it] 68%|██████▊   | 6637/9822 [2:58:18<1:15:55,  1.43s/it] 68%|██████▊   | 6638/9822 [2:58:20<1:15:54,  1.43s/it] 68%|██████▊   | 6639/9822 [2:58:21<1:17:05,  1.45s/it] 68%|██████▊   | 6640/9822 [2:58:23<1:16:36,  1.44s/it] 68%|██████▊   | 6641/9822 [2:58:24<1:16:16,  1.44s/it] 68%|██████▊   | 6642/9822 [2:58:26<1:16:19,  1.44s/it] 68%|██████▊   | 6643/9822 [2:58:27<1:16:05,  1.44s/it] 68%|██████▊   | 6644/9822 [2:58:29<1:16:07,  1.44s/it] 68%|██████▊   | 6645/9822 [2:58:30<1:16:08,  1.44s/it] 68%|██████▊   | 6646/9822 [2:58:31<1:16:06,  1.44s/it] 68%|██████▊   | 6647/9822 [2:58:33<1:16:06,  1.44s/it] 68%|██████▊   | 6648/9822 [2:58:34<1:16:00,  1.44s/it] 68%|██████▊   | 6649/9822 [2:58:36<1:16:33,  1.45s/it] 68%|██████▊   | 6650/9822 [2:58:37<1:16:18,  1.44s/it] 68%|██████▊   | 6651/9822 [2:58:39<1:16:05,  1.44s/it] 68%|██████▊   | 6652/9822 [2:58:40<1:16:02,  1.44s/it] 68%|██████▊   | 6653/9822 [2:58:41<1:15:59,  1.44s/it] 68%|██████▊   | 6654/9822 [2:58:43<1:15:51,  1.44s/it] 68%|██████▊   | 6655/9822 [2:58:44<1:15:45,  1.44s/it] 68%|██████▊   | 6656/9822 [2:58:46<1:15:38,  1.43s/it] 68%|██████▊   | 6657/9822 [2:58:47<1:15:50,  1.44s/it] 68%|██████▊   | 6658/9822 [2:58:49<1:15:32,  1.43s/it] 68%|██████▊   | 6659/9822 [2:58:50<1:15:21,  1.43s/it] 68%|██████▊   | 6660/9822 [2:58:51<1:15:18,  1.43s/it] 68%|██████▊   | 6661/9822 [2:58:53<1:15:12,  1.43s/it] 68%|██████▊   | 6662/9822 [2:58:54<1:15:18,  1.43s/it] 68%|██████▊   | 6663/9822 [2:58:56<1:15:13,  1.43s/it] 68%|██████▊   | 6664/9822 [2:58:57<1:15:14,  1.43s/it] 68%|██████▊   | 6665/9822 [2:58:59<1:15:41,  1.44s/it] 68%|██████▊   | 6666/9822 [2:59:00<1:16:21,  1.45s/it] 68%|██████▊   | 6667/9822 [2:59:02<1:16:04,  1.45s/it] 68%|██████▊   | 6668/9822 [2:59:03<1:16:04,  1.45s/it] 68%|██████▊   | 6669/9822 [2:59:04<1:15:52,  1.44s/it] 68%|██████▊   | 6670/9822 [2:59:06<1:15:46,  1.44s/it] 68%|██████▊   | 6671/9822 [2:59:07<1:16:50,  1.46s/it] 68%|██████▊   | 6672/9822 [2:59:09<1:16:25,  1.46s/it] 68%|██████▊   | 6673/9822 [2:59:10<1:16:09,  1.45s/it] 68%|██████▊   | 6674/9822 [2:59:12<1:15:38,  1.44s/it] 68%|██████▊   | 6675/9822 [2:59:13<1:15:34,  1.44s/it] 68%|██████▊   | 6676/9822 [2:59:15<1:15:33,  1.44s/it] 68%|██████▊   | 6677/9822 [2:59:16<1:15:27,  1.44s/it] 68%|██████▊   | 6678/9822 [2:59:17<1:15:36,  1.44s/it] 68%|██████▊   | 6679/9822 [2:59:19<1:15:24,  1.44s/it] 68%|██████▊   | 6680/9822 [2:59:20<1:15:22,  1.44s/it] 68%|██████▊   | 6681/9822 [2:59:22<1:15:27,  1.44s/it] 68%|██████▊   | 6682/9822 [2:59:23<1:15:31,  1.44s/it] 68%|██████▊   | 6683/9822 [2:59:25<1:15:23,  1.44s/it] 68%|██████▊   | 6684/9822 [2:59:26<1:15:16,  1.44s/it] 68%|██████▊   | 6685/9822 [2:59:28<1:15:14,  1.44s/it] 68%|██████▊   | 6686/9822 [2:59:29<1:14:55,  1.43s/it] 68%|██████▊   | 6687/9822 [2:59:30<1:15:05,  1.44s/it] 68%|██████▊   | 6688/9822 [2:59:32<1:15:10,  1.44s/it] 68%|██████▊   | 6689/9822 [2:59:33<1:15:17,  1.44s/it] 68%|██████▊   | 6690/9822 [2:59:35<1:15:19,  1.44s/it] 68%|██████▊   | 6691/9822 [2:59:36<1:15:05,  1.44s/it] 68%|██████▊   | 6692/9822 [2:59:38<1:15:03,  1.44s/it] 68%|██████▊   | 6693/9822 [2:59:39<1:14:43,  1.43s/it] 68%|██████▊   | 6694/9822 [2:59:40<1:14:52,  1.44s/it] 68%|██████▊   | 6695/9822 [2:59:42<1:14:53,  1.44s/it] 68%|██████▊   | 6696/9822 [2:59:43<1:15:00,  1.44s/it] 68%|██████▊   | 6697/9822 [2:59:45<1:14:57,  1.44s/it] 68%|██████▊   | 6698/9822 [2:59:46<1:14:49,  1.44s/it] 68%|██████▊   | 6699/9822 [2:59:48<1:14:57,  1.44s/it] 68%|██████▊   | 6700/9822 [2:59:49<1:15:03,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:47:36 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:47:36 - INFO - __main__ - ***** test Results*****
01/08/2024 00:47:36 - INFO - __main__ -   Training step = 6700
01/08/2024 00:47:36 - INFO - __main__ -  test_accuracy:0.8766471449487555 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:47:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:47:42 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:47:42 - INFO - __main__ -   Training step = 6700
01/08/2024 00:47:42 - INFO - __main__ -  eval_accuracy:0.8604906627608935 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8612229952398389}
test:
{'accuracy': 0.8821376281112738}
01/08/2024 00:47:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:47:47 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:47:47 - INFO - __main__ -   Training step = 6700
01/08/2024 00:47:47 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 68%|██████▊   | 6701/9822 [3:00:07<5:39:09,  6.52s/it] 68%|██████▊   | 6702/9822 [3:00:09<4:19:37,  4.99s/it] 68%|██████▊   | 6703/9822 [3:00:10<3:25:22,  3.95s/it] 68%|██████▊   | 6704/9822 [3:00:12<2:46:00,  3.19s/it] 68%|██████▊   | 6705/9822 [3:00:13<2:18:41,  2.67s/it] 68%|██████▊   | 6706/9822 [3:00:15<1:59:33,  2.30s/it] 68%|██████▊   | 6707/9822 [3:00:16<1:46:04,  2.04s/it] 68%|██████▊   | 6708/9822 [3:00:18<1:35:46,  1.85s/it] 68%|██████▊   | 6709/9822 [3:00:19<1:29:20,  1.72s/it] 68%|██████▊   | 6710/9822 [3:00:20<1:24:41,  1.63s/it] 68%|██████▊   | 6711/9822 [3:00:22<1:21:27,  1.57s/it] 68%|██████▊   | 6712/9822 [3:00:23<1:19:27,  1.53s/it] 68%|██████▊   | 6713/9822 [3:00:25<1:17:52,  1.50s/it] 68%|██████▊   | 6714/9822 [3:00:26<1:17:02,  1.49s/it] 68%|██████▊   | 6715/9822 [3:00:28<1:16:12,  1.47s/it] 68%|██████▊   | 6716/9822 [3:00:29<1:15:46,  1.46s/it] 68%|██████▊   | 6717/9822 [3:00:31<1:15:18,  1.46s/it] 68%|██████▊   | 6718/9822 [3:00:32<1:14:56,  1.45s/it] 68%|██████▊   | 6719/9822 [3:00:33<1:14:48,  1.45s/it] 68%|██████▊   | 6720/9822 [3:00:35<1:14:36,  1.44s/it] 68%|██████▊   | 6721/9822 [3:00:36<1:14:19,  1.44s/it] 68%|██████▊   | 6722/9822 [3:00:38<1:14:16,  1.44s/it] 68%|██████▊   | 6723/9822 [3:00:39<1:14:09,  1.44s/it] 68%|██████▊   | 6724/9822 [3:00:41<1:14:14,  1.44s/it] 68%|██████▊   | 6725/9822 [3:00:42<1:14:09,  1.44s/it] 68%|██████▊   | 6726/9822 [3:00:43<1:14:04,  1.44s/it] 68%|██████▊   | 6727/9822 [3:00:45<1:14:14,  1.44s/it] 68%|██████▊   | 6728/9822 [3:00:46<1:14:10,  1.44s/it] 69%|██████▊   | 6729/9822 [3:00:48<1:14:05,  1.44s/it] 69%|██████▊   | 6730/9822 [3:00:49<1:14:09,  1.44s/it] 69%|██████▊   | 6731/9822 [3:00:51<1:14:09,  1.44s/it] 69%|██████▊   | 6732/9822 [3:00:52<1:14:03,  1.44s/it] 69%|██████▊   | 6733/9822 [3:00:54<1:13:55,  1.44s/it] 69%|██████▊   | 6734/9822 [3:00:55<1:13:52,  1.44s/it] 69%|██████▊   | 6735/9822 [3:00:56<1:15:08,  1.46s/it] 69%|██████▊   | 6736/9822 [3:00:58<1:14:36,  1.45s/it] 69%|██████▊   | 6737/9822 [3:00:59<1:14:22,  1.45s/it] 69%|██████▊   | 6738/9822 [3:01:01<1:14:04,  1.44s/it] 69%|██████▊   | 6739/9822 [3:01:02<1:13:54,  1.44s/it] 69%|██████▊   | 6740/9822 [3:01:04<1:13:47,  1.44s/it] 69%|██████▊   | 6741/9822 [3:01:05<1:13:49,  1.44s/it] 69%|██████▊   | 6742/9822 [3:01:06<1:13:48,  1.44s/it] 69%|██████▊   | 6743/9822 [3:01:08<1:13:49,  1.44s/it] 69%|██████▊   | 6744/9822 [3:01:09<1:13:48,  1.44s/it] 69%|██████▊   | 6745/9822 [3:01:11<1:13:49,  1.44s/it] 69%|██████▊   | 6746/9822 [3:01:12<1:13:36,  1.44s/it] 69%|██████▊   | 6747/9822 [3:01:14<1:13:33,  1.44s/it] 69%|██████▊   | 6748/9822 [3:01:15<1:13:27,  1.43s/it] 69%|██████▊   | 6749/9822 [3:01:17<1:13:23,  1.43s/it] 69%|██████▊   | 6750/9822 [3:01:18<1:13:20,  1.43s/it] 69%|██████▊   | 6751/9822 [3:01:19<1:13:17,  1.43s/it] 69%|██████▊   | 6752/9822 [3:01:21<1:13:17,  1.43s/it] 69%|██████▉   | 6753/9822 [3:01:22<1:13:07,  1.43s/it] 69%|██████▉   | 6754/9822 [3:01:24<1:13:02,  1.43s/it] 69%|██████▉   | 6755/9822 [3:01:25<1:13:09,  1.43s/it] 69%|██████▉   | 6756/9822 [3:01:27<1:13:18,  1.43s/it] 69%|██████▉   | 6757/9822 [3:01:28<1:13:19,  1.44s/it] 69%|██████▉   | 6758/9822 [3:01:29<1:13:21,  1.44s/it] 69%|██████▉   | 6759/9822 [3:01:31<1:13:17,  1.44s/it] 69%|██████▉   | 6760/9822 [3:01:32<1:14:25,  1.46s/it] 69%|██████▉   | 6761/9822 [3:01:34<1:14:03,  1.45s/it] 69%|██████▉   | 6762/9822 [3:01:35<1:13:41,  1.44s/it] 69%|██████▉   | 6763/9822 [3:01:37<1:13:40,  1.45s/it] 69%|██████▉   | 6764/9822 [3:01:38<1:13:37,  1.44s/it] 69%|██████▉   | 6765/9822 [3:01:40<1:13:21,  1.44s/it] 69%|██████▉   | 6766/9822 [3:01:41<1:13:13,  1.44s/it] 69%|██████▉   | 6767/9822 [3:01:42<1:13:10,  1.44s/it] 69%|██████▉   | 6768/9822 [3:01:44<1:13:08,  1.44s/it] 69%|██████▉   | 6769/9822 [3:01:45<1:13:01,  1.44s/it] 69%|██████▉   | 6770/9822 [3:01:47<1:12:58,  1.43s/it] 69%|██████▉   | 6771/9822 [3:01:48<1:12:51,  1.43s/it] 69%|██████▉   | 6772/9822 [3:01:50<1:12:51,  1.43s/it] 69%|██████▉   | 6773/9822 [3:01:51<1:12:51,  1.43s/it] 69%|██████▉   | 6774/9822 [3:01:52<1:12:48,  1.43s/it] 69%|██████▉   | 6775/9822 [3:01:54<1:12:43,  1.43s/it] 69%|██████▉   | 6776/9822 [3:01:55<1:12:55,  1.44s/it] 69%|██████▉   | 6777/9822 [3:01:57<1:13:06,  1.44s/it] 69%|██████▉   | 6778/9822 [3:01:58<1:13:30,  1.45s/it] 69%|██████▉   | 6779/9822 [3:02:00<1:13:12,  1.44s/it] 69%|██████▉   | 6780/9822 [3:02:01<1:12:59,  1.44s/it] 69%|██████▉   | 6781/9822 [3:02:03<1:12:50,  1.44s/it] 69%|██████▉   | 6782/9822 [3:02:04<1:12:49,  1.44s/it] 69%|██████▉   | 6783/9822 [3:02:05<1:12:48,  1.44s/it] 69%|██████▉   | 6784/9822 [3:02:07<1:12:42,  1.44s/it] 69%|██████▉   | 6785/9822 [3:02:08<1:12:49,  1.44s/it] 69%|██████▉   | 6786/9822 [3:02:10<1:12:43,  1.44s/it] 69%|██████▉   | 6787/9822 [3:02:11<1:12:31,  1.43s/it] 69%|██████▉   | 6788/9822 [3:02:13<1:12:22,  1.43s/it] 69%|██████▉   | 6789/9822 [3:02:14<1:12:20,  1.43s/it] 69%|██████▉   | 6790/9822 [3:02:15<1:12:19,  1.43s/it] 69%|██████▉   | 6791/9822 [3:02:17<1:12:14,  1.43s/it] 69%|██████▉   | 6792/9822 [3:02:18<1:13:30,  1.46s/it] 69%|██████▉   | 6793/9822 [3:02:20<1:13:27,  1.46s/it] 69%|██████▉   | 6794/9822 [3:02:21<1:12:11,  1.43s/it] 69%|██████▉   | 6795/9822 [3:02:23<1:12:04,  1.43s/it] 69%|██████▉   | 6796/9822 [3:02:24<1:12:01,  1.43s/it] 69%|██████▉   | 6797/9822 [3:02:26<1:12:05,  1.43s/it] 69%|██████▉   | 6798/9822 [3:02:27<1:12:06,  1.43s/it] 69%|██████▉   | 6799/9822 [3:02:28<1:11:59,  1.43s/it] 69%|██████▉   | 6800/9822 [3:02:30<1:12:02,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1020, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0909, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:50:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:50:17 - INFO - __main__ - ***** test Results*****
01/08/2024 00:50:17 - INFO - __main__ -   Training step = 6800
01/08/2024 00:50:17 - INFO - __main__ -  test_accuracy:0.8766471449487555 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:50:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:50:23 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:50:23 - INFO - __main__ -   Training step = 6800
01/08/2024 00:50:23 - INFO - __main__ -  eval_accuracy:0.8615891614793116 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 00:50:23,227 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 00:50:23,227 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 00:50:23,264 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 00:50:24,841 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 00:50:29 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:50:29 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:50:29 - INFO - __main__ -   Training step = 6800
01/08/2024 00:50:29 - INFO - __main__ -  eval_accuracy:0.9102892713291835 
 69%|██████▉   | 6801/9822 [3:02:50<5:51:52,  6.99s/it] 69%|██████▉   | 6802/9822 [3:02:51<4:27:44,  5.32s/it] 69%|██████▉   | 6803/9822 [3:02:53<3:28:54,  4.15s/it] 69%|██████▉   | 6804/9822 [3:02:54<2:47:41,  3.33s/it] 69%|██████▉   | 6805/9822 [3:02:55<2:18:54,  2.76s/it] 69%|██████▉   | 6806/9822 [3:02:57<1:58:44,  2.36s/it] 69%|██████▉   | 6807/9822 [3:02:58<1:44:39,  2.08s/it] 69%|██████▉   | 6808/9822 [3:03:00<1:34:40,  1.88s/it] 69%|██████▉   | 6809/9822 [3:03:01<1:27:43,  1.75s/it] 69%|██████▉   | 6810/9822 [3:03:03<1:22:50,  1.65s/it] 69%|██████▉   | 6811/9822 [3:03:04<1:19:36,  1.59s/it] 69%|██████▉   | 6812/9822 [3:03:05<1:17:13,  1.54s/it] 69%|██████▉   | 6813/9822 [3:03:07<1:15:33,  1.51s/it] 69%|██████▉   | 6814/9822 [3:03:08<1:14:18,  1.48s/it] 69%|██████▉   | 6815/9822 [3:03:10<1:13:38,  1.47s/it] 69%|██████▉   | 6816/9822 [3:03:11<1:13:18,  1.46s/it] 69%|██████▉   | 6817/9822 [3:03:13<1:12:50,  1.45s/it] 69%|██████▉   | 6818/9822 [3:03:14<1:12:21,  1.45s/it] 69%|██████▉   | 6819/9822 [3:03:15<1:12:09,  1.44s/it] 69%|██████▉   | 6820/9822 [3:03:17<1:13:26,  1.47s/it] 69%|██████▉   | 6821/9822 [3:03:18<1:13:10,  1.46s/it] 69%|██████▉   | 6822/9822 [3:03:20<1:12:52,  1.46s/it] 69%|██████▉   | 6823/9822 [3:03:21<1:12:46,  1.46s/it] 69%|██████▉   | 6824/9822 [3:03:23<1:12:30,  1.45s/it] 69%|██████▉   | 6825/9822 [3:03:24<1:12:21,  1.45s/it] 69%|██████▉   | 6826/9822 [3:03:26<1:12:52,  1.46s/it] 70%|██████▉   | 6827/9822 [3:03:27<1:13:12,  1.47s/it] 70%|██████▉   | 6828/9822 [3:03:29<1:13:12,  1.47s/it] 70%|██████▉   | 6829/9822 [3:03:30<1:12:43,  1.46s/it] 70%|██████▉   | 6830/9822 [3:03:32<1:12:19,  1.45s/it] 70%|██████▉   | 6831/9822 [3:03:33<1:11:55,  1.44s/it] 70%|██████▉   | 6832/9822 [3:03:34<1:11:42,  1.44s/it] 70%|██████▉   | 6833/9822 [3:03:36<1:11:44,  1.44s/it] 70%|██████▉   | 6834/9822 [3:03:37<1:11:46,  1.44s/it] 70%|██████▉   | 6835/9822 [3:03:39<1:11:39,  1.44s/it] 70%|██████▉   | 6836/9822 [3:03:40<1:12:22,  1.45s/it] 70%|██████▉   | 6837/9822 [3:03:42<1:12:50,  1.46s/it] 70%|██████▉   | 6838/9822 [3:03:43<1:12:34,  1.46s/it] 70%|██████▉   | 6839/9822 [3:03:45<1:12:21,  1.46s/it] 70%|██████▉   | 6840/9822 [3:03:46<1:12:16,  1.45s/it] 70%|██████▉   | 6841/9822 [3:03:48<1:12:08,  1.45s/it] 70%|██████▉   | 6842/9822 [3:03:49<1:12:04,  1.45s/it] 70%|██████▉   | 6843/9822 [3:03:50<1:11:56,  1.45s/it] 70%|██████▉   | 6844/9822 [3:03:52<1:11:57,  1.45s/it] 70%|██████▉   | 6845/9822 [3:03:53<1:11:50,  1.45s/it] 70%|██████▉   | 6846/9822 [3:03:55<1:11:54,  1.45s/it] 70%|██████▉   | 6847/9822 [3:03:56<1:11:46,  1.45s/it] 70%|██████▉   | 6848/9822 [3:03:58<1:11:41,  1.45s/it] 70%|██████▉   | 6849/9822 [3:03:59<1:11:34,  1.44s/it] 70%|██████▉   | 6850/9822 [3:04:01<1:11:28,  1.44s/it] 70%|██████▉   | 6851/9822 [3:04:02<1:11:10,  1.44s/it] 70%|██████▉   | 6852/9822 [3:04:03<1:12:20,  1.46s/it] 70%|██████▉   | 6853/9822 [3:04:05<1:12:48,  1.47s/it] 70%|██████▉   | 6854/9822 [3:04:06<1:12:53,  1.47s/it] 70%|██████▉   | 6855/9822 [3:04:08<1:12:42,  1.47s/it] 70%|██████▉   | 6856/9822 [3:04:09<1:13:00,  1.48s/it] 70%|██████▉   | 6857/9822 [3:04:11<1:13:16,  1.48s/it] 70%|██████▉   | 6858/9822 [3:04:12<1:13:12,  1.48s/it] 70%|██████▉   | 6859/9822 [3:04:14<1:12:31,  1.47s/it] 70%|██████▉   | 6860/9822 [3:04:15<1:11:51,  1.46s/it] 70%|██████▉   | 6861/9822 [3:04:17<1:11:27,  1.45s/it] 70%|██████▉   | 6862/9822 [3:04:18<1:11:05,  1.44s/it] 70%|██████▉   | 6863/9822 [3:04:20<1:10:54,  1.44s/it] 70%|██████▉   | 6864/9822 [3:04:21<1:10:51,  1.44s/it] 70%|██████▉   | 6865/9822 [3:04:22<1:10:48,  1.44s/it] 70%|██████▉   | 6866/9822 [3:04:24<1:10:43,  1.44s/it] 70%|██████▉   | 6867/9822 [3:04:25<1:10:40,  1.44s/it] 70%|██████▉   | 6868/9822 [3:04:27<1:10:58,  1.44s/it] 70%|██████▉   | 6869/9822 [3:04:28<1:10:49,  1.44s/it] 70%|██████▉   | 6870/9822 [3:04:30<1:10:46,  1.44s/it] 70%|██████▉   | 6871/9822 [3:04:31<1:10:46,  1.44s/it] 70%|██████▉   | 6872/9822 [3:04:32<1:10:34,  1.44s/it] 70%|██████▉   | 6873/9822 [3:04:34<1:10:32,  1.44s/it] 70%|██████▉   | 6874/9822 [3:04:35<1:10:26,  1.43s/it] 70%|██████▉   | 6875/9822 [3:04:37<1:10:15,  1.43s/it] 70%|███████   | 6876/9822 [3:04:38<1:10:18,  1.43s/it] 70%|███████   | 6877/9822 [3:04:40<1:10:10,  1.43s/it] 70%|███████   | 6878/9822 [3:04:41<1:10:07,  1.43s/it] 70%|███████   | 6879/9822 [3:04:42<1:10:13,  1.43s/it] 70%|███████   | 6880/9822 [3:04:44<1:09:34,  1.42s/it] 70%|███████   | 6881/9822 [3:04:45<1:09:34,  1.42s/it] 70%|███████   | 6882/9822 [3:04:47<1:10:55,  1.45s/it] 70%|███████   | 6883/9822 [3:04:48<1:10:44,  1.44s/it] 70%|███████   | 6884/9822 [3:04:50<1:10:51,  1.45s/it] 70%|███████   | 6885/9822 [3:04:51<1:10:43,  1.44s/it] 70%|███████   | 6886/9822 [3:04:53<1:10:58,  1.45s/it] 70%|███████   | 6887/9822 [3:04:54<1:10:33,  1.44s/it] 70%|███████   | 6888/9822 [3:04:55<1:10:17,  1.44s/it] 70%|███████   | 6889/9822 [3:04:57<1:10:09,  1.44s/it] 70%|███████   | 6890/9822 [3:04:58<1:10:09,  1.44s/it] 70%|███████   | 6891/9822 [3:05:00<1:10:07,  1.44s/it] 70%|███████   | 6892/9822 [3:05:01<1:10:06,  1.44s/it] 70%|███████   | 6893/9822 [3:05:03<1:10:10,  1.44s/it] 70%|███████   | 6894/9822 [3:05:04<1:10:06,  1.44s/it] 70%|███████   | 6895/9822 [3:05:05<1:10:02,  1.44s/it] 70%|███████   | 6896/9822 [3:05:07<1:09:55,  1.43s/it] 70%|███████   | 6897/9822 [3:05:08<1:09:47,  1.43s/it] 70%|███████   | 6898/9822 [3:05:10<1:09:39,  1.43s/it] 70%|███████   | 6899/9822 [3:05:11<1:09:37,  1.43s/it] 70%|███████   | 6900/9822 [3:05:13<1:09:34,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:52:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:52:59 - INFO - __main__ - ***** test Results*****
01/08/2024 00:52:59 - INFO - __main__ -   Training step = 6900
01/08/2024 00:52:59 - INFO - __main__ -  test_accuracy:0.8810395314787701 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:53:06 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:53:06 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:53:06 - INFO - __main__ -   Training step = 6900
01/08/2024 00:53:06 - INFO - __main__ -  eval_accuracy:0.859758330281948 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 00:53:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:53:10 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:53:10 - INFO - __main__ -   Training step = 6900
01/08/2024 00:53:10 - INFO - __main__ -  eval_accuracy:0.9154155986818016 
 70%|███████   | 6901/9822 [3:05:31<5:16:57,  6.51s/it] 70%|███████   | 6902/9822 [3:05:32<4:02:48,  4.99s/it] 70%|███████   | 6903/9822 [3:05:34<3:10:53,  3.92s/it] 70%|███████   | 6904/9822 [3:05:35<2:34:26,  3.18s/it] 70%|███████   | 6905/9822 [3:05:37<2:09:12,  2.66s/it] 70%|███████   | 6906/9822 [3:05:38<1:51:53,  2.30s/it] 70%|███████   | 6907/9822 [3:05:40<1:39:10,  2.04s/it] 70%|███████   | 6908/9822 [3:05:41<1:30:21,  1.86s/it] 70%|███████   | 6909/9822 [3:05:43<1:24:03,  1.73s/it] 70%|███████   | 6910/9822 [3:05:44<1:19:51,  1.65s/it] 70%|███████   | 6911/9822 [3:05:45<1:17:46,  1.60s/it] 70%|███████   | 6912/9822 [3:05:47<1:15:24,  1.55s/it] 70%|███████   | 6913/9822 [3:05:48<1:13:38,  1.52s/it] 70%|███████   | 6914/9822 [3:05:50<1:12:13,  1.49s/it] 70%|███████   | 6915/9822 [3:05:51<1:11:24,  1.47s/it] 70%|███████   | 6916/9822 [3:05:53<1:11:09,  1.47s/it] 70%|███████   | 6917/9822 [3:05:54<1:10:40,  1.46s/it] 70%|███████   | 6918/9822 [3:05:56<1:10:45,  1.46s/it] 70%|███████   | 6919/9822 [3:05:57<1:10:39,  1.46s/it] 70%|███████   | 6920/9822 [3:05:58<1:10:23,  1.46s/it] 70%|███████   | 6921/9822 [3:06:00<1:10:03,  1.45s/it] 70%|███████   | 6922/9822 [3:06:01<1:09:57,  1.45s/it] 70%|███████   | 6923/9822 [3:06:03<1:09:42,  1.44s/it] 70%|███████   | 6924/9822 [3:06:04<1:09:32,  1.44s/it] 71%|███████   | 6925/9822 [3:06:06<1:09:39,  1.44s/it] 71%|███████   | 6926/9822 [3:06:07<1:09:23,  1.44s/it] 71%|███████   | 6927/9822 [3:06:09<1:09:27,  1.44s/it] 71%|███████   | 6928/9822 [3:06:10<1:09:26,  1.44s/it] 71%|███████   | 6929/9822 [3:06:11<1:09:22,  1.44s/it] 71%|███████   | 6930/9822 [3:06:13<1:09:24,  1.44s/it] 71%|███████   | 6931/9822 [3:06:14<1:09:48,  1.45s/it] 71%|███████   | 6932/9822 [3:06:16<1:09:57,  1.45s/it] 71%|███████   | 6933/9822 [3:06:17<1:09:46,  1.45s/it] 71%|███████   | 6934/9822 [3:06:19<1:09:57,  1.45s/it] 71%|███████   | 6935/9822 [3:06:20<1:09:37,  1.45s/it] 71%|███████   | 6936/9822 [3:06:22<1:09:12,  1.44s/it] 71%|███████   | 6937/9822 [3:06:23<1:09:04,  1.44s/it] 71%|███████   | 6938/9822 [3:06:24<1:09:01,  1.44s/it] 71%|███████   | 6939/9822 [3:06:26<1:09:05,  1.44s/it] 71%|███████   | 6940/9822 [3:06:27<1:08:51,  1.43s/it] 71%|███████   | 6941/9822 [3:06:29<1:08:44,  1.43s/it] 71%|███████   | 6942/9822 [3:06:30<1:08:54,  1.44s/it] 71%|███████   | 6943/9822 [3:06:32<1:09:58,  1.46s/it] 71%|███████   | 6944/9822 [3:06:33<1:09:45,  1.45s/it] 71%|███████   | 6945/9822 [3:06:35<1:09:19,  1.45s/it] 71%|███████   | 6946/9822 [3:06:36<1:09:00,  1.44s/it] 71%|███████   | 6947/9822 [3:06:37<1:08:59,  1.44s/it] 71%|███████   | 6948/9822 [3:06:39<1:09:01,  1.44s/it] 71%|███████   | 6949/9822 [3:06:40<1:09:01,  1.44s/it] 71%|███████   | 6950/9822 [3:06:42<1:08:50,  1.44s/it] 71%|███████   | 6951/9822 [3:06:43<1:08:46,  1.44s/it] 71%|███████   | 6952/9822 [3:06:45<1:08:34,  1.43s/it] 71%|███████   | 6953/9822 [3:06:46<1:08:32,  1.43s/it] 71%|███████   | 6954/9822 [3:06:47<1:08:28,  1.43s/it] 71%|███████   | 6955/9822 [3:06:49<1:08:23,  1.43s/it] 71%|███████   | 6956/9822 [3:06:50<1:08:18,  1.43s/it] 71%|███████   | 6957/9822 [3:06:52<1:08:18,  1.43s/it] 71%|███████   | 6958/9822 [3:06:53<1:08:23,  1.43s/it] 71%|███████   | 6959/9822 [3:06:55<1:08:20,  1.43s/it] 71%|███████   | 6960/9822 [3:06:56<1:08:27,  1.44s/it] 71%|███████   | 6961/9822 [3:06:57<1:08:24,  1.43s/it] 71%|███████   | 6962/9822 [3:06:59<1:08:16,  1.43s/it] 71%|███████   | 6963/9822 [3:07:00<1:08:14,  1.43s/it] 71%|███████   | 6964/9822 [3:07:02<1:08:10,  1.43s/it] 71%|███████   | 6965/9822 [3:07:03<1:08:08,  1.43s/it] 71%|███████   | 6966/9822 [3:07:05<1:07:26,  1.42s/it] 71%|███████   | 6967/9822 [3:07:06<1:07:40,  1.42s/it] 71%|███████   | 6968/9822 [3:07:07<1:07:47,  1.43s/it] 71%|███████   | 6969/9822 [3:07:09<1:07:53,  1.43s/it] 71%|███████   | 6970/9822 [3:07:10<1:07:53,  1.43s/it] 71%|███████   | 6971/9822 [3:07:12<1:08:03,  1.43s/it] 71%|███████   | 6972/9822 [3:07:13<1:07:53,  1.43s/it] 71%|███████   | 6973/9822 [3:07:15<1:07:54,  1.43s/it] 71%|███████   | 6974/9822 [3:07:16<1:07:55,  1.43s/it] 71%|███████   | 6975/9822 [3:07:18<1:09:11,  1.46s/it] 71%|███████   | 6976/9822 [3:07:19<1:08:53,  1.45s/it] 71%|███████   | 6977/9822 [3:07:20<1:08:54,  1.45s/it] 71%|███████   | 6978/9822 [3:07:22<1:08:29,  1.45s/it] 71%|███████   | 6979/9822 [3:07:23<1:08:12,  1.44s/it] 71%|███████   | 6980/9822 [3:07:25<1:08:00,  1.44s/it] 71%|███████   | 6981/9822 [3:07:26<1:08:00,  1.44s/it] 71%|███████   | 6982/9822 [3:07:28<1:07:46,  1.43s/it] 71%|███████   | 6983/9822 [3:07:29<1:07:43,  1.43s/it] 71%|███████   | 6984/9822 [3:07:30<1:07:43,  1.43s/it] 71%|███████   | 6985/9822 [3:07:32<1:08:01,  1.44s/it] 71%|███████   | 6986/9822 [3:07:33<1:08:01,  1.44s/it] 71%|███████   | 6987/9822 [3:07:35<1:08:02,  1.44s/it] 71%|███████   | 6988/9822 [3:07:36<1:07:54,  1.44s/it] 71%|███████   | 6989/9822 [3:07:38<1:07:44,  1.43s/it] 71%|███████   | 6990/9822 [3:07:39<1:07:44,  1.44s/it] 71%|███████   | 6991/9822 [3:07:40<1:07:40,  1.43s/it] 71%|███████   | 6992/9822 [3:07:42<1:07:41,  1.44s/it] 71%|███████   | 6993/9822 [3:07:43<1:07:42,  1.44s/it] 71%|███████   | 6994/9822 [3:07:45<1:07:37,  1.43s/it] 71%|███████   | 6995/9822 [3:07:46<1:07:43,  1.44s/it] 71%|███████   | 6996/9822 [3:07:48<1:07:41,  1.44s/it] 71%|███████   | 6997/9822 [3:07:49<1:07:34,  1.44s/it] 71%|███████   | 6998/9822 [3:07:51<1:07:30,  1.43s/it] 71%|███████▏  | 6999/9822 [3:07:52<1:07:41,  1.44s/it] 71%|███████▏  | 7000/9822 [3:07:54<1:08:44,  1.46s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2286, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1197, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:55:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:55:40 - INFO - __main__ - ***** test Results*****
01/08/2024 00:55:40 - INFO - __main__ -   Training step = 7000
01/08/2024 00:55:40 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:55:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:55:46 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:55:46 - INFO - __main__ -   Training step = 7000
01/08/2024 00:55:46 - INFO - __main__ -  eval_accuracy:0.8590259978030026 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 00:55:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:55:51 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:55:51 - INFO - __main__ -   Training step = 7000
01/08/2024 00:55:51 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 71%|███████▏  | 7001/9822 [3:08:12<5:06:49,  6.53s/it] 71%|███████▏  | 7002/9822 [3:08:13<3:55:31,  5.01s/it] 71%|███████▏  | 7003/9822 [3:08:15<3:05:04,  3.94s/it] 71%|███████▏  | 7004/9822 [3:08:16<2:29:48,  3.19s/it] 71%|███████▏  | 7005/9822 [3:08:18<2:04:56,  2.66s/it] 71%|███████▏  | 7006/9822 [3:08:19<1:47:41,  2.29s/it] 71%|███████▏  | 7007/9822 [3:08:21<1:35:40,  2.04s/it] 71%|███████▏  | 7008/9822 [3:08:22<1:27:08,  1.86s/it] 71%|███████▏  | 7009/9822 [3:08:23<1:21:03,  1.73s/it] 71%|███████▏  | 7010/9822 [3:08:25<1:16:58,  1.64s/it] 71%|███████▏  | 7011/9822 [3:08:26<1:14:08,  1.58s/it] 71%|███████▏  | 7012/9822 [3:08:28<1:12:08,  1.54s/it] 71%|███████▏  | 7013/9822 [3:08:29<1:10:35,  1.51s/it] 71%|███████▏  | 7014/9822 [3:08:31<1:09:41,  1.49s/it] 71%|███████▏  | 7015/9822 [3:08:32<1:08:52,  1.47s/it] 71%|███████▏  | 7016/9822 [3:08:33<1:08:21,  1.46s/it] 71%|███████▏  | 7017/9822 [3:08:35<1:08:01,  1.46s/it] 71%|███████▏  | 7018/9822 [3:08:36<1:07:47,  1.45s/it] 71%|███████▏  | 7019/9822 [3:08:38<1:07:28,  1.44s/it] 71%|███████▏  | 7020/9822 [3:08:39<1:07:24,  1.44s/it] 71%|███████▏  | 7021/9822 [3:08:41<1:07:23,  1.44s/it] 71%|███████▏  | 7022/9822 [3:08:42<1:07:19,  1.44s/it] 72%|███████▏  | 7023/9822 [3:08:44<1:07:14,  1.44s/it] 72%|███████▏  | 7024/9822 [3:08:45<1:07:06,  1.44s/it] 72%|███████▏  | 7025/9822 [3:08:46<1:07:03,  1.44s/it] 72%|███████▏  | 7026/9822 [3:08:48<1:06:58,  1.44s/it] 72%|███████▏  | 7027/9822 [3:08:49<1:08:08,  1.46s/it] 72%|███████▏  | 7028/9822 [3:08:51<1:07:52,  1.46s/it] 72%|███████▏  | 7029/9822 [3:08:52<1:07:53,  1.46s/it] 72%|███████▏  | 7030/9822 [3:08:54<1:08:01,  1.46s/it] 72%|███████▏  | 7031/9822 [3:08:55<1:07:36,  1.45s/it] 72%|███████▏  | 7032/9822 [3:08:57<1:07:11,  1.45s/it] 72%|███████▏  | 7033/9822 [3:08:58<1:07:14,  1.45s/it] 72%|███████▏  | 7034/9822 [3:09:00<1:07:48,  1.46s/it] 72%|███████▏  | 7035/9822 [3:09:01<1:07:29,  1.45s/it] 72%|███████▏  | 7036/9822 [3:09:02<1:07:07,  1.45s/it] 72%|███████▏  | 7037/9822 [3:09:04<1:06:58,  1.44s/it] 72%|███████▏  | 7038/9822 [3:09:05<1:06:58,  1.44s/it] 72%|███████▏  | 7039/9822 [3:09:07<1:06:55,  1.44s/it] 72%|███████▏  | 7040/9822 [3:09:08<1:06:49,  1.44s/it] 72%|███████▏  | 7041/9822 [3:09:10<1:06:36,  1.44s/it] 72%|███████▏  | 7042/9822 [3:09:11<1:06:29,  1.44s/it] 72%|███████▏  | 7043/9822 [3:09:12<1:06:19,  1.43s/it] 72%|███████▏  | 7044/9822 [3:09:14<1:06:17,  1.43s/it] 72%|███████▏  | 7045/9822 [3:09:15<1:06:22,  1.43s/it] 72%|███████▏  | 7046/9822 [3:09:17<1:06:17,  1.43s/it] 72%|███████▏  | 7047/9822 [3:09:18<1:06:21,  1.43s/it] 72%|███████▏  | 7048/9822 [3:09:20<1:06:10,  1.43s/it] 72%|███████▏  | 7049/9822 [3:09:21<1:06:00,  1.43s/it] 72%|███████▏  | 7050/9822 [3:09:22<1:06:06,  1.43s/it] 72%|███████▏  | 7051/9822 [3:09:24<1:06:12,  1.43s/it] 72%|███████▏  | 7052/9822 [3:09:25<1:05:27,  1.42s/it] 72%|███████▏  | 7053/9822 [3:09:27<1:05:47,  1.43s/it] 72%|███████▏  | 7054/9822 [3:09:28<1:06:06,  1.43s/it] 72%|███████▏  | 7055/9822 [3:09:30<1:06:15,  1.44s/it] 72%|███████▏  | 7056/9822 [3:09:31<1:06:11,  1.44s/it] 72%|███████▏  | 7057/9822 [3:09:32<1:06:01,  1.43s/it] 72%|███████▏  | 7058/9822 [3:09:34<1:05:51,  1.43s/it] 72%|███████▏  | 7059/9822 [3:09:35<1:07:00,  1.46s/it] 72%|███████▏  | 7060/9822 [3:09:37<1:06:45,  1.45s/it] 72%|███████▏  | 7061/9822 [3:09:38<1:06:35,  1.45s/it] 72%|███████▏  | 7062/9822 [3:09:40<1:06:29,  1.45s/it] 72%|███████▏  | 7063/9822 [3:09:41<1:06:24,  1.44s/it] 72%|███████▏  | 7064/9822 [3:09:43<1:06:06,  1.44s/it] 72%|███████▏  | 7065/9822 [3:09:44<1:06:05,  1.44s/it] 72%|███████▏  | 7066/9822 [3:09:45<1:06:02,  1.44s/it] 72%|███████▏  | 7067/9822 [3:09:47<1:05:50,  1.43s/it] 72%|███████▏  | 7068/9822 [3:09:48<1:05:49,  1.43s/it] 72%|███████▏  | 7069/9822 [3:09:50<1:05:48,  1.43s/it] 72%|███████▏  | 7070/9822 [3:09:51<1:05:53,  1.44s/it] 72%|███████▏  | 7071/9822 [3:09:53<1:05:53,  1.44s/it] 72%|███████▏  | 7072/9822 [3:09:54<1:05:51,  1.44s/it] 72%|███████▏  | 7073/9822 [3:09:56<1:05:38,  1.43s/it] 72%|███████▏  | 7074/9822 [3:09:57<1:05:35,  1.43s/it] 72%|███████▏  | 7075/9822 [3:09:58<1:05:28,  1.43s/it] 72%|███████▏  | 7076/9822 [3:10:00<1:05:39,  1.43s/it] 72%|███████▏  | 7077/9822 [3:10:01<1:05:45,  1.44s/it] 72%|███████▏  | 7078/9822 [3:10:03<1:05:31,  1.43s/it] 72%|███████▏  | 7079/9822 [3:10:04<1:05:25,  1.43s/it] 72%|███████▏  | 7080/9822 [3:10:06<1:05:23,  1.43s/it] 72%|███████▏  | 7081/9822 [3:10:07<1:05:35,  1.44s/it] 72%|███████▏  | 7082/9822 [3:10:08<1:05:35,  1.44s/it] 72%|███████▏  | 7083/9822 [3:10:10<1:05:32,  1.44s/it] 72%|███████▏  | 7084/9822 [3:10:11<1:05:22,  1.43s/it] 72%|███████▏  | 7085/9822 [3:10:13<1:05:30,  1.44s/it] 72%|███████▏  | 7086/9822 [3:10:14<1:05:32,  1.44s/it] 72%|███████▏  | 7087/9822 [3:10:16<1:05:19,  1.43s/it] 72%|███████▏  | 7088/9822 [3:10:17<1:05:17,  1.43s/it] 72%|███████▏  | 7089/9822 [3:10:19<1:06:24,  1.46s/it] 72%|███████▏  | 7090/9822 [3:10:20<1:06:06,  1.45s/it] 72%|███████▏  | 7091/9822 [3:10:21<1:05:48,  1.45s/it] 72%|███████▏  | 7092/9822 [3:10:23<1:05:44,  1.44s/it] 72%|███████▏  | 7093/9822 [3:10:24<1:05:44,  1.45s/it] 72%|███████▏  | 7094/9822 [3:10:26<1:05:36,  1.44s/it] 72%|███████▏  | 7095/9822 [3:10:27<1:05:31,  1.44s/it] 72%|███████▏  | 7096/9822 [3:10:29<1:05:18,  1.44s/it] 72%|███████▏  | 7097/9822 [3:10:30<1:05:16,  1.44s/it] 72%|███████▏  | 7098/9822 [3:10:31<1:05:25,  1.44s/it] 72%|███████▏  | 7099/9822 [3:10:33<1:05:13,  1.44s/it] 72%|███████▏  | 7100/9822 [3:10:34<1:05:09,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1219, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0381, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1211, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 00:58:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:58:21 - INFO - __main__ - ***** test Results*****
01/08/2024 00:58:21 - INFO - __main__ -   Training step = 7100
01/08/2024 00:58:21 - INFO - __main__ -  test_accuracy:0.87298682284041 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:58:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:58:27 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 00:58:27 - INFO - __main__ -   Training step = 7100
01/08/2024 00:58:27 - INFO - __main__ -  eval_accuracy:0.8586598315635299 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 00:58:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 00:58:32 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 00:58:32 - INFO - __main__ -   Training step = 7100
01/08/2024 00:58:32 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 72%|███████▏  | 7101/9822 [3:10:53<4:55:05,  6.51s/it] 72%|███████▏  | 7102/9822 [3:10:54<3:46:02,  4.99s/it] 72%|███████▏  | 7103/9822 [3:10:56<2:57:34,  3.92s/it] 72%|███████▏  | 7104/9822 [3:10:57<2:23:32,  3.17s/it] 72%|███████▏  | 7105/9822 [3:10:58<1:59:50,  2.65s/it] 72%|███████▏  | 7106/9822 [3:11:00<1:44:06,  2.30s/it] 72%|███████▏  | 7107/9822 [3:11:01<1:33:07,  2.06s/it] 72%|███████▏  | 7108/9822 [3:11:03<1:24:48,  1.87s/it] 72%|███████▏  | 7109/9822 [3:11:04<1:19:01,  1.75s/it] 72%|███████▏  | 7110/9822 [3:11:06<1:14:55,  1.66s/it] 72%|███████▏  | 7111/9822 [3:11:07<1:12:03,  1.59s/it] 72%|███████▏  | 7112/9822 [3:11:09<1:10:01,  1.55s/it] 72%|███████▏  | 7113/9822 [3:11:10<1:08:39,  1.52s/it] 72%|███████▏  | 7114/9822 [3:11:12<1:07:50,  1.50s/it] 72%|███████▏  | 7115/9822 [3:11:13<1:07:00,  1.49s/it] 72%|███████▏  | 7116/9822 [3:11:15<1:07:37,  1.50s/it] 72%|███████▏  | 7117/9822 [3:11:16<1:06:43,  1.48s/it] 72%|███████▏  | 7118/9822 [3:11:17<1:06:07,  1.47s/it] 72%|███████▏  | 7119/9822 [3:11:19<1:05:43,  1.46s/it] 72%|███████▏  | 7120/9822 [3:11:20<1:05:17,  1.45s/it] 73%|███████▎  | 7121/9822 [3:11:22<1:05:09,  1.45s/it] 73%|███████▎  | 7122/9822 [3:11:23<1:05:01,  1.45s/it] 73%|███████▎  | 7123/9822 [3:11:25<1:04:49,  1.44s/it] 73%|███████▎  | 7124/9822 [3:11:26<1:04:41,  1.44s/it] 73%|███████▎  | 7125/9822 [3:11:27<1:04:36,  1.44s/it] 73%|███████▎  | 7126/9822 [3:11:29<1:04:32,  1.44s/it] 73%|███████▎  | 7127/9822 [3:11:30<1:04:29,  1.44s/it] 73%|███████▎  | 7128/9822 [3:11:32<1:04:26,  1.44s/it] 73%|███████▎  | 7129/9822 [3:11:33<1:04:19,  1.43s/it] 73%|███████▎  | 7130/9822 [3:11:35<1:04:14,  1.43s/it] 73%|███████▎  | 7131/9822 [3:11:36<1:04:20,  1.43s/it] 73%|███████▎  | 7132/9822 [3:11:37<1:04:09,  1.43s/it] 73%|███████▎  | 7133/9822 [3:11:39<1:04:05,  1.43s/it] 73%|███████▎  | 7134/9822 [3:11:40<1:04:09,  1.43s/it] 73%|███████▎  | 7135/9822 [3:11:42<1:04:10,  1.43s/it] 73%|███████▎  | 7136/9822 [3:11:43<1:04:07,  1.43s/it] 73%|███████▎  | 7137/9822 [3:11:45<1:04:13,  1.44s/it] 73%|███████▎  | 7138/9822 [3:11:46<1:03:27,  1.42s/it] 73%|███████▎  | 7139/9822 [3:11:47<1:03:40,  1.42s/it] 73%|███████▎  | 7140/9822 [3:11:49<1:03:58,  1.43s/it] 73%|███████▎  | 7141/9822 [3:11:50<1:04:01,  1.43s/it] 73%|███████▎  | 7142/9822 [3:11:52<1:04:02,  1.43s/it] 73%|███████▎  | 7143/9822 [3:11:53<1:03:55,  1.43s/it] 73%|███████▎  | 7144/9822 [3:11:55<1:03:57,  1.43s/it] 73%|███████▎  | 7145/9822 [3:11:56<1:03:55,  1.43s/it] 73%|███████▎  | 7146/9822 [3:11:58<1:04:04,  1.44s/it] 73%|███████▎  | 7147/9822 [3:11:59<1:03:58,  1.43s/it] 73%|███████▎  | 7148/9822 [3:12:00<1:05:08,  1.46s/it] 73%|███████▎  | 7149/9822 [3:12:02<1:05:02,  1.46s/it] 73%|███████▎  | 7150/9822 [3:12:03<1:04:46,  1.45s/it] 73%|███████▎  | 7151/9822 [3:12:05<1:04:24,  1.45s/it] 73%|███████▎  | 7152/9822 [3:12:06<1:04:10,  1.44s/it] 73%|███████▎  | 7153/9822 [3:12:08<1:03:59,  1.44s/it] 73%|███████▎  | 7154/9822 [3:12:09<1:03:44,  1.43s/it] 73%|███████▎  | 7155/9822 [3:12:10<1:03:40,  1.43s/it] 73%|███████▎  | 7156/9822 [3:12:12<1:03:59,  1.44s/it] 73%|███████▎  | 7157/9822 [3:12:13<1:03:56,  1.44s/it] 73%|███████▎  | 7158/9822 [3:12:15<1:03:49,  1.44s/it] 73%|███████▎  | 7159/9822 [3:12:16<1:03:55,  1.44s/it] 73%|███████▎  | 7160/9822 [3:12:18<1:03:42,  1.44s/it] 73%|███████▎  | 7161/9822 [3:12:19<1:03:34,  1.43s/it] 73%|███████▎  | 7162/9822 [3:12:21<1:03:45,  1.44s/it] 73%|███████▎  | 7163/9822 [3:12:22<1:03:35,  1.44s/it] 73%|███████▎  | 7164/9822 [3:12:23<1:03:22,  1.43s/it] 73%|███████▎  | 7165/9822 [3:12:25<1:03:24,  1.43s/it] 73%|███████▎  | 7166/9822 [3:12:26<1:03:24,  1.43s/it] 73%|███████▎  | 7167/9822 [3:12:28<1:03:28,  1.43s/it] 73%|███████▎  | 7168/9822 [3:12:29<1:03:33,  1.44s/it] 73%|███████▎  | 7169/9822 [3:12:31<1:03:29,  1.44s/it] 73%|███████▎  | 7170/9822 [3:12:32<1:03:19,  1.43s/it] 73%|███████▎  | 7171/9822 [3:12:33<1:03:14,  1.43s/it] 73%|███████▎  | 7172/9822 [3:12:35<1:03:30,  1.44s/it] 73%|███████▎  | 7173/9822 [3:12:36<1:03:26,  1.44s/it] 73%|███████▎  | 7174/9822 [3:12:38<1:03:19,  1.43s/it] 73%|███████▎  | 7175/9822 [3:12:39<1:03:14,  1.43s/it] 73%|███████▎  | 7176/9822 [3:12:41<1:03:09,  1.43s/it] 73%|███████▎  | 7177/9822 [3:12:42<1:03:04,  1.43s/it] 73%|███████▎  | 7178/9822 [3:12:44<1:04:03,  1.45s/it] 73%|███████▎  | 7179/9822 [3:12:45<1:03:44,  1.45s/it] 73%|███████▎  | 7180/9822 [3:12:46<1:03:30,  1.44s/it] 73%|███████▎  | 7181/9822 [3:12:48<1:03:20,  1.44s/it] 73%|███████▎  | 7182/9822 [3:12:49<1:03:27,  1.44s/it] 73%|███████▎  | 7183/9822 [3:12:51<1:03:27,  1.44s/it] 73%|███████▎  | 7184/9822 [3:12:52<1:03:14,  1.44s/it] 73%|███████▎  | 7185/9822 [3:12:54<1:02:57,  1.43s/it] 73%|███████▎  | 7186/9822 [3:12:55<1:02:58,  1.43s/it] 73%|███████▎  | 7187/9822 [3:12:56<1:02:59,  1.43s/it] 73%|███████▎  | 7188/9822 [3:12:58<1:03:02,  1.44s/it] 73%|███████▎  | 7189/9822 [3:12:59<1:03:06,  1.44s/it] 73%|███████▎  | 7190/9822 [3:13:01<1:03:04,  1.44s/it] 73%|███████▎  | 7191/9822 [3:13:02<1:03:01,  1.44s/it] 73%|███████▎  | 7192/9822 [3:13:04<1:02:50,  1.43s/it] 73%|███████▎  | 7193/9822 [3:13:05<1:02:45,  1.43s/it] 73%|███████▎  | 7194/9822 [3:13:07<1:02:44,  1.43s/it] 73%|███████▎  | 7195/9822 [3:13:08<1:02:37,  1.43s/it] 73%|███████▎  | 7196/9822 [3:13:09<1:02:34,  1.43s/it] 73%|███████▎  | 7197/9822 [3:13:11<1:02:36,  1.43s/it] 73%|███████▎  | 7198/9822 [3:13:12<1:02:30,  1.43s/it] 73%|███████▎  | 7199/9822 [3:13:14<1:02:43,  1.43s/it] 73%|███████▎  | 7200/9822 [3:13:15<1:02:49,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:01:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:01:02 - INFO - __main__ - ***** test Results*****
01/08/2024 01:01:02 - INFO - __main__ -   Training step = 7200
01/08/2024 01:01:02 - INFO - __main__ -  test_accuracy:0.8777452415812591 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:01:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:01:08 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:01:08 - INFO - __main__ -   Training step = 7200
01/08/2024 01:01:08 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:01:13 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:01:13 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:01:13 - INFO - __main__ -   Training step = 7200
01/08/2024 01:01:13 - INFO - __main__ -  eval_accuracy:0.9106554375686562 
 73%|███████▎  | 7201/9822 [3:13:33<4:44:19,  6.51s/it] 73%|███████▎  | 7202/9822 [3:13:35<3:37:38,  4.98s/it] 73%|███████▎  | 7203/9822 [3:13:36<2:51:06,  3.92s/it] 73%|███████▎  | 7204/9822 [3:13:38<2:18:36,  3.18s/it] 73%|███████▎  | 7205/9822 [3:13:39<1:55:43,  2.65s/it] 73%|███████▎  | 7206/9822 [3:13:41<1:39:50,  2.29s/it] 73%|███████▎  | 7207/9822 [3:13:42<1:28:34,  2.03s/it] 73%|███████▎  | 7208/9822 [3:13:44<1:21:50,  1.88s/it] 73%|███████▎  | 7209/9822 [3:13:45<1:16:06,  1.75s/it] 73%|███████▎  | 7210/9822 [3:13:46<1:12:00,  1.65s/it] 73%|███████▎  | 7211/9822 [3:13:48<1:09:08,  1.59s/it] 73%|███████▎  | 7212/9822 [3:13:49<1:07:06,  1.54s/it] 73%|███████▎  | 7213/9822 [3:13:51<1:05:49,  1.51s/it] 73%|███████▎  | 7214/9822 [3:13:52<1:04:45,  1.49s/it] 73%|███████▎  | 7215/9822 [3:13:54<1:03:58,  1.47s/it] 73%|███████▎  | 7216/9822 [3:13:55<1:03:18,  1.46s/it] 73%|███████▎  | 7217/9822 [3:13:57<1:02:54,  1.45s/it] 73%|███████▎  | 7218/9822 [3:13:58<1:02:41,  1.44s/it] 73%|███████▎  | 7219/9822 [3:13:59<1:02:45,  1.45s/it] 74%|███████▎  | 7220/9822 [3:14:01<1:02:41,  1.45s/it] 74%|███████▎  | 7221/9822 [3:14:02<1:02:50,  1.45s/it] 74%|███████▎  | 7222/9822 [3:14:04<1:02:42,  1.45s/it] 74%|███████▎  | 7223/9822 [3:14:05<1:02:25,  1.44s/it] 74%|███████▎  | 7224/9822 [3:14:07<1:01:52,  1.43s/it] 74%|███████▎  | 7225/9822 [3:14:08<1:01:49,  1.43s/it] 74%|███████▎  | 7226/9822 [3:14:09<1:01:42,  1.43s/it] 74%|███████▎  | 7227/9822 [3:14:11<1:01:35,  1.42s/it] 74%|███████▎  | 7228/9822 [3:14:12<1:01:36,  1.42s/it] 74%|███████▎  | 7229/9822 [3:14:14<1:01:30,  1.42s/it] 74%|███████▎  | 7230/9822 [3:14:15<1:01:55,  1.43s/it] 74%|███████▎  | 7231/9822 [3:14:17<1:02:08,  1.44s/it] 74%|███████▎  | 7232/9822 [3:14:18<1:02:06,  1.44s/it] 74%|███████▎  | 7233/9822 [3:14:19<1:01:57,  1.44s/it] 74%|███████▎  | 7234/9822 [3:14:21<1:01:55,  1.44s/it] 74%|███████▎  | 7235/9822 [3:14:22<1:01:51,  1.43s/it] 74%|███████▎  | 7236/9822 [3:14:24<1:01:54,  1.44s/it] 74%|███████▎  | 7237/9822 [3:14:25<1:01:51,  1.44s/it] 74%|███████▎  | 7238/9822 [3:14:27<1:01:44,  1.43s/it] 74%|███████▎  | 7239/9822 [3:14:28<1:01:36,  1.43s/it] 74%|███████▎  | 7240/9822 [3:14:30<1:02:36,  1.46s/it] 74%|███████▎  | 7241/9822 [3:14:31<1:02:22,  1.45s/it] 74%|███████▎  | 7242/9822 [3:14:32<1:02:05,  1.44s/it] 74%|███████▎  | 7243/9822 [3:14:34<1:01:51,  1.44s/it] 74%|███████▍  | 7244/9822 [3:14:35<1:01:45,  1.44s/it] 74%|███████▍  | 7245/9822 [3:14:37<1:01:35,  1.43s/it] 74%|███████▍  | 7246/9822 [3:14:38<1:01:45,  1.44s/it] 74%|███████▍  | 7247/9822 [3:14:40<1:01:45,  1.44s/it] 74%|███████▍  | 7248/9822 [3:14:41<1:01:51,  1.44s/it] 74%|███████▍  | 7249/9822 [3:14:42<1:01:41,  1.44s/it] 74%|███████▍  | 7250/9822 [3:14:44<1:01:38,  1.44s/it] 74%|███████▍  | 7251/9822 [3:14:45<1:01:33,  1.44s/it] 74%|███████▍  | 7252/9822 [3:14:47<1:01:52,  1.44s/it] 74%|███████▍  | 7253/9822 [3:14:48<1:01:47,  1.44s/it] 74%|███████▍  | 7254/9822 [3:14:50<1:01:40,  1.44s/it] 74%|███████▍  | 7255/9822 [3:14:51<1:01:28,  1.44s/it] 74%|███████▍  | 7256/9822 [3:14:53<1:01:24,  1.44s/it] 74%|███████▍  | 7257/9822 [3:14:54<1:01:14,  1.43s/it] 74%|███████▍  | 7258/9822 [3:14:55<1:01:16,  1.43s/it] 74%|███████▍  | 7259/9822 [3:14:57<1:01:10,  1.43s/it] 74%|███████▍  | 7260/9822 [3:14:58<1:01:06,  1.43s/it] 74%|███████▍  | 7261/9822 [3:15:00<1:01:08,  1.43s/it] 74%|███████▍  | 7262/9822 [3:15:01<1:01:20,  1.44s/it] 74%|███████▍  | 7263/9822 [3:15:03<1:01:23,  1.44s/it] 74%|███████▍  | 7264/9822 [3:15:04<1:01:48,  1.45s/it] 74%|███████▍  | 7265/9822 [3:15:06<1:02:21,  1.46s/it] 74%|███████▍  | 7266/9822 [3:15:07<1:02:01,  1.46s/it] 74%|███████▍  | 7267/9822 [3:15:08<1:01:43,  1.45s/it] 74%|███████▍  | 7268/9822 [3:15:10<1:01:34,  1.45s/it] 74%|███████▍  | 7269/9822 [3:15:11<1:01:18,  1.44s/it] 74%|███████▍  | 7270/9822 [3:15:13<1:02:21,  1.47s/it] 74%|███████▍  | 7271/9822 [3:15:14<1:02:01,  1.46s/it] 74%|███████▍  | 7272/9822 [3:15:16<1:02:17,  1.47s/it] 74%|███████▍  | 7273/9822 [3:15:17<1:01:57,  1.46s/it] 74%|███████▍  | 7274/9822 [3:15:19<1:01:30,  1.45s/it] 74%|███████▍  | 7275/9822 [3:15:20<1:01:16,  1.44s/it] 74%|███████▍  | 7276/9822 [3:15:22<1:01:16,  1.44s/it] 74%|███████▍  | 7277/9822 [3:15:23<1:01:08,  1.44s/it] 74%|███████▍  | 7278/9822 [3:15:24<1:00:59,  1.44s/it] 74%|███████▍  | 7279/9822 [3:15:26<1:01:03,  1.44s/it] 74%|███████▍  | 7280/9822 [3:15:27<1:01:02,  1.44s/it] 74%|███████▍  | 7281/9822 [3:15:29<1:01:00,  1.44s/it] 74%|███████▍  | 7282/9822 [3:15:30<1:00:55,  1.44s/it] 74%|███████▍  | 7283/9822 [3:15:32<1:00:57,  1.44s/it] 74%|███████▍  | 7284/9822 [3:15:33<1:00:45,  1.44s/it] 74%|███████▍  | 7285/9822 [3:15:34<1:01:07,  1.45s/it] 74%|███████▍  | 7286/9822 [3:15:36<1:01:09,  1.45s/it] 74%|███████▍  | 7287/9822 [3:15:37<1:00:58,  1.44s/it] 74%|███████▍  | 7288/9822 [3:15:39<1:00:52,  1.44s/it] 74%|███████▍  | 7289/9822 [3:15:40<1:00:42,  1.44s/it] 74%|███████▍  | 7290/9822 [3:15:42<1:00:38,  1.44s/it] 74%|███████▍  | 7291/9822 [3:15:43<1:00:30,  1.43s/it] 74%|███████▍  | 7292/9822 [3:15:45<1:00:24,  1.43s/it] 74%|███████▍  | 7293/9822 [3:15:46<1:00:27,  1.43s/it] 74%|███████▍  | 7294/9822 [3:15:47<1:00:23,  1.43s/it] 74%|███████▍  | 7295/9822 [3:15:49<1:00:28,  1.44s/it] 74%|███████▍  | 7296/9822 [3:15:50<1:00:37,  1.44s/it] 74%|███████▍  | 7297/9822 [3:15:52<1:00:29,  1.44s/it] 74%|███████▍  | 7298/9822 [3:15:53<1:00:26,  1.44s/it] 74%|███████▍  | 7299/9822 [3:15:55<1:00:13,  1.43s/it] 74%|███████▍  | 7300/9822 [3:15:56<1:00:14,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1103, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2053, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1183, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:03:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:03:43 - INFO - __main__ - ***** test Results*****
01/08/2024 01:03:43 - INFO - __main__ -   Training step = 7300
01/08/2024 01:03:43 - INFO - __main__ -  test_accuracy:0.8755490483162518 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:03:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:03:49 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:03:49 - INFO - __main__ -   Training step = 7300
01/08/2024 01:03:49 - INFO - __main__ -  eval_accuracy:0.859758330281948 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:03:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:03:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:03:54 - INFO - __main__ -   Training step = 7300
01/08/2024 01:03:54 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 74%|███████▍  | 7301/9822 [3:16:14<4:33:51,  6.52s/it] 74%|███████▍  | 7302/9822 [3:16:16<3:30:49,  5.02s/it] 74%|███████▍  | 7303/9822 [3:16:17<2:45:31,  3.94s/it] 74%|███████▍  | 7304/9822 [3:16:19<2:13:50,  3.19s/it] 74%|███████▍  | 7305/9822 [3:16:20<1:51:47,  2.66s/it] 74%|███████▍  | 7306/9822 [3:16:22<1:36:57,  2.31s/it] 74%|███████▍  | 7307/9822 [3:16:23<1:26:03,  2.05s/it] 74%|███████▍  | 7308/9822 [3:16:25<1:18:25,  1.87s/it] 74%|███████▍  | 7309/9822 [3:16:26<1:13:09,  1.75s/it] 74%|███████▍  | 7310/9822 [3:16:27<1:08:48,  1.64s/it] 74%|███████▍  | 7311/9822 [3:16:29<1:06:13,  1.58s/it] 74%|███████▍  | 7312/9822 [3:16:30<1:04:24,  1.54s/it] 74%|███████▍  | 7313/9822 [3:16:32<1:03:01,  1.51s/it] 74%|███████▍  | 7314/9822 [3:16:33<1:02:01,  1.48s/it] 74%|███████▍  | 7315/9822 [3:16:35<1:01:17,  1.47s/it] 74%|███████▍  | 7316/9822 [3:16:36<1:00:53,  1.46s/it] 74%|███████▍  | 7317/9822 [3:16:37<1:00:34,  1.45s/it] 75%|███████▍  | 7318/9822 [3:16:39<1:00:17,  1.44s/it] 75%|███████▍  | 7319/9822 [3:16:40<1:00:07,  1.44s/it] 75%|███████▍  | 7320/9822 [3:16:42<59:58,  1.44s/it]   75%|███████▍  | 7321/9822 [3:16:43<59:43,  1.43s/it] 75%|███████▍  | 7322/9822 [3:16:45<59:46,  1.43s/it] 75%|███████▍  | 7323/9822 [3:16:46<59:49,  1.44s/it] 75%|███████▍  | 7324/9822 [3:16:48<59:33,  1.43s/it] 75%|███████▍  | 7325/9822 [3:16:49<59:30,  1.43s/it] 75%|███████▍  | 7326/9822 [3:16:50<59:36,  1.43s/it] 75%|███████▍  | 7327/9822 [3:16:52<59:35,  1.43s/it] 75%|███████▍  | 7328/9822 [3:16:53<59:22,  1.43s/it] 75%|███████▍  | 7329/9822 [3:16:55<59:16,  1.43s/it] 75%|███████▍  | 7330/9822 [3:16:56<59:31,  1.43s/it] 75%|███████▍  | 7331/9822 [3:16:58<59:33,  1.43s/it] 75%|███████▍  | 7332/9822 [3:16:59<59:33,  1.44s/it] 75%|███████▍  | 7333/9822 [3:17:00<59:33,  1.44s/it] 75%|███████▍  | 7334/9822 [3:17:02<1:00:28,  1.46s/it] 75%|███████▍  | 7335/9822 [3:17:03<1:00:09,  1.45s/it] 75%|███████▍  | 7336/9822 [3:17:05<59:51,  1.44s/it]   75%|███████▍  | 7337/9822 [3:17:06<59:51,  1.45s/it] 75%|███████▍  | 7338/9822 [3:17:08<59:44,  1.44s/it] 75%|███████▍  | 7339/9822 [3:17:09<59:25,  1.44s/it] 75%|███████▍  | 7340/9822 [3:17:11<59:20,  1.43s/it] 75%|███████▍  | 7341/9822 [3:17:12<59:20,  1.44s/it] 75%|███████▍  | 7342/9822 [3:17:13<59:14,  1.43s/it] 75%|███████▍  | 7343/9822 [3:17:15<59:16,  1.43s/it] 75%|███████▍  | 7344/9822 [3:17:16<59:12,  1.43s/it] 75%|███████▍  | 7345/9822 [3:17:18<59:17,  1.44s/it] 75%|███████▍  | 7346/9822 [3:17:19<59:23,  1.44s/it] 75%|███████▍  | 7347/9822 [3:17:21<59:20,  1.44s/it] 75%|███████▍  | 7348/9822 [3:17:22<59:31,  1.44s/it] 75%|███████▍  | 7349/9822 [3:17:23<59:21,  1.44s/it] 75%|███████▍  | 7350/9822 [3:17:25<59:19,  1.44s/it] 75%|███████▍  | 7351/9822 [3:17:26<59:21,  1.44s/it] 75%|███████▍  | 7352/9822 [3:17:28<59:16,  1.44s/it] 75%|███████▍  | 7353/9822 [3:17:29<59:04,  1.44s/it] 75%|███████▍  | 7354/9822 [3:17:31<59:00,  1.43s/it] 75%|███████▍  | 7355/9822 [3:17:32<59:02,  1.44s/it] 75%|███████▍  | 7356/9822 [3:17:34<58:57,  1.43s/it] 75%|███████▍  | 7357/9822 [3:17:35<59:03,  1.44s/it] 75%|███████▍  | 7358/9822 [3:17:36<59:17,  1.44s/it] 75%|███████▍  | 7359/9822 [3:17:38<59:38,  1.45s/it] 75%|███████▍  | 7360/9822 [3:17:39<1:00:10,  1.47s/it] 75%|███████▍  | 7361/9822 [3:17:41<1:00:02,  1.46s/it] 75%|███████▍  | 7362/9822 [3:17:42<1:00:14,  1.47s/it] 75%|███████▍  | 7363/9822 [3:17:44<59:54,  1.46s/it]   75%|███████▍  | 7364/9822 [3:17:45<59:49,  1.46s/it] 75%|███████▍  | 7365/9822 [3:17:47<59:41,  1.46s/it] 75%|███████▍  | 7366/9822 [3:17:48<1:00:37,  1.48s/it] 75%|███████▌  | 7367/9822 [3:17:50<1:00:15,  1.47s/it] 75%|███████▌  | 7368/9822 [3:17:51<59:59,  1.47s/it]   75%|███████▌  | 7369/9822 [3:17:53<59:38,  1.46s/it] 75%|███████▌  | 7370/9822 [3:17:54<59:21,  1.45s/it] 75%|███████▌  | 7371/9822 [3:17:55<59:04,  1.45s/it] 75%|███████▌  | 7372/9822 [3:17:57<58:59,  1.44s/it] 75%|███████▌  | 7373/9822 [3:17:58<58:51,  1.44s/it] 75%|███████▌  | 7374/9822 [3:18:00<58:54,  1.44s/it] 75%|███████▌  | 7375/9822 [3:18:01<58:57,  1.45s/it] 75%|███████▌  | 7376/9822 [3:18:03<59:10,  1.45s/it] 75%|███████▌  | 7377/9822 [3:18:04<59:00,  1.45s/it] 75%|███████▌  | 7378/9822 [3:18:06<58:47,  1.44s/it] 75%|███████▌  | 7379/9822 [3:18:07<58:38,  1.44s/it] 75%|███████▌  | 7380/9822 [3:18:08<58:37,  1.44s/it] 75%|███████▌  | 7381/9822 [3:18:10<58:24,  1.44s/it] 75%|███████▌  | 7382/9822 [3:18:11<58:20,  1.43s/it] 75%|███████▌  | 7383/9822 [3:18:13<58:21,  1.44s/it] 75%|███████▌  | 7384/9822 [3:18:14<58:26,  1.44s/it] 75%|███████▌  | 7385/9822 [3:18:16<58:32,  1.44s/it] 75%|███████▌  | 7386/9822 [3:18:17<58:32,  1.44s/it] 75%|███████▌  | 7387/9822 [3:18:18<58:30,  1.44s/it] 75%|███████▌  | 7388/9822 [3:18:20<58:22,  1.44s/it] 75%|███████▌  | 7389/9822 [3:18:21<58:21,  1.44s/it] 75%|███████▌  | 7390/9822 [3:18:23<58:21,  1.44s/it] 75%|███████▌  | 7391/9822 [3:18:24<59:51,  1.48s/it] 75%|███████▌  | 7392/9822 [3:18:26<59:17,  1.46s/it] 75%|███████▌  | 7393/9822 [3:18:27<58:50,  1.45s/it] 75%|███████▌  | 7394/9822 [3:18:29<58:40,  1.45s/it] 75%|███████▌  | 7395/9822 [3:18:30<58:30,  1.45s/it] 75%|███████▌  | 7396/9822 [3:18:31<57:43,  1.43s/it] 75%|███████▌  | 7397/9822 [3:18:33<57:45,  1.43s/it] 75%|███████▌  | 7398/9822 [3:18:34<57:51,  1.43s/it] 75%|███████▌  | 7399/9822 [3:18:36<57:58,  1.44s/it] 75%|███████▌  | 7400/9822 [3:18:37<58:01,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1380, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:06:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:06:24 - INFO - __main__ - ***** test Results*****
01/08/2024 01:06:24 - INFO - __main__ -   Training step = 7400
01/08/2024 01:06:24 - INFO - __main__ -  test_accuracy:0.8766471449487555 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:06:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:06:30 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:06:30 - INFO - __main__ -   Training step = 7400
01/08/2024 01:06:30 - INFO - __main__ -  eval_accuracy:0.8586598315635299 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:06:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:06:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:06:35 - INFO - __main__ -   Training step = 7400
01/08/2024 01:06:35 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 75%|███████▌  | 7401/9822 [3:18:56<4:23:30,  6.53s/it] 75%|███████▌  | 7402/9822 [3:18:57<3:21:41,  5.00s/it] 75%|███████▌  | 7403/9822 [3:18:59<2:38:21,  3.93s/it] 75%|███████▌  | 7404/9822 [3:19:00<2:08:14,  3.18s/it] 75%|███████▌  | 7405/9822 [3:19:01<1:47:13,  2.66s/it] 75%|███████▌  | 7406/9822 [3:19:03<1:32:20,  2.29s/it] 75%|███████▌  | 7407/9822 [3:19:04<1:21:54,  2.04s/it] 75%|███████▌  | 7408/9822 [3:19:06<1:14:34,  1.85s/it] 75%|███████▌  | 7409/9822 [3:19:07<1:09:35,  1.73s/it] 75%|███████▌  | 7410/9822 [3:19:09<1:06:04,  1.64s/it] 75%|███████▌  | 7411/9822 [3:19:10<1:03:41,  1.58s/it] 75%|███████▌  | 7412/9822 [3:19:11<1:01:50,  1.54s/it] 75%|███████▌  | 7413/9822 [3:19:13<1:00:31,  1.51s/it] 75%|███████▌  | 7414/9822 [3:19:14<59:30,  1.48s/it]   75%|███████▌  | 7415/9822 [3:19:16<58:51,  1.47s/it] 76%|███████▌  | 7416/9822 [3:19:17<58:30,  1.46s/it] 76%|███████▌  | 7417/9822 [3:19:19<59:05,  1.47s/it] 76%|███████▌  | 7418/9822 [3:19:20<58:39,  1.46s/it] 76%|███████▌  | 7419/9822 [3:19:22<58:16,  1.46s/it] 76%|███████▌  | 7420/9822 [3:19:23<57:59,  1.45s/it] 76%|███████▌  | 7421/9822 [3:19:24<57:48,  1.44s/it] 76%|███████▌  | 7422/9822 [3:19:26<57:42,  1.44s/it] 76%|███████▌  | 7423/9822 [3:19:27<57:38,  1.44s/it] 76%|███████▌  | 7424/9822 [3:19:29<57:25,  1.44s/it] 76%|███████▌  | 7425/9822 [3:19:30<57:18,  1.43s/it] 76%|███████▌  | 7426/9822 [3:19:32<57:25,  1.44s/it] 76%|███████▌  | 7427/9822 [3:19:33<57:16,  1.43s/it] 76%|███████▌  | 7428/9822 [3:19:34<57:10,  1.43s/it] 76%|███████▌  | 7429/9822 [3:19:36<57:27,  1.44s/it] 76%|███████▌  | 7430/9822 [3:19:37<57:21,  1.44s/it] 76%|███████▌  | 7431/9822 [3:19:39<57:13,  1.44s/it] 76%|███████▌  | 7432/9822 [3:19:40<57:10,  1.44s/it] 76%|███████▌  | 7433/9822 [3:19:42<57:12,  1.44s/it] 76%|███████▌  | 7434/9822 [3:19:43<57:08,  1.44s/it] 76%|███████▌  | 7435/9822 [3:19:45<57:04,  1.43s/it] 76%|███████▌  | 7436/9822 [3:19:46<56:56,  1.43s/it] 76%|███████▌  | 7437/9822 [3:19:47<56:52,  1.43s/it] 76%|███████▌  | 7438/9822 [3:19:49<57:02,  1.44s/it] 76%|███████▌  | 7439/9822 [3:19:50<57:04,  1.44s/it] 76%|███████▌  | 7440/9822 [3:19:52<57:13,  1.44s/it] 76%|███████▌  | 7441/9822 [3:19:53<57:47,  1.46s/it] 76%|███████▌  | 7442/9822 [3:19:55<58:13,  1.47s/it] 76%|███████▌  | 7443/9822 [3:19:56<57:57,  1.46s/it] 76%|███████▌  | 7444/9822 [3:19:58<57:48,  1.46s/it] 76%|███████▌  | 7445/9822 [3:19:59<57:41,  1.46s/it] 76%|███████▌  | 7446/9822 [3:20:01<57:38,  1.46s/it] 76%|███████▌  | 7447/9822 [3:20:02<57:31,  1.45s/it] 76%|███████▌  | 7448/9822 [3:20:03<57:29,  1.45s/it] 76%|███████▌  | 7449/9822 [3:20:05<58:22,  1.48s/it] 76%|███████▌  | 7450/9822 [3:20:06<58:05,  1.47s/it] 76%|███████▌  | 7451/9822 [3:20:08<57:52,  1.46s/it] 76%|███████▌  | 7452/9822 [3:20:09<57:47,  1.46s/it] 76%|███████▌  | 7453/9822 [3:20:11<58:07,  1.47s/it] 76%|███████▌  | 7454/9822 [3:20:12<57:53,  1.47s/it] 76%|███████▌  | 7455/9822 [3:20:14<57:41,  1.46s/it] 76%|███████▌  | 7456/9822 [3:20:15<57:20,  1.45s/it] 76%|███████▌  | 7457/9822 [3:20:17<57:09,  1.45s/it] 76%|███████▌  | 7458/9822 [3:20:18<57:12,  1.45s/it] 76%|███████▌  | 7459/9822 [3:20:19<56:55,  1.45s/it] 76%|███████▌  | 7460/9822 [3:20:21<56:41,  1.44s/it] 76%|███████▌  | 7461/9822 [3:20:22<56:31,  1.44s/it] 76%|███████▌  | 7462/9822 [3:20:24<56:32,  1.44s/it] 76%|███████▌  | 7463/9822 [3:20:25<56:40,  1.44s/it] 76%|███████▌  | 7464/9822 [3:20:27<56:34,  1.44s/it] 76%|███████▌  | 7465/9822 [3:20:28<56:28,  1.44s/it] 76%|███████▌  | 7466/9822 [3:20:30<56:21,  1.44s/it] 76%|███████▌  | 7467/9822 [3:20:31<56:21,  1.44s/it] 76%|███████▌  | 7468/9822 [3:20:32<56:14,  1.43s/it] 76%|███████▌  | 7469/9822 [3:20:34<56:09,  1.43s/it] 76%|███████▌  | 7470/9822 [3:20:35<56:08,  1.43s/it] 76%|███████▌  | 7471/9822 [3:20:37<56:09,  1.43s/it] 76%|███████▌  | 7472/9822 [3:20:38<56:12,  1.44s/it] 76%|███████▌  | 7473/9822 [3:20:40<56:13,  1.44s/it] 76%|███████▌  | 7474/9822 [3:20:41<56:06,  1.43s/it] 76%|███████▌  | 7475/9822 [3:20:42<56:08,  1.44s/it] 76%|███████▌  | 7476/9822 [3:20:44<56:03,  1.43s/it] 76%|███████▌  | 7477/9822 [3:20:45<56:06,  1.44s/it] 76%|███████▌  | 7478/9822 [3:20:47<56:02,  1.43s/it] 76%|███████▌  | 7479/9822 [3:20:48<56:04,  1.44s/it] 76%|███████▌  | 7480/9822 [3:20:50<55:56,  1.43s/it] 76%|███████▌  | 7481/9822 [3:20:51<56:49,  1.46s/it] 76%|███████▌  | 7482/9822 [3:20:53<56:02,  1.44s/it] 76%|███████▌  | 7483/9822 [3:20:54<56:13,  1.44s/it] 76%|███████▌  | 7484/9822 [3:20:55<56:08,  1.44s/it] 76%|███████▌  | 7485/9822 [3:20:57<55:55,  1.44s/it] 76%|███████▌  | 7486/9822 [3:20:58<55:54,  1.44s/it] 76%|███████▌  | 7487/9822 [3:21:00<56:00,  1.44s/it] 76%|███████▌  | 7488/9822 [3:21:01<55:49,  1.43s/it] 76%|███████▌  | 7489/9822 [3:21:03<55:50,  1.44s/it] 76%|███████▋  | 7490/9822 [3:21:04<55:43,  1.43s/it] 76%|███████▋  | 7491/9822 [3:21:05<55:47,  1.44s/it] 76%|███████▋  | 7492/9822 [3:21:07<55:39,  1.43s/it] 76%|███████▋  | 7493/9822 [3:21:08<55:35,  1.43s/it] 76%|███████▋  | 7494/9822 [3:21:10<55:25,  1.43s/it] 76%|███████▋  | 7495/9822 [3:21:11<55:18,  1.43s/it] 76%|███████▋  | 7496/9822 [3:21:13<55:12,  1.42s/it] 76%|███████▋  | 7497/9822 [3:21:14<55:13,  1.43s/it] 76%|███████▋  | 7498/9822 [3:21:15<55:29,  1.43s/it] 76%|███████▋  | 7499/9822 [3:21:17<55:42,  1.44s/it] 76%|███████▋  | 7500/9822 [3:21:18<55:35,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1165, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1063, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1086, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1081, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1807, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1058, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1108, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:09:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:09:05 - INFO - __main__ - ***** test Results*****
01/08/2024 01:09:05 - INFO - __main__ -   Training step = 7500
01/08/2024 01:09:05 - INFO - __main__ -  test_accuracy:0.8740849194729137 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:09:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:09:11 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:09:11 - INFO - __main__ -   Training step = 7500
01/08/2024 01:09:11 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:09:16 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:09:16 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:09:16 - INFO - __main__ -   Training step = 7500
01/08/2024 01:09:16 - INFO - __main__ -  eval_accuracy:0.9113877700476016 
 76%|███████▋  | 7501/9822 [3:21:37<4:11:50,  6.51s/it] 76%|███████▋  | 7502/9822 [3:21:38<3:12:54,  4.99s/it] 76%|███████▋  | 7503/9822 [3:21:40<2:31:50,  3.93s/it] 76%|███████▋  | 7504/9822 [3:21:41<2:03:00,  3.18s/it] 76%|███████▋  | 7505/9822 [3:21:42<1:42:38,  2.66s/it] 76%|███████▋  | 7506/9822 [3:21:44<1:28:37,  2.30s/it] 76%|███████▋  | 7507/9822 [3:21:45<1:18:47,  2.04s/it] 76%|███████▋  | 7508/9822 [3:21:47<1:12:43,  1.89s/it] 76%|███████▋  | 7509/9822 [3:21:48<1:07:19,  1.75s/it] 76%|███████▋  | 7510/9822 [3:21:50<1:03:40,  1.65s/it] 76%|███████▋  | 7511/9822 [3:21:51<1:01:31,  1.60s/it] 76%|███████▋  | 7512/9822 [3:21:53<59:37,  1.55s/it]   76%|███████▋  | 7513/9822 [3:21:54<58:23,  1.52s/it] 77%|███████▋  | 7514/9822 [3:21:56<57:34,  1.50s/it] 77%|███████▋  | 7515/9822 [3:21:57<56:47,  1.48s/it] 77%|███████▋  | 7516/9822 [3:21:58<56:19,  1.47s/it] 77%|███████▋  | 7517/9822 [3:22:00<55:56,  1.46s/it] 77%|███████▋  | 7518/9822 [3:22:01<55:37,  1.45s/it] 77%|███████▋  | 7519/9822 [3:22:03<55:25,  1.44s/it] 77%|███████▋  | 7520/9822 [3:22:04<55:15,  1.44s/it] 77%|███████▋  | 7521/9822 [3:22:06<55:19,  1.44s/it] 77%|███████▋  | 7522/9822 [3:22:07<55:03,  1.44s/it] 77%|███████▋  | 7523/9822 [3:22:08<55:25,  1.45s/it] 77%|███████▋  | 7524/9822 [3:22:10<55:11,  1.44s/it] 77%|███████▋  | 7525/9822 [3:22:11<55:03,  1.44s/it] 77%|███████▋  | 7526/9822 [3:22:13<55:08,  1.44s/it] 77%|███████▋  | 7527/9822 [3:22:14<55:21,  1.45s/it] 77%|███████▋  | 7528/9822 [3:22:16<55:23,  1.45s/it] 77%|███████▋  | 7529/9822 [3:22:17<55:26,  1.45s/it] 77%|███████▋  | 7530/9822 [3:22:19<55:19,  1.45s/it] 77%|███████▋  | 7531/9822 [3:22:20<55:10,  1.44s/it] 77%|███████▋  | 7532/9822 [3:22:21<55:10,  1.45s/it] 77%|███████▋  | 7533/9822 [3:22:23<55:04,  1.44s/it] 77%|███████▋  | 7534/9822 [3:22:24<54:55,  1.44s/it] 77%|███████▋  | 7535/9822 [3:22:26<54:53,  1.44s/it] 77%|███████▋  | 7536/9822 [3:22:27<54:41,  1.44s/it] 77%|███████▋  | 7537/9822 [3:22:29<54:38,  1.43s/it] 77%|███████▋  | 7538/9822 [3:22:30<55:33,  1.46s/it] 77%|███████▋  | 7539/9822 [3:22:32<55:16,  1.45s/it] 77%|███████▋  | 7540/9822 [3:22:33<55:12,  1.45s/it] 77%|███████▋  | 7541/9822 [3:22:34<55:08,  1.45s/it] 77%|███████▋  | 7542/9822 [3:22:36<55:00,  1.45s/it] 77%|███████▋  | 7543/9822 [3:22:37<55:02,  1.45s/it] 77%|███████▋  | 7544/9822 [3:22:39<55:05,  1.45s/it] 77%|███████▋  | 7545/9822 [3:22:40<54:52,  1.45s/it] 77%|███████▋  | 7546/9822 [3:22:42<54:49,  1.45s/it] 77%|███████▋  | 7547/9822 [3:22:43<54:49,  1.45s/it] 77%|███████▋  | 7548/9822 [3:22:45<54:45,  1.44s/it] 77%|███████▋  | 7549/9822 [3:22:46<54:51,  1.45s/it] 77%|███████▋  | 7550/9822 [3:22:47<54:41,  1.44s/it] 77%|███████▋  | 7551/9822 [3:22:49<54:34,  1.44s/it] 77%|███████▋  | 7552/9822 [3:22:50<54:39,  1.44s/it] 77%|███████▋  | 7553/9822 [3:22:52<54:37,  1.44s/it] 77%|███████▋  | 7554/9822 [3:22:53<54:31,  1.44s/it] 77%|███████▋  | 7555/9822 [3:22:55<54:21,  1.44s/it] 77%|███████▋  | 7556/9822 [3:22:56<54:20,  1.44s/it] 77%|███████▋  | 7557/9822 [3:22:58<54:09,  1.43s/it] 77%|███████▋  | 7558/9822 [3:22:59<54:01,  1.43s/it] 77%|███████▋  | 7559/9822 [3:23:00<54:08,  1.44s/it] 77%|███████▋  | 7560/9822 [3:23:02<54:16,  1.44s/it] 77%|███████▋  | 7561/9822 [3:23:03<54:33,  1.45s/it] 77%|███████▋  | 7562/9822 [3:23:05<54:23,  1.44s/it] 77%|███████▋  | 7563/9822 [3:23:06<54:16,  1.44s/it] 77%|███████▋  | 7564/9822 [3:23:08<54:11,  1.44s/it] 77%|███████▋  | 7565/9822 [3:23:09<54:07,  1.44s/it] 77%|███████▋  | 7566/9822 [3:23:11<54:05,  1.44s/it] 77%|███████▋  | 7567/9822 [3:23:12<54:08,  1.44s/it] 77%|███████▋  | 7568/9822 [3:23:13<53:30,  1.42s/it] 77%|███████▋  | 7569/9822 [3:23:15<53:31,  1.43s/it] 77%|███████▋  | 7570/9822 [3:23:16<54:24,  1.45s/it] 77%|███████▋  | 7571/9822 [3:23:18<54:10,  1.44s/it] 77%|███████▋  | 7572/9822 [3:23:19<53:57,  1.44s/it] 77%|███████▋  | 7573/9822 [3:23:21<53:46,  1.43s/it] 77%|███████▋  | 7574/9822 [3:23:22<53:45,  1.43s/it] 77%|███████▋  | 7575/9822 [3:23:23<53:47,  1.44s/it] 77%|███████▋  | 7576/9822 [3:23:25<53:48,  1.44s/it] 77%|███████▋  | 7577/9822 [3:23:26<53:44,  1.44s/it] 77%|███████▋  | 7578/9822 [3:23:28<53:38,  1.43s/it] 77%|███████▋  | 7579/9822 [3:23:29<53:43,  1.44s/it] 77%|███████▋  | 7580/9822 [3:23:31<53:43,  1.44s/it] 77%|███████▋  | 7581/9822 [3:23:32<53:43,  1.44s/it] 77%|███████▋  | 7582/9822 [3:23:34<53:38,  1.44s/it] 77%|███████▋  | 7583/9822 [3:23:35<53:32,  1.43s/it] 77%|███████▋  | 7584/9822 [3:23:36<53:37,  1.44s/it] 77%|███████▋  | 7585/9822 [3:23:38<53:32,  1.44s/it] 77%|███████▋  | 7586/9822 [3:23:39<53:20,  1.43s/it] 77%|███████▋  | 7587/9822 [3:23:41<53:19,  1.43s/it] 77%|███████▋  | 7588/9822 [3:23:42<53:16,  1.43s/it] 77%|███████▋  | 7589/9822 [3:23:44<53:20,  1.43s/it] 77%|███████▋  | 7590/9822 [3:23:45<53:18,  1.43s/it] 77%|███████▋  | 7591/9822 [3:23:46<53:10,  1.43s/it] 77%|███████▋  | 7592/9822 [3:23:48<53:14,  1.43s/it] 77%|███████▋  | 7593/9822 [3:23:49<53:08,  1.43s/it] 77%|███████▋  | 7594/9822 [3:23:51<53:14,  1.43s/it] 77%|███████▋  | 7595/9822 [3:23:52<53:04,  1.43s/it] 77%|███████▋  | 7596/9822 [3:23:54<53:04,  1.43s/it] 77%|███████▋  | 7597/9822 [3:23:55<53:13,  1.44s/it] 77%|███████▋  | 7598/9822 [3:23:56<53:12,  1.44s/it] 77%|███████▋  | 7599/9822 [3:23:58<53:00,  1.43s/it] 77%|███████▋  | 7600/9822 [3:23:59<53:02,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1178, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1390, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1478, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0914, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:11:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:11:46 - INFO - __main__ - ***** test Results*****
01/08/2024 01:11:46 - INFO - __main__ -   Training step = 7600
01/08/2024 01:11:46 - INFO - __main__ -  test_accuracy:0.8825036603221084 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:11:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:11:52 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:11:52 - INFO - __main__ -   Training step = 7600
01/08/2024 01:11:52 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:11:57 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:11:57 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:11:57 - INFO - __main__ -   Training step = 7600
01/08/2024 01:11:57 - INFO - __main__ -  eval_accuracy:0.9139509337239107 
 77%|███████▋  | 7601/9822 [3:24:18<4:01:11,  6.52s/it] 77%|███████▋  | 7602/9822 [3:24:19<3:05:51,  5.02s/it] 77%|███████▋  | 7603/9822 [3:24:21<2:25:56,  3.95s/it] 77%|███████▋  | 7604/9822 [3:24:22<1:58:05,  3.19s/it] 77%|███████▋  | 7605/9822 [3:24:24<1:38:35,  2.67s/it] 77%|███████▋  | 7606/9822 [3:24:25<1:24:49,  2.30s/it] 77%|███████▋  | 7607/9822 [3:24:26<1:15:16,  2.04s/it] 77%|███████▋  | 7608/9822 [3:24:28<1:08:49,  1.87s/it] 77%|███████▋  | 7609/9822 [3:24:29<1:03:57,  1.73s/it] 77%|███████▋  | 7610/9822 [3:24:31<1:00:45,  1.65s/it] 77%|███████▋  | 7611/9822 [3:24:32<58:29,  1.59s/it]   77%|███████▋  | 7612/9822 [3:24:34<56:49,  1.54s/it] 78%|███████▊  | 7613/9822 [3:24:35<55:38,  1.51s/it] 78%|███████▊  | 7614/9822 [3:24:36<54:44,  1.49s/it] 78%|███████▊  | 7615/9822 [3:24:38<54:08,  1.47s/it] 78%|███████▊  | 7616/9822 [3:24:39<53:40,  1.46s/it] 78%|███████▊  | 7617/9822 [3:24:41<53:22,  1.45s/it] 78%|███████▊  | 7618/9822 [3:24:42<53:08,  1.45s/it] 78%|███████▊  | 7619/9822 [3:24:44<52:57,  1.44s/it] 78%|███████▊  | 7620/9822 [3:24:45<52:55,  1.44s/it] 78%|███████▊  | 7621/9822 [3:24:47<52:54,  1.44s/it] 78%|███████▊  | 7622/9822 [3:24:48<52:43,  1.44s/it] 78%|███████▊  | 7623/9822 [3:24:49<52:30,  1.43s/it] 78%|███████▊  | 7624/9822 [3:24:51<52:33,  1.43s/it] 78%|███████▊  | 7625/9822 [3:24:52<52:35,  1.44s/it] 78%|███████▊  | 7626/9822 [3:24:54<52:28,  1.43s/it] 78%|███████▊  | 7627/9822 [3:24:55<52:25,  1.43s/it] 78%|███████▊  | 7628/9822 [3:24:57<52:34,  1.44s/it] 78%|███████▊  | 7629/9822 [3:24:58<52:23,  1.43s/it] 78%|███████▊  | 7630/9822 [3:24:59<52:36,  1.44s/it] 78%|███████▊  | 7631/9822 [3:25:01<52:30,  1.44s/it] 78%|███████▊  | 7632/9822 [3:25:02<52:34,  1.44s/it] 78%|███████▊  | 7633/9822 [3:25:04<52:34,  1.44s/it] 78%|███████▊  | 7634/9822 [3:25:05<53:22,  1.46s/it] 78%|███████▊  | 7635/9822 [3:25:07<52:57,  1.45s/it] 78%|███████▊  | 7636/9822 [3:25:08<52:40,  1.45s/it] 78%|███████▊  | 7637/9822 [3:25:10<52:29,  1.44s/it] 78%|███████▊  | 7638/9822 [3:25:11<52:23,  1.44s/it] 78%|███████▊  | 7639/9822 [3:25:12<52:17,  1.44s/it] 78%|███████▊  | 7640/9822 [3:25:14<52:17,  1.44s/it] 78%|███████▊  | 7641/9822 [3:25:15<52:19,  1.44s/it] 78%|███████▊  | 7642/9822 [3:25:17<52:09,  1.44s/it] 78%|███████▊  | 7643/9822 [3:25:18<52:07,  1.44s/it] 78%|███████▊  | 7644/9822 [3:25:20<51:57,  1.43s/it] 78%|███████▊  | 7645/9822 [3:25:21<52:01,  1.43s/it] 78%|███████▊  | 7646/9822 [3:25:23<52:27,  1.45s/it] 78%|███████▊  | 7647/9822 [3:25:24<52:12,  1.44s/it] 78%|███████▊  | 7648/9822 [3:25:25<52:09,  1.44s/it] 78%|███████▊  | 7649/9822 [3:25:27<52:05,  1.44s/it] 78%|███████▊  | 7650/9822 [3:25:28<52:18,  1.45s/it] 78%|███████▊  | 7651/9822 [3:25:30<52:20,  1.45s/it] 78%|███████▊  | 7652/9822 [3:25:31<52:13,  1.44s/it] 78%|███████▊  | 7653/9822 [3:25:33<52:00,  1.44s/it] 78%|███████▊  | 7654/9822 [3:25:34<51:18,  1.42s/it] 78%|███████▊  | 7655/9822 [3:25:35<51:45,  1.43s/it] 78%|███████▊  | 7656/9822 [3:25:37<51:44,  1.43s/it] 78%|███████▊  | 7657/9822 [3:25:38<51:43,  1.43s/it] 78%|███████▊  | 7658/9822 [3:25:40<51:48,  1.44s/it] 78%|███████▊  | 7659/9822 [3:25:41<52:45,  1.46s/it] 78%|███████▊  | 7660/9822 [3:25:43<52:21,  1.45s/it] 78%|███████▊  | 7661/9822 [3:25:44<52:17,  1.45s/it] 78%|███████▊  | 7662/9822 [3:25:46<52:42,  1.46s/it] 78%|███████▊  | 7663/9822 [3:25:47<52:25,  1.46s/it] 78%|███████▊  | 7664/9822 [3:25:49<52:20,  1.46s/it] 78%|███████▊  | 7665/9822 [3:25:50<52:03,  1.45s/it] 78%|███████▊  | 7666/9822 [3:25:51<51:58,  1.45s/it] 78%|███████▊  | 7667/9822 [3:25:53<51:46,  1.44s/it] 78%|███████▊  | 7668/9822 [3:25:54<51:37,  1.44s/it] 78%|███████▊  | 7669/9822 [3:25:56<51:33,  1.44s/it] 78%|███████▊  | 7670/9822 [3:25:57<51:31,  1.44s/it] 78%|███████▊  | 7671/9822 [3:25:59<51:46,  1.44s/it] 78%|███████▊  | 7672/9822 [3:26:00<51:40,  1.44s/it] 78%|███████▊  | 7673/9822 [3:26:01<51:55,  1.45s/it] 78%|███████▊  | 7674/9822 [3:26:03<51:45,  1.45s/it] 78%|███████▊  | 7675/9822 [3:26:04<51:38,  1.44s/it] 78%|███████▊  | 7676/9822 [3:26:06<51:31,  1.44s/it] 78%|███████▊  | 7677/9822 [3:26:07<51:19,  1.44s/it] 78%|███████▊  | 7678/9822 [3:26:09<51:31,  1.44s/it] 78%|███████▊  | 7679/9822 [3:26:10<51:23,  1.44s/it] 78%|███████▊  | 7680/9822 [3:26:12<51:35,  1.45s/it] 78%|███████▊  | 7681/9822 [3:26:13<51:56,  1.46s/it] 78%|███████▊  | 7682/9822 [3:26:15<51:52,  1.45s/it] 78%|███████▊  | 7683/9822 [3:26:16<51:36,  1.45s/it] 78%|███████▊  | 7684/9822 [3:26:17<51:31,  1.45s/it] 78%|███████▊  | 7685/9822 [3:26:19<51:28,  1.45s/it] 78%|███████▊  | 7686/9822 [3:26:20<51:30,  1.45s/it] 78%|███████▊  | 7687/9822 [3:26:22<51:25,  1.45s/it] 78%|███████▊  | 7688/9822 [3:26:23<51:21,  1.44s/it] 78%|███████▊  | 7689/9822 [3:26:25<51:13,  1.44s/it] 78%|███████▊  | 7690/9822 [3:26:26<51:04,  1.44s/it] 78%|███████▊  | 7691/9822 [3:26:28<51:45,  1.46s/it] 78%|███████▊  | 7692/9822 [3:26:29<51:43,  1.46s/it] 78%|███████▊  | 7693/9822 [3:26:30<51:55,  1.46s/it] 78%|███████▊  | 7694/9822 [3:26:32<51:43,  1.46s/it] 78%|███████▊  | 7695/9822 [3:26:33<51:31,  1.45s/it] 78%|███████▊  | 7696/9822 [3:26:35<51:26,  1.45s/it] 78%|███████▊  | 7697/9822 [3:26:36<51:16,  1.45s/it] 78%|███████▊  | 7698/9822 [3:26:38<51:01,  1.44s/it] 78%|███████▊  | 7699/9822 [3:26:39<50:59,  1.44s/it] 78%|███████▊  | 7700/9822 [3:26:41<50:58,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1171, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0651, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0711, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1107, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1443, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0982, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:14:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:14:27 - INFO - __main__ - ***** test Results*****
01/08/2024 01:14:27 - INFO - __main__ -   Training step = 7700
01/08/2024 01:14:27 - INFO - __main__ -  test_accuracy:0.8803074670571011 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:14:33 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:14:33 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:14:33 - INFO - __main__ -   Training step = 7700
01/08/2024 01:14:33 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:14:38 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:14:38 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:14:38 - INFO - __main__ -   Training step = 7700
01/08/2024 01:14:38 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 78%|███████▊  | 7701/9822 [3:26:59<3:50:13,  6.51s/it] 78%|███████▊  | 7702/9822 [3:27:00<2:56:35,  5.00s/it] 78%|███████▊  | 7703/9822 [3:27:02<2:19:16,  3.94s/it] 78%|███████▊  | 7704/9822 [3:27:03<1:52:42,  3.19s/it] 78%|███████▊  | 7705/9822 [3:27:05<1:34:00,  2.66s/it] 78%|███████▊  | 7706/9822 [3:27:06<1:21:06,  2.30s/it] 78%|███████▊  | 7707/9822 [3:27:08<1:12:02,  2.04s/it] 78%|███████▊  | 7708/9822 [3:27:09<1:05:38,  1.86s/it] 78%|███████▊  | 7709/9822 [3:27:11<1:01:16,  1.74s/it] 78%|███████▊  | 7710/9822 [3:27:12<58:06,  1.65s/it]   79%|███████▊  | 7711/9822 [3:27:13<55:41,  1.58s/it] 79%|███████▊  | 7712/9822 [3:27:15<54:05,  1.54s/it] 79%|███████▊  | 7713/9822 [3:27:16<52:55,  1.51s/it] 79%|███████▊  | 7714/9822 [3:27:18<52:11,  1.49s/it] 79%|███████▊  | 7715/9822 [3:27:19<51:38,  1.47s/it] 79%|███████▊  | 7716/9822 [3:27:21<51:09,  1.46s/it] 79%|███████▊  | 7717/9822 [3:27:22<50:58,  1.45s/it] 79%|███████▊  | 7718/9822 [3:27:23<50:48,  1.45s/it] 79%|███████▊  | 7719/9822 [3:27:25<50:58,  1.45s/it] 79%|███████▊  | 7720/9822 [3:27:26<51:25,  1.47s/it] 79%|███████▊  | 7721/9822 [3:27:28<51:43,  1.48s/it] 79%|███████▊  | 7722/9822 [3:27:29<51:53,  1.48s/it] 79%|███████▊  | 7723/9822 [3:27:31<51:29,  1.47s/it] 79%|███████▊  | 7724/9822 [3:27:32<51:11,  1.46s/it] 79%|███████▊  | 7725/9822 [3:27:34<51:01,  1.46s/it] 79%|███████▊  | 7726/9822 [3:27:35<50:51,  1.46s/it] 79%|███████▊  | 7727/9822 [3:27:37<50:37,  1.45s/it] 79%|███████▊  | 7728/9822 [3:27:38<50:25,  1.44s/it] 79%|███████▊  | 7729/9822 [3:27:40<51:05,  1.46s/it] 79%|███████▊  | 7730/9822 [3:27:41<50:42,  1.45s/it] 79%|███████▊  | 7731/9822 [3:27:42<50:26,  1.45s/it] 79%|███████▊  | 7732/9822 [3:27:44<50:21,  1.45s/it] 79%|███████▊  | 7733/9822 [3:27:45<50:12,  1.44s/it] 79%|███████▊  | 7734/9822 [3:27:47<50:06,  1.44s/it] 79%|███████▉  | 7735/9822 [3:27:48<49:53,  1.43s/it] 79%|███████▉  | 7736/9822 [3:27:50<49:48,  1.43s/it] 79%|███████▉  | 7737/9822 [3:27:51<49:49,  1.43s/it] 79%|███████▉  | 7738/9822 [3:27:52<49:54,  1.44s/it] 79%|███████▉  | 7739/9822 [3:27:54<49:42,  1.43s/it] 79%|███████▉  | 7740/9822 [3:27:55<49:07,  1.42s/it] 79%|███████▉  | 7741/9822 [3:27:57<49:12,  1.42s/it] 79%|███████▉  | 7742/9822 [3:27:58<49:10,  1.42s/it] 79%|███████▉  | 7743/9822 [3:28:00<49:14,  1.42s/it] 79%|███████▉  | 7744/9822 [3:28:01<49:23,  1.43s/it] 79%|███████▉  | 7745/9822 [3:28:02<49:28,  1.43s/it] 79%|███████▉  | 7746/9822 [3:28:04<49:28,  1.43s/it] 79%|███████▉  | 7747/9822 [3:28:05<49:26,  1.43s/it] 79%|███████▉  | 7748/9822 [3:28:07<49:24,  1.43s/it] 79%|███████▉  | 7749/9822 [3:28:08<49:24,  1.43s/it] 79%|███████▉  | 7750/9822 [3:28:10<49:34,  1.44s/it] 79%|███████▉  | 7751/9822 [3:28:11<49:32,  1.44s/it] 79%|███████▉  | 7752/9822 [3:28:12<49:41,  1.44s/it] 79%|███████▉  | 7753/9822 [3:28:14<49:39,  1.44s/it] 79%|███████▉  | 7754/9822 [3:28:15<49:33,  1.44s/it] 79%|███████▉  | 7755/9822 [3:28:17<49:25,  1.43s/it] 79%|███████▉  | 7756/9822 [3:28:18<49:23,  1.43s/it] 79%|███████▉  | 7757/9822 [3:28:20<49:26,  1.44s/it] 79%|███████▉  | 7758/9822 [3:28:21<49:18,  1.43s/it] 79%|███████▉  | 7759/9822 [3:28:22<49:19,  1.43s/it] 79%|███████▉  | 7760/9822 [3:28:24<49:16,  1.43s/it] 79%|███████▉  | 7761/9822 [3:28:25<50:17,  1.46s/it] 79%|███████▉  | 7762/9822 [3:28:27<49:59,  1.46s/it] 79%|███████▉  | 7763/9822 [3:28:28<49:42,  1.45s/it] 79%|███████▉  | 7764/9822 [3:28:30<49:33,  1.44s/it] 79%|███████▉  | 7765/9822 [3:28:31<49:26,  1.44s/it] 79%|███████▉  | 7766/9822 [3:28:33<49:13,  1.44s/it] 79%|███████▉  | 7767/9822 [3:28:34<49:03,  1.43s/it] 79%|███████▉  | 7768/9822 [3:28:35<49:06,  1.43s/it] 79%|███████▉  | 7769/9822 [3:28:37<49:03,  1.43s/it] 79%|███████▉  | 7770/9822 [3:28:38<49:00,  1.43s/it] 79%|███████▉  | 7771/9822 [3:28:40<49:05,  1.44s/it] 79%|███████▉  | 7772/9822 [3:28:41<48:58,  1.43s/it] 79%|███████▉  | 7773/9822 [3:28:43<49:02,  1.44s/it] 79%|███████▉  | 7774/9822 [3:28:44<48:58,  1.43s/it] 79%|███████▉  | 7775/9822 [3:28:46<48:50,  1.43s/it] 79%|███████▉  | 7776/9822 [3:28:47<48:52,  1.43s/it] 79%|███████▉  | 7777/9822 [3:28:48<49:07,  1.44s/it] 79%|███████▉  | 7778/9822 [3:28:50<49:12,  1.44s/it] 79%|███████▉  | 7779/9822 [3:28:51<49:14,  1.45s/it] 79%|███████▉  | 7780/9822 [3:28:53<49:04,  1.44s/it] 79%|███████▉  | 7781/9822 [3:28:54<48:53,  1.44s/it] 79%|███████▉  | 7782/9822 [3:28:56<48:45,  1.43s/it] 79%|███████▉  | 7783/9822 [3:28:57<48:35,  1.43s/it] 79%|███████▉  | 7784/9822 [3:28:58<48:33,  1.43s/it] 79%|███████▉  | 7785/9822 [3:29:00<48:37,  1.43s/it] 79%|███████▉  | 7786/9822 [3:29:01<49:22,  1.45s/it] 79%|███████▉  | 7787/9822 [3:29:03<49:02,  1.45s/it] 79%|███████▉  | 7788/9822 [3:29:04<48:53,  1.44s/it] 79%|███████▉  | 7789/9822 [3:29:06<48:46,  1.44s/it] 79%|███████▉  | 7790/9822 [3:29:07<48:49,  1.44s/it] 79%|███████▉  | 7791/9822 [3:29:09<48:47,  1.44s/it] 79%|███████▉  | 7792/9822 [3:29:10<48:49,  1.44s/it] 79%|███████▉  | 7793/9822 [3:29:11<48:50,  1.44s/it] 79%|███████▉  | 7794/9822 [3:29:13<48:47,  1.44s/it] 79%|███████▉  | 7795/9822 [3:29:14<48:43,  1.44s/it] 79%|███████▉  | 7796/9822 [3:29:16<48:40,  1.44s/it] 79%|███████▉  | 7797/9822 [3:29:17<48:43,  1.44s/it] 79%|███████▉  | 7798/9822 [3:29:19<48:38,  1.44s/it] 79%|███████▉  | 7799/9822 [3:29:20<48:30,  1.44s/it] 79%|███████▉  | 7800/9822 [3:29:22<48:25,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:17:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:17:08 - INFO - __main__ - ***** test Results*****
01/08/2024 01:17:08 - INFO - __main__ -   Training step = 7800
01/08/2024 01:17:08 - INFO - __main__ -  test_accuracy:0.8799414348462665 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:17:14 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:17:14 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:17:14 - INFO - __main__ -   Training step = 7800
01/08/2024 01:17:14 - INFO - __main__ -  eval_accuracy:0.8593921640424753 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:17:19 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:17:19 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:17:19 - INFO - __main__ -   Training step = 7800
01/08/2024 01:17:19 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 79%|███████▉  | 7801/9822 [3:29:40<3:39:43,  6.52s/it] 79%|███████▉  | 7802/9822 [3:29:41<2:48:10,  5.00s/it] 79%|███████▉  | 7803/9822 [3:29:43<2:12:10,  3.93s/it] 79%|███████▉  | 7804/9822 [3:29:44<1:47:04,  3.18s/it] 79%|███████▉  | 7805/9822 [3:29:46<1:29:24,  2.66s/it] 79%|███████▉  | 7806/9822 [3:29:47<1:17:03,  2.29s/it] 79%|███████▉  | 7807/9822 [3:29:49<1:08:33,  2.04s/it] 79%|███████▉  | 7808/9822 [3:29:50<1:02:21,  1.86s/it] 80%|███████▉  | 7809/9822 [3:29:51<58:04,  1.73s/it]   80%|███████▉  | 7810/9822 [3:29:53<55:09,  1.64s/it] 80%|███████▉  | 7811/9822 [3:29:54<52:52,  1.58s/it] 80%|███████▉  | 7812/9822 [3:29:56<51:22,  1.53s/it] 80%|███████▉  | 7813/9822 [3:29:57<50:31,  1.51s/it] 80%|███████▉  | 7814/9822 [3:29:59<49:42,  1.49s/it] 80%|███████▉  | 7815/9822 [3:30:00<50:01,  1.50s/it] 80%|███████▉  | 7816/9822 [3:30:02<49:33,  1.48s/it] 80%|███████▉  | 7817/9822 [3:30:03<49:13,  1.47s/it] 80%|███████▉  | 7818/9822 [3:30:04<48:52,  1.46s/it] 80%|███████▉  | 7819/9822 [3:30:06<48:29,  1.45s/it] 80%|███████▉  | 7820/9822 [3:30:07<48:25,  1.45s/it] 80%|███████▉  | 7821/9822 [3:30:09<48:11,  1.44s/it] 80%|███████▉  | 7822/9822 [3:30:10<47:54,  1.44s/it] 80%|███████▉  | 7823/9822 [3:30:12<47:53,  1.44s/it] 80%|███████▉  | 7824/9822 [3:30:13<47:52,  1.44s/it] 80%|███████▉  | 7825/9822 [3:30:14<47:44,  1.43s/it] 80%|███████▉  | 7826/9822 [3:30:16<47:20,  1.42s/it] 80%|███████▉  | 7827/9822 [3:30:17<47:28,  1.43s/it] 80%|███████▉  | 7828/9822 [3:30:19<47:45,  1.44s/it] 80%|███████▉  | 7829/9822 [3:30:20<48:10,  1.45s/it] 80%|███████▉  | 7830/9822 [3:30:22<48:09,  1.45s/it] 80%|███████▉  | 7831/9822 [3:30:23<47:57,  1.45s/it] 80%|███████▉  | 7832/9822 [3:30:25<47:49,  1.44s/it] 80%|███████▉  | 7833/9822 [3:30:26<47:45,  1.44s/it] 80%|███████▉  | 7834/9822 [3:30:27<47:52,  1.45s/it] 80%|███████▉  | 7835/9822 [3:30:29<47:52,  1.45s/it] 80%|███████▉  | 7836/9822 [3:30:30<47:55,  1.45s/it] 80%|███████▉  | 7837/9822 [3:30:32<47:59,  1.45s/it] 80%|███████▉  | 7838/9822 [3:30:33<48:05,  1.45s/it] 80%|███████▉  | 7839/9822 [3:30:35<48:02,  1.45s/it] 80%|███████▉  | 7840/9822 [3:30:36<47:49,  1.45s/it] 80%|███████▉  | 7841/9822 [3:30:38<47:39,  1.44s/it] 80%|███████▉  | 7842/9822 [3:30:39<47:38,  1.44s/it] 80%|███████▉  | 7843/9822 [3:30:41<47:39,  1.45s/it] 80%|███████▉  | 7844/9822 [3:30:42<47:37,  1.44s/it] 80%|███████▉  | 7845/9822 [3:30:43<47:40,  1.45s/it] 80%|███████▉  | 7846/9822 [3:30:45<47:31,  1.44s/it] 80%|███████▉  | 7847/9822 [3:30:46<48:21,  1.47s/it] 80%|███████▉  | 7848/9822 [3:30:48<48:00,  1.46s/it] 80%|███████▉  | 7849/9822 [3:30:49<47:44,  1.45s/it] 80%|███████▉  | 7850/9822 [3:30:51<47:27,  1.44s/it] 80%|███████▉  | 7851/9822 [3:30:52<47:16,  1.44s/it] 80%|███████▉  | 7852/9822 [3:30:54<47:19,  1.44s/it] 80%|███████▉  | 7853/9822 [3:30:55<47:12,  1.44s/it] 80%|███████▉  | 7854/9822 [3:30:56<47:06,  1.44s/it] 80%|███████▉  | 7855/9822 [3:30:58<47:13,  1.44s/it] 80%|███████▉  | 7856/9822 [3:30:59<47:18,  1.44s/it] 80%|███████▉  | 7857/9822 [3:31:01<47:19,  1.44s/it] 80%|████████  | 7858/9822 [3:31:02<47:17,  1.44s/it] 80%|████████  | 7859/9822 [3:31:04<47:20,  1.45s/it] 80%|████████  | 7860/9822 [3:31:05<47:12,  1.44s/it] 80%|████████  | 7861/9822 [3:31:07<47:26,  1.45s/it] 80%|████████  | 7862/9822 [3:31:08<47:31,  1.45s/it] 80%|████████  | 7863/9822 [3:31:09<47:23,  1.45s/it] 80%|████████  | 7864/9822 [3:31:11<47:15,  1.45s/it] 80%|████████  | 7865/9822 [3:31:12<47:04,  1.44s/it] 80%|████████  | 7866/9822 [3:31:14<46:56,  1.44s/it] 80%|████████  | 7867/9822 [3:31:15<46:49,  1.44s/it] 80%|████████  | 7868/9822 [3:31:17<46:46,  1.44s/it] 80%|████████  | 7869/9822 [3:31:18<46:43,  1.44s/it] 80%|████████  | 7870/9822 [3:31:20<46:44,  1.44s/it] 80%|████████  | 7871/9822 [3:31:21<46:47,  1.44s/it] 80%|████████  | 7872/9822 [3:31:22<46:40,  1.44s/it] 80%|████████  | 7873/9822 [3:31:24<46:39,  1.44s/it] 80%|████████  | 7874/9822 [3:31:25<46:46,  1.44s/it] 80%|████████  | 7875/9822 [3:31:27<46:42,  1.44s/it] 80%|████████  | 7876/9822 [3:31:28<46:37,  1.44s/it] 80%|████████  | 7877/9822 [3:31:30<46:36,  1.44s/it] 80%|████████  | 7878/9822 [3:31:31<46:43,  1.44s/it] 80%|████████  | 7879/9822 [3:31:33<47:34,  1.47s/it] 80%|████████  | 7880/9822 [3:31:34<47:16,  1.46s/it] 80%|████████  | 7881/9822 [3:31:35<47:09,  1.46s/it] 80%|████████  | 7882/9822 [3:31:37<47:04,  1.46s/it] 80%|████████  | 7883/9822 [3:31:38<46:57,  1.45s/it] 80%|████████  | 7884/9822 [3:31:40<46:50,  1.45s/it] 80%|████████  | 7885/9822 [3:31:41<46:38,  1.45s/it] 80%|████████  | 7886/9822 [3:31:43<46:36,  1.44s/it] 80%|████████  | 7887/9822 [3:31:44<46:28,  1.44s/it] 80%|████████  | 7888/9822 [3:31:46<46:29,  1.44s/it] 80%|████████  | 7889/9822 [3:31:47<46:31,  1.44s/it] 80%|████████  | 7890/9822 [3:31:48<46:25,  1.44s/it] 80%|████████  | 7891/9822 [3:31:50<46:21,  1.44s/it] 80%|████████  | 7892/9822 [3:31:51<46:22,  1.44s/it] 80%|████████  | 7893/9822 [3:31:53<46:12,  1.44s/it] 80%|████████  | 7894/9822 [3:31:54<46:03,  1.43s/it] 80%|████████  | 7895/9822 [3:31:56<46:04,  1.43s/it] 80%|████████  | 7896/9822 [3:31:57<46:08,  1.44s/it] 80%|████████  | 7897/9822 [3:31:58<46:05,  1.44s/it] 80%|████████  | 7898/9822 [3:32:00<46:19,  1.44s/it] 80%|████████  | 7899/9822 [3:32:01<46:29,  1.45s/it] 80%|████████  | 7900/9822 [3:32:03<46:22,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1068, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1140, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0588, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1382, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:19:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:19:50 - INFO - __main__ - ***** test Results*****
01/08/2024 01:19:50 - INFO - __main__ -   Training step = 7900
01/08/2024 01:19:50 - INFO - __main__ -  test_accuracy:0.8799414348462665 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:19:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:19:56 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:19:56 - INFO - __main__ -   Training step = 7900
01/08/2024 01:19:56 - INFO - __main__ -  eval_accuracy:0.859758330281948 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:20:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:20:01 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:20:01 - INFO - __main__ -   Training step = 7900
01/08/2024 01:20:01 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 80%|████████  | 7901/9822 [3:32:21<3:28:51,  6.52s/it] 80%|████████  | 7902/9822 [3:32:23<2:39:56,  5.00s/it] 80%|████████  | 7903/9822 [3:32:24<2:05:45,  3.93s/it] 80%|████████  | 7904/9822 [3:32:26<1:41:40,  3.18s/it] 80%|████████  | 7905/9822 [3:32:27<1:25:39,  2.68s/it] 80%|████████  | 7906/9822 [3:32:28<1:13:40,  2.31s/it] 81%|████████  | 7907/9822 [3:32:30<1:05:20,  2.05s/it] 81%|████████  | 7908/9822 [3:32:31<59:24,  1.86s/it]   81%|████████  | 7909/9822 [3:32:33<55:24,  1.74s/it] 81%|████████  | 7910/9822 [3:32:34<52:28,  1.65s/it] 81%|████████  | 7911/9822 [3:32:36<50:40,  1.59s/it] 81%|████████  | 7912/9822 [3:32:37<48:45,  1.53s/it] 81%|████████  | 7913/9822 [3:32:39<47:46,  1.50s/it] 81%|████████  | 7914/9822 [3:32:40<47:06,  1.48s/it] 81%|████████  | 7915/9822 [3:32:41<46:49,  1.47s/it] 81%|████████  | 7916/9822 [3:32:43<46:30,  1.46s/it] 81%|████████  | 7917/9822 [3:32:44<46:12,  1.46s/it] 81%|████████  | 7918/9822 [3:32:46<45:58,  1.45s/it] 81%|████████  | 7919/9822 [3:32:47<45:44,  1.44s/it] 81%|████████  | 7920/9822 [3:32:49<45:40,  1.44s/it] 81%|████████  | 7921/9822 [3:32:50<45:30,  1.44s/it] 81%|████████  | 7922/9822 [3:32:51<45:31,  1.44s/it] 81%|████████  | 7923/9822 [3:32:53<45:22,  1.43s/it] 81%|████████  | 7924/9822 [3:32:54<45:18,  1.43s/it] 81%|████████  | 7925/9822 [3:32:56<45:24,  1.44s/it] 81%|████████  | 7926/9822 [3:32:57<45:22,  1.44s/it] 81%|████████  | 7927/9822 [3:32:59<45:24,  1.44s/it] 81%|████████  | 7928/9822 [3:33:00<45:16,  1.43s/it] 81%|████████  | 7929/9822 [3:33:01<45:14,  1.43s/it] 81%|████████  | 7930/9822 [3:33:03<45:20,  1.44s/it] 81%|████████  | 7931/9822 [3:33:04<45:18,  1.44s/it] 81%|████████  | 7932/9822 [3:33:06<45:16,  1.44s/it] 81%|████████  | 7933/9822 [3:33:07<45:19,  1.44s/it] 81%|████████  | 7934/9822 [3:33:09<45:11,  1.44s/it] 81%|████████  | 7935/9822 [3:33:10<45:04,  1.43s/it] 81%|████████  | 7936/9822 [3:33:12<45:24,  1.44s/it] 81%|████████  | 7937/9822 [3:33:13<45:59,  1.46s/it] 81%|████████  | 7938/9822 [3:33:15<45:40,  1.45s/it] 81%|████████  | 7939/9822 [3:33:16<45:49,  1.46s/it] 81%|████████  | 7940/9822 [3:33:17<45:38,  1.46s/it] 81%|████████  | 7941/9822 [3:33:19<45:34,  1.45s/it] 81%|████████  | 7942/9822 [3:33:20<45:48,  1.46s/it] 81%|████████  | 7943/9822 [3:33:22<45:36,  1.46s/it] 81%|████████  | 7944/9822 [3:33:23<45:19,  1.45s/it] 81%|████████  | 7945/9822 [3:33:25<45:10,  1.44s/it] 81%|████████  | 7946/9822 [3:33:26<45:13,  1.45s/it] 81%|████████  | 7947/9822 [3:33:28<45:05,  1.44s/it] 81%|████████  | 7948/9822 [3:33:29<44:56,  1.44s/it] 81%|████████  | 7949/9822 [3:33:30<44:48,  1.44s/it] 81%|████████  | 7950/9822 [3:33:32<45:03,  1.44s/it] 81%|████████  | 7951/9822 [3:33:33<45:03,  1.45s/it] 81%|████████  | 7952/9822 [3:33:35<45:00,  1.44s/it] 81%|████████  | 7953/9822 [3:33:36<44:50,  1.44s/it] 81%|████████  | 7954/9822 [3:33:38<44:47,  1.44s/it] 81%|████████  | 7955/9822 [3:33:39<44:40,  1.44s/it] 81%|████████  | 7956/9822 [3:33:40<44:35,  1.43s/it] 81%|████████  | 7957/9822 [3:33:42<44:37,  1.44s/it] 81%|████████  | 7958/9822 [3:33:43<44:29,  1.43s/it] 81%|████████  | 7959/9822 [3:33:45<44:24,  1.43s/it] 81%|████████  | 7960/9822 [3:33:46<44:19,  1.43s/it] 81%|████████  | 7961/9822 [3:33:48<44:27,  1.43s/it] 81%|████████  | 7962/9822 [3:33:49<45:12,  1.46s/it] 81%|████████  | 7963/9822 [3:33:51<44:57,  1.45s/it] 81%|████████  | 7964/9822 [3:33:52<44:46,  1.45s/it] 81%|████████  | 7965/9822 [3:33:53<44:39,  1.44s/it] 81%|████████  | 7966/9822 [3:33:55<44:33,  1.44s/it] 81%|████████  | 7967/9822 [3:33:56<44:26,  1.44s/it] 81%|████████  | 7968/9822 [3:33:58<44:26,  1.44s/it] 81%|████████  | 7969/9822 [3:33:59<44:28,  1.44s/it] 81%|████████  | 7970/9822 [3:34:01<44:35,  1.44s/it] 81%|████████  | 7971/9822 [3:34:02<44:33,  1.44s/it] 81%|████████  | 7972/9822 [3:34:04<44:43,  1.45s/it] 81%|████████  | 7973/9822 [3:34:05<44:31,  1.44s/it] 81%|████████  | 7974/9822 [3:34:06<44:22,  1.44s/it] 81%|████████  | 7975/9822 [3:34:08<44:15,  1.44s/it] 81%|████████  | 7976/9822 [3:34:09<44:13,  1.44s/it] 81%|████████  | 7977/9822 [3:34:11<44:16,  1.44s/it] 81%|████████  | 7978/9822 [3:34:12<44:10,  1.44s/it] 81%|████████  | 7979/9822 [3:34:14<44:05,  1.44s/it] 81%|████████  | 7980/9822 [3:34:15<44:07,  1.44s/it] 81%|████████▏ | 7981/9822 [3:34:17<44:00,  1.43s/it] 81%|████████▏ | 7982/9822 [3:34:18<43:59,  1.43s/it] 81%|████████▏ | 7983/9822 [3:34:19<44:01,  1.44s/it] 81%|████████▏ | 7984/9822 [3:34:21<43:51,  1.43s/it] 81%|████████▏ | 7985/9822 [3:34:22<43:49,  1.43s/it] 81%|████████▏ | 7986/9822 [3:34:24<43:59,  1.44s/it] 81%|████████▏ | 7987/9822 [3:34:25<44:28,  1.45s/it] 81%|████████▏ | 7988/9822 [3:34:27<44:22,  1.45s/it] 81%|████████▏ | 7989/9822 [3:34:28<44:15,  1.45s/it] 81%|████████▏ | 7990/9822 [3:34:29<44:05,  1.44s/it] 81%|████████▏ | 7991/9822 [3:34:31<43:51,  1.44s/it] 81%|████████▏ | 7992/9822 [3:34:32<43:44,  1.43s/it] 81%|████████▏ | 7993/9822 [3:34:34<43:37,  1.43s/it] 81%|████████▏ | 7994/9822 [3:34:35<44:25,  1.46s/it] 81%|████████▏ | 7995/9822 [3:34:37<44:04,  1.45s/it] 81%|████████▏ | 7996/9822 [3:34:38<43:51,  1.44s/it] 81%|████████▏ | 7997/9822 [3:34:40<43:44,  1.44s/it] 81%|████████▏ | 7998/9822 [3:34:41<43:11,  1.42s/it] 81%|████████▏ | 7999/9822 [3:34:42<43:20,  1.43s/it] 81%|████████▏ | 8000/9822 [3:34:44<43:22,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1311, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1268, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1647, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1237, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1015, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1587, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1362, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:22:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:22:31 - INFO - __main__ - ***** test Results*****
01/08/2024 01:22:31 - INFO - __main__ -   Training step = 8000
01/08/2024 01:22:31 - INFO - __main__ -  test_accuracy:0.8810395314787701 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:22:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:22:37 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:22:37 - INFO - __main__ -   Training step = 8000
01/08/2024 01:22:37 - INFO - __main__ -  eval_accuracy:0.8615891614793116 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:22:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:22:41 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:22:41 - INFO - __main__ -   Training step = 8000
01/08/2024 01:22:41 - INFO - __main__ -  eval_accuracy:0.9146832662028561 
 81%|████████▏ | 8001/9822 [3:35:02<3:17:19,  6.50s/it] 81%|████████▏ | 8002/9822 [3:35:04<2:31:01,  4.98s/it] 81%|████████▏ | 8003/9822 [3:35:05<1:58:39,  3.91s/it] 81%|████████▏ | 8004/9822 [3:35:06<1:36:11,  3.17s/it] 82%|████████▏ | 8005/9822 [3:35:08<1:20:23,  2.65s/it] 82%|████████▏ | 8006/9822 [3:35:09<1:09:23,  2.29s/it] 82%|████████▏ | 8007/9822 [3:35:11<1:02:06,  2.05s/it] 82%|████████▏ | 8008/9822 [3:35:12<56:42,  1.88s/it]   82%|████████▏ | 8009/9822 [3:35:14<52:51,  1.75s/it] 82%|████████▏ | 8010/9822 [3:35:15<50:08,  1.66s/it] 82%|████████▏ | 8011/9822 [3:35:17<48:14,  1.60s/it] 82%|████████▏ | 8012/9822 [3:35:18<47:06,  1.56s/it] 82%|████████▏ | 8013/9822 [3:35:20<46:03,  1.53s/it] 82%|████████▏ | 8014/9822 [3:35:21<45:14,  1.50s/it] 82%|████████▏ | 8015/9822 [3:35:22<44:33,  1.48s/it] 82%|████████▏ | 8016/9822 [3:35:24<44:22,  1.47s/it] 82%|████████▏ | 8017/9822 [3:35:25<44:23,  1.48s/it] 82%|████████▏ | 8018/9822 [3:35:27<44:08,  1.47s/it] 82%|████████▏ | 8019/9822 [3:35:28<43:55,  1.46s/it] 82%|████████▏ | 8020/9822 [3:35:30<43:39,  1.45s/it] 82%|████████▏ | 8021/9822 [3:35:31<43:28,  1.45s/it] 82%|████████▏ | 8022/9822 [3:35:33<43:29,  1.45s/it] 82%|████████▏ | 8023/9822 [3:35:34<43:18,  1.44s/it] 82%|████████▏ | 8024/9822 [3:35:35<43:04,  1.44s/it] 82%|████████▏ | 8025/9822 [3:35:37<42:58,  1.43s/it] 82%|████████▏ | 8026/9822 [3:35:38<42:50,  1.43s/it] 82%|████████▏ | 8027/9822 [3:35:40<42:47,  1.43s/it] 82%|████████▏ | 8028/9822 [3:35:41<42:42,  1.43s/it] 82%|████████▏ | 8029/9822 [3:35:43<42:45,  1.43s/it] 82%|████████▏ | 8030/9822 [3:35:44<42:46,  1.43s/it] 82%|████████▏ | 8031/9822 [3:35:45<42:52,  1.44s/it] 82%|████████▏ | 8032/9822 [3:35:47<42:54,  1.44s/it] 82%|████████▏ | 8033/9822 [3:35:48<43:32,  1.46s/it] 82%|████████▏ | 8034/9822 [3:35:50<43:13,  1.45s/it] 82%|████████▏ | 8035/9822 [3:35:51<43:06,  1.45s/it] 82%|████████▏ | 8036/9822 [3:35:53<42:59,  1.44s/it] 82%|████████▏ | 8037/9822 [3:35:54<42:50,  1.44s/it] 82%|████████▏ | 8038/9822 [3:35:56<42:44,  1.44s/it] 82%|████████▏ | 8039/9822 [3:35:57<42:49,  1.44s/it] 82%|████████▏ | 8040/9822 [3:35:58<42:39,  1.44s/it] 82%|████████▏ | 8041/9822 [3:36:00<42:35,  1.43s/it] 82%|████████▏ | 8042/9822 [3:36:01<42:31,  1.43s/it] 82%|████████▏ | 8043/9822 [3:36:03<42:37,  1.44s/it] 82%|████████▏ | 8044/9822 [3:36:04<42:34,  1.44s/it] 82%|████████▏ | 8045/9822 [3:36:06<42:35,  1.44s/it] 82%|████████▏ | 8046/9822 [3:36:07<42:29,  1.44s/it] 82%|████████▏ | 8047/9822 [3:36:09<42:28,  1.44s/it] 82%|████████▏ | 8048/9822 [3:36:10<42:18,  1.43s/it] 82%|████████▏ | 8049/9822 [3:36:11<42:42,  1.45s/it] 82%|████████▏ | 8050/9822 [3:36:13<42:37,  1.44s/it] 82%|████████▏ | 8051/9822 [3:36:14<42:27,  1.44s/it] 82%|████████▏ | 8052/9822 [3:36:16<42:33,  1.44s/it] 82%|████████▏ | 8053/9822 [3:36:17<42:26,  1.44s/it] 82%|████████▏ | 8054/9822 [3:36:19<42:18,  1.44s/it] 82%|████████▏ | 8055/9822 [3:36:20<42:16,  1.44s/it] 82%|████████▏ | 8056/9822 [3:36:21<42:18,  1.44s/it] 82%|████████▏ | 8057/9822 [3:36:23<42:17,  1.44s/it] 82%|████████▏ | 8058/9822 [3:36:24<42:17,  1.44s/it] 82%|████████▏ | 8059/9822 [3:36:26<42:12,  1.44s/it] 82%|████████▏ | 8060/9822 [3:36:27<42:04,  1.43s/it] 82%|████████▏ | 8061/9822 [3:36:29<42:00,  1.43s/it] 82%|████████▏ | 8062/9822 [3:36:30<42:02,  1.43s/it] 82%|████████▏ | 8063/9822 [3:36:32<41:57,  1.43s/it] 82%|████████▏ | 8064/9822 [3:36:33<42:56,  1.47s/it] 82%|████████▏ | 8065/9822 [3:36:35<42:42,  1.46s/it] 82%|████████▏ | 8066/9822 [3:36:36<42:26,  1.45s/it] 82%|████████▏ | 8067/9822 [3:36:37<42:14,  1.44s/it] 82%|████████▏ | 8068/9822 [3:36:39<42:03,  1.44s/it] 82%|████████▏ | 8069/9822 [3:36:40<42:00,  1.44s/it] 82%|████████▏ | 8070/9822 [3:36:42<41:55,  1.44s/it] 82%|████████▏ | 8071/9822 [3:36:43<41:54,  1.44s/it] 82%|████████▏ | 8072/9822 [3:36:45<41:57,  1.44s/it] 82%|████████▏ | 8073/9822 [3:36:46<41:56,  1.44s/it] 82%|████████▏ | 8074/9822 [3:36:47<41:57,  1.44s/it] 82%|████████▏ | 8075/9822 [3:36:49<41:56,  1.44s/it] 82%|████████▏ | 8076/9822 [3:36:50<41:53,  1.44s/it] 82%|████████▏ | 8077/9822 [3:36:52<41:48,  1.44s/it] 82%|████████▏ | 8078/9822 [3:36:53<41:48,  1.44s/it] 82%|████████▏ | 8079/9822 [3:36:55<41:40,  1.43s/it] 82%|████████▏ | 8080/9822 [3:36:56<41:47,  1.44s/it] 82%|████████▏ | 8081/9822 [3:36:58<42:06,  1.45s/it] 82%|████████▏ | 8082/9822 [3:36:59<42:04,  1.45s/it] 82%|████████▏ | 8083/9822 [3:37:00<42:02,  1.45s/it] 82%|████████▏ | 8084/9822 [3:37:02<41:29,  1.43s/it] 82%|████████▏ | 8085/9822 [3:37:03<41:36,  1.44s/it] 82%|████████▏ | 8086/9822 [3:37:05<41:34,  1.44s/it] 82%|████████▏ | 8087/9822 [3:37:06<41:32,  1.44s/it] 82%|████████▏ | 8088/9822 [3:37:08<41:36,  1.44s/it] 82%|████████▏ | 8089/9822 [3:37:09<42:13,  1.46s/it] 82%|████████▏ | 8090/9822 [3:37:11<41:53,  1.45s/it] 82%|████████▏ | 8091/9822 [3:37:12<41:45,  1.45s/it] 82%|████████▏ | 8092/9822 [3:37:13<41:37,  1.44s/it] 82%|████████▏ | 8093/9822 [3:37:15<41:31,  1.44s/it] 82%|████████▏ | 8094/9822 [3:37:16<41:22,  1.44s/it] 82%|████████▏ | 8095/9822 [3:37:18<41:22,  1.44s/it] 82%|████████▏ | 8096/9822 [3:37:19<41:19,  1.44s/it] 82%|████████▏ | 8097/9822 [3:37:21<41:17,  1.44s/it] 82%|████████▏ | 8098/9822 [3:37:22<41:15,  1.44s/it] 82%|████████▏ | 8099/9822 [3:37:23<41:23,  1.44s/it] 82%|████████▏ | 8100/9822 [3:37:25<41:16,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0680, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0464, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:25:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:25:12 - INFO - __main__ - ***** test Results*****
01/08/2024 01:25:12 - INFO - __main__ -   Training step = 8100
01/08/2024 01:25:12 - INFO - __main__ -  test_accuracy:0.8784773060029283 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:25:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:25:18 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:25:18 - INFO - __main__ -   Training step = 8100
01/08/2024 01:25:18 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:25:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:25:23 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:25:23 - INFO - __main__ -   Training step = 8100
01/08/2024 01:25:23 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 82%|████████▏ | 8101/9822 [3:37:43<3:06:50,  6.51s/it] 82%|████████▏ | 8102/9822 [3:37:45<2:23:07,  4.99s/it] 82%|████████▏ | 8103/9822 [3:37:46<1:52:25,  3.92s/it] 83%|████████▎ | 8104/9822 [3:37:48<1:30:56,  3.18s/it] 83%|████████▎ | 8105/9822 [3:37:49<1:15:54,  2.65s/it] 83%|████████▎ | 8106/9822 [3:37:50<1:05:24,  2.29s/it] 83%|████████▎ | 8107/9822 [3:37:52<58:10,  2.04s/it]   83%|████████▎ | 8108/9822 [3:37:53<53:07,  1.86s/it] 83%|████████▎ | 8109/9822 [3:37:55<49:30,  1.73s/it] 83%|████████▎ | 8110/9822 [3:37:56<46:53,  1.64s/it] 83%|████████▎ | 8111/9822 [3:37:58<45:14,  1.59s/it] 83%|████████▎ | 8112/9822 [3:37:59<44:01,  1.54s/it] 83%|████████▎ | 8113/9822 [3:38:01<43:09,  1.52s/it] 83%|████████▎ | 8114/9822 [3:38:02<42:30,  1.49s/it] 83%|████████▎ | 8115/9822 [3:38:03<41:52,  1.47s/it] 83%|████████▎ | 8116/9822 [3:38:05<42:20,  1.49s/it] 83%|████████▎ | 8117/9822 [3:38:06<41:56,  1.48s/it] 83%|████████▎ | 8118/9822 [3:38:08<41:35,  1.46s/it] 83%|████████▎ | 8119/9822 [3:38:09<41:24,  1.46s/it] 83%|████████▎ | 8120/9822 [3:38:11<41:13,  1.45s/it] 83%|████████▎ | 8121/9822 [3:38:12<41:27,  1.46s/it] 83%|████████▎ | 8122/9822 [3:38:14<41:37,  1.47s/it] 83%|████████▎ | 8123/9822 [3:38:15<41:25,  1.46s/it] 83%|████████▎ | 8124/9822 [3:38:17<41:15,  1.46s/it] 83%|████████▎ | 8125/9822 [3:38:18<41:08,  1.45s/it] 83%|████████▎ | 8126/9822 [3:38:19<41:05,  1.45s/it] 83%|████████▎ | 8127/9822 [3:38:21<41:01,  1.45s/it] 83%|████████▎ | 8128/9822 [3:38:22<40:55,  1.45s/it] 83%|████████▎ | 8129/9822 [3:38:24<40:55,  1.45s/it] 83%|████████▎ | 8130/9822 [3:38:25<40:54,  1.45s/it] 83%|████████▎ | 8131/9822 [3:38:27<41:12,  1.46s/it] 83%|████████▎ | 8132/9822 [3:38:28<40:57,  1.45s/it] 83%|████████▎ | 8133/9822 [3:38:30<41:06,  1.46s/it] 83%|████████▎ | 8134/9822 [3:38:31<40:53,  1.45s/it] 83%|████████▎ | 8135/9822 [3:38:33<40:40,  1.45s/it] 83%|████████▎ | 8136/9822 [3:38:34<40:32,  1.44s/it] 83%|████████▎ | 8137/9822 [3:38:35<40:22,  1.44s/it] 83%|████████▎ | 8138/9822 [3:38:37<40:25,  1.44s/it] 83%|████████▎ | 8139/9822 [3:38:38<40:21,  1.44s/it] 83%|████████▎ | 8140/9822 [3:38:40<40:18,  1.44s/it] 83%|████████▎ | 8141/9822 [3:38:41<40:17,  1.44s/it] 83%|████████▎ | 8142/9822 [3:38:43<40:16,  1.44s/it] 83%|████████▎ | 8143/9822 [3:38:44<40:17,  1.44s/it] 83%|████████▎ | 8144/9822 [3:38:45<40:15,  1.44s/it] 83%|████████▎ | 8145/9822 [3:38:47<40:34,  1.45s/it] 83%|████████▎ | 8146/9822 [3:38:48<40:21,  1.44s/it] 83%|████████▎ | 8147/9822 [3:38:50<40:13,  1.44s/it] 83%|████████▎ | 8148/9822 [3:38:51<40:50,  1.46s/it] 83%|████████▎ | 8149/9822 [3:38:53<40:33,  1.45s/it] 83%|████████▎ | 8150/9822 [3:38:54<40:23,  1.45s/it] 83%|████████▎ | 8151/9822 [3:38:56<40:11,  1.44s/it] 83%|████████▎ | 8152/9822 [3:38:57<40:05,  1.44s/it] 83%|████████▎ | 8153/9822 [3:38:59<40:06,  1.44s/it] 83%|████████▎ | 8154/9822 [3:39:00<39:59,  1.44s/it] 83%|████████▎ | 8155/9822 [3:39:01<39:59,  1.44s/it] 83%|████████▎ | 8156/9822 [3:39:03<39:54,  1.44s/it] 83%|████████▎ | 8157/9822 [3:39:04<39:47,  1.43s/it] 83%|████████▎ | 8158/9822 [3:39:06<39:44,  1.43s/it] 83%|████████▎ | 8159/9822 [3:39:07<39:43,  1.43s/it] 83%|████████▎ | 8160/9822 [3:39:09<39:39,  1.43s/it] 83%|████████▎ | 8161/9822 [3:39:10<39:38,  1.43s/it] 83%|████████▎ | 8162/9822 [3:39:11<39:46,  1.44s/it] 83%|████████▎ | 8163/9822 [3:39:13<39:57,  1.45s/it] 83%|████████▎ | 8164/9822 [3:39:14<39:57,  1.45s/it] 83%|████████▎ | 8165/9822 [3:39:16<39:49,  1.44s/it] 83%|████████▎ | 8166/9822 [3:39:17<39:45,  1.44s/it] 83%|████████▎ | 8167/9822 [3:39:19<39:39,  1.44s/it] 83%|████████▎ | 8168/9822 [3:39:20<39:41,  1.44s/it] 83%|████████▎ | 8169/9822 [3:39:22<39:39,  1.44s/it] 83%|████████▎ | 8170/9822 [3:39:23<39:13,  1.42s/it] 83%|████████▎ | 8171/9822 [3:39:24<39:13,  1.43s/it] 83%|████████▎ | 8172/9822 [3:39:26<39:17,  1.43s/it] 83%|████████▎ | 8173/9822 [3:39:27<39:22,  1.43s/it] 83%|████████▎ | 8174/9822 [3:39:29<39:20,  1.43s/it] 83%|████████▎ | 8175/9822 [3:39:30<39:21,  1.43s/it] 83%|████████▎ | 8176/9822 [3:39:32<39:19,  1.43s/it] 83%|████████▎ | 8177/9822 [3:39:33<39:18,  1.43s/it] 83%|████████▎ | 8178/9822 [3:39:34<39:57,  1.46s/it] 83%|████████▎ | 8179/9822 [3:39:36<39:46,  1.45s/it] 83%|████████▎ | 8180/9822 [3:39:37<39:40,  1.45s/it] 83%|████████▎ | 8181/9822 [3:39:39<39:35,  1.45s/it] 83%|████████▎ | 8182/9822 [3:39:40<39:28,  1.44s/it] 83%|████████▎ | 8183/9822 [3:39:42<39:21,  1.44s/it] 83%|████████▎ | 8184/9822 [3:39:43<39:18,  1.44s/it] 83%|████████▎ | 8185/9822 [3:39:45<39:17,  1.44s/it] 83%|████████▎ | 8186/9822 [3:39:46<39:08,  1.44s/it] 83%|████████▎ | 8187/9822 [3:39:47<39:01,  1.43s/it] 83%|████████▎ | 8188/9822 [3:39:49<39:01,  1.43s/it] 83%|████████▎ | 8189/9822 [3:39:50<38:59,  1.43s/it] 83%|████████▎ | 8190/9822 [3:39:52<38:55,  1.43s/it] 83%|████████▎ | 8191/9822 [3:39:53<39:00,  1.44s/it] 83%|████████▎ | 8192/9822 [3:39:55<38:57,  1.43s/it] 83%|████████▎ | 8193/9822 [3:39:56<38:56,  1.43s/it] 83%|████████▎ | 8194/9822 [3:39:57<38:56,  1.44s/it] 83%|████████▎ | 8195/9822 [3:39:59<38:54,  1.43s/it] 83%|████████▎ | 8196/9822 [3:40:00<38:50,  1.43s/it] 83%|████████▎ | 8197/9822 [3:40:02<38:50,  1.43s/it] 83%|████████▎ | 8198/9822 [3:40:03<38:49,  1.43s/it] 83%|████████▎ | 8199/9822 [3:40:05<38:47,  1.43s/it] 83%|████████▎ | 8200/9822 [3:40:06<38:45,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1524, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1663, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1274, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1592, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1372, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1175, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1066, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:27:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:27:53 - INFO - __main__ - ***** test Results*****
01/08/2024 01:27:53 - INFO - __main__ -   Training step = 8200
01/08/2024 01:27:53 - INFO - __main__ -  test_accuracy:0.8806734992679356 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:27:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:27:59 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:27:59 - INFO - __main__ -   Training step = 8200
01/08/2024 01:27:59 - INFO - __main__ -  eval_accuracy:0.8601244965214208 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:28:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:28:04 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:28:04 - INFO - __main__ -   Training step = 8200
01/08/2024 01:28:04 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 83%|████████▎ | 8201/9822 [3:40:24<2:55:53,  6.51s/it] 84%|████████▎ | 8202/9822 [3:40:26<2:14:38,  4.99s/it] 84%|████████▎ | 8203/9822 [3:40:27<1:45:47,  3.92s/it] 84%|████████▎ | 8204/9822 [3:40:29<1:25:29,  3.17s/it] 84%|████████▎ | 8205/9822 [3:40:30<1:11:24,  2.65s/it] 84%|████████▎ | 8206/9822 [3:40:32<1:01:33,  2.29s/it] 84%|████████▎ | 8207/9822 [3:40:33<54:31,  2.03s/it]   84%|████████▎ | 8208/9822 [3:40:34<50:21,  1.87s/it] 84%|████████▎ | 8209/9822 [3:40:36<46:51,  1.74s/it] 84%|████████▎ | 8210/9822 [3:40:37<44:23,  1.65s/it] 84%|████████▎ | 8211/9822 [3:40:39<42:35,  1.59s/it] 84%|████████▎ | 8212/9822 [3:40:40<41:16,  1.54s/it] 84%|████████▎ | 8213/9822 [3:40:42<40:24,  1.51s/it] 84%|████████▎ | 8214/9822 [3:40:43<39:52,  1.49s/it] 84%|████████▎ | 8215/9822 [3:40:45<39:24,  1.47s/it] 84%|████████▎ | 8216/9822 [3:40:46<39:02,  1.46s/it] 84%|████████▎ | 8217/9822 [3:40:47<38:51,  1.45s/it] 84%|████████▎ | 8218/9822 [3:40:49<38:47,  1.45s/it] 84%|████████▎ | 8219/9822 [3:40:50<39:08,  1.47s/it] 84%|████████▎ | 8220/9822 [3:40:52<39:23,  1.48s/it] 84%|████████▎ | 8221/9822 [3:40:53<39:27,  1.48s/it] 84%|████████▎ | 8222/9822 [3:40:55<39:12,  1.47s/it] 84%|████████▎ | 8223/9822 [3:40:56<39:00,  1.46s/it] 84%|████████▎ | 8224/9822 [3:40:58<38:42,  1.45s/it] 84%|████████▎ | 8225/9822 [3:40:59<38:30,  1.45s/it] 84%|████████▍ | 8226/9822 [3:41:01<38:23,  1.44s/it] 84%|████████▍ | 8227/9822 [3:41:02<38:15,  1.44s/it] 84%|████████▍ | 8228/9822 [3:41:03<38:06,  1.43s/it] 84%|████████▍ | 8229/9822 [3:41:05<37:59,  1.43s/it] 84%|████████▍ | 8230/9822 [3:41:06<37:58,  1.43s/it] 84%|████████▍ | 8231/9822 [3:41:08<37:59,  1.43s/it] 84%|████████▍ | 8232/9822 [3:41:09<38:12,  1.44s/it] 84%|████████▍ | 8233/9822 [3:41:11<38:08,  1.44s/it] 84%|████████▍ | 8234/9822 [3:41:12<37:59,  1.44s/it] 84%|████████▍ | 8235/9822 [3:41:13<37:55,  1.43s/it] 84%|████████▍ | 8236/9822 [3:41:15<37:53,  1.43s/it] 84%|████████▍ | 8237/9822 [3:41:16<37:50,  1.43s/it] 84%|████████▍ | 8238/9822 [3:41:18<37:45,  1.43s/it] 84%|████████▍ | 8239/9822 [3:41:19<37:47,  1.43s/it] 84%|████████▍ | 8240/9822 [3:41:21<38:31,  1.46s/it] 84%|████████▍ | 8241/9822 [3:41:22<38:15,  1.45s/it] 84%|████████▍ | 8242/9822 [3:41:24<38:02,  1.44s/it] 84%|████████▍ | 8243/9822 [3:41:25<37:54,  1.44s/it] 84%|████████▍ | 8244/9822 [3:41:26<37:49,  1.44s/it] 84%|████████▍ | 8245/9822 [3:41:28<37:39,  1.43s/it] 84%|████████▍ | 8246/9822 [3:41:29<37:43,  1.44s/it] 84%|████████▍ | 8247/9822 [3:41:31<37:38,  1.43s/it] 84%|████████▍ | 8248/9822 [3:41:32<37:41,  1.44s/it] 84%|████████▍ | 8249/9822 [3:41:34<37:42,  1.44s/it] 84%|████████▍ | 8250/9822 [3:41:35<37:39,  1.44s/it] 84%|████████▍ | 8251/9822 [3:41:36<37:38,  1.44s/it] 84%|████████▍ | 8252/9822 [3:41:38<37:34,  1.44s/it] 84%|████████▍ | 8253/9822 [3:41:39<37:32,  1.44s/it] 84%|████████▍ | 8254/9822 [3:41:41<37:32,  1.44s/it] 84%|████████▍ | 8255/9822 [3:41:42<37:37,  1.44s/it] 84%|████████▍ | 8256/9822 [3:41:44<37:08,  1.42s/it] 84%|████████▍ | 8257/9822 [3:41:45<37:10,  1.42s/it] 84%|████████▍ | 8258/9822 [3:41:46<37:10,  1.43s/it] 84%|████████▍ | 8259/9822 [3:41:48<37:10,  1.43s/it] 84%|████████▍ | 8260/9822 [3:41:49<37:09,  1.43s/it] 84%|████████▍ | 8261/9822 [3:41:51<37:07,  1.43s/it] 84%|████████▍ | 8262/9822 [3:41:52<37:10,  1.43s/it] 84%|████████▍ | 8263/9822 [3:41:54<37:22,  1.44s/it] 84%|████████▍ | 8264/9822 [3:41:55<37:21,  1.44s/it] 84%|████████▍ | 8265/9822 [3:41:56<37:17,  1.44s/it] 84%|████████▍ | 8266/9822 [3:41:58<37:17,  1.44s/it] 84%|████████▍ | 8267/9822 [3:41:59<37:13,  1.44s/it] 84%|████████▍ | 8268/9822 [3:42:01<37:14,  1.44s/it] 84%|████████▍ | 8269/9822 [3:42:02<37:07,  1.43s/it] 84%|████████▍ | 8270/9822 [3:42:04<37:44,  1.46s/it] 84%|████████▍ | 8271/9822 [3:42:05<37:39,  1.46s/it] 84%|████████▍ | 8272/9822 [3:42:07<37:28,  1.45s/it] 84%|████████▍ | 8273/9822 [3:42:08<37:32,  1.45s/it] 84%|████████▍ | 8274/9822 [3:42:10<37:18,  1.45s/it] 84%|████████▍ | 8275/9822 [3:42:11<37:13,  1.44s/it] 84%|████████▍ | 8276/9822 [3:42:12<37:14,  1.45s/it] 84%|████████▍ | 8277/9822 [3:42:14<37:09,  1.44s/it] 84%|████████▍ | 8278/9822 [3:42:15<37:08,  1.44s/it] 84%|████████▍ | 8279/9822 [3:42:17<37:01,  1.44s/it] 84%|████████▍ | 8280/9822 [3:42:18<37:03,  1.44s/it] 84%|████████▍ | 8281/9822 [3:42:20<37:03,  1.44s/it] 84%|████████▍ | 8282/9822 [3:42:21<37:19,  1.45s/it] 84%|████████▍ | 8283/9822 [3:42:23<37:03,  1.45s/it] 84%|████████▍ | 8284/9822 [3:42:24<36:58,  1.44s/it] 84%|████████▍ | 8285/9822 [3:42:25<36:47,  1.44s/it] 84%|████████▍ | 8286/9822 [3:42:27<36:44,  1.44s/it] 84%|████████▍ | 8287/9822 [3:42:28<36:38,  1.43s/it] 84%|████████▍ | 8288/9822 [3:42:30<36:33,  1.43s/it] 84%|████████▍ | 8289/9822 [3:42:31<36:33,  1.43s/it] 84%|████████▍ | 8290/9822 [3:42:33<36:39,  1.44s/it] 84%|████████▍ | 8291/9822 [3:42:34<36:43,  1.44s/it] 84%|████████▍ | 8292/9822 [3:42:35<36:43,  1.44s/it] 84%|████████▍ | 8293/9822 [3:42:37<36:55,  1.45s/it] 84%|████████▍ | 8294/9822 [3:42:38<36:46,  1.44s/it] 84%|████████▍ | 8295/9822 [3:42:40<36:48,  1.45s/it] 84%|████████▍ | 8296/9822 [3:42:41<36:38,  1.44s/it] 84%|████████▍ | 8297/9822 [3:42:43<36:43,  1.44s/it] 84%|████████▍ | 8298/9822 [3:42:44<36:41,  1.44s/it] 84%|████████▍ | 8299/9822 [3:42:46<36:33,  1.44s/it] 85%|████████▍ | 8300/9822 [3:42:47<36:43,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1500, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1523, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1648, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1543, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:30:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:30:34 - INFO - __main__ - ***** test Results*****
01/08/2024 01:30:34 - INFO - __main__ -   Training step = 8300
01/08/2024 01:30:34 - INFO - __main__ -  test_accuracy:0.8810395314787701 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:30:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:30:40 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:30:40 - INFO - __main__ -   Training step = 8300
01/08/2024 01:30:40 - INFO - __main__ -  eval_accuracy:0.8593921640424753 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8615891614793116}
test:
{'accuracy': 0.8766471449487555}
01/08/2024 01:30:45 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:30:45 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:30:45 - INFO - __main__ -   Training step = 8300
01/08/2024 01:30:45 - INFO - __main__ -  eval_accuracy:0.9168802636396924 
 85%|████████▍ | 8301/9822 [3:43:05<2:45:31,  6.53s/it] 85%|████████▍ | 8302/9822 [3:43:07<2:07:14,  5.02s/it] 85%|████████▍ | 8303/9822 [3:43:08<1:40:11,  3.96s/it] 85%|████████▍ | 8304/9822 [3:43:10<1:20:57,  3.20s/it] 85%|████████▍ | 8305/9822 [3:43:11<1:07:34,  2.67s/it] 85%|████████▍ | 8306/9822 [3:43:13<58:11,  2.30s/it]   85%|████████▍ | 8307/9822 [3:43:14<51:37,  2.04s/it] 85%|████████▍ | 8308/9822 [3:43:16<47:02,  1.86s/it] 85%|████████▍ | 8309/9822 [3:43:17<43:47,  1.74s/it] 85%|████████▍ | 8310/9822 [3:43:18<41:32,  1.65s/it] 85%|████████▍ | 8311/9822 [3:43:20<39:50,  1.58s/it] 85%|████████▍ | 8312/9822 [3:43:21<38:42,  1.54s/it] 85%|████████▍ | 8313/9822 [3:43:23<37:52,  1.51s/it] 85%|████████▍ | 8314/9822 [3:43:24<37:37,  1.50s/it] 85%|████████▍ | 8315/9822 [3:43:26<37:04,  1.48s/it] 85%|████████▍ | 8316/9822 [3:43:27<36:45,  1.46s/it] 85%|████████▍ | 8317/9822 [3:43:29<36:32,  1.46s/it] 85%|████████▍ | 8318/9822 [3:43:30<36:16,  1.45s/it] 85%|████████▍ | 8319/9822 [3:43:31<36:06,  1.44s/it] 85%|████████▍ | 8320/9822 [3:43:33<35:57,  1.44s/it] 85%|████████▍ | 8321/9822 [3:43:34<35:59,  1.44s/it] 85%|████████▍ | 8322/9822 [3:43:36<35:56,  1.44s/it] 85%|████████▍ | 8323/9822 [3:43:37<35:52,  1.44s/it] 85%|████████▍ | 8324/9822 [3:43:39<35:45,  1.43s/it] 85%|████████▍ | 8325/9822 [3:43:40<36:03,  1.45s/it] 85%|████████▍ | 8326/9822 [3:43:41<36:00,  1.44s/it] 85%|████████▍ | 8327/9822 [3:43:43<35:54,  1.44s/it] 85%|████████▍ | 8328/9822 [3:43:44<35:49,  1.44s/it] 85%|████████▍ | 8329/9822 [3:43:46<35:45,  1.44s/it] 85%|████████▍ | 8330/9822 [3:43:47<35:47,  1.44s/it] 85%|████████▍ | 8331/9822 [3:43:49<35:39,  1.43s/it] 85%|████████▍ | 8332/9822 [3:43:50<35:39,  1.44s/it] 85%|████████▍ | 8333/9822 [3:43:51<35:34,  1.43s/it] 85%|████████▍ | 8334/9822 [3:43:53<36:18,  1.46s/it] 85%|████████▍ | 8335/9822 [3:43:54<36:12,  1.46s/it] 85%|████████▍ | 8336/9822 [3:43:56<36:03,  1.46s/it] 85%|████████▍ | 8337/9822 [3:43:57<35:54,  1.45s/it] 85%|████████▍ | 8338/9822 [3:43:59<35:47,  1.45s/it] 85%|████████▍ | 8339/9822 [3:44:00<35:39,  1.44s/it] 85%|████████▍ | 8340/9822 [3:44:02<35:36,  1.44s/it] 85%|████████▍ | 8341/9822 [3:44:03<35:33,  1.44s/it] 85%|████████▍ | 8342/9822 [3:44:04<35:05,  1.42s/it] 85%|████████▍ | 8343/9822 [3:44:06<35:15,  1.43s/it] 85%|████████▍ | 8344/9822 [3:44:07<35:19,  1.43s/it] 85%|████████▍ | 8345/9822 [3:44:09<35:18,  1.43s/it] 85%|████████▍ | 8346/9822 [3:44:10<35:15,  1.43s/it] 85%|████████▍ | 8347/9822 [3:44:12<35:15,  1.43s/it] 85%|████████▍ | 8348/9822 [3:44:13<35:11,  1.43s/it] 85%|████████▌ | 8349/9822 [3:44:15<35:12,  1.43s/it] 85%|████████▌ | 8350/9822 [3:44:16<35:15,  1.44s/it] 85%|████████▌ | 8351/9822 [3:44:17<35:22,  1.44s/it] 85%|████████▌ | 8352/9822 [3:44:19<35:31,  1.45s/it] 85%|████████▌ | 8353/9822 [3:44:20<35:50,  1.46s/it] 85%|████████▌ | 8354/9822 [3:44:22<35:42,  1.46s/it] 85%|████████▌ | 8355/9822 [3:44:23<35:28,  1.45s/it] 85%|████████▌ | 8356/9822 [3:44:25<35:15,  1.44s/it] 85%|████████▌ | 8357/9822 [3:44:26<35:13,  1.44s/it] 85%|████████▌ | 8358/9822 [3:44:28<35:08,  1.44s/it] 85%|████████▌ | 8359/9822 [3:44:29<35:03,  1.44s/it] 85%|████████▌ | 8360/9822 [3:44:30<34:59,  1.44s/it] 85%|████████▌ | 8361/9822 [3:44:32<35:02,  1.44s/it] 85%|████████▌ | 8362/9822 [3:44:33<35:04,  1.44s/it] 85%|████████▌ | 8363/9822 [3:44:35<35:06,  1.44s/it] 85%|████████▌ | 8364/9822 [3:44:36<35:00,  1.44s/it] 85%|████████▌ | 8365/9822 [3:44:38<34:52,  1.44s/it] 85%|████████▌ | 8366/9822 [3:44:39<35:40,  1.47s/it] 85%|████████▌ | 8367/9822 [3:44:41<35:20,  1.46s/it] 85%|████████▌ | 8368/9822 [3:44:42<35:15,  1.46s/it] 85%|████████▌ | 8369/9822 [3:44:44<35:08,  1.45s/it] 85%|████████▌ | 8370/9822 [3:44:45<34:58,  1.44s/it] 85%|████████▌ | 8371/9822 [3:44:46<34:47,  1.44s/it] 85%|████████▌ | 8372/9822 [3:44:48<34:48,  1.44s/it] 85%|████████▌ | 8373/9822 [3:44:49<34:46,  1.44s/it] 85%|████████▌ | 8374/9822 [3:44:51<34:44,  1.44s/it] 85%|████████▌ | 8375/9822 [3:44:52<34:44,  1.44s/it] 85%|████████▌ | 8376/9822 [3:44:54<34:42,  1.44s/it] 85%|████████▌ | 8377/9822 [3:44:55<34:43,  1.44s/it] 85%|████████▌ | 8378/9822 [3:44:57<34:57,  1.45s/it] 85%|████████▌ | 8379/9822 [3:44:58<34:55,  1.45s/it] 85%|████████▌ | 8380/9822 [3:44:59<34:49,  1.45s/it] 85%|████████▌ | 8381/9822 [3:45:01<34:49,  1.45s/it] 85%|████████▌ | 8382/9822 [3:45:02<34:42,  1.45s/it] 85%|████████▌ | 8383/9822 [3:45:04<34:39,  1.45s/it] 85%|████████▌ | 8384/9822 [3:45:05<34:34,  1.44s/it] 85%|████████▌ | 8385/9822 [3:45:07<34:39,  1.45s/it] 85%|████████▌ | 8386/9822 [3:45:08<34:55,  1.46s/it] 85%|████████▌ | 8387/9822 [3:45:10<34:58,  1.46s/it] 85%|████████▌ | 8388/9822 [3:45:11<34:53,  1.46s/it] 85%|████████▌ | 8389/9822 [3:45:12<34:43,  1.45s/it] 85%|████████▌ | 8390/9822 [3:45:14<34:32,  1.45s/it] 85%|████████▌ | 8391/9822 [3:45:15<35:00,  1.47s/it] 85%|████████▌ | 8392/9822 [3:45:17<34:47,  1.46s/it] 85%|████████▌ | 8393/9822 [3:45:18<34:43,  1.46s/it] 85%|████████▌ | 8394/9822 [3:45:20<34:34,  1.45s/it] 85%|████████▌ | 8395/9822 [3:45:21<34:26,  1.45s/it] 85%|████████▌ | 8396/9822 [3:45:23<34:22,  1.45s/it] 85%|████████▌ | 8397/9822 [3:45:24<34:18,  1.44s/it] 86%|████████▌ | 8398/9822 [3:45:26<34:20,  1.45s/it] 86%|████████▌ | 8399/9822 [3:45:27<34:13,  1.44s/it] 86%|████████▌ | 8400/9822 [3:45:28<34:06,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1645, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1195, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1259, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1353, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1943, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1558, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2204, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1370, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1233, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:33:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:33:15 - INFO - __main__ - ***** test Results*****
01/08/2024 01:33:15 - INFO - __main__ -   Training step = 8400
01/08/2024 01:33:15 - INFO - __main__ -  test_accuracy:0.876281112737921 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:33:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:33:21 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:33:21 - INFO - __main__ -   Training step = 8400
01/08/2024 01:33:21 - INFO - __main__ -  eval_accuracy:0.8619553277187844 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 01:33:21,825 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 01:33:21,825 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 01:33:21,862 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 01:33:23,462 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8619553277187844}
test:
{'accuracy': 0.876281112737921}
01/08/2024 01:33:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:33:28 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:33:28 - INFO - __main__ -   Training step = 8400
01/08/2024 01:33:28 - INFO - __main__ -  eval_accuracy:0.9157817649212743 
 86%|████████▌ | 8401/9822 [3:45:48<2:45:56,  7.01s/it] 86%|████████▌ | 8402/9822 [3:45:50<2:06:09,  5.33s/it] 86%|████████▌ | 8403/9822 [3:45:51<1:38:27,  4.16s/it] 86%|████████▌ | 8404/9822 [3:45:53<1:19:02,  3.34s/it] 86%|████████▌ | 8405/9822 [3:45:54<1:05:26,  2.77s/it] 86%|████████▌ | 8406/9822 [3:45:56<55:58,  2.37s/it]   86%|████████▌ | 8407/9822 [3:45:57<49:20,  2.09s/it] 86%|████████▌ | 8408/9822 [3:45:58<44:40,  1.90s/it] 86%|████████▌ | 8409/9822 [3:46:00<41:22,  1.76s/it] 86%|████████▌ | 8410/9822 [3:46:01<38:59,  1.66s/it] 86%|████████▌ | 8411/9822 [3:46:03<37:25,  1.59s/it] 86%|████████▌ | 8412/9822 [3:46:04<36:19,  1.55s/it] 86%|████████▌ | 8413/9822 [3:46:06<35:31,  1.51s/it] 86%|████████▌ | 8414/9822 [3:46:07<35:01,  1.49s/it] 86%|████████▌ | 8415/9822 [3:46:08<34:37,  1.48s/it] 86%|████████▌ | 8416/9822 [3:46:10<34:19,  1.46s/it] 86%|████████▌ | 8417/9822 [3:46:11<34:04,  1.46s/it] 86%|████████▌ | 8418/9822 [3:46:13<33:58,  1.45s/it] 86%|████████▌ | 8419/9822 [3:46:14<33:57,  1.45s/it] 86%|████████▌ | 8420/9822 [3:46:16<34:04,  1.46s/it] 86%|████████▌ | 8421/9822 [3:46:17<33:53,  1.45s/it] 86%|████████▌ | 8422/9822 [3:46:19<33:41,  1.44s/it] 86%|████████▌ | 8423/9822 [3:46:20<33:37,  1.44s/it] 86%|████████▌ | 8424/9822 [3:46:21<33:36,  1.44s/it] 86%|████████▌ | 8425/9822 [3:46:23<33:45,  1.45s/it] 86%|████████▌ | 8426/9822 [3:46:24<33:36,  1.44s/it] 86%|████████▌ | 8427/9822 [3:46:26<33:33,  1.44s/it] 86%|████████▌ | 8428/9822 [3:46:27<33:08,  1.43s/it] 86%|████████▌ | 8429/9822 [3:46:29<33:10,  1.43s/it] 86%|████████▌ | 8430/9822 [3:46:30<33:10,  1.43s/it] 86%|████████▌ | 8431/9822 [3:46:32<33:14,  1.43s/it] 86%|████████▌ | 8432/9822 [3:46:33<33:09,  1.43s/it] 86%|████████▌ | 8433/9822 [3:46:34<33:12,  1.43s/it] 86%|████████▌ | 8434/9822 [3:46:36<33:12,  1.44s/it] 86%|████████▌ | 8435/9822 [3:46:37<33:09,  1.43s/it] 86%|████████▌ | 8436/9822 [3:46:39<33:12,  1.44s/it] 86%|████████▌ | 8437/9822 [3:46:40<33:08,  1.44s/it] 86%|████████▌ | 8438/9822 [3:46:42<33:07,  1.44s/it] 86%|████████▌ | 8439/9822 [3:46:43<33:05,  1.44s/it] 86%|████████▌ | 8440/9822 [3:46:44<33:01,  1.43s/it] 86%|████████▌ | 8441/9822 [3:46:46<32:58,  1.43s/it] 86%|████████▌ | 8442/9822 [3:46:47<32:59,  1.43s/it] 86%|████████▌ | 8443/9822 [3:46:49<32:56,  1.43s/it] 86%|████████▌ | 8444/9822 [3:46:50<32:55,  1.43s/it] 86%|████████▌ | 8445/9822 [3:46:52<32:55,  1.43s/it] 86%|████████▌ | 8446/9822 [3:46:53<32:56,  1.44s/it] 86%|████████▌ | 8447/9822 [3:46:54<32:54,  1.44s/it] 86%|████████▌ | 8448/9822 [3:46:56<32:46,  1.43s/it] 86%|████████▌ | 8449/9822 [3:46:57<32:53,  1.44s/it] 86%|████████▌ | 8450/9822 [3:46:59<32:52,  1.44s/it] 86%|████████▌ | 8451/9822 [3:47:00<32:49,  1.44s/it] 86%|████████▌ | 8452/9822 [3:47:02<32:47,  1.44s/it] 86%|████████▌ | 8453/9822 [3:47:03<32:48,  1.44s/it] 86%|████████▌ | 8454/9822 [3:47:05<32:40,  1.43s/it] 86%|████████▌ | 8455/9822 [3:47:06<32:38,  1.43s/it] 86%|████████▌ | 8456/9822 [3:47:07<32:40,  1.43s/it] 86%|████████▌ | 8457/9822 [3:47:09<32:39,  1.44s/it] 86%|████████▌ | 8458/9822 [3:47:10<32:39,  1.44s/it] 86%|████████▌ | 8459/9822 [3:47:12<32:38,  1.44s/it] 86%|████████▌ | 8460/9822 [3:47:13<33:06,  1.46s/it] 86%|████████▌ | 8461/9822 [3:47:15<32:56,  1.45s/it] 86%|████████▌ | 8462/9822 [3:47:16<32:49,  1.45s/it] 86%|████████▌ | 8463/9822 [3:47:18<32:41,  1.44s/it] 86%|████████▌ | 8464/9822 [3:47:19<32:34,  1.44s/it] 86%|████████▌ | 8465/9822 [3:47:20<32:35,  1.44s/it] 86%|████████▌ | 8466/9822 [3:47:22<32:37,  1.44s/it] 86%|████████▌ | 8467/9822 [3:47:23<32:48,  1.45s/it] 86%|████████▌ | 8468/9822 [3:47:25<32:35,  1.44s/it] 86%|████████▌ | 8469/9822 [3:47:26<32:28,  1.44s/it] 86%|████████▌ | 8470/9822 [3:47:28<32:23,  1.44s/it] 86%|████████▌ | 8471/9822 [3:47:29<32:15,  1.43s/it] 86%|████████▋ | 8472/9822 [3:47:30<32:17,  1.44s/it] 86%|████████▋ | 8473/9822 [3:47:32<32:13,  1.43s/it] 86%|████████▋ | 8474/9822 [3:47:33<32:11,  1.43s/it] 86%|████████▋ | 8475/9822 [3:47:35<32:12,  1.43s/it] 86%|████████▋ | 8476/9822 [3:47:36<32:10,  1.43s/it] 86%|████████▋ | 8477/9822 [3:47:38<32:12,  1.44s/it] 86%|████████▋ | 8478/9822 [3:47:39<32:21,  1.44s/it] 86%|████████▋ | 8479/9822 [3:47:41<32:14,  1.44s/it] 86%|████████▋ | 8480/9822 [3:47:42<32:13,  1.44s/it] 86%|████████▋ | 8481/9822 [3:47:43<32:10,  1.44s/it] 86%|████████▋ | 8482/9822 [3:47:45<32:02,  1.43s/it] 86%|████████▋ | 8483/9822 [3:47:46<32:05,  1.44s/it] 86%|████████▋ | 8484/9822 [3:47:48<32:03,  1.44s/it] 86%|████████▋ | 8485/9822 [3:47:49<31:59,  1.44s/it] 86%|████████▋ | 8486/9822 [3:47:51<31:55,  1.43s/it] 86%|████████▋ | 8487/9822 [3:47:52<31:49,  1.43s/it] 86%|████████▋ | 8488/9822 [3:47:53<31:50,  1.43s/it] 86%|████████▋ | 8489/9822 [3:47:55<31:53,  1.44s/it] 86%|████████▋ | 8490/9822 [3:47:56<31:54,  1.44s/it] 86%|████████▋ | 8491/9822 [3:47:58<32:27,  1.46s/it] 86%|████████▋ | 8492/9822 [3:47:59<32:13,  1.45s/it] 86%|████████▋ | 8493/9822 [3:48:01<32:05,  1.45s/it] 86%|████████▋ | 8494/9822 [3:48:02<31:53,  1.44s/it] 86%|████████▋ | 8495/9822 [3:48:04<31:55,  1.44s/it] 86%|████████▋ | 8496/9822 [3:48:05<31:51,  1.44s/it] 87%|████████▋ | 8497/9822 [3:48:06<31:50,  1.44s/it] 87%|████████▋ | 8498/9822 [3:48:08<31:43,  1.44s/it] 87%|████████▋ | 8499/9822 [3:48:09<31:34,  1.43s/it] 87%|████████▋ | 8500/9822 [3:48:11<31:30,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0553, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1393, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2039, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1190, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0903, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1366, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1538, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0980, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1310, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0974, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1306, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1218, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:35:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:35:58 - INFO - __main__ - ***** test Results*****
01/08/2024 01:35:58 - INFO - __main__ -   Training step = 8500
01/08/2024 01:35:58 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:36:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:36:04 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:36:04 - INFO - __main__ -   Training step = 8500
01/08/2024 01:36:04 - INFO - __main__ -  eval_accuracy:0.863786158916148 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 01:36:04,146 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 01:36:04,146 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 01:36:04,182 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 01:36:05,724 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:36:10 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:36:10 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:36:10 - INFO - __main__ -   Training step = 8500
01/08/2024 01:36:10 - INFO - __main__ -  eval_accuracy:0.9157817649212743 
 87%|████████▋ | 8501/9822 [3:48:31<2:33:35,  6.98s/it] 87%|████████▋ | 8502/9822 [3:48:32<1:57:03,  5.32s/it] 87%|████████▋ | 8503/9822 [3:48:34<1:31:32,  4.16s/it] 87%|████████▋ | 8504/9822 [3:48:35<1:13:29,  3.35s/it] 87%|████████▋ | 8505/9822 [3:48:36<1:00:47,  2.77s/it] 87%|████████▋ | 8506/9822 [3:48:38<51:57,  2.37s/it]   87%|████████▋ | 8507/9822 [3:48:39<45:47,  2.09s/it] 87%|████████▋ | 8508/9822 [3:48:41<41:30,  1.90s/it] 87%|████████▋ | 8509/9822 [3:48:42<38:29,  1.76s/it] 87%|████████▋ | 8510/9822 [3:48:44<36:17,  1.66s/it] 87%|████████▋ | 8511/9822 [3:48:45<34:51,  1.60s/it] 87%|████████▋ | 8512/9822 [3:48:47<34:00,  1.56s/it] 87%|████████▋ | 8513/9822 [3:48:48<33:20,  1.53s/it] 87%|████████▋ | 8514/9822 [3:48:49<32:35,  1.50s/it] 87%|████████▋ | 8515/9822 [3:48:51<32:10,  1.48s/it] 87%|████████▋ | 8516/9822 [3:48:52<31:56,  1.47s/it] 87%|████████▋ | 8517/9822 [3:48:54<31:40,  1.46s/it] 87%|████████▋ | 8518/9822 [3:48:55<31:28,  1.45s/it] 87%|████████▋ | 8519/9822 [3:48:57<31:26,  1.45s/it] 87%|████████▋ | 8520/9822 [3:48:58<31:23,  1.45s/it] 87%|████████▋ | 8521/9822 [3:48:59<31:20,  1.45s/it] 87%|████████▋ | 8522/9822 [3:49:01<31:18,  1.45s/it] 87%|████████▋ | 8523/9822 [3:49:02<31:15,  1.44s/it] 87%|████████▋ | 8524/9822 [3:49:04<31:05,  1.44s/it] 87%|████████▋ | 8525/9822 [3:49:05<31:37,  1.46s/it] 87%|████████▋ | 8526/9822 [3:49:07<31:22,  1.45s/it] 87%|████████▋ | 8527/9822 [3:49:08<31:13,  1.45s/it] 87%|████████▋ | 8528/9822 [3:49:10<31:13,  1.45s/it] 87%|████████▋ | 8529/9822 [3:49:11<31:07,  1.44s/it] 87%|████████▋ | 8530/9822 [3:49:13<31:05,  1.44s/it] 87%|████████▋ | 8531/9822 [3:49:14<30:59,  1.44s/it] 87%|████████▋ | 8532/9822 [3:49:15<30:50,  1.43s/it] 87%|████████▋ | 8533/9822 [3:49:17<30:51,  1.44s/it] 87%|████████▋ | 8534/9822 [3:49:18<30:48,  1.44s/it] 87%|████████▋ | 8535/9822 [3:49:20<30:45,  1.43s/it] 87%|████████▋ | 8536/9822 [3:49:21<30:56,  1.44s/it] 87%|████████▋ | 8537/9822 [3:49:23<30:53,  1.44s/it] 87%|████████▋ | 8538/9822 [3:49:24<30:43,  1.44s/it] 87%|████████▋ | 8539/9822 [3:49:25<30:38,  1.43s/it] 87%|████████▋ | 8540/9822 [3:49:27<30:36,  1.43s/it] 87%|████████▋ | 8541/9822 [3:49:28<30:33,  1.43s/it] 87%|████████▋ | 8542/9822 [3:49:30<30:37,  1.44s/it] 87%|████████▋ | 8543/9822 [3:49:31<30:36,  1.44s/it] 87%|████████▋ | 8544/9822 [3:49:33<30:43,  1.44s/it] 87%|████████▋ | 8545/9822 [3:49:34<30:52,  1.45s/it] 87%|████████▋ | 8546/9822 [3:49:36<30:45,  1.45s/it] 87%|████████▋ | 8547/9822 [3:49:37<30:36,  1.44s/it] 87%|████████▋ | 8548/9822 [3:49:38<30:32,  1.44s/it] 87%|████████▋ | 8549/9822 [3:49:40<30:27,  1.44s/it] 87%|████████▋ | 8550/9822 [3:49:41<30:23,  1.43s/it] 87%|████████▋ | 8551/9822 [3:49:43<30:28,  1.44s/it] 87%|████████▋ | 8552/9822 [3:49:44<30:24,  1.44s/it] 87%|████████▋ | 8553/9822 [3:49:46<30:21,  1.44s/it] 87%|████████▋ | 8554/9822 [3:49:47<30:20,  1.44s/it] 87%|████████▋ | 8555/9822 [3:49:48<30:19,  1.44s/it] 87%|████████▋ | 8556/9822 [3:49:50<30:49,  1.46s/it] 87%|████████▋ | 8557/9822 [3:49:51<30:35,  1.45s/it] 87%|████████▋ | 8558/9822 [3:49:53<30:26,  1.45s/it] 87%|████████▋ | 8559/9822 [3:49:54<30:19,  1.44s/it] 87%|████████▋ | 8560/9822 [3:49:56<30:15,  1.44s/it] 87%|████████▋ | 8561/9822 [3:49:57<30:07,  1.43s/it] 87%|████████▋ | 8562/9822 [3:49:59<30:06,  1.43s/it] 87%|████████▋ | 8563/9822 [3:50:00<30:16,  1.44s/it] 87%|████████▋ | 8564/9822 [3:50:01<30:17,  1.45s/it] 87%|████████▋ | 8565/9822 [3:50:03<30:12,  1.44s/it] 87%|████████▋ | 8566/9822 [3:50:04<30:09,  1.44s/it] 87%|████████▋ | 8567/9822 [3:50:06<30:03,  1.44s/it] 87%|████████▋ | 8568/9822 [3:50:07<29:58,  1.43s/it] 87%|████████▋ | 8569/9822 [3:50:09<29:57,  1.43s/it] 87%|████████▋ | 8570/9822 [3:50:10<29:52,  1.43s/it] 87%|████████▋ | 8571/9822 [3:50:11<29:48,  1.43s/it] 87%|████████▋ | 8572/9822 [3:50:13<29:44,  1.43s/it] 87%|████████▋ | 8573/9822 [3:50:14<29:48,  1.43s/it] 87%|████████▋ | 8574/9822 [3:50:16<29:45,  1.43s/it] 87%|████████▋ | 8575/9822 [3:50:17<29:49,  1.43s/it] 87%|████████▋ | 8576/9822 [3:50:19<29:47,  1.43s/it] 87%|████████▋ | 8577/9822 [3:50:20<29:42,  1.43s/it] 87%|████████▋ | 8578/9822 [3:50:21<29:37,  1.43s/it] 87%|████████▋ | 8579/9822 [3:50:23<29:36,  1.43s/it] 87%|████████▋ | 8580/9822 [3:50:24<29:34,  1.43s/it] 87%|████████▋ | 8581/9822 [3:50:26<30:02,  1.45s/it] 87%|████████▋ | 8582/9822 [3:50:27<29:54,  1.45s/it] 87%|████████▋ | 8583/9822 [3:50:29<29:43,  1.44s/it] 87%|████████▋ | 8584/9822 [3:50:30<29:36,  1.43s/it] 87%|████████▋ | 8585/9822 [3:50:32<29:36,  1.44s/it] 87%|████████▋ | 8586/9822 [3:50:33<29:33,  1.43s/it] 87%|████████▋ | 8587/9822 [3:50:34<29:33,  1.44s/it] 87%|████████▋ | 8588/9822 [3:50:36<29:36,  1.44s/it] 87%|████████▋ | 8589/9822 [3:50:37<29:29,  1.43s/it] 87%|████████▋ | 8590/9822 [3:50:39<29:26,  1.43s/it] 87%|████████▋ | 8591/9822 [3:50:40<29:35,  1.44s/it] 87%|████████▋ | 8592/9822 [3:50:42<29:26,  1.44s/it] 87%|████████▋ | 8593/9822 [3:50:43<29:24,  1.44s/it] 87%|████████▋ | 8594/9822 [3:50:44<29:21,  1.43s/it] 88%|████████▊ | 8595/9822 [3:50:46<29:22,  1.44s/it] 88%|████████▊ | 8596/9822 [3:50:47<29:21,  1.44s/it] 88%|████████▊ | 8597/9822 [3:50:49<29:17,  1.43s/it] 88%|████████▊ | 8598/9822 [3:50:50<29:21,  1.44s/it] 88%|████████▊ | 8599/9822 [3:50:52<29:19,  1.44s/it] 88%|████████▊ | 8600/9822 [3:50:53<28:59,  1.42s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1354, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1288, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0400, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1118, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0483, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0563, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2355, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0644, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0666, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:38:40 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:38:40 - INFO - __main__ - ***** test Results*****
01/08/2024 01:38:40 - INFO - __main__ -   Training step = 8600
01/08/2024 01:38:40 - INFO - __main__ -  test_accuracy:0.8751830161054173 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:38:46 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:38:46 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:38:46 - INFO - __main__ -   Training step = 8600
01/08/2024 01:38:46 - INFO - __main__ -  eval_accuracy:0.8623214939582571 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:38:51 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:38:51 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:38:51 - INFO - __main__ -   Training step = 8600
01/08/2024 01:38:51 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 88%|████████▊ | 8601/9822 [3:51:11<2:12:25,  6.51s/it] 88%|████████▊ | 8602/9822 [3:51:13<1:41:19,  4.98s/it] 88%|████████▊ | 8603/9822 [3:51:14<1:19:38,  3.92s/it] 88%|████████▊ | 8604/9822 [3:51:16<1:04:26,  3.17s/it] 88%|████████▊ | 8605/9822 [3:51:17<53:49,  2.65s/it]   88%|████████▊ | 8606/9822 [3:51:19<46:26,  2.29s/it] 88%|████████▊ | 8607/9822 [3:51:20<41:11,  2.03s/it] 88%|████████▊ | 8608/9822 [3:51:21<37:31,  1.85s/it] 88%|████████▊ | 8609/9822 [3:51:23<34:52,  1.73s/it] 88%|████████▊ | 8610/9822 [3:51:24<33:05,  1.64s/it] 88%|████████▊ | 8611/9822 [3:51:26<32:25,  1.61s/it] 88%|████████▊ | 8612/9822 [3:51:27<31:24,  1.56s/it] 88%|████████▊ | 8613/9822 [3:51:29<30:36,  1.52s/it] 88%|████████▊ | 8614/9822 [3:51:30<30:01,  1.49s/it] 88%|████████▊ | 8615/9822 [3:51:32<29:40,  1.47s/it] 88%|████████▊ | 8616/9822 [3:51:33<29:22,  1.46s/it] 88%|████████▊ | 8617/9822 [3:51:34<29:09,  1.45s/it] 88%|████████▊ | 8618/9822 [3:51:36<29:06,  1.45s/it] 88%|████████▊ | 8619/9822 [3:51:37<28:57,  1.44s/it] 88%|████████▊ | 8620/9822 [3:51:39<28:51,  1.44s/it] 88%|████████▊ | 8621/9822 [3:51:40<28:49,  1.44s/it] 88%|████████▊ | 8622/9822 [3:51:42<28:45,  1.44s/it] 88%|████████▊ | 8623/9822 [3:51:43<28:45,  1.44s/it] 88%|████████▊ | 8624/9822 [3:51:45<28:41,  1.44s/it] 88%|████████▊ | 8625/9822 [3:51:46<28:41,  1.44s/it] 88%|████████▊ | 8626/9822 [3:51:47<28:39,  1.44s/it] 88%|████████▊ | 8627/9822 [3:51:49<28:38,  1.44s/it] 88%|████████▊ | 8628/9822 [3:51:50<28:37,  1.44s/it] 88%|████████▊ | 8629/9822 [3:51:52<28:39,  1.44s/it] 88%|████████▊ | 8630/9822 [3:51:53<28:36,  1.44s/it] 88%|████████▊ | 8631/9822 [3:51:55<28:32,  1.44s/it] 88%|████████▊ | 8632/9822 [3:51:56<28:26,  1.43s/it] 88%|████████▊ | 8633/9822 [3:51:57<28:28,  1.44s/it] 88%|████████▊ | 8634/9822 [3:51:59<28:25,  1.44s/it] 88%|████████▊ | 8635/9822 [3:52:00<28:22,  1.43s/it] 88%|████████▊ | 8636/9822 [3:52:02<28:20,  1.43s/it] 88%|████████▊ | 8637/9822 [3:52:03<28:22,  1.44s/it] 88%|████████▊ | 8638/9822 [3:52:05<28:22,  1.44s/it] 88%|████████▊ | 8639/9822 [3:52:06<28:26,  1.44s/it] 88%|████████▊ | 8640/9822 [3:52:08<28:22,  1.44s/it] 88%|████████▊ | 8641/9822 [3:52:09<28:14,  1.44s/it] 88%|████████▊ | 8642/9822 [3:52:10<28:13,  1.43s/it] 88%|████████▊ | 8643/9822 [3:52:12<28:43,  1.46s/it] 88%|████████▊ | 8644/9822 [3:52:13<28:34,  1.46s/it] 88%|████████▊ | 8645/9822 [3:52:15<28:23,  1.45s/it] 88%|████████▊ | 8646/9822 [3:52:16<28:26,  1.45s/it] 88%|████████▊ | 8647/9822 [3:52:18<28:15,  1.44s/it] 88%|████████▊ | 8648/9822 [3:52:19<28:11,  1.44s/it] 88%|████████▊ | 8649/9822 [3:52:21<28:09,  1.44s/it] 88%|████████▊ | 8650/9822 [3:52:22<28:03,  1.44s/it] 88%|████████▊ | 8651/9822 [3:52:23<28:00,  1.44s/it] 88%|████████▊ | 8652/9822 [3:52:25<27:57,  1.43s/it] 88%|████████▊ | 8653/9822 [3:52:26<27:57,  1.43s/it] 88%|████████▊ | 8654/9822 [3:52:28<28:00,  1.44s/it] 88%|████████▊ | 8655/9822 [3:52:29<27:59,  1.44s/it] 88%|████████▊ | 8656/9822 [3:52:31<27:58,  1.44s/it] 88%|████████▊ | 8657/9822 [3:52:32<27:55,  1.44s/it] 88%|████████▊ | 8658/9822 [3:52:33<27:50,  1.44s/it] 88%|████████▊ | 8659/9822 [3:52:35<27:47,  1.43s/it] 88%|████████▊ | 8660/9822 [3:52:36<27:44,  1.43s/it] 88%|████████▊ | 8661/9822 [3:52:38<27:43,  1.43s/it] 88%|████████▊ | 8662/9822 [3:52:39<27:43,  1.43s/it] 88%|████████▊ | 8663/9822 [3:52:41<27:43,  1.44s/it] 88%|████████▊ | 8664/9822 [3:52:42<27:43,  1.44s/it] 88%|████████▊ | 8665/9822 [3:52:44<27:48,  1.44s/it] 88%|████████▊ | 8666/9822 [3:52:45<27:44,  1.44s/it] 88%|████████▊ | 8667/9822 [3:52:46<27:40,  1.44s/it] 88%|████████▊ | 8668/9822 [3:52:48<27:42,  1.44s/it] 88%|████████▊ | 8669/9822 [3:52:49<27:36,  1.44s/it] 88%|████████▊ | 8670/9822 [3:52:51<27:39,  1.44s/it] 88%|████████▊ | 8671/9822 [3:52:52<27:38,  1.44s/it] 88%|████████▊ | 8672/9822 [3:52:54<27:35,  1.44s/it] 88%|████████▊ | 8673/9822 [3:52:55<28:10,  1.47s/it] 88%|████████▊ | 8674/9822 [3:52:57<27:57,  1.46s/it] 88%|████████▊ | 8675/9822 [3:52:58<27:47,  1.45s/it] 88%|████████▊ | 8676/9822 [3:52:59<27:43,  1.45s/it] 88%|████████▊ | 8677/9822 [3:53:01<27:34,  1.44s/it] 88%|████████▊ | 8678/9822 [3:53:02<27:25,  1.44s/it] 88%|████████▊ | 8679/9822 [3:53:04<27:24,  1.44s/it] 88%|████████▊ | 8680/9822 [3:53:05<27:24,  1.44s/it] 88%|████████▊ | 8681/9822 [3:53:07<27:21,  1.44s/it] 88%|████████▊ | 8682/9822 [3:53:08<27:29,  1.45s/it] 88%|████████▊ | 8683/9822 [3:53:10<27:19,  1.44s/it] 88%|████████▊ | 8684/9822 [3:53:11<27:13,  1.44s/it] 88%|████████▊ | 8685/9822 [3:53:12<27:08,  1.43s/it] 88%|████████▊ | 8686/9822 [3:53:14<26:50,  1.42s/it] 88%|████████▊ | 8687/9822 [3:53:15<26:57,  1.43s/it] 88%|████████▊ | 8688/9822 [3:53:17<26:56,  1.43s/it] 88%|████████▊ | 8689/9822 [3:53:18<27:06,  1.44s/it] 88%|████████▊ | 8690/9822 [3:53:20<27:24,  1.45s/it] 88%|████████▊ | 8691/9822 [3:53:21<27:14,  1.45s/it] 88%|████████▊ | 8692/9822 [3:53:22<27:07,  1.44s/it] 89%|████████▊ | 8693/9822 [3:53:24<27:04,  1.44s/it] 89%|████████▊ | 8694/9822 [3:53:25<27:01,  1.44s/it] 89%|████████▊ | 8695/9822 [3:53:27<27:01,  1.44s/it] 89%|████████▊ | 8696/9822 [3:53:28<27:10,  1.45s/it] 89%|████████▊ | 8697/9822 [3:53:30<27:11,  1.45s/it] 89%|████████▊ | 8698/9822 [3:53:31<27:09,  1.45s/it] 89%|████████▊ | 8699/9822 [3:53:33<27:06,  1.45s/it] 89%|████████▊ | 8700/9822 [3:53:34<27:00,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1144, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1133, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0371, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1575, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0995, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0977, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0944, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0942, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1002, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2120, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1613, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:41:21 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:41:21 - INFO - __main__ - ***** test Results*****
01/08/2024 01:41:21 - INFO - __main__ -   Training step = 8700
01/08/2024 01:41:21 - INFO - __main__ -  test_accuracy:0.8821376281112738 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:41:27 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:41:27 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:41:27 - INFO - __main__ -   Training step = 8700
01/08/2024 01:41:27 - INFO - __main__ -  eval_accuracy:0.8623214939582571 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:41:32 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:41:32 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:41:32 - INFO - __main__ -   Training step = 8700
01/08/2024 01:41:32 - INFO - __main__ -  eval_accuracy:0.9121201025265471 
 89%|████████▊ | 8701/9822 [3:53:52<2:01:49,  6.52s/it] 89%|████████▊ | 8702/9822 [3:53:54<1:33:15,  5.00s/it] 89%|████████▊ | 8703/9822 [3:53:55<1:13:41,  3.95s/it] 89%|████████▊ | 8704/9822 [3:53:57<59:34,  3.20s/it]   89%|████████▊ | 8705/9822 [3:53:58<49:47,  2.67s/it] 89%|████████▊ | 8706/9822 [3:54:00<42:51,  2.30s/it] 89%|████████▊ | 8707/9822 [3:54:01<38:03,  2.05s/it] 89%|████████▊ | 8708/9822 [3:54:03<34:48,  1.87s/it] 89%|████████▊ | 8709/9822 [3:54:04<32:19,  1.74s/it] 89%|████████▊ | 8710/9822 [3:54:05<30:37,  1.65s/it] 89%|████████▊ | 8711/9822 [3:54:07<29:20,  1.58s/it] 89%|████████▊ | 8712/9822 [3:54:08<28:29,  1.54s/it] 89%|████████▊ | 8713/9822 [3:54:10<27:54,  1.51s/it] 89%|████████▊ | 8714/9822 [3:54:11<27:30,  1.49s/it] 89%|████████▊ | 8715/9822 [3:54:13<27:10,  1.47s/it] 89%|████████▊ | 8716/9822 [3:54:14<26:54,  1.46s/it] 89%|████████▊ | 8717/9822 [3:54:15<26:42,  1.45s/it] 89%|████████▉ | 8718/9822 [3:54:17<26:40,  1.45s/it] 89%|████████▉ | 8719/9822 [3:54:18<26:40,  1.45s/it] 89%|████████▉ | 8720/9822 [3:54:20<26:33,  1.45s/it] 89%|████████▉ | 8721/9822 [3:54:21<26:28,  1.44s/it] 89%|████████▉ | 8722/9822 [3:54:23<26:21,  1.44s/it] 89%|████████▉ | 8723/9822 [3:54:24<26:19,  1.44s/it] 89%|████████▉ | 8724/9822 [3:54:26<26:18,  1.44s/it] 89%|████████▉ | 8725/9822 [3:54:27<26:18,  1.44s/it] 89%|████████▉ | 8726/9822 [3:54:28<26:16,  1.44s/it] 89%|████████▉ | 8727/9822 [3:54:30<26:10,  1.43s/it] 89%|████████▉ | 8728/9822 [3:54:31<26:06,  1.43s/it] 89%|████████▉ | 8729/9822 [3:54:33<26:04,  1.43s/it] 89%|████████▉ | 8730/9822 [3:54:34<26:06,  1.43s/it] 89%|████████▉ | 8731/9822 [3:54:36<26:04,  1.43s/it] 89%|████████▉ | 8732/9822 [3:54:37<26:01,  1.43s/it] 89%|████████▉ | 8733/9822 [3:54:38<26:03,  1.44s/it] 89%|████████▉ | 8734/9822 [3:54:40<26:03,  1.44s/it] 89%|████████▉ | 8735/9822 [3:54:41<26:24,  1.46s/it] 89%|████████▉ | 8736/9822 [3:54:43<26:12,  1.45s/it] 89%|████████▉ | 8737/9822 [3:54:44<26:04,  1.44s/it] 89%|████████▉ | 8738/9822 [3:54:46<26:02,  1.44s/it] 89%|████████▉ | 8739/9822 [3:54:47<25:58,  1.44s/it] 89%|████████▉ | 8740/9822 [3:54:49<25:53,  1.44s/it] 89%|████████▉ | 8741/9822 [3:54:50<25:51,  1.43s/it] 89%|████████▉ | 8742/9822 [3:54:51<25:50,  1.44s/it] 89%|████████▉ | 8743/9822 [3:54:53<25:57,  1.44s/it] 89%|████████▉ | 8744/9822 [3:54:54<25:55,  1.44s/it] 89%|████████▉ | 8745/9822 [3:54:56<25:52,  1.44s/it] 89%|████████▉ | 8746/9822 [3:54:57<25:46,  1.44s/it] 89%|████████▉ | 8747/9822 [3:54:59<25:43,  1.44s/it] 89%|████████▉ | 8748/9822 [3:55:00<25:44,  1.44s/it] 89%|████████▉ | 8749/9822 [3:55:02<25:42,  1.44s/it] 89%|████████▉ | 8750/9822 [3:55:03<25:42,  1.44s/it] 89%|████████▉ | 8751/9822 [3:55:04<25:41,  1.44s/it] 89%|████████▉ | 8752/9822 [3:55:06<25:39,  1.44s/it] 89%|████████▉ | 8753/9822 [3:55:07<25:39,  1.44s/it] 89%|████████▉ | 8754/9822 [3:55:09<25:34,  1.44s/it] 89%|████████▉ | 8755/9822 [3:55:10<25:34,  1.44s/it] 89%|████████▉ | 8756/9822 [3:55:12<25:33,  1.44s/it] 89%|████████▉ | 8757/9822 [3:55:13<25:35,  1.44s/it] 89%|████████▉ | 8758/9822 [3:55:14<25:32,  1.44s/it] 89%|████████▉ | 8759/9822 [3:55:16<25:30,  1.44s/it] 89%|████████▉ | 8760/9822 [3:55:17<25:23,  1.43s/it] 89%|████████▉ | 8761/9822 [3:55:19<25:26,  1.44s/it] 89%|████████▉ | 8762/9822 [3:55:20<25:21,  1.44s/it] 89%|████████▉ | 8763/9822 [3:55:22<25:19,  1.43s/it] 89%|████████▉ | 8764/9822 [3:55:23<25:18,  1.44s/it] 89%|████████▉ | 8765/9822 [3:55:25<25:39,  1.46s/it] 89%|████████▉ | 8766/9822 [3:55:26<25:30,  1.45s/it] 89%|████████▉ | 8767/9822 [3:55:27<25:25,  1.45s/it] 89%|████████▉ | 8768/9822 [3:55:29<25:21,  1.44s/it] 89%|████████▉ | 8769/9822 [3:55:30<25:16,  1.44s/it] 89%|████████▉ | 8770/9822 [3:55:32<25:13,  1.44s/it] 89%|████████▉ | 8771/9822 [3:55:33<25:06,  1.43s/it] 89%|████████▉ | 8772/9822 [3:55:35<24:52,  1.42s/it] 89%|████████▉ | 8773/9822 [3:55:36<24:57,  1.43s/it] 89%|████████▉ | 8774/9822 [3:55:37<25:02,  1.43s/it] 89%|████████▉ | 8775/9822 [3:55:39<25:01,  1.43s/it] 89%|████████▉ | 8776/9822 [3:55:40<25:03,  1.44s/it] 89%|████████▉ | 8777/9822 [3:55:42<25:00,  1.44s/it] 89%|████████▉ | 8778/9822 [3:55:43<25:00,  1.44s/it] 89%|████████▉ | 8779/9822 [3:55:45<24:55,  1.43s/it] 89%|████████▉ | 8780/9822 [3:55:46<24:50,  1.43s/it] 89%|████████▉ | 8781/9822 [3:55:48<24:51,  1.43s/it] 89%|████████▉ | 8782/9822 [3:55:49<24:51,  1.43s/it] 89%|████████▉ | 8783/9822 [3:55:50<24:53,  1.44s/it] 89%|████████▉ | 8784/9822 [3:55:52<24:55,  1.44s/it] 89%|████████▉ | 8785/9822 [3:55:53<24:54,  1.44s/it] 89%|████████▉ | 8786/9822 [3:55:55<24:56,  1.44s/it] 89%|████████▉ | 8787/9822 [3:55:56<24:54,  1.44s/it] 89%|████████▉ | 8788/9822 [3:55:58<24:52,  1.44s/it] 89%|████████▉ | 8789/9822 [3:55:59<24:50,  1.44s/it] 89%|████████▉ | 8790/9822 [3:56:00<24:45,  1.44s/it] 90%|████████▉ | 8791/9822 [3:56:02<24:42,  1.44s/it] 90%|████████▉ | 8792/9822 [3:56:03<24:39,  1.44s/it] 90%|████████▉ | 8793/9822 [3:56:05<24:40,  1.44s/it] 90%|████████▉ | 8794/9822 [3:56:06<24:35,  1.44s/it] 90%|████████▉ | 8795/9822 [3:56:08<24:32,  1.43s/it] 90%|████████▉ | 8796/9822 [3:56:09<24:28,  1.43s/it] 90%|████████▉ | 8797/9822 [3:56:11<24:52,  1.46s/it] 90%|████████▉ | 8798/9822 [3:56:12<24:48,  1.45s/it] 90%|████████▉ | 8799/9822 [3:56:13<24:41,  1.45s/it] 90%|████████▉ | 8800/9822 [3:56:15<24:32,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2099, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1948, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1329, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1449, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1025, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1766, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1153, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1091, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1111, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1109, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1491, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:44:02 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:44:02 - INFO - __main__ - ***** test Results*****
01/08/2024 01:44:02 - INFO - __main__ -   Training step = 8800
01/08/2024 01:44:02 - INFO - __main__ -  test_accuracy:0.8792093704245973 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:44:08 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:44:08 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:44:08 - INFO - __main__ -   Training step = 8800
01/08/2024 01:44:08 - INFO - __main__ -  eval_accuracy:0.8604906627608935 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:44:13 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:44:13 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:44:13 - INFO - __main__ -   Training step = 8800
01/08/2024 01:44:13 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 90%|████████▉ | 8801/9822 [3:56:33<1:50:50,  6.51s/it] 90%|████████▉ | 8802/9822 [3:56:35<1:24:47,  4.99s/it] 90%|████████▉ | 8803/9822 [3:56:36<1:06:37,  3.92s/it] 90%|████████▉ | 8804/9822 [3:56:38<53:54,  3.18s/it]   90%|████████▉ | 8805/9822 [3:56:39<45:00,  2.66s/it] 90%|████████▉ | 8806/9822 [3:56:40<38:51,  2.29s/it] 90%|████████▉ | 8807/9822 [3:56:42<34:37,  2.05s/it] 90%|████████▉ | 8808/9822 [3:56:43<31:38,  1.87s/it] 90%|████████▉ | 8809/9822 [3:56:45<29:26,  1.74s/it] 90%|████████▉ | 8810/9822 [3:56:46<27:50,  1.65s/it] 90%|████████▉ | 8811/9822 [3:56:48<26:41,  1.58s/it] 90%|████████▉ | 8812/9822 [3:56:49<25:57,  1.54s/it] 90%|████████▉ | 8813/9822 [3:56:51<25:25,  1.51s/it] 90%|████████▉ | 8814/9822 [3:56:52<25:02,  1.49s/it] 90%|████████▉ | 8815/9822 [3:56:53<24:46,  1.48s/it] 90%|████████▉ | 8816/9822 [3:56:55<24:45,  1.48s/it] 90%|████████▉ | 8817/9822 [3:56:56<24:29,  1.46s/it] 90%|████████▉ | 8818/9822 [3:56:58<24:17,  1.45s/it] 90%|████████▉ | 8819/9822 [3:56:59<24:11,  1.45s/it] 90%|████████▉ | 8820/9822 [3:57:01<24:05,  1.44s/it] 90%|████████▉ | 8821/9822 [3:57:02<24:01,  1.44s/it] 90%|████████▉ | 8822/9822 [3:57:04<23:57,  1.44s/it] 90%|████████▉ | 8823/9822 [3:57:05<23:58,  1.44s/it] 90%|████████▉ | 8824/9822 [3:57:06<23:55,  1.44s/it] 90%|████████▉ | 8825/9822 [3:57:08<23:50,  1.44s/it] 90%|████████▉ | 8826/9822 [3:57:09<23:49,  1.43s/it] 90%|████████▉ | 8827/9822 [3:57:11<24:04,  1.45s/it] 90%|████████▉ | 8828/9822 [3:57:12<23:59,  1.45s/it] 90%|████████▉ | 8829/9822 [3:57:14<23:51,  1.44s/it] 90%|████████▉ | 8830/9822 [3:57:15<23:48,  1.44s/it] 90%|████████▉ | 8831/9822 [3:57:16<23:43,  1.44s/it] 90%|████████▉ | 8832/9822 [3:57:18<23:44,  1.44s/it] 90%|████████▉ | 8833/9822 [3:57:19<23:40,  1.44s/it] 90%|████████▉ | 8834/9822 [3:57:21<23:37,  1.43s/it] 90%|████████▉ | 8835/9822 [3:57:22<23:37,  1.44s/it] 90%|████████▉ | 8836/9822 [3:57:24<23:59,  1.46s/it] 90%|████████▉ | 8837/9822 [3:57:25<23:47,  1.45s/it] 90%|████████▉ | 8838/9822 [3:57:27<23:43,  1.45s/it] 90%|████████▉ | 8839/9822 [3:57:28<23:37,  1.44s/it] 90%|█████████ | 8840/9822 [3:57:30<23:43,  1.45s/it] 90%|█████████ | 8841/9822 [3:57:31<23:37,  1.44s/it] 90%|█████████ | 8842/9822 [3:57:32<23:52,  1.46s/it] 90%|█████████ | 8843/9822 [3:57:34<24:00,  1.47s/it] 90%|█████████ | 8844/9822 [3:57:35<23:52,  1.46s/it] 90%|█████████ | 8845/9822 [3:57:37<23:47,  1.46s/it] 90%|█████████ | 8846/9822 [3:57:38<23:41,  1.46s/it] 90%|█████████ | 8847/9822 [3:57:40<23:38,  1.45s/it] 90%|█████████ | 8848/9822 [3:57:41<23:36,  1.45s/it] 90%|█████████ | 8849/9822 [3:57:43<23:33,  1.45s/it] 90%|█████████ | 8850/9822 [3:57:44<23:31,  1.45s/it] 90%|█████████ | 8851/9822 [3:57:46<23:28,  1.45s/it] 90%|█████████ | 8852/9822 [3:57:47<23:26,  1.45s/it] 90%|█████████ | 8853/9822 [3:57:48<23:23,  1.45s/it] 90%|█████████ | 8854/9822 [3:57:50<23:22,  1.45s/it] 90%|█████████ | 8855/9822 [3:57:51<23:21,  1.45s/it] 90%|█████████ | 8856/9822 [3:57:53<23:19,  1.45s/it] 90%|█████████ | 8857/9822 [3:57:54<23:18,  1.45s/it] 90%|█████████ | 8858/9822 [3:57:56<23:04,  1.44s/it] 90%|█████████ | 8859/9822 [3:57:57<23:16,  1.45s/it] 90%|█████████ | 8860/9822 [3:57:59<23:24,  1.46s/it] 90%|█████████ | 8861/9822 [3:58:00<23:23,  1.46s/it] 90%|█████████ | 8862/9822 [3:58:02<23:16,  1.45s/it] 90%|█████████ | 8863/9822 [3:58:03<23:05,  1.44s/it] 90%|█████████ | 8864/9822 [3:58:04<23:01,  1.44s/it] 90%|█████████ | 8865/9822 [3:58:06<23:02,  1.44s/it] 90%|█████████ | 8866/9822 [3:58:07<23:00,  1.44s/it] 90%|█████████ | 8867/9822 [3:58:09<23:19,  1.46s/it] 90%|█████████ | 8868/9822 [3:58:10<23:04,  1.45s/it] 90%|█████████ | 8869/9822 [3:58:12<22:59,  1.45s/it] 90%|█████████ | 8870/9822 [3:58:13<22:55,  1.45s/it] 90%|█████████ | 8871/9822 [3:58:15<22:52,  1.44s/it] 90%|█████████ | 8872/9822 [3:58:16<22:49,  1.44s/it] 90%|█████████ | 8873/9822 [3:58:17<22:46,  1.44s/it] 90%|█████████ | 8874/9822 [3:58:19<22:41,  1.44s/it] 90%|█████████ | 8875/9822 [3:58:20<22:41,  1.44s/it] 90%|█████████ | 8876/9822 [3:58:22<22:37,  1.44s/it] 90%|█████████ | 8877/9822 [3:58:23<22:39,  1.44s/it] 90%|█████████ | 8878/9822 [3:58:25<22:34,  1.44s/it] 90%|█████████ | 8879/9822 [3:58:26<22:35,  1.44s/it] 90%|█████████ | 8880/9822 [3:58:27<22:31,  1.43s/it] 90%|█████████ | 8881/9822 [3:58:29<22:31,  1.44s/it] 90%|█████████ | 8882/9822 [3:58:30<22:33,  1.44s/it] 90%|█████████ | 8883/9822 [3:58:32<22:30,  1.44s/it] 90%|█████████ | 8884/9822 [3:58:33<22:27,  1.44s/it] 90%|█████████ | 8885/9822 [3:58:35<22:27,  1.44s/it] 90%|█████████ | 8886/9822 [3:58:36<22:24,  1.44s/it] 90%|█████████ | 8887/9822 [3:58:37<22:20,  1.43s/it] 90%|█████████ | 8888/9822 [3:58:39<22:18,  1.43s/it] 91%|█████████ | 8889/9822 [3:58:40<22:14,  1.43s/it] 91%|█████████ | 8890/9822 [3:58:42<22:13,  1.43s/it] 91%|█████████ | 8891/9822 [3:58:43<22:11,  1.43s/it] 91%|█████████ | 8892/9822 [3:58:45<22:34,  1.46s/it] 91%|█████████ | 8893/9822 [3:58:46<22:26,  1.45s/it] 91%|█████████ | 8894/9822 [3:58:48<22:23,  1.45s/it] 91%|█████████ | 8895/9822 [3:58:49<22:16,  1.44s/it] 91%|█████████ | 8896/9822 [3:58:50<22:11,  1.44s/it] 91%|█████████ | 8897/9822 [3:58:52<22:08,  1.44s/it] 91%|█████████ | 8898/9822 [3:58:53<22:04,  1.43s/it] 91%|█████████ | 8899/9822 [3:58:55<22:01,  1.43s/it] 91%|█████████ | 8900/9822 [3:58:56<22:02,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1251, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1630, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0299, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1319, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1152, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1661, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1830, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0579, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1062, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1182, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0924, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:46:43 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:46:43 - INFO - __main__ - ***** test Results*****
01/08/2024 01:46:43 - INFO - __main__ -   Training step = 8900
01/08/2024 01:46:43 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:46:49 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:46:49 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:46:49 - INFO - __main__ -   Training step = 8900
01/08/2024 01:46:49 - INFO - __main__ -  eval_accuracy:0.8619553277187844 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:46:54 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:46:54 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:46:54 - INFO - __main__ -   Training step = 8900
01/08/2024 01:46:54 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 91%|█████████ | 8901/9822 [3:59:15<1:39:52,  6.51s/it] 91%|█████████ | 8902/9822 [3:59:16<1:16:28,  4.99s/it] 91%|█████████ | 8903/9822 [3:59:17<1:00:02,  3.92s/it] 91%|█████████ | 8904/9822 [3:59:19<48:32,  3.17s/it]   91%|█████████ | 8905/9822 [3:59:20<40:30,  2.65s/it] 91%|█████████ | 8906/9822 [3:59:22<34:50,  2.28s/it] 91%|█████████ | 8907/9822 [3:59:23<30:53,  2.03s/it] 91%|█████████ | 8908/9822 [3:59:25<28:06,  1.85s/it] 91%|█████████ | 8909/9822 [3:59:26<26:12,  1.72s/it] 91%|█████████ | 8910/9822 [3:59:27<24:51,  1.64s/it] 91%|█████████ | 8911/9822 [3:59:29<23:58,  1.58s/it] 91%|█████████ | 8912/9822 [3:59:30<23:20,  1.54s/it] 91%|█████████ | 8913/9822 [3:59:32<22:51,  1.51s/it] 91%|█████████ | 8914/9822 [3:59:33<22:28,  1.49s/it] 91%|█████████ | 8915/9822 [3:59:35<22:11,  1.47s/it] 91%|█████████ | 8916/9822 [3:59:36<22:00,  1.46s/it] 91%|█████████ | 8917/9822 [3:59:37<21:53,  1.45s/it] 91%|█████████ | 8918/9822 [3:59:39<22:12,  1.47s/it] 91%|█████████ | 8919/9822 [3:59:40<22:07,  1.47s/it] 91%|█████████ | 8920/9822 [3:59:42<21:59,  1.46s/it] 91%|█████████ | 8921/9822 [3:59:43<21:50,  1.45s/it] 91%|█████████ | 8922/9822 [3:59:45<21:42,  1.45s/it] 91%|█████████ | 8923/9822 [3:59:46<21:39,  1.44s/it] 91%|█████████ | 8924/9822 [3:59:48<21:36,  1.44s/it] 91%|█████████ | 8925/9822 [3:59:49<21:32,  1.44s/it] 91%|█████████ | 8926/9822 [3:59:51<21:31,  1.44s/it] 91%|█████████ | 8927/9822 [3:59:52<21:28,  1.44s/it] 91%|█████████ | 8928/9822 [3:59:53<21:26,  1.44s/it] 91%|█████████ | 8929/9822 [3:59:55<21:21,  1.44s/it] 91%|█████████ | 8930/9822 [3:59:56<21:22,  1.44s/it] 91%|█████████ | 8931/9822 [3:59:58<21:21,  1.44s/it] 91%|█████████ | 8932/9822 [3:59:59<21:22,  1.44s/it] 91%|█████████ | 8933/9822 [4:00:01<21:17,  1.44s/it] 91%|█████████ | 8934/9822 [4:00:02<21:15,  1.44s/it] 91%|█████████ | 8935/9822 [4:00:03<21:14,  1.44s/it] 91%|█████████ | 8936/9822 [4:00:05<21:16,  1.44s/it] 91%|█████████ | 8937/9822 [4:00:06<21:16,  1.44s/it] 91%|█████████ | 8938/9822 [4:00:08<21:15,  1.44s/it] 91%|█████████ | 8939/9822 [4:00:09<21:10,  1.44s/it] 91%|█████████ | 8940/9822 [4:00:11<21:12,  1.44s/it] 91%|█████████ | 8941/9822 [4:00:12<21:11,  1.44s/it] 91%|█████████ | 8942/9822 [4:00:14<21:06,  1.44s/it] 91%|█████████ | 8943/9822 [4:00:15<21:00,  1.43s/it] 91%|█████████ | 8944/9822 [4:00:16<20:45,  1.42s/it] 91%|█████████ | 8945/9822 [4:00:18<20:45,  1.42s/it] 91%|█████████ | 8946/9822 [4:00:19<20:48,  1.43s/it] 91%|█████████ | 8947/9822 [4:00:21<20:52,  1.43s/it] 91%|█████████ | 8948/9822 [4:00:22<20:50,  1.43s/it] 91%|█████████ | 8949/9822 [4:00:24<20:50,  1.43s/it] 91%|█████████ | 8950/9822 [4:00:25<21:10,  1.46s/it] 91%|█████████ | 8951/9822 [4:00:26<21:02,  1.45s/it] 91%|█████████ | 8952/9822 [4:00:28<20:56,  1.44s/it] 91%|█████████ | 8953/9822 [4:00:29<20:50,  1.44s/it] 91%|█████████ | 8954/9822 [4:00:31<20:45,  1.44s/it] 91%|█████████ | 8955/9822 [4:00:32<20:43,  1.43s/it] 91%|█████████ | 8956/9822 [4:00:34<20:44,  1.44s/it] 91%|█████████ | 8957/9822 [4:00:35<20:43,  1.44s/it] 91%|█████████ | 8958/9822 [4:00:36<20:42,  1.44s/it] 91%|█████████ | 8959/9822 [4:00:38<20:43,  1.44s/it] 91%|█████████ | 8960/9822 [4:00:39<20:37,  1.44s/it] 91%|█████████ | 8961/9822 [4:00:41<20:34,  1.43s/it] 91%|█████████ | 8962/9822 [4:00:42<20:37,  1.44s/it] 91%|█████████▏| 8963/9822 [4:00:44<20:36,  1.44s/it] 91%|█████████▏| 8964/9822 [4:00:45<20:33,  1.44s/it] 91%|█████████▏| 8965/9822 [4:00:47<20:30,  1.44s/it] 91%|█████████▏| 8966/9822 [4:00:48<20:29,  1.44s/it] 91%|█████████▏| 8967/9822 [4:00:49<20:28,  1.44s/it] 91%|█████████▏| 8968/9822 [4:00:51<20:29,  1.44s/it] 91%|█████████▏| 8969/9822 [4:00:52<20:29,  1.44s/it] 91%|█████████▏| 8970/9822 [4:00:54<20:26,  1.44s/it] 91%|█████████▏| 8971/9822 [4:00:55<20:24,  1.44s/it] 91%|█████████▏| 8972/9822 [4:00:57<20:21,  1.44s/it] 91%|█████████▏| 8973/9822 [4:00:58<20:16,  1.43s/it] 91%|█████████▏| 8974/9822 [4:00:59<20:17,  1.44s/it] 91%|█████████▏| 8975/9822 [4:01:01<20:15,  1.43s/it] 91%|█████████▏| 8976/9822 [4:01:02<20:15,  1.44s/it] 91%|█████████▏| 8977/9822 [4:01:04<20:10,  1.43s/it] 91%|█████████▏| 8978/9822 [4:01:05<20:10,  1.43s/it] 91%|█████████▏| 8979/9822 [4:01:07<20:07,  1.43s/it] 91%|█████████▏| 8980/9822 [4:01:08<20:06,  1.43s/it] 91%|█████████▏| 8981/9822 [4:01:10<20:08,  1.44s/it] 91%|█████████▏| 8982/9822 [4:01:11<20:24,  1.46s/it] 91%|█████████▏| 8983/9822 [4:01:12<20:21,  1.46s/it] 91%|█████████▏| 8984/9822 [4:01:14<20:15,  1.45s/it] 91%|█████████▏| 8985/9822 [4:01:15<20:10,  1.45s/it] 91%|█████████▏| 8986/9822 [4:01:17<20:08,  1.45s/it] 91%|█████████▏| 8987/9822 [4:01:18<20:03,  1.44s/it] 92%|█████████▏| 8988/9822 [4:01:20<19:59,  1.44s/it] 92%|█████████▏| 8989/9822 [4:01:21<19:54,  1.43s/it] 92%|█████████▏| 8990/9822 [4:01:23<19:56,  1.44s/it] 92%|█████████▏| 8991/9822 [4:01:24<19:53,  1.44s/it] 92%|█████████▏| 8992/9822 [4:01:25<19:55,  1.44s/it] 92%|█████████▏| 8993/9822 [4:01:27<19:53,  1.44s/it] 92%|█████████▏| 8994/9822 [4:01:28<19:48,  1.44s/it] 92%|█████████▏| 8995/9822 [4:01:30<19:59,  1.45s/it] 92%|█████████▏| 8996/9822 [4:01:31<20:01,  1.45s/it] 92%|█████████▏| 8997/9822 [4:01:33<19:56,  1.45s/it] 92%|█████████▏| 8998/9822 [4:01:34<19:55,  1.45s/it] 92%|█████████▏| 8999/9822 [4:01:36<19:54,  1.45s/it] 92%|█████████▏| 9000/9822 [4:01:37<19:51,  1.45s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1331, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1238, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1232, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1278, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1173, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1361, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1134, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1145, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:49:24 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:49:24 - INFO - __main__ - ***** test Results*****
01/08/2024 01:49:24 - INFO - __main__ -   Training step = 9000
01/08/2024 01:49:24 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:49:30 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:49:30 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:49:30 - INFO - __main__ -   Training step = 9000
01/08/2024 01:49:30 - INFO - __main__ -  eval_accuracy:0.8626876601977298 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.863786158916148}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:49:35 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:49:35 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:49:35 - INFO - __main__ -   Training step = 9000
01/08/2024 01:49:35 - INFO - __main__ -  eval_accuracy:0.9124862687660198 
 92%|█████████▏| 9001/9822 [4:01:55<1:29:12,  6.52s/it] 92%|█████████▏| 9002/9822 [4:01:57<1:08:17,  5.00s/it] 92%|█████████▏| 9003/9822 [4:01:58<53:40,  3.93s/it]   92%|█████████▏| 9004/9822 [4:02:00<43:24,  3.18s/it] 92%|█████████▏| 9005/9822 [4:02:01<36:14,  2.66s/it] 92%|█████████▏| 9006/9822 [4:02:03<31:15,  2.30s/it] 92%|█████████▏| 9007/9822 [4:02:04<27:43,  2.04s/it] 92%|█████████▏| 9008/9822 [4:02:05<25:13,  1.86s/it] 92%|█████████▏| 9009/9822 [4:02:07<23:45,  1.75s/it] 92%|█████████▏| 9010/9822 [4:02:08<22:26,  1.66s/it] 92%|█████████▏| 9011/9822 [4:02:10<21:30,  1.59s/it] 92%|█████████▏| 9012/9822 [4:02:11<20:51,  1.55s/it] 92%|█████████▏| 9013/9822 [4:02:13<20:23,  1.51s/it] 92%|█████████▏| 9014/9822 [4:02:14<20:04,  1.49s/it] 92%|█████████▏| 9015/9822 [4:02:16<19:47,  1.47s/it] 92%|█████████▏| 9016/9822 [4:02:17<19:37,  1.46s/it] 92%|█████████▏| 9017/9822 [4:02:18<19:29,  1.45s/it] 92%|█████████▏| 9018/9822 [4:02:20<19:20,  1.44s/it] 92%|█████████▏| 9019/9822 [4:02:21<19:15,  1.44s/it] 92%|█████████▏| 9020/9822 [4:02:23<19:18,  1.44s/it] 92%|█████████▏| 9021/9822 [4:02:24<19:28,  1.46s/it] 92%|█████████▏| 9022/9822 [4:02:26<19:22,  1.45s/it] 92%|█████████▏| 9023/9822 [4:02:27<19:17,  1.45s/it] 92%|█████████▏| 9024/9822 [4:02:29<19:12,  1.44s/it] 92%|█████████▏| 9025/9822 [4:02:30<19:10,  1.44s/it] 92%|█████████▏| 9026/9822 [4:02:31<19:07,  1.44s/it] 92%|█████████▏| 9027/9822 [4:02:33<19:04,  1.44s/it] 92%|█████████▏| 9028/9822 [4:02:34<19:01,  1.44s/it] 92%|█████████▏| 9029/9822 [4:02:36<18:58,  1.44s/it] 92%|█████████▏| 9030/9822 [4:02:37<18:47,  1.42s/it] 92%|█████████▏| 9031/9822 [4:02:39<18:48,  1.43s/it] 92%|█████████▏| 9032/9822 [4:02:40<18:51,  1.43s/it] 92%|█████████▏| 9033/9822 [4:02:41<18:56,  1.44s/it] 92%|█████████▏| 9034/9822 [4:02:43<18:53,  1.44s/it] 92%|█████████▏| 9035/9822 [4:02:44<18:50,  1.44s/it] 92%|█████████▏| 9036/9822 [4:02:46<18:48,  1.44s/it] 92%|█████████▏| 9037/9822 [4:02:47<18:46,  1.44s/it] 92%|█████████▏| 9038/9822 [4:02:49<18:45,  1.44s/it] 92%|█████████▏| 9039/9822 [4:02:50<19:05,  1.46s/it] 92%|█████████▏| 9040/9822 [4:02:52<18:56,  1.45s/it] 92%|█████████▏| 9041/9822 [4:02:53<18:52,  1.45s/it] 92%|█████████▏| 9042/9822 [4:02:54<18:47,  1.45s/it] 92%|█████████▏| 9043/9822 [4:02:56<18:44,  1.44s/it] 92%|█████████▏| 9044/9822 [4:02:57<18:38,  1.44s/it] 92%|█████████▏| 9045/9822 [4:02:59<18:37,  1.44s/it] 92%|█████████▏| 9046/9822 [4:03:00<18:35,  1.44s/it] 92%|█████████▏| 9047/9822 [4:03:02<18:34,  1.44s/it] 92%|█████████▏| 9048/9822 [4:03:03<18:31,  1.44s/it] 92%|█████████▏| 9049/9822 [4:03:05<18:29,  1.44s/it] 92%|█████████▏| 9050/9822 [4:03:06<18:29,  1.44s/it] 92%|█████████▏| 9051/9822 [4:03:07<18:28,  1.44s/it] 92%|█████████▏| 9052/9822 [4:03:09<18:25,  1.44s/it] 92%|█████████▏| 9053/9822 [4:03:10<18:22,  1.43s/it] 92%|█████████▏| 9054/9822 [4:03:12<18:19,  1.43s/it] 92%|█████████▏| 9055/9822 [4:03:13<18:20,  1.43s/it] 92%|█████████▏| 9056/9822 [4:03:15<18:16,  1.43s/it] 92%|█████████▏| 9057/9822 [4:03:16<18:16,  1.43s/it] 92%|█████████▏| 9058/9822 [4:03:17<18:13,  1.43s/it] 92%|█████████▏| 9059/9822 [4:03:19<18:11,  1.43s/it] 92%|█████████▏| 9060/9822 [4:03:20<18:09,  1.43s/it] 92%|█████████▏| 9061/9822 [4:03:22<18:08,  1.43s/it] 92%|█████████▏| 9062/9822 [4:03:23<18:09,  1.43s/it] 92%|█████████▏| 9063/9822 [4:03:25<18:08,  1.43s/it] 92%|█████████▏| 9064/9822 [4:03:26<18:08,  1.44s/it] 92%|█████████▏| 9065/9822 [4:03:27<18:09,  1.44s/it] 92%|█████████▏| 9066/9822 [4:03:29<18:04,  1.43s/it] 92%|█████████▏| 9067/9822 [4:03:30<18:06,  1.44s/it] 92%|█████████▏| 9068/9822 [4:03:32<18:08,  1.44s/it] 92%|█████████▏| 9069/9822 [4:03:33<18:06,  1.44s/it] 92%|█████████▏| 9070/9822 [4:03:35<18:06,  1.44s/it] 92%|█████████▏| 9071/9822 [4:03:36<18:20,  1.47s/it] 92%|█████████▏| 9072/9822 [4:03:38<18:11,  1.46s/it] 92%|█████████▏| 9073/9822 [4:03:39<18:03,  1.45s/it] 92%|█████████▏| 9074/9822 [4:03:41<17:56,  1.44s/it] 92%|█████████▏| 9075/9822 [4:03:42<17:57,  1.44s/it] 92%|█████████▏| 9076/9822 [4:03:43<18:03,  1.45s/it] 92%|█████████▏| 9077/9822 [4:03:45<17:59,  1.45s/it] 92%|█████████▏| 9078/9822 [4:03:46<17:56,  1.45s/it] 92%|█████████▏| 9079/9822 [4:03:48<17:54,  1.45s/it] 92%|█████████▏| 9080/9822 [4:03:49<17:49,  1.44s/it] 92%|█████████▏| 9081/9822 [4:03:51<17:46,  1.44s/it] 92%|█████████▏| 9082/9822 [4:03:52<17:42,  1.44s/it] 92%|█████████▏| 9083/9822 [4:03:53<17:39,  1.43s/it] 92%|█████████▏| 9084/9822 [4:03:55<17:40,  1.44s/it] 92%|█████████▏| 9085/9822 [4:03:56<17:38,  1.44s/it] 93%|█████████▎| 9086/9822 [4:03:58<17:35,  1.43s/it] 93%|█████████▎| 9087/9822 [4:03:59<17:32,  1.43s/it] 93%|█████████▎| 9088/9822 [4:04:01<17:33,  1.44s/it] 93%|█████████▎| 9089/9822 [4:04:02<17:31,  1.44s/it] 93%|█████████▎| 9090/9822 [4:04:04<17:31,  1.44s/it] 93%|█████████▎| 9091/9822 [4:04:05<17:28,  1.43s/it] 93%|█████████▎| 9092/9822 [4:04:06<17:26,  1.43s/it] 93%|█████████▎| 9093/9822 [4:04:08<17:26,  1.44s/it] 93%|█████████▎| 9094/9822 [4:04:09<17:26,  1.44s/it] 93%|█████████▎| 9095/9822 [4:04:11<17:23,  1.44s/it] 93%|█████████▎| 9096/9822 [4:04:12<17:23,  1.44s/it] 93%|█████████▎| 9097/9822 [4:04:14<17:21,  1.44s/it] 93%|█████████▎| 9098/9822 [4:04:15<17:22,  1.44s/it] 93%|█████████▎| 9099/9822 [4:04:16<17:19,  1.44s/it] 93%|█████████▎| 9100/9822 [4:04:18<17:15,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1641, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1270, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1717, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1138, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1000, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0599, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1367, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1586, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0994, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2248, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1675, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1398, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1301, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1583, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:52:05 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:52:05 - INFO - __main__ - ***** test Results*****
01/08/2024 01:52:05 - INFO - __main__ -   Training step = 9100
01/08/2024 01:52:05 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:52:11 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:52:11 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:52:11 - INFO - __main__ -   Training step = 9100
01/08/2024 01:52:11 - INFO - __main__ -  eval_accuracy:0.8652508238740388 
[INFO|tokenization_utils_base.py:2094] 2024-01-08 01:52:11,316 >> tokenizer config file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2024-01-08 01:52:11,317 >> Special tokens file saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/special_tokens_map.json
[INFO|configuration_utils.py:439] 2024-01-08 01:52:11,353 >> Configuration saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/config.json
[INFO|modeling_utils.py:1084] 2024-01-08 01:52:12,981 >> Model weights saved in /mnt/zhanyuliang/data/checkpoint/nlp/lgtm/qnli_output/pytorch_model.bin
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:52:17 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:52:17 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:52:17 - INFO - __main__ -   Training step = 9100
01/08/2024 01:52:17 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 93%|█████████▎| 9101/9822 [4:04:38<1:24:12,  7.01s/it] 93%|█████████▎| 9102/9822 [4:04:39<1:04:18,  5.36s/it] 93%|█████████▎| 9103/9822 [4:04:41<50:07,  4.18s/it]   93%|█████████▎| 9104/9822 [4:04:42<40:12,  3.36s/it] 93%|█████████▎| 9105/9822 [4:04:44<33:16,  2.78s/it] 93%|█████████▎| 9106/9822 [4:04:45<28:22,  2.38s/it] 93%|█████████▎| 9107/9822 [4:04:47<24:59,  2.10s/it] 93%|█████████▎| 9108/9822 [4:04:48<22:33,  1.90s/it] 93%|█████████▎| 9109/9822 [4:04:49<20:51,  1.76s/it] 93%|█████████▎| 9110/9822 [4:04:51<19:40,  1.66s/it] 93%|█████████▎| 9111/9822 [4:04:52<18:49,  1.59s/it] 93%|█████████▎| 9112/9822 [4:04:54<18:17,  1.55s/it] 93%|█████████▎| 9113/9822 [4:04:55<17:53,  1.51s/it] 93%|█████████▎| 9114/9822 [4:04:57<17:33,  1.49s/it] 93%|█████████▎| 9115/9822 [4:04:58<17:20,  1.47s/it] 93%|█████████▎| 9116/9822 [4:04:59<16:59,  1.44s/it] 93%|█████████▎| 9117/9822 [4:05:01<16:53,  1.44s/it] 93%|█████████▎| 9118/9822 [4:05:02<16:51,  1.44s/it] 93%|█████████▎| 9119/9822 [4:05:04<16:49,  1.44s/it] 93%|█████████▎| 9120/9822 [4:05:05<16:44,  1.43s/it] 93%|█████████▎| 9121/9822 [4:05:07<16:44,  1.43s/it] 93%|█████████▎| 9122/9822 [4:05:08<16:43,  1.43s/it] 93%|█████████▎| 9123/9822 [4:05:09<16:39,  1.43s/it] 93%|█████████▎| 9124/9822 [4:05:11<16:40,  1.43s/it] 93%|█████████▎| 9125/9822 [4:05:12<16:37,  1.43s/it] 93%|█████████▎| 9126/9822 [4:05:14<16:35,  1.43s/it] 93%|█████████▎| 9127/9822 [4:05:15<16:34,  1.43s/it] 93%|█████████▎| 9128/9822 [4:05:17<16:35,  1.43s/it] 93%|█████████▎| 9129/9822 [4:05:18<16:34,  1.44s/it] 93%|█████████▎| 9130/9822 [4:05:19<16:32,  1.43s/it] 93%|█████████▎| 9131/9822 [4:05:21<16:30,  1.43s/it] 93%|█████████▎| 9132/9822 [4:05:22<16:29,  1.43s/it] 93%|█████████▎| 9133/9822 [4:05:24<16:44,  1.46s/it] 93%|█████████▎| 9134/9822 [4:05:25<16:36,  1.45s/it] 93%|█████████▎| 9135/9822 [4:05:27<16:30,  1.44s/it] 93%|█████████▎| 9136/9822 [4:05:28<16:28,  1.44s/it] 93%|█████████▎| 9137/9822 [4:05:30<16:24,  1.44s/it] 93%|█████████▎| 9138/9822 [4:05:31<16:20,  1.43s/it] 93%|█████████▎| 9139/9822 [4:05:32<16:18,  1.43s/it] 93%|█████████▎| 9140/9822 [4:05:34<16:18,  1.44s/it] 93%|█████████▎| 9141/9822 [4:05:35<16:19,  1.44s/it] 93%|█████████▎| 9142/9822 [4:05:37<16:15,  1.44s/it] 93%|█████████▎| 9143/9822 [4:05:38<16:12,  1.43s/it] 93%|█████████▎| 9144/9822 [4:05:40<16:08,  1.43s/it] 93%|█████████▎| 9145/9822 [4:05:41<16:08,  1.43s/it] 93%|█████████▎| 9146/9822 [4:05:42<16:07,  1.43s/it] 93%|█████████▎| 9147/9822 [4:05:44<16:07,  1.43s/it] 93%|█████████▎| 9148/9822 [4:05:45<16:08,  1.44s/it] 93%|█████████▎| 9149/9822 [4:05:47<16:06,  1.44s/it] 93%|█████████▎| 9150/9822 [4:05:48<16:04,  1.44s/it] 93%|█████████▎| 9151/9822 [4:05:50<16:04,  1.44s/it] 93%|█████████▎| 9152/9822 [4:05:51<16:00,  1.43s/it] 93%|█████████▎| 9153/9822 [4:05:53<16:02,  1.44s/it] 93%|█████████▎| 9154/9822 [4:05:54<16:03,  1.44s/it] 93%|█████████▎| 9155/9822 [4:05:55<16:01,  1.44s/it] 93%|█████████▎| 9156/9822 [4:05:57<15:57,  1.44s/it] 93%|█████████▎| 9157/9822 [4:05:58<15:58,  1.44s/it] 93%|█████████▎| 9158/9822 [4:06:00<16:12,  1.47s/it] 93%|█████████▎| 9159/9822 [4:06:01<16:04,  1.46s/it] 93%|█████████▎| 9160/9822 [4:06:03<15:59,  1.45s/it] 93%|█████████▎| 9161/9822 [4:06:04<15:56,  1.45s/it] 93%|█████████▎| 9162/9822 [4:06:06<15:52,  1.44s/it] 93%|█████████▎| 9163/9822 [4:06:07<15:48,  1.44s/it] 93%|█████████▎| 9164/9822 [4:06:08<15:45,  1.44s/it] 93%|█████████▎| 9165/9822 [4:06:10<15:45,  1.44s/it] 93%|█████████▎| 9166/9822 [4:06:11<15:42,  1.44s/it] 93%|█████████▎| 9167/9822 [4:06:13<15:43,  1.44s/it] 93%|█████████▎| 9168/9822 [4:06:14<15:42,  1.44s/it] 93%|█████████▎| 9169/9822 [4:06:16<15:38,  1.44s/it] 93%|█████████▎| 9170/9822 [4:06:17<15:36,  1.44s/it] 93%|█████████▎| 9171/9822 [4:06:19<15:36,  1.44s/it] 93%|█████████▎| 9172/9822 [4:06:20<15:33,  1.44s/it] 93%|█████████▎| 9173/9822 [4:06:21<15:30,  1.43s/it] 93%|█████████▎| 9174/9822 [4:06:23<15:29,  1.43s/it] 93%|█████████▎| 9175/9822 [4:06:24<15:27,  1.43s/it] 93%|█████████▎| 9176/9822 [4:06:26<15:25,  1.43s/it] 93%|█████████▎| 9177/9822 [4:06:27<15:23,  1.43s/it] 93%|█████████▎| 9178/9822 [4:06:29<15:24,  1.44s/it] 93%|█████████▎| 9179/9822 [4:06:30<15:22,  1.44s/it] 93%|█████████▎| 9180/9822 [4:06:31<15:22,  1.44s/it] 93%|█████████▎| 9181/9822 [4:06:33<15:18,  1.43s/it] 93%|█████████▎| 9182/9822 [4:06:34<15:16,  1.43s/it] 93%|█████████▎| 9183/9822 [4:06:36<15:16,  1.43s/it] 94%|█████████▎| 9184/9822 [4:06:37<15:17,  1.44s/it] 94%|█████████▎| 9185/9822 [4:06:39<15:16,  1.44s/it] 94%|█████████▎| 9186/9822 [4:06:40<15:14,  1.44s/it] 94%|█████████▎| 9187/9822 [4:06:41<15:12,  1.44s/it] 94%|█████████▎| 9188/9822 [4:06:43<15:08,  1.43s/it] 94%|█████████▎| 9189/9822 [4:06:44<15:08,  1.43s/it] 94%|█████████▎| 9190/9822 [4:06:46<15:23,  1.46s/it] 94%|█████████▎| 9191/9822 [4:06:47<15:24,  1.47s/it] 94%|█████████▎| 9192/9822 [4:06:49<15:18,  1.46s/it] 94%|█████████▎| 9193/9822 [4:06:50<15:12,  1.45s/it] 94%|█████████▎| 9194/9822 [4:06:52<15:07,  1.45s/it] 94%|█████████▎| 9195/9822 [4:06:53<15:04,  1.44s/it] 94%|█████████▎| 9196/9822 [4:06:55<15:01,  1.44s/it] 94%|█████████▎| 9197/9822 [4:06:56<14:58,  1.44s/it] 94%|█████████▎| 9198/9822 [4:06:57<14:56,  1.44s/it] 94%|█████████▎| 9199/9822 [4:06:59<14:53,  1.43s/it] 94%|█████████▎| 9200/9822 [4:07:00<14:51,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1391, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1213, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1061, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1411, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1581, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1598, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1658, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0915, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1265, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.3341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1228, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1277, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1537, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1026, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1269, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1548, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0953, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1357, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1050, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:54:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:54:47 - INFO - __main__ - ***** test Results*****
01/08/2024 01:54:47 - INFO - __main__ -   Training step = 9200
01/08/2024 01:54:47 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:54:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:54:53 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:54:53 - INFO - __main__ -   Training step = 9200
01/08/2024 01:54:53 - INFO - __main__ -  eval_accuracy:0.8634199926766752 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:54:58 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:54:58 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:54:58 - INFO - __main__ -   Training step = 9200
01/08/2024 01:54:58 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 94%|█████████▎| 9201/9822 [4:07:19<1:07:23,  6.51s/it] 94%|█████████▎| 9202/9822 [4:07:20<51:22,  4.97s/it]   94%|█████████▎| 9203/9822 [4:07:21<40:22,  3.91s/it] 94%|█████████▎| 9204/9822 [4:07:23<32:38,  3.17s/it] 94%|█████████▎| 9205/9822 [4:07:24<27:13,  2.65s/it] 94%|█████████▎| 9206/9822 [4:07:26<23:27,  2.29s/it] 94%|█████████▎| 9207/9822 [4:07:27<20:52,  2.04s/it] 94%|█████████▎| 9208/9822 [4:07:29<19:00,  1.86s/it] 94%|█████████▍| 9209/9822 [4:07:30<17:42,  1.73s/it] 94%|█████████▍| 9210/9822 [4:07:31<16:45,  1.64s/it] 94%|█████████▍| 9211/9822 [4:07:33<16:05,  1.58s/it] 94%|█████████▍| 9212/9822 [4:07:34<15:36,  1.54s/it] 94%|█████████▍| 9213/9822 [4:07:36<15:16,  1.50s/it] 94%|█████████▍| 9214/9822 [4:07:37<15:00,  1.48s/it] 94%|█████████▍| 9215/9822 [4:07:39<14:50,  1.47s/it] 94%|█████████▍| 9216/9822 [4:07:40<14:42,  1.46s/it] 94%|█████████▍| 9217/9822 [4:07:42<14:39,  1.45s/it] 94%|█████████▍| 9218/9822 [4:07:43<14:40,  1.46s/it] 94%|█████████▍| 9219/9822 [4:07:44<14:37,  1.46s/it] 94%|█████████▍| 9220/9822 [4:07:46<14:31,  1.45s/it] 94%|█████████▍| 9221/9822 [4:07:47<14:29,  1.45s/it] 94%|█████████▍| 9222/9822 [4:07:49<14:24,  1.44s/it] 94%|█████████▍| 9223/9822 [4:07:50<14:19,  1.43s/it] 94%|█████████▍| 9224/9822 [4:07:52<14:17,  1.43s/it] 94%|█████████▍| 9225/9822 [4:07:53<14:17,  1.44s/it] 94%|█████████▍| 9226/9822 [4:07:54<14:19,  1.44s/it] 94%|█████████▍| 9227/9822 [4:07:56<14:25,  1.45s/it] 94%|█████████▍| 9228/9822 [4:07:57<14:34,  1.47s/it] 94%|█████████▍| 9229/9822 [4:07:59<14:26,  1.46s/it] 94%|█████████▍| 9230/9822 [4:08:00<14:17,  1.45s/it] 94%|█████████▍| 9231/9822 [4:08:02<14:14,  1.45s/it] 94%|█████████▍| 9232/9822 [4:08:03<14:10,  1.44s/it] 94%|█████████▍| 9233/9822 [4:08:05<14:08,  1.44s/it] 94%|█████████▍| 9234/9822 [4:08:06<14:07,  1.44s/it] 94%|█████████▍| 9235/9822 [4:08:08<14:04,  1.44s/it] 94%|█████████▍| 9236/9822 [4:08:09<14:02,  1.44s/it] 94%|█████████▍| 9237/9822 [4:08:10<14:00,  1.44s/it] 94%|█████████▍| 9238/9822 [4:08:12<13:59,  1.44s/it] 94%|█████████▍| 9239/9822 [4:08:13<13:55,  1.43s/it] 94%|█████████▍| 9240/9822 [4:08:15<13:54,  1.43s/it] 94%|█████████▍| 9241/9822 [4:08:16<13:55,  1.44s/it] 94%|█████████▍| 9242/9822 [4:08:18<13:53,  1.44s/it] 94%|█████████▍| 9243/9822 [4:08:19<13:50,  1.43s/it] 94%|█████████▍| 9244/9822 [4:08:20<13:47,  1.43s/it] 94%|█████████▍| 9245/9822 [4:08:22<13:46,  1.43s/it] 94%|█████████▍| 9246/9822 [4:08:23<13:46,  1.43s/it] 94%|█████████▍| 9247/9822 [4:08:25<13:48,  1.44s/it] 94%|█████████▍| 9248/9822 [4:08:26<13:47,  1.44s/it] 94%|█████████▍| 9249/9822 [4:08:28<13:45,  1.44s/it] 94%|█████████▍| 9250/9822 [4:08:29<13:42,  1.44s/it] 94%|█████████▍| 9251/9822 [4:08:31<13:41,  1.44s/it] 94%|█████████▍| 9252/9822 [4:08:32<13:40,  1.44s/it] 94%|█████████▍| 9253/9822 [4:08:33<13:36,  1.44s/it] 94%|█████████▍| 9254/9822 [4:08:35<13:36,  1.44s/it] 94%|█████████▍| 9255/9822 [4:08:36<13:39,  1.45s/it] 94%|█████████▍| 9256/9822 [4:08:38<13:36,  1.44s/it] 94%|█████████▍| 9257/9822 [4:08:39<13:32,  1.44s/it] 94%|█████████▍| 9258/9822 [4:08:41<13:30,  1.44s/it] 94%|█████████▍| 9259/9822 [4:08:42<13:28,  1.44s/it] 94%|█████████▍| 9260/9822 [4:08:44<13:44,  1.47s/it] 94%|█████████▍| 9261/9822 [4:08:45<13:37,  1.46s/it] 94%|█████████▍| 9262/9822 [4:08:46<13:33,  1.45s/it] 94%|█████████▍| 9263/9822 [4:08:48<13:30,  1.45s/it] 94%|█████████▍| 9264/9822 [4:08:49<13:28,  1.45s/it] 94%|█████████▍| 9265/9822 [4:08:51<13:28,  1.45s/it] 94%|█████████▍| 9266/9822 [4:08:52<13:25,  1.45s/it] 94%|█████████▍| 9267/9822 [4:08:54<13:22,  1.45s/it] 94%|█████████▍| 9268/9822 [4:08:55<13:19,  1.44s/it] 94%|█████████▍| 9269/9822 [4:08:57<13:15,  1.44s/it] 94%|█████████▍| 9270/9822 [4:08:58<13:14,  1.44s/it] 94%|█████████▍| 9271/9822 [4:08:59<13:13,  1.44s/it] 94%|█████████▍| 9272/9822 [4:09:01<13:12,  1.44s/it] 94%|█████████▍| 9273/9822 [4:09:02<13:09,  1.44s/it] 94%|█████████▍| 9274/9822 [4:09:04<13:07,  1.44s/it] 94%|█████████▍| 9275/9822 [4:09:05<13:03,  1.43s/it] 94%|█████████▍| 9276/9822 [4:09:07<13:01,  1.43s/it] 94%|█████████▍| 9277/9822 [4:09:08<12:59,  1.43s/it] 94%|█████████▍| 9278/9822 [4:09:09<12:58,  1.43s/it] 94%|█████████▍| 9279/9822 [4:09:11<13:00,  1.44s/it] 94%|█████████▍| 9280/9822 [4:09:12<12:58,  1.44s/it] 94%|█████████▍| 9281/9822 [4:09:14<12:59,  1.44s/it] 95%|█████████▍| 9282/9822 [4:09:15<12:57,  1.44s/it] 95%|█████████▍| 9283/9822 [4:09:17<12:54,  1.44s/it] 95%|█████████▍| 9284/9822 [4:09:18<12:52,  1.44s/it] 95%|█████████▍| 9285/9822 [4:09:20<13:04,  1.46s/it] 95%|█████████▍| 9286/9822 [4:09:21<13:00,  1.46s/it] 95%|█████████▍| 9287/9822 [4:09:22<12:57,  1.45s/it] 95%|█████████▍| 9288/9822 [4:09:24<12:48,  1.44s/it] 95%|█████████▍| 9289/9822 [4:09:25<12:44,  1.43s/it] 95%|█████████▍| 9290/9822 [4:09:27<12:41,  1.43s/it] 95%|█████████▍| 9291/9822 [4:09:28<12:39,  1.43s/it] 95%|█████████▍| 9292/9822 [4:09:30<12:39,  1.43s/it] 95%|█████████▍| 9293/9822 [4:09:31<12:38,  1.43s/it] 95%|█████████▍| 9294/9822 [4:09:32<12:36,  1.43s/it] 95%|█████████▍| 9295/9822 [4:09:34<12:37,  1.44s/it] 95%|█████████▍| 9296/9822 [4:09:35<12:33,  1.43s/it] 95%|█████████▍| 9297/9822 [4:09:37<12:32,  1.43s/it] 95%|█████████▍| 9298/9822 [4:09:38<12:29,  1.43s/it] 95%|█████████▍| 9299/9822 [4:09:40<12:28,  1.43s/it] 95%|█████████▍| 9300/9822 [4:09:41<12:26,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0686, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1217, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1741, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1351, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1328, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0998, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1733, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1570, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1383, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 01:57:28 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:57:28 - INFO - __main__ - ***** test Results*****
01/08/2024 01:57:28 - INFO - __main__ -   Training step = 9300
01/08/2024 01:57:28 - INFO - __main__ -  test_accuracy:0.8803074670571011 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:57:34 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:57:34 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 01:57:34 - INFO - __main__ -   Training step = 9300
01/08/2024 01:57:34 - INFO - __main__ -  eval_accuracy:0.8615891614793116 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 01:57:39 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 01:57:39 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 01:57:39 - INFO - __main__ -   Training step = 9300
01/08/2024 01:57:39 - INFO - __main__ -  eval_accuracy:0.9128524350054925 
 95%|█████████▍| 9301/9822 [4:09:59<56:34,  6.51s/it] 95%|█████████▍| 9302/9822 [4:10:01<43:17,  5.00s/it] 95%|█████████▍| 9303/9822 [4:10:02<33:57,  3.93s/it] 95%|█████████▍| 9304/9822 [4:10:04<27:25,  3.18s/it] 95%|█████████▍| 9305/9822 [4:10:05<22:51,  2.65s/it] 95%|█████████▍| 9306/9822 [4:10:07<19:41,  2.29s/it] 95%|█████████▍| 9307/9822 [4:10:08<17:30,  2.04s/it] 95%|█████████▍| 9308/9822 [4:10:10<15:55,  1.86s/it] 95%|█████████▍| 9309/9822 [4:10:11<14:49,  1.73s/it] 95%|█████████▍| 9310/9822 [4:10:12<14:02,  1.65s/it] 95%|█████████▍| 9311/9822 [4:10:14<13:28,  1.58s/it] 95%|█████████▍| 9312/9822 [4:10:15<13:16,  1.56s/it] 95%|█████████▍| 9313/9822 [4:10:17<12:56,  1.52s/it] 95%|█████████▍| 9314/9822 [4:10:18<12:40,  1.50s/it] 95%|█████████▍| 9315/9822 [4:10:20<12:30,  1.48s/it] 95%|█████████▍| 9316/9822 [4:10:21<12:21,  1.47s/it] 95%|█████████▍| 9317/9822 [4:10:23<12:17,  1.46s/it] 95%|█████████▍| 9318/9822 [4:10:24<12:17,  1.46s/it] 95%|█████████▍| 9319/9822 [4:10:25<12:13,  1.46s/it] 95%|█████████▍| 9320/9822 [4:10:27<12:08,  1.45s/it] 95%|█████████▍| 9321/9822 [4:10:28<12:07,  1.45s/it] 95%|█████████▍| 9322/9822 [4:10:30<12:02,  1.44s/it] 95%|█████████▍| 9323/9822 [4:10:31<11:59,  1.44s/it] 95%|█████████▍| 9324/9822 [4:10:33<11:56,  1.44s/it] 95%|█████████▍| 9325/9822 [4:10:34<11:54,  1.44s/it] 95%|█████████▍| 9326/9822 [4:10:35<11:51,  1.44s/it] 95%|█████████▍| 9327/9822 [4:10:37<11:50,  1.44s/it] 95%|█████████▍| 9328/9822 [4:10:38<11:49,  1.44s/it] 95%|█████████▍| 9329/9822 [4:10:40<11:48,  1.44s/it] 95%|█████████▍| 9330/9822 [4:10:41<11:45,  1.43s/it] 95%|█████████▌| 9331/9822 [4:10:43<11:45,  1.44s/it] 95%|█████████▌| 9332/9822 [4:10:44<11:43,  1.44s/it] 95%|█████████▌| 9333/9822 [4:10:46<11:42,  1.44s/it] 95%|█████████▌| 9334/9822 [4:10:47<11:39,  1.43s/it] 95%|█████████▌| 9335/9822 [4:10:48<11:40,  1.44s/it] 95%|█████████▌| 9336/9822 [4:10:50<11:39,  1.44s/it] 95%|█████████▌| 9337/9822 [4:10:51<11:36,  1.44s/it] 95%|█████████▌| 9338/9822 [4:10:53<11:34,  1.43s/it] 95%|█████████▌| 9339/9822 [4:10:54<11:39,  1.45s/it] 95%|█████████▌| 9340/9822 [4:10:56<11:34,  1.44s/it] 95%|█████████▌| 9341/9822 [4:10:57<11:35,  1.45s/it] 95%|█████████▌| 9342/9822 [4:10:59<11:34,  1.45s/it] 95%|█████████▌| 9343/9822 [4:11:00<11:31,  1.44s/it] 95%|█████████▌| 9344/9822 [4:11:02<11:41,  1.47s/it] 95%|█████████▌| 9345/9822 [4:11:03<11:35,  1.46s/it] 95%|█████████▌| 9346/9822 [4:11:04<11:31,  1.45s/it] 95%|█████████▌| 9347/9822 [4:11:06<11:27,  1.45s/it] 95%|█████████▌| 9348/9822 [4:11:07<11:24,  1.44s/it] 95%|█████████▌| 9349/9822 [4:11:09<11:23,  1.45s/it] 95%|█████████▌| 9350/9822 [4:11:10<11:22,  1.45s/it] 95%|█████████▌| 9351/9822 [4:11:12<11:20,  1.44s/it] 95%|█████████▌| 9352/9822 [4:11:13<11:16,  1.44s/it] 95%|█████████▌| 9353/9822 [4:11:14<11:14,  1.44s/it] 95%|█████████▌| 9354/9822 [4:11:16<11:13,  1.44s/it] 95%|█████████▌| 9355/9822 [4:11:17<11:19,  1.45s/it] 95%|█████████▌| 9356/9822 [4:11:19<11:23,  1.47s/it] 95%|█████████▌| 9357/9822 [4:11:20<11:19,  1.46s/it] 95%|█████████▌| 9358/9822 [4:11:22<11:16,  1.46s/it] 95%|█████████▌| 9359/9822 [4:11:23<11:13,  1.46s/it] 95%|█████████▌| 9360/9822 [4:11:25<11:11,  1.45s/it] 95%|█████████▌| 9361/9822 [4:11:26<11:09,  1.45s/it] 95%|█████████▌| 9362/9822 [4:11:28<11:07,  1.45s/it] 95%|█████████▌| 9363/9822 [4:11:29<11:05,  1.45s/it] 95%|█████████▌| 9364/9822 [4:11:30<11:03,  1.45s/it] 95%|█████████▌| 9365/9822 [4:11:32<11:05,  1.46s/it] 95%|█████████▌| 9366/9822 [4:11:33<11:04,  1.46s/it] 95%|█████████▌| 9367/9822 [4:11:35<11:00,  1.45s/it] 95%|█████████▌| 9368/9822 [4:11:36<10:58,  1.45s/it] 95%|█████████▌| 9369/9822 [4:11:38<11:02,  1.46s/it] 95%|█████████▌| 9370/9822 [4:11:39<11:03,  1.47s/it] 95%|█████████▌| 9371/9822 [4:11:41<10:58,  1.46s/it] 95%|█████████▌| 9372/9822 [4:11:42<10:55,  1.46s/it] 95%|█████████▌| 9373/9822 [4:11:44<10:53,  1.45s/it] 95%|█████████▌| 9374/9822 [4:11:45<10:52,  1.46s/it] 95%|█████████▌| 9375/9822 [4:11:46<10:48,  1.45s/it] 95%|█████████▌| 9376/9822 [4:11:48<10:42,  1.44s/it] 95%|█████████▌| 9377/9822 [4:11:49<10:40,  1.44s/it] 95%|█████████▌| 9378/9822 [4:11:51<10:39,  1.44s/it] 95%|█████████▌| 9379/9822 [4:11:52<10:36,  1.44s/it] 95%|█████████▌| 9380/9822 [4:11:54<10:32,  1.43s/it] 96%|█████████▌| 9381/9822 [4:11:55<10:31,  1.43s/it] 96%|█████████▌| 9382/9822 [4:11:57<10:32,  1.44s/it] 96%|█████████▌| 9383/9822 [4:11:58<10:30,  1.44s/it] 96%|█████████▌| 9384/9822 [4:11:59<10:30,  1.44s/it] 96%|█████████▌| 9385/9822 [4:12:01<10:27,  1.44s/it] 96%|█████████▌| 9386/9822 [4:12:02<10:24,  1.43s/it] 96%|█████████▌| 9387/9822 [4:12:04<10:22,  1.43s/it] 96%|█████████▌| 9388/9822 [4:12:05<10:20,  1.43s/it] 96%|█████████▌| 9389/9822 [4:12:07<10:18,  1.43s/it] 96%|█████████▌| 9390/9822 [4:12:08<10:16,  1.43s/it] 96%|█████████▌| 9391/9822 [4:12:09<10:16,  1.43s/it] 96%|█████████▌| 9392/9822 [4:12:11<10:17,  1.44s/it] 96%|█████████▌| 9393/9822 [4:12:12<10:15,  1.44s/it] 96%|█████████▌| 9394/9822 [4:12:14<10:12,  1.43s/it] 96%|█████████▌| 9395/9822 [4:12:15<10:12,  1.43s/it] 96%|█████████▌| 9396/9822 [4:12:17<10:09,  1.43s/it] 96%|█████████▌| 9397/9822 [4:12:18<10:07,  1.43s/it] 96%|█████████▌| 9398/9822 [4:12:19<10:06,  1.43s/it] 96%|█████████▌| 9399/9822 [4:12:21<10:06,  1.43s/it] 96%|█████████▌| 9400/9822 [4:12:22<10:05,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1895, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1234, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1422, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1514, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1695, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1791, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0697, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1186, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1410, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1652, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1191, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1358, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1751, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1258, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1038, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1384, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0937, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1633, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1792, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1155, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1542, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0939, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1264, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:00:09 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:00:09 - INFO - __main__ - ***** test Results*****
01/08/2024 02:00:09 - INFO - __main__ -   Training step = 9400
01/08/2024 02:00:09 - INFO - __main__ -  test_accuracy:0.8792093704245973 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:00:15 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:00:15 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:00:15 - INFO - __main__ -   Training step = 9400
01/08/2024 02:00:15 - INFO - __main__ -  eval_accuracy:0.8615891614793116 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:00:20 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:00:20 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:00:20 - INFO - __main__ -   Training step = 9400
01/08/2024 02:00:20 - INFO - __main__ -  eval_accuracy:0.9117539362870744 
 96%|█████████▌| 9401/9822 [4:12:41<45:41,  6.51s/it] 96%|█████████▌| 9402/9822 [4:12:42<34:54,  4.99s/it] 96%|█████████▌| 9403/9822 [4:12:44<27:22,  3.92s/it] 96%|█████████▌| 9404/9822 [4:12:45<22:17,  3.20s/it] 96%|█████████▌| 9405/9822 [4:12:46<18:33,  2.67s/it] 96%|█████████▌| 9406/9822 [4:12:48<15:55,  2.30s/it] 96%|█████████▌| 9407/9822 [4:12:49<14:04,  2.04s/it] 96%|█████████▌| 9408/9822 [4:12:51<12:49,  1.86s/it] 96%|█████████▌| 9409/9822 [4:12:52<11:54,  1.73s/it] 96%|█████████▌| 9410/9822 [4:12:54<11:15,  1.64s/it] 96%|█████████▌| 9411/9822 [4:12:55<10:48,  1.58s/it] 96%|█████████▌| 9412/9822 [4:12:57<10:29,  1.54s/it] 96%|█████████▌| 9413/9822 [4:12:58<10:16,  1.51s/it] 96%|█████████▌| 9414/9822 [4:12:59<10:05,  1.48s/it] 96%|█████████▌| 9415/9822 [4:13:01<09:58,  1.47s/it] 96%|█████████▌| 9416/9822 [4:13:02<09:56,  1.47s/it] 96%|█████████▌| 9417/9822 [4:13:04<09:50,  1.46s/it] 96%|█████████▌| 9418/9822 [4:13:05<09:46,  1.45s/it] 96%|█████████▌| 9419/9822 [4:13:07<09:43,  1.45s/it] 96%|█████████▌| 9420/9822 [4:13:08<09:41,  1.45s/it] 96%|█████████▌| 9421/9822 [4:13:10<09:43,  1.45s/it] 96%|█████████▌| 9422/9822 [4:13:11<09:40,  1.45s/it] 96%|█████████▌| 9423/9822 [4:13:12<09:38,  1.45s/it] 96%|█████████▌| 9424/9822 [4:13:14<09:38,  1.45s/it] 96%|█████████▌| 9425/9822 [4:13:15<09:38,  1.46s/it] 96%|█████████▌| 9426/9822 [4:13:17<09:34,  1.45s/it] 96%|█████████▌| 9427/9822 [4:13:18<09:31,  1.45s/it] 96%|█████████▌| 9428/9822 [4:13:20<09:28,  1.44s/it] 96%|█████████▌| 9429/9822 [4:13:21<09:27,  1.45s/it] 96%|█████████▌| 9430/9822 [4:13:23<09:31,  1.46s/it] 96%|█████████▌| 9431/9822 [4:13:24<09:28,  1.45s/it] 96%|█████████▌| 9432/9822 [4:13:25<09:25,  1.45s/it] 96%|█████████▌| 9433/9822 [4:13:27<09:21,  1.44s/it] 96%|█████████▌| 9434/9822 [4:13:28<09:19,  1.44s/it] 96%|█████████▌| 9435/9822 [4:13:30<09:17,  1.44s/it] 96%|█████████▌| 9436/9822 [4:13:31<09:24,  1.46s/it] 96%|█████████▌| 9437/9822 [4:13:33<09:19,  1.45s/it] 96%|█████████▌| 9438/9822 [4:13:34<09:14,  1.44s/it] 96%|█████████▌| 9439/9822 [4:13:36<09:10,  1.44s/it] 96%|█████████▌| 9440/9822 [4:13:37<09:08,  1.44s/it] 96%|█████████▌| 9441/9822 [4:13:38<09:07,  1.44s/it] 96%|█████████▌| 9442/9822 [4:13:40<09:06,  1.44s/it] 96%|█████████▌| 9443/9822 [4:13:41<09:04,  1.44s/it] 96%|█████████▌| 9444/9822 [4:13:43<09:03,  1.44s/it] 96%|█████████▌| 9445/9822 [4:13:44<09:01,  1.44s/it] 96%|█████████▌| 9446/9822 [4:13:46<08:58,  1.43s/it] 96%|█████████▌| 9447/9822 [4:13:47<08:56,  1.43s/it] 96%|█████████▌| 9448/9822 [4:13:48<08:56,  1.43s/it] 96%|█████████▌| 9449/9822 [4:13:50<08:57,  1.44s/it] 96%|█████████▌| 9450/9822 [4:13:51<08:55,  1.44s/it] 96%|█████████▌| 9451/9822 [4:13:53<08:54,  1.44s/it] 96%|█████████▌| 9452/9822 [4:13:54<08:50,  1.43s/it] 96%|█████████▌| 9453/9822 [4:13:56<08:49,  1.43s/it] 96%|█████████▋| 9454/9822 [4:13:57<08:48,  1.44s/it] 96%|█████████▋| 9455/9822 [4:13:59<08:47,  1.44s/it] 96%|█████████▋| 9456/9822 [4:14:00<08:46,  1.44s/it] 96%|█████████▋| 9457/9822 [4:14:01<08:44,  1.44s/it] 96%|█████████▋| 9458/9822 [4:14:03<08:42,  1.44s/it] 96%|█████████▋| 9459/9822 [4:14:04<08:41,  1.44s/it] 96%|█████████▋| 9460/9822 [4:14:06<08:33,  1.42s/it] 96%|█████████▋| 9461/9822 [4:14:07<08:34,  1.42s/it] 96%|█████████▋| 9462/9822 [4:14:09<08:35,  1.43s/it] 96%|█████████▋| 9463/9822 [4:14:10<08:33,  1.43s/it] 96%|█████████▋| 9464/9822 [4:14:11<08:32,  1.43s/it] 96%|█████████▋| 9465/9822 [4:14:13<08:31,  1.43s/it] 96%|█████████▋| 9466/9822 [4:14:14<08:38,  1.46s/it] 96%|█████████▋| 9467/9822 [4:14:16<08:38,  1.46s/it] 96%|█████████▋| 9468/9822 [4:14:17<08:33,  1.45s/it] 96%|█████████▋| 9469/9822 [4:14:19<08:30,  1.45s/it] 96%|█████████▋| 9470/9822 [4:14:20<08:28,  1.45s/it] 96%|█████████▋| 9471/9822 [4:14:22<08:27,  1.45s/it] 96%|█████████▋| 9472/9822 [4:14:23<08:25,  1.44s/it] 96%|█████████▋| 9473/9822 [4:14:24<08:23,  1.44s/it] 96%|█████████▋| 9474/9822 [4:14:26<08:20,  1.44s/it] 96%|█████████▋| 9475/9822 [4:14:27<08:17,  1.44s/it] 96%|█████████▋| 9476/9822 [4:14:29<08:16,  1.43s/it] 96%|█████████▋| 9477/9822 [4:14:30<08:14,  1.43s/it] 96%|█████████▋| 9478/9822 [4:14:32<08:13,  1.43s/it] 97%|█████████▋| 9479/9822 [4:14:33<08:11,  1.43s/it] 97%|█████████▋| 9480/9822 [4:14:34<08:09,  1.43s/it] 97%|█████████▋| 9481/9822 [4:14:36<08:07,  1.43s/it] 97%|█████████▋| 9482/9822 [4:14:37<08:05,  1.43s/it] 97%|█████████▋| 9483/9822 [4:14:39<08:03,  1.43s/it] 97%|█████████▋| 9484/9822 [4:14:40<08:01,  1.42s/it] 97%|█████████▋| 9485/9822 [4:14:42<08:01,  1.43s/it] 97%|█████████▋| 9486/9822 [4:14:43<07:59,  1.43s/it] 97%|█████████▋| 9487/9822 [4:14:44<07:57,  1.42s/it] 97%|█████████▋| 9488/9822 [4:14:46<07:57,  1.43s/it] 97%|█████████▋| 9489/9822 [4:14:47<07:56,  1.43s/it] 97%|█████████▋| 9490/9822 [4:14:49<07:54,  1.43s/it] 97%|█████████▋| 9491/9822 [4:14:50<07:53,  1.43s/it] 97%|█████████▋| 9492/9822 [4:14:52<07:51,  1.43s/it] 97%|█████████▋| 9493/9822 [4:14:53<07:50,  1.43s/it] 97%|█████████▋| 9494/9822 [4:14:54<07:47,  1.43s/it] 97%|█████████▋| 9495/9822 [4:14:56<07:46,  1.43s/it] 97%|█████████▋| 9496/9822 [4:14:57<07:45,  1.43s/it] 97%|█████████▋| 9497/9822 [4:14:59<07:45,  1.43s/it] 97%|█████████▋| 9498/9822 [4:15:00<07:52,  1.46s/it] 97%|█████████▋| 9499/9822 [4:15:02<07:48,  1.45s/it] 97%|█████████▋| 9500/9822 [4:15:03<07:44,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1462, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1223, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1100, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1375, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1424, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1805, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1210, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1117, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1085, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1036, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1322, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1528, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0990, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1428, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0911, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1239, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0933, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1851, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1438, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0703, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0950, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1201, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:02:50 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:02:50 - INFO - __main__ - ***** test Results*****
01/08/2024 02:02:50 - INFO - __main__ -   Training step = 9500
01/08/2024 02:02:50 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:02:56 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:02:56 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:02:56 - INFO - __main__ -   Training step = 9500
01/08/2024 02:02:56 - INFO - __main__ -  eval_accuracy:0.863786158916148 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:03:01 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:03:01 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:03:01 - INFO - __main__ -   Training step = 9500
01/08/2024 02:03:01 - INFO - __main__ -  eval_accuracy:0.913584767484438 
 97%|█████████▋| 9501/9822 [4:15:21<34:51,  6.52s/it] 97%|█████████▋| 9502/9822 [4:15:23<26:36,  4.99s/it] 97%|█████████▋| 9503/9822 [4:15:24<20:51,  3.92s/it] 97%|█████████▋| 9504/9822 [4:15:26<16:50,  3.18s/it] 97%|█████████▋| 9505/9822 [4:15:27<14:08,  2.68s/it] 97%|█████████▋| 9506/9822 [4:15:29<12:10,  2.31s/it] 97%|█████████▋| 9507/9822 [4:15:30<10:49,  2.06s/it] 97%|█████████▋| 9508/9822 [4:15:32<09:49,  1.88s/it] 97%|█████████▋| 9509/9822 [4:15:33<09:07,  1.75s/it] 97%|█████████▋| 9510/9822 [4:15:35<08:36,  1.66s/it] 97%|█████████▋| 9511/9822 [4:15:36<08:16,  1.60s/it] 97%|█████████▋| 9512/9822 [4:15:37<08:01,  1.55s/it] 97%|█████████▋| 9513/9822 [4:15:39<07:54,  1.53s/it] 97%|█████████▋| 9514/9822 [4:15:40<07:43,  1.51s/it] 97%|█████████▋| 9515/9822 [4:15:42<07:37,  1.49s/it] 97%|█████████▋| 9516/9822 [4:15:43<07:31,  1.48s/it] 97%|█████████▋| 9517/9822 [4:15:45<07:27,  1.47s/it] 97%|█████████▋| 9518/9822 [4:15:46<07:24,  1.46s/it] 97%|█████████▋| 9519/9822 [4:15:48<07:21,  1.46s/it] 97%|█████████▋| 9520/9822 [4:15:49<07:19,  1.46s/it] 97%|█████████▋| 9521/9822 [4:15:51<07:16,  1.45s/it] 97%|█████████▋| 9522/9822 [4:15:52<07:14,  1.45s/it] 97%|█████████▋| 9523/9822 [4:15:53<07:12,  1.45s/it] 97%|█████████▋| 9524/9822 [4:15:55<07:10,  1.45s/it] 97%|█████████▋| 9525/9822 [4:15:56<07:08,  1.44s/it] 97%|█████████▋| 9526/9822 [4:15:58<07:07,  1.44s/it] 97%|█████████▋| 9527/9822 [4:15:59<07:06,  1.44s/it] 97%|█████████▋| 9528/9822 [4:16:01<07:05,  1.45s/it] 97%|█████████▋| 9529/9822 [4:16:02<07:03,  1.44s/it] 97%|█████████▋| 9530/9822 [4:16:04<07:01,  1.44s/it] 97%|█████████▋| 9531/9822 [4:16:05<06:59,  1.44s/it] 97%|█████████▋| 9532/9822 [4:16:06<06:57,  1.44s/it] 97%|█████████▋| 9533/9822 [4:16:08<06:55,  1.44s/it] 97%|█████████▋| 9534/9822 [4:16:09<07:02,  1.47s/it] 97%|█████████▋| 9535/9822 [4:16:11<06:57,  1.46s/it] 97%|█████████▋| 9536/9822 [4:16:12<06:54,  1.45s/it] 97%|█████████▋| 9537/9822 [4:16:14<06:52,  1.45s/it] 97%|█████████▋| 9538/9822 [4:16:15<06:50,  1.44s/it] 97%|█████████▋| 9539/9822 [4:16:17<06:47,  1.44s/it] 97%|█████████▋| 9540/9822 [4:16:18<06:44,  1.44s/it] 97%|█████████▋| 9541/9822 [4:16:19<06:43,  1.43s/it] 97%|█████████▋| 9542/9822 [4:16:21<06:42,  1.44s/it] 97%|█████████▋| 9543/9822 [4:16:22<06:40,  1.44s/it] 97%|█████████▋| 9544/9822 [4:16:24<06:40,  1.44s/it] 97%|█████████▋| 9545/9822 [4:16:25<06:38,  1.44s/it] 97%|█████████▋| 9546/9822 [4:16:27<06:32,  1.42s/it] 97%|█████████▋| 9547/9822 [4:16:28<06:32,  1.43s/it] 97%|█████████▋| 9548/9822 [4:16:29<06:32,  1.43s/it] 97%|█████████▋| 9549/9822 [4:16:31<06:31,  1.43s/it] 97%|█████████▋| 9550/9822 [4:16:32<06:32,  1.44s/it] 97%|█████████▋| 9551/9822 [4:16:34<06:35,  1.46s/it] 97%|█████████▋| 9552/9822 [4:16:35<06:33,  1.46s/it] 97%|█████████▋| 9553/9822 [4:16:37<06:31,  1.45s/it] 97%|█████████▋| 9554/9822 [4:16:38<06:28,  1.45s/it] 97%|█████████▋| 9555/9822 [4:16:40<06:26,  1.45s/it] 97%|█████████▋| 9556/9822 [4:16:41<06:23,  1.44s/it] 97%|█████████▋| 9557/9822 [4:16:42<06:21,  1.44s/it] 97%|█████████▋| 9558/9822 [4:16:44<06:19,  1.44s/it] 97%|█████████▋| 9559/9822 [4:16:45<06:18,  1.44s/it] 97%|█████████▋| 9560/9822 [4:16:47<06:17,  1.44s/it] 97%|█████████▋| 9561/9822 [4:16:48<06:15,  1.44s/it] 97%|█████████▋| 9562/9822 [4:16:50<06:14,  1.44s/it] 97%|█████████▋| 9563/9822 [4:16:51<06:12,  1.44s/it] 97%|█████████▋| 9564/9822 [4:16:53<06:11,  1.44s/it] 97%|█████████▋| 9565/9822 [4:16:54<06:15,  1.46s/it] 97%|█████████▋| 9566/9822 [4:16:55<06:12,  1.46s/it] 97%|█████████▋| 9567/9822 [4:16:57<06:10,  1.45s/it] 97%|█████████▋| 9568/9822 [4:16:58<06:08,  1.45s/it] 97%|█████████▋| 9569/9822 [4:17:00<06:06,  1.45s/it] 97%|█████████▋| 9570/9822 [4:17:01<06:04,  1.44s/it] 97%|█████████▋| 9571/9822 [4:17:03<06:01,  1.44s/it] 97%|█████████▋| 9572/9822 [4:17:04<06:00,  1.44s/it] 97%|█████████▋| 9573/9822 [4:17:06<05:58,  1.44s/it] 97%|█████████▋| 9574/9822 [4:17:07<05:56,  1.44s/it] 97%|█████████▋| 9575/9822 [4:17:08<05:54,  1.44s/it] 97%|█████████▋| 9576/9822 [4:17:10<05:52,  1.43s/it] 98%|█████████▊| 9577/9822 [4:17:11<05:52,  1.44s/it] 98%|█████████▊| 9578/9822 [4:17:13<05:50,  1.44s/it] 98%|█████████▊| 9579/9822 [4:17:14<05:48,  1.44s/it] 98%|█████████▊| 9580/9822 [4:17:16<05:46,  1.43s/it] 98%|█████████▊| 9581/9822 [4:17:17<05:44,  1.43s/it] 98%|█████████▊| 9582/9822 [4:17:18<05:43,  1.43s/it] 98%|█████████▊| 9583/9822 [4:17:20<05:44,  1.44s/it] 98%|█████████▊| 9584/9822 [4:17:21<05:45,  1.45s/it] 98%|█████████▊| 9585/9822 [4:17:23<05:41,  1.44s/it] 98%|█████████▊| 9586/9822 [4:17:24<05:39,  1.44s/it] 98%|█████████▊| 9587/9822 [4:17:26<05:37,  1.43s/it] 98%|█████████▊| 9588/9822 [4:17:27<05:34,  1.43s/it] 98%|█████████▊| 9589/9822 [4:17:29<05:34,  1.43s/it] 98%|█████████▊| 9590/9822 [4:17:30<05:38,  1.46s/it] 98%|█████████▊| 9591/9822 [4:17:31<05:35,  1.45s/it] 98%|█████████▊| 9592/9822 [4:17:33<05:32,  1.44s/it] 98%|█████████▊| 9593/9822 [4:17:34<05:29,  1.44s/it] 98%|█████████▊| 9594/9822 [4:17:36<05:28,  1.44s/it] 98%|█████████▊| 9595/9822 [4:17:37<05:27,  1.44s/it] 98%|█████████▊| 9596/9822 [4:17:39<05:26,  1.44s/it] 98%|█████████▊| 9597/9822 [4:17:40<05:24,  1.44s/it] 98%|█████████▊| 9598/9822 [4:17:42<05:22,  1.44s/it] 98%|█████████▊| 9599/9822 [4:17:43<05:21,  1.44s/it] 98%|█████████▊| 9600/9822 [4:17:44<05:19,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1350, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1122, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0516, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1724, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2048, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1203, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1373, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1174, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1157, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1499, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2154, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1179, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1659, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1484, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1416, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1678, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1317, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1479, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1421, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1518, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1135, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2005, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0571, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1684, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:05:31 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:05:31 - INFO - __main__ - ***** test Results*****
01/08/2024 02:05:31 - INFO - __main__ -   Training step = 9600
01/08/2024 02:05:31 - INFO - __main__ -  test_accuracy:0.8799414348462665 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:05:37 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:05:37 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:05:37 - INFO - __main__ -   Training step = 9600
01/08/2024 02:05:37 - INFO - __main__ -  eval_accuracy:0.8626876601977298 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:05:42 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:05:42 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:05:42 - INFO - __main__ -   Training step = 9600
01/08/2024 02:05:42 - INFO - __main__ -  eval_accuracy:0.9143170999633834 
 98%|█████████▊| 9601/9822 [4:18:03<23:59,  6.51s/it] 98%|█████████▊| 9602/9822 [4:18:04<18:17,  4.99s/it] 98%|█████████▊| 9603/9822 [4:18:06<14:19,  3.92s/it] 98%|█████████▊| 9604/9822 [4:18:07<11:33,  3.18s/it] 98%|█████████▊| 9605/9822 [4:18:09<09:37,  2.66s/it] 98%|█████████▊| 9606/9822 [4:18:10<08:15,  2.29s/it] 98%|█████████▊| 9607/9822 [4:18:11<07:17,  2.04s/it] 98%|█████████▊| 9608/9822 [4:18:13<06:36,  1.85s/it] 98%|█████████▊| 9609/9822 [4:18:14<06:08,  1.73s/it] 98%|█████████▊| 9610/9822 [4:18:16<05:49,  1.65s/it] 98%|█████████▊| 9611/9822 [4:18:17<05:33,  1.58s/it] 98%|█████████▊| 9612/9822 [4:18:19<05:21,  1.53s/it] 98%|█████████▊| 9613/9822 [4:18:20<05:14,  1.51s/it] 98%|█████████▊| 9614/9822 [4:18:21<05:08,  1.48s/it] 98%|█████████▊| 9615/9822 [4:18:23<05:03,  1.47s/it] 98%|█████████▊| 9616/9822 [4:18:24<05:00,  1.46s/it] 98%|█████████▊| 9617/9822 [4:18:26<04:58,  1.45s/it] 98%|█████████▊| 9618/9822 [4:18:27<04:55,  1.45s/it] 98%|█████████▊| 9619/9822 [4:18:29<04:59,  1.48s/it] 98%|█████████▊| 9620/9822 [4:18:30<04:56,  1.47s/it] 98%|█████████▊| 9621/9822 [4:18:32<04:52,  1.46s/it] 98%|█████████▊| 9622/9822 [4:18:33<04:50,  1.45s/it] 98%|█████████▊| 9623/9822 [4:18:35<04:47,  1.45s/it] 98%|█████████▊| 9624/9822 [4:18:36<04:45,  1.44s/it] 98%|█████████▊| 9625/9822 [4:18:37<04:43,  1.44s/it] 98%|█████████▊| 9626/9822 [4:18:39<04:41,  1.43s/it] 98%|█████████▊| 9627/9822 [4:18:40<04:40,  1.44s/it] 98%|█████████▊| 9628/9822 [4:18:42<04:38,  1.44s/it] 98%|█████████▊| 9629/9822 [4:18:43<04:37,  1.44s/it] 98%|█████████▊| 9630/9822 [4:18:45<04:36,  1.44s/it] 98%|█████████▊| 9631/9822 [4:18:46<04:34,  1.44s/it] 98%|█████████▊| 9632/9822 [4:18:47<04:30,  1.42s/it] 98%|█████████▊| 9633/9822 [4:18:49<04:30,  1.43s/it] 98%|█████████▊| 9634/9822 [4:18:50<04:30,  1.44s/it] 98%|█████████▊| 9635/9822 [4:18:52<04:29,  1.44s/it] 98%|█████████▊| 9636/9822 [4:18:53<04:27,  1.44s/it] 98%|█████████▊| 9637/9822 [4:18:55<04:25,  1.44s/it] 98%|█████████▊| 9638/9822 [4:18:56<04:24,  1.44s/it] 98%|█████████▊| 9639/9822 [4:18:57<04:23,  1.44s/it] 98%|█████████▊| 9640/9822 [4:18:59<04:21,  1.44s/it] 98%|█████████▊| 9641/9822 [4:19:00<04:19,  1.43s/it] 98%|█████████▊| 9642/9822 [4:19:02<04:18,  1.44s/it] 98%|█████████▊| 9643/9822 [4:19:03<04:17,  1.44s/it] 98%|█████████▊| 9644/9822 [4:19:05<04:15,  1.43s/it] 98%|█████████▊| 9645/9822 [4:19:06<04:13,  1.43s/it] 98%|█████████▊| 9646/9822 [4:19:08<04:12,  1.44s/it] 98%|█████████▊| 9647/9822 [4:19:09<04:11,  1.44s/it] 98%|█████████▊| 9648/9822 [4:19:10<04:10,  1.44s/it] 98%|█████████▊| 9649/9822 [4:19:12<04:09,  1.44s/it] 98%|█████████▊| 9650/9822 [4:19:13<04:09,  1.45s/it] 98%|█████████▊| 9651/9822 [4:19:15<04:11,  1.47s/it] 98%|█████████▊| 9652/9822 [4:19:16<04:10,  1.48s/it] 98%|█████████▊| 9653/9822 [4:19:18<04:07,  1.47s/it] 98%|█████████▊| 9654/9822 [4:19:19<04:04,  1.46s/it] 98%|█████████▊| 9655/9822 [4:19:21<04:02,  1.45s/it] 98%|█████████▊| 9656/9822 [4:19:22<04:00,  1.45s/it] 98%|█████████▊| 9657/9822 [4:19:24<03:58,  1.44s/it] 98%|█████████▊| 9658/9822 [4:19:25<03:57,  1.45s/it] 98%|█████████▊| 9659/9822 [4:19:26<03:58,  1.46s/it] 98%|█████████▊| 9660/9822 [4:19:28<03:58,  1.47s/it] 98%|█████████▊| 9661/9822 [4:19:29<03:57,  1.48s/it] 98%|█████████▊| 9662/9822 [4:19:31<03:56,  1.48s/it] 98%|█████████▊| 9663/9822 [4:19:32<03:55,  1.48s/it] 98%|█████████▊| 9664/9822 [4:19:34<03:51,  1.47s/it] 98%|█████████▊| 9665/9822 [4:19:35<03:48,  1.46s/it] 98%|█████████▊| 9666/9822 [4:19:37<03:45,  1.45s/it] 98%|█████████▊| 9667/9822 [4:19:38<03:43,  1.44s/it] 98%|█████████▊| 9668/9822 [4:19:40<03:41,  1.44s/it] 98%|█████████▊| 9669/9822 [4:19:41<03:39,  1.44s/it] 98%|█████████▊| 9670/9822 [4:19:42<03:37,  1.43s/it] 98%|█████████▊| 9671/9822 [4:19:44<03:36,  1.43s/it] 98%|█████████▊| 9672/9822 [4:19:45<03:35,  1.44s/it] 98%|█████████▊| 9673/9822 [4:19:47<03:33,  1.43s/it] 98%|█████████▊| 9674/9822 [4:19:48<03:32,  1.43s/it] 99%|█████████▊| 9675/9822 [4:19:50<03:30,  1.43s/it] 99%|█████████▊| 9676/9822 [4:19:51<03:29,  1.43s/it] 99%|█████████▊| 9677/9822 [4:19:52<03:27,  1.43s/it] 99%|█████████▊| 9678/9822 [4:19:54<03:25,  1.43s/it] 99%|█████████▊| 9679/9822 [4:19:55<03:24,  1.43s/it] 99%|█████████▊| 9680/9822 [4:19:57<03:22,  1.43s/it] 99%|█████████▊| 9681/9822 [4:19:58<03:21,  1.43s/it] 99%|█████████▊| 9682/9822 [4:20:00<03:20,  1.43s/it] 99%|█████████▊| 9683/9822 [4:20:01<03:22,  1.45s/it] 99%|█████████▊| 9684/9822 [4:20:03<03:19,  1.45s/it] 99%|█████████▊| 9685/9822 [4:20:04<03:17,  1.44s/it] 99%|█████████▊| 9686/9822 [4:20:05<03:16,  1.44s/it] 99%|█████████▊| 9687/9822 [4:20:07<03:14,  1.44s/it] 99%|█████████▊| 9688/9822 [4:20:08<03:12,  1.44s/it] 99%|█████████▊| 9689/9822 [4:20:10<03:10,  1.43s/it] 99%|█████████▊| 9690/9822 [4:20:11<03:09,  1.43s/it] 99%|█████████▊| 9691/9822 [4:20:13<03:07,  1.43s/it] 99%|█████████▊| 9692/9822 [4:20:14<03:05,  1.43s/it] 99%|█████████▊| 9693/9822 [4:20:15<03:04,  1.43s/it] 99%|█████████▊| 9694/9822 [4:20:17<03:03,  1.43s/it] 99%|█████████▊| 9695/9822 [4:20:18<03:01,  1.43s/it] 99%|█████████▊| 9696/9822 [4:20:20<03:00,  1.43s/it] 99%|█████████▊| 9697/9822 [4:20:21<02:58,  1.43s/it] 99%|█████████▊| 9698/9822 [4:20:23<02:57,  1.43s/it] 99%|█████████▊| 9699/9822 [4:20:24<02:56,  1.43s/it] 99%|█████████▉| 9700/9822 [4:20:25<02:54,  1.43s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1193, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0465, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1067, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1513, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1466, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1019, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1257, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1745, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1021, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2399, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1087, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1413, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1242, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1092, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1363, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0938, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1044, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1112, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0497, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1076, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0679, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0959, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1621, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1925, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1007, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1642, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1158, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:08:12 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:08:12 - INFO - __main__ - ***** test Results*****
01/08/2024 02:08:12 - INFO - __main__ -   Training step = 9700
01/08/2024 02:08:12 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:08:18 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:08:18 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:08:18 - INFO - __main__ -   Training step = 9700
01/08/2024 02:08:18 - INFO - __main__ -  eval_accuracy:0.8630538264372025 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:08:23 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:08:23 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:08:23 - INFO - __main__ -   Training step = 9700
01/08/2024 02:08:23 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
 99%|█████████▉| 9701/9822 [4:20:44<13:06,  6.50s/it] 99%|█████████▉| 9702/9822 [4:20:45<09:58,  4.99s/it] 99%|█████████▉| 9703/9822 [4:20:47<07:46,  3.92s/it] 99%|█████████▉| 9704/9822 [4:20:48<06:14,  3.18s/it] 99%|█████████▉| 9705/9822 [4:20:50<05:10,  2.66s/it] 99%|█████████▉| 9706/9822 [4:20:51<04:25,  2.29s/it] 99%|█████████▉| 9707/9822 [4:20:52<03:53,  2.03s/it] 99%|█████████▉| 9708/9822 [4:20:54<03:31,  1.85s/it] 99%|█████████▉| 9709/9822 [4:20:55<03:15,  1.73s/it] 99%|█████████▉| 9710/9822 [4:20:57<03:06,  1.66s/it] 99%|█████████▉| 9711/9822 [4:20:58<02:57,  1.60s/it] 99%|█████████▉| 9712/9822 [4:21:00<02:50,  1.55s/it] 99%|█████████▉| 9713/9822 [4:21:01<02:46,  1.52s/it] 99%|█████████▉| 9714/9822 [4:21:03<02:41,  1.50s/it] 99%|█████████▉| 9715/9822 [4:21:04<02:37,  1.47s/it] 99%|█████████▉| 9716/9822 [4:21:05<02:34,  1.46s/it] 99%|█████████▉| 9717/9822 [4:21:07<02:32,  1.45s/it] 99%|█████████▉| 9718/9822 [4:21:08<02:29,  1.43s/it] 99%|█████████▉| 9719/9822 [4:21:10<02:27,  1.44s/it] 99%|█████████▉| 9720/9822 [4:21:11<02:26,  1.44s/it] 99%|█████████▉| 9721/9822 [4:21:13<02:24,  1.44s/it] 99%|█████████▉| 9722/9822 [4:21:14<02:23,  1.43s/it] 99%|█████████▉| 9723/9822 [4:21:15<02:22,  1.44s/it] 99%|█████████▉| 9724/9822 [4:21:17<02:20,  1.44s/it] 99%|█████████▉| 9725/9822 [4:21:18<02:19,  1.44s/it] 99%|█████████▉| 9726/9822 [4:21:20<02:18,  1.45s/it] 99%|█████████▉| 9727/9822 [4:21:21<02:17,  1.44s/it] 99%|█████████▉| 9728/9822 [4:21:23<02:15,  1.44s/it] 99%|█████████▉| 9729/9822 [4:21:24<02:13,  1.44s/it] 99%|█████████▉| 9730/9822 [4:21:26<02:12,  1.44s/it] 99%|█████████▉| 9731/9822 [4:21:27<02:10,  1.44s/it] 99%|█████████▉| 9732/9822 [4:21:28<02:09,  1.44s/it] 99%|█████████▉| 9733/9822 [4:21:30<02:07,  1.44s/it] 99%|█████████▉| 9734/9822 [4:21:31<02:06,  1.44s/it] 99%|█████████▉| 9735/9822 [4:21:33<02:04,  1.43s/it] 99%|█████████▉| 9736/9822 [4:21:34<02:03,  1.44s/it] 99%|█████████▉| 9737/9822 [4:21:36<02:02,  1.44s/it] 99%|█████████▉| 9738/9822 [4:21:37<02:00,  1.44s/it] 99%|█████████▉| 9739/9822 [4:21:38<01:59,  1.44s/it] 99%|█████████▉| 9740/9822 [4:21:40<01:59,  1.46s/it] 99%|█████████▉| 9741/9822 [4:21:41<01:58,  1.46s/it] 99%|█████████▉| 9742/9822 [4:21:43<01:57,  1.46s/it] 99%|█████████▉| 9743/9822 [4:21:44<01:54,  1.46s/it] 99%|█████████▉| 9744/9822 [4:21:46<01:52,  1.45s/it] 99%|█████████▉| 9745/9822 [4:21:47<01:51,  1.44s/it] 99%|█████████▉| 9746/9822 [4:21:49<01:49,  1.44s/it] 99%|█████████▉| 9747/9822 [4:21:50<01:47,  1.44s/it] 99%|█████████▉| 9748/9822 [4:21:52<01:46,  1.44s/it] 99%|█████████▉| 9749/9822 [4:21:53<01:44,  1.44s/it] 99%|█████████▉| 9750/9822 [4:21:54<01:43,  1.44s/it] 99%|█████████▉| 9751/9822 [4:21:56<01:41,  1.43s/it] 99%|█████████▉| 9752/9822 [4:21:57<01:40,  1.43s/it] 99%|█████████▉| 9753/9822 [4:21:59<01:38,  1.43s/it] 99%|█████████▉| 9754/9822 [4:22:00<01:37,  1.44s/it] 99%|█████████▉| 9755/9822 [4:22:02<01:36,  1.44s/it] 99%|█████████▉| 9756/9822 [4:22:03<01:34,  1.44s/it] 99%|█████████▉| 9757/9822 [4:22:04<01:33,  1.43s/it] 99%|█████████▉| 9758/9822 [4:22:06<01:31,  1.44s/it] 99%|█████████▉| 9759/9822 [4:22:07<01:30,  1.43s/it] 99%|█████████▉| 9760/9822 [4:22:09<01:28,  1.43s/it] 99%|█████████▉| 9761/9822 [4:22:10<01:27,  1.43s/it] 99%|█████████▉| 9762/9822 [4:22:12<01:25,  1.43s/it] 99%|█████████▉| 9763/9822 [4:22:13<01:24,  1.44s/it] 99%|█████████▉| 9764/9822 [4:22:14<01:23,  1.44s/it] 99%|█████████▉| 9765/9822 [4:22:16<01:21,  1.44s/it] 99%|█████████▉| 9766/9822 [4:22:17<01:20,  1.44s/it] 99%|█████████▉| 9767/9822 [4:22:19<01:19,  1.44s/it] 99%|█████████▉| 9768/9822 [4:22:20<01:17,  1.44s/it] 99%|█████████▉| 9769/9822 [4:22:22<01:15,  1.43s/it] 99%|█████████▉| 9770/9822 [4:22:23<01:14,  1.43s/it] 99%|█████████▉| 9771/9822 [4:22:25<01:13,  1.44s/it] 99%|█████████▉| 9772/9822 [4:22:26<01:13,  1.46s/it]100%|█████████▉| 9773/9822 [4:22:27<01:11,  1.46s/it]100%|█████████▉| 9774/9822 [4:22:29<01:09,  1.45s/it]100%|█████████▉| 9775/9822 [4:22:30<01:07,  1.44s/it]100%|█████████▉| 9776/9822 [4:22:32<01:05,  1.43s/it]100%|█████████▉| 9777/9822 [4:22:33<01:04,  1.43s/it]100%|█████████▉| 9778/9822 [4:22:35<01:03,  1.43s/it]100%|█████████▉| 9779/9822 [4:22:36<01:01,  1.43s/it]100%|█████████▉| 9780/9822 [4:22:37<01:00,  1.43s/it]100%|█████████▉| 9781/9822 [4:22:39<00:58,  1.43s/it]100%|█████████▉| 9782/9822 [4:22:40<00:57,  1.43s/it]100%|█████████▉| 9783/9822 [4:22:42<00:55,  1.43s/it]100%|█████████▉| 9784/9822 [4:22:43<00:54,  1.43s/it]100%|█████████▉| 9785/9822 [4:22:45<00:53,  1.44s/it]100%|█████████▉| 9786/9822 [4:22:46<00:51,  1.43s/it]100%|█████████▉| 9787/9822 [4:22:48<00:50,  1.44s/it]100%|█████████▉| 9788/9822 [4:22:49<00:48,  1.44s/it]100%|█████████▉| 9789/9822 [4:22:50<00:47,  1.43s/it]100%|█████████▉| 9790/9822 [4:22:52<00:45,  1.43s/it]100%|█████████▉| 9791/9822 [4:22:53<00:44,  1.43s/it]100%|█████████▉| 9792/9822 [4:22:55<00:42,  1.43s/it]100%|█████████▉| 9793/9822 [4:22:56<00:41,  1.43s/it]100%|█████████▉| 9794/9822 [4:22:58<00:40,  1.44s/it]100%|█████████▉| 9795/9822 [4:22:59<00:38,  1.43s/it]100%|█████████▉| 9796/9822 [4:23:00<00:37,  1.43s/it]100%|█████████▉| 9797/9822 [4:23:02<00:35,  1.44s/it]100%|█████████▉| 9798/9822 [4:23:03<00:34,  1.44s/it]100%|█████████▉| 9799/9822 [4:23:05<00:33,  1.44s/it]100%|█████████▉| 9800/9822 [4:23:06<00:31,  1.44s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1284, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1132, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0267, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2256, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0900, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1018, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1337, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1429, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1283, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2181, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1356, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0985, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1940, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1395, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1677, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1341, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2376, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1408, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0339, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1014, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1027, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2131, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1293, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0963, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1079, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0983, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1029, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1840, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0975, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1194, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1216, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:10:53 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:10:53 - INFO - __main__ - ***** test Results*****
01/08/2024 02:10:53 - INFO - __main__ -   Training step = 9800
01/08/2024 02:10:53 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:10:59 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:10:59 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:10:59 - INFO - __main__ -   Training step = 9800
01/08/2024 02:10:59 - INFO - __main__ -  eval_accuracy:0.8634199926766752 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:11:04 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:11:04 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:11:04 - INFO - __main__ -   Training step = 9800
01/08/2024 02:11:04 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
100%|█████████▉| 9801/9822 [4:23:25<02:17,  6.53s/it]100%|█████████▉| 9802/9822 [4:23:26<01:40,  5.00s/it]100%|█████████▉| 9803/9822 [4:23:27<01:14,  3.93s/it]100%|█████████▉| 9804/9822 [4:23:29<00:57,  3.17s/it]100%|█████████▉| 9805/9822 [4:23:30<00:45,  2.67s/it]100%|█████████▉| 9806/9822 [4:23:32<00:36,  2.30s/it]100%|█████████▉| 9807/9822 [4:23:33<00:30,  2.04s/it]100%|█████████▉| 9808/9822 [4:23:35<00:25,  1.86s/it]100%|█████████▉| 9809/9822 [4:23:36<00:22,  1.73s/it]100%|█████████▉| 9810/9822 [4:23:38<00:19,  1.64s/it]100%|█████████▉| 9811/9822 [4:23:39<00:17,  1.58s/it]100%|█████████▉| 9812/9822 [4:23:40<00:15,  1.53s/it]100%|█████████▉| 9813/9822 [4:23:42<00:13,  1.50s/it]100%|█████████▉| 9814/9822 [4:23:43<00:11,  1.49s/it]100%|█████████▉| 9815/9822 [4:23:45<00:10,  1.47s/it]100%|█████████▉| 9816/9822 [4:23:46<00:08,  1.46s/it]100%|█████████▉| 9817/9822 [4:23:48<00:07,  1.45s/it]100%|█████████▉| 9818/9822 [4:23:49<00:05,  1.45s/it]100%|█████████▉| 9819/9822 [4:23:50<00:04,  1.44s/it]100%|█████████▉| 9820/9822 [4:23:52<00:02,  1.44s/it]100%|█████████▉| 9821/9822 [4:23:53<00:01,  1.44s/it]100%|██████████| 9822/9822 [4:23:54<00:00,  1.28s/it]Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
loss:
tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1060, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1121, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1332, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.1017, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)
loss:
tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)
01/08/2024 02:11:41 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:11:41 - INFO - __main__ - ***** test Results*****
01/08/2024 02:11:41 - INFO - __main__ -   Training step = 9822
01/08/2024 02:11:41 - INFO - __main__ -  test_accuracy:0.8788433382137628 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:11:47 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:11:47 - INFO - __main__ - ***** Evaluation Results*****
01/08/2024 02:11:47 - INFO - __main__ -   Training step = 9822
01/08/2024 02:11:47 - INFO - __main__ -  eval_accuracy:0.8634199926766752 
Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
dev:
{'accuracy': 0.8652508238740388}
test:
{'accuracy': 0.8788433382137628}
01/08/2024 02:11:52 - INFO - datasets.metric - Removing /home/zhanyuliang/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow
01/08/2024 02:11:52 - INFO - __main__ - ***** Teacher Evaluation Results*****
01/08/2024 02:11:52 - INFO - __main__ -   Training step = 9822
01/08/2024 02:11:52 - INFO - __main__ -  eval_accuracy:0.9132186012449652 
100%|██████████| 9822/9822 [4:24:11<00:00,  1.61s/it]